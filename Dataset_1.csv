id,text
0,"<p>What does ""backprop"" mean? I've Googled it, but it's showing backpropagation.</p>

<p>Is the ""backprop"" term basically the same as ""backpropagation"" or does it have a different meaning?</p>
"
1,"<p>Does increasing the noise in data help to improve the learning ability of a network? Does it make any difference or does it depend on the problem being solved? How is it affect the generalization process overall?</p>
"
2,"<p>When you're writing your algorithm, how do you know how many neurons you need per single layer? Are there any methods for finding the optimal number of them, or is it a rule of thumb?</p>
"
3,"<p>I have a LEGO Mindstorms EV3 and I'm wondering if there's any way I could start coding the bot in Python rather than the default drag-and-drop system. Is a Mindstorm considered AI?</p>

<p>Is this possible?</p>

<hr>

<p>My goal is to write a basic walking program in Python. The bot is the EV3RSTORM. I searched and found <a href=""http://bitsandbricks.no/2014/01/19/getting-started-with-python-on-ev3/"" rel=""nofollow"">this</a>, but don't understand it. </p>
"
4,"<p>The intelligent agent definition of intelligence states that an agent is intelligent if it acts so to maximize the expected value of a performance measure based on past experience and knowledge. (paraphrased from <a href=""http://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence#Intelligent_agent_definition"" rel=""nofollow"">Wikipedia</a>)</p>

<p>Does this mean that humans are not intelligent? I think we all make mistakes that imply that we are not maximizing the expected value of a performance measure.</p>
"
5,"<p>This quote by Stephen Hawking has been in headlines for quite some time:</p>

<blockquote>
  <p>Artificial Intelligence could wipe out humanity when it gets too clever as humans will be like ants.</p>
</blockquote>

<p>Why does he say this? To put it simply in layman terms: what are the possible threats from AI? If we know that AI is so dangerous why are we still promoting it? Why is it not banned?</p>

<p>What are the adverse consequences of the so called <a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""noreferrer"">Technological Singularity</a>? </p>
"
6,"<p>I'm new to A.I. and I'd like to know in simple words, what is the fuzzy logic concept? How does it help, and when is it used?</p>
"
7,"<p>In particular, an embedded computer (with limited resources) analyzes live video stream from a traffic camera, trying to pick good frames that contain license plate numbers of passing cars. Once a plate is located, the frame is handed over to an OCR library to extract the registration and use it further.</p>

<p>In my country two types of license plates are in common use - rectangular (the typical) and square - actually, somewhat rectangular but ""higher than wider"", with the registration split over two rows.</p>

<p>(there are some more types, but let us disregard them; they are a small percent and usually belong to vehicles that lie outside our interest.)</p>

<p>Due to the limited resources and need for rapid, real-time processing, the maximum size of the network (number of cells and connections) the system can handle is fixed.</p>

<p>Would it be better to split this into two smaller networks, each recognizing one type of registration plates, or will the larger single network handle the two types better?</p>
"
8,"<p>The <a href=""https://en.wikipedia.org/wiki/Turing_test"">Turing Test</a> was the first test of artificial intelligence and is now a bit outdated. The <a href=""https://en.wikipedia.org/wiki/Turing_test#Total_Turing_test"">Total Turing Test</a> aims to be a more modern test which requires a much more sophisticated system. What techniques can we use to identify an artificial intelligence (weak AI) and an <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"">artificial general intelligence</a> (strong AI)?</p>
"
9,"<p>What is ""early stopping"" and what are the advantages using this method? How does it help exactly?</p>

<p>I've <a href=""https://en.wikipedia.org/wiki/Early_stopping"" rel=""nofollow noreferrer"">read the wiki</a>, but I'd be interested in perspectives, and links to recent research.</p>
"
10,"<p>I've heard the idea of the technological singularity, what is it and how does it relate to Artificial Intelligence?  Is this the theoretical point where Artificial Intelligence machines have progressed to the point where they grow and learn on their own beyond what humans can do and their growth takes off?  How would we know when we reach this point?</p>
"
11,"<p>I'm worrying that my network has become too complex. I don't want to end up with half of the network doing nothing but just take up space and resources.</p>

<p>So, what are the techniques for detecting and preventing overfitting to avoid such problems?</p>
"
12,"<p>I've seen emotional intelligence defined as the capacity to be aware of, control, and express one's emotions, and to handle interpersonal relationships judiciously and empathetically.  </p>

<ol>
<li><p>What are some strategies for artificial intelligence to begin to tackle this problem and develop emotional intelligence for computers?  </p></li>
<li><p>Are there examples where this is already happening to a degree today?  </p></li>
<li><p>Wouldn't a computer that passes a Turing test necessarily express emotional intelligence or it would be seen as an obvious computer?  </p>

<p>Perhaps that is why early programs that pass the test represented young people, who presumably have lower emotional intelligence.</p></li>
</ol>
"
13,"<p>Since human intelligence presumably is a function of a natural genetic algorithm in nature, is using a genetic algorithm in a computer an example of artificial intelligence?  If not, how do they differ?  Or perhaps some are and some are not expressing artificial intelligence depending upon the scale of the algorithm and what it evolves into?</p>
"
14,"<p>These two terms seem to be related, especially in their application in computer science and software engineering.  Is one a subset of another?  Is one a tool used to build a system for the other?  What are their differences and why are they significant?</p>
"
15,"<p>What aspects of quantum computers, if any, can help to further develop Artificial Intelligence?</p>
"
16,"<p>I believe a Markov chain is a sequence of events where each subsequent event depends probabilistically on the current event.  What are examples of the application of a Markov chain and can it be used to create artificial intelligence?  Would a genetic algorithm be an example of a Markov chain since each generation depends upon the state of the prior generation?</p>
"
17,"<p>What purpose does the ""dropout"" method serve and how does it improve the overall performance of the neural network?</p>
"
18,"<p>Can an AI program have an IQ?</p>

<p>In other words, can the IQ of an AI program be measured?</p>

<p>Like how humans can do an IQ test.</p>
"
19,"<p>Why would anybody want to use ""hidden layers""? How do they enhance the learning ability of the network in comparison to the network which doesn't have them (linear models)?</p>
"
20,"<p>When did research into Artificial Intelligence first begin?  Was it called Artificial Intelligence then or was there another name?</p>
"
21,"<p>How would you estimate the generalisation error? What are the methods of achieving this?</p>
"
22,"<p>I've implemented <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">the reinforcement learning alogrithm</a> for an agent to play <a href=""https://github.com/admonkey/snappybird"" rel=""nofollow noreferrer"">snappy bird</a> (a shameless cheap ripoff of flappy bird) utilizing a q-table for storing the history for future lookups. It works and eventually achieves perfect convergence after enough training.</p>

<p>Is it possible to implement a neural network to do function approximation in order to accomplish the purpose of the q-table? Obviously, storage is a concern with the q-table, but it doesn't seem to ever train with the neural net alone. Perhaps training the NN on an existing q-table would work, but I would like to not use a q-table at all if possible.</p>
"
23,"<p>I read that in the spring of 2016 a computer <a href=""https://en.wikipedia.org/wiki/Computer_Go"" rel=""nofollow"">Go program</a> was finally able to beat a professional human for the first time.  Now that this milestone has been reached, does that represent a significant advance in artificial intelligence techniques or was it just a matter of even more processing power being applied to the problem?  What are some of the methods used to program the successful Go playing program, and are those methods considered to be artificial intelligence?</p>
"
24,"<p>Who first coined the term Artificial Intelligence, is there a published research paper which is the first to use that term?</p>
"
25,"<p>I have a background in Computer Engineering and have been working on developing better algorithms to mimic human thought. (One of my favorites is Analogical Modeling as applied to language processing and decision making.) However, the more I research, the more I realize just <em>how</em> complicated AI is.</p>

<p>I have tried to tackle many problems in this field, but sometimes I find that I am reinventing the wheel or am trying to solve a problem that has already been proven to be unsolvable (ie. the halting problem). So, to help in furthering AI, I want to better understand the current obstacles that are hindering our progress in this field.</p>

<p>For example, time and space complexity of some machine learning algorithms is super-polynomial which means that even with fast computers, it can take a while for the program to complete. Even still, some algorithms may be fast on a desktop or other computer while dealing with a small data set, but when increasing the size of the data, the algorithm becomes intractable.</p>

<p>What are other issues currently facing AI development?</p>
"
26,"<p>I've read that the most of the problems can be solved with 1-2 hidden layers.</p>

<p>How do you know you need more than 2? For what kind of problems you would need them (give me an example)?</p>
"
27,"<p>What were the first areas of research into Artificial Intelligence and what were some early successes?  More recently we've had:</p>

<ol>
<li>Beating a human at the game of chess</li>
<li>Convincing a human that a person was conversing with them (passing the Turing test)</li>
<li>Beating a human at Jeopardy game show</li>
<li>Beating a human at the game of go.</li>
</ol>

<p>Were there milestones that were considered major in the field before the 1990s?</p>
"
28,"<p>Why somebody would use SAT solvers (<a href=""https://en.wikipedia.org/wiki/Boolean_satisfiability_problem"" rel=""nofollow"">Boolean satisfiability problem</a>) to solve their real world problems?</p>

<p>Are there any examples of the real uses of this model?</p>
"
29,"<p>What designs for genetic algorithms are there, if they are classified differently and/or have different names, that leverage models for epigenetics in evolution? What are the pros/cons of the designs? Are there vast insufficiencies or wide-open questions about their usefulness? </p>
"
30,"<p>Can a Convolutional Neural Network be used for pattern recognition in a problem domain where there are no pre-existing images, say by representing abstract data graphically? Would that always be less efficient?</p>

<p><a href=""https://youtu.be/py5byOOHZM8?t=815"">This developer</a> says current development could go further but not if there's a limit outside image recognition. </p>
"
31,"<p>I've heard the terms strong-AI and weak-AI used.  Are these well defined terms or subjective ones?  How are they generally defined?</p>
"
32,"<p>As AI gains capabilities, and becomes more prevalent in society, our legal system will encounter questions it has not encountered before.  For example, if a self-driving car is involved in an accident while being controlled by the AI, who is at fault?  The ""driver"" (who's really just a passenger), the programmer(s) who made the AI, or the AI itself?</p>

<p>So, what's on the cutting edge in terms of these kinds of issues at the intersection of law and artificial intelligence?</p>
"
33,"<p>I know that language of Lisp was used early on when working on artificial intelligence problems. Is it still being used today for significant work? If not, is there a new language that has taken its place as the most common one being used for work in AI today?</p>
"
34,"<p>What are the specific requirements of the Turing Test? </p>

<ul>
<li>What requirements if any must the evaluator fulfill in order to be qualified to give the test?</li>
<li>Must there always be two participants in the conversation (one human and one computer) or can there be more?</li>
<li>Are placebo tests (where there is not actually a computer involved) allowed or encouraged?</li>
<li>Can there be multiple evaluators? If so does the decision need to be unanimous among all evaluators in order for the machine to have passed the test?</li>
</ul>
"
35,"<p>I believe that statistical AI uses inductive thought processes. For example, deducing a trend from a pattern, after training.</p>

<p>What are some examples of successfully applying statistical AI to real world problems?</p>
"
36,"<p>How do the basic components <a href=""https://en.wikipedia.org/wiki/Optimality_theory"" rel=""nofollow"">optimality theory</a> apply to artificial intelligence?</p>

<p>How is optimality theory related to neural network research?</p>
"
37,"<p>Some programs do exhaustive searches for a solution while others do heuristic searches for a similar answer. For example, in chess, the search for the best next move tends to be more exhaustive in nature whereas, in Go, the search for the best next move tends to be more heuristic in nature due to the much larger search space.</p>

<p>Is the technique of brute force exhaustive searching for a good answer considered to be AI or is it generally required that heuristic algorithms be used before being deemed AI? If so, is the chess-playing computer beating a human professional seen as a meaningful milestone?</p>
"
38,"<p>How is a neural network having the ""deep"" adjective actually distinguished from other similar networks?</p>
"
39,"<p>What is the effectiveness of pre-training of unsupervised deep learning?</p>

<p>Does unsupervised deep learning actually work?</p>
"
40,"<p>Are search engines considered AI because of the way they analyze what you search for and remember it? Or how they send you ads of what you've searched for recently? </p>

<p>Is this considered AI or just smart?</p>
"
41,"<p>The following <a href=""http://www.evolvingai.org/fooling"" rel=""noreferrer"">page</a>/<a href=""http://www.evolvingai.org/files/DNNsEasilyFooled_cvpr15.pdf"" rel=""noreferrer"">study</a> demonstrates that the deep neural networks are easily fooled by giving high confidence predictions for unrecognisable images, e.g.</p>

<p><a href=""https://i.stack.imgur.com/7pgrH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7pgrH.jpg"" alt=""Evolved images that are unrecognisable to humans, but that state-of-the-art DNNs trained on ImageNet believe with &gt;= 99.6% certainty to be a familiar object. This result highlights differences between how DNNs and humans recognise objects. Directly and indirectly encoded images""></a></p>

<p><a href=""https://i.stack.imgur.com/pBm48.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pBm48.png"" alt=""Evolving images to match DNN classes produces a tremendous diversity of images. The mean DNN confidence scores for these images is 99.12% for the listed class, meaning that the DNN believes with near-certainty that the image is that type of thing. Shown are images selected to showcase diversity from 5 independent evolutionary runs. The images shed light on what the DNN network cares about, and what it does not, when classifying an image. For example, a school bus is alternating yellow and black lines, but does not need to have a windshield or wheels""></a></p>

<p>How this is possible? Can you please explain ideally in plain English?</p>
"
42,"<p>In a feedforward neural network the inputs are fed directly to the outputs via a series of <strong>weights</strong>.</p>

<p>What purpose do the weights serve and how are they significant in this neural network?</p>
"
43,"<p>I'm pretty sure this a noob-y question, but what is Deep Network? As of now it is the most popular tag on AI. Is there a reason for this? </p>

<hr>

<p>Please note, I am not asking how to distinguish a deep network from a neural network, I am simply asking for the definition of deep network.</p>
"
44,"<p>I believe that Classical AI uses deductive thought processes. For example, given as a set of constraints, deduce a conclusion.  What are some examples of successfully applying Classical AI to real world problems.</p>
"
45,"<p>In <a href=""https://youtu.be/oSdPmxRCWws?t=30"">this video</a> an expert says, ""One way of thinking about what intelligence is [specifically with regard to artificial intelligence], is as an optimization process.""</p>

<p>Can intelligence always be thought of as an optimization process, and can artificial intelligence always be modeled as an optimization problem? What about pattern recognition? Or is he mischaracterizing?</p>
"
46,"<p>What specific advantages of declarative languages make them more applicable to AI than imperative languages?  </p>

<p>What can declarative languages do easily that other languages styles find difficult for this kind of problem?</p>
"
47,"<p>In years past, GOFAI (Good Old Fashioned AI) was heavily based on ""rules"" and <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow noreferrer"">symbolic computation</a> based on rules.  Unfortunately, that approach ran into stumbling blocks, and the world moved heavily towards statistical/probabilistic approaches leading to the current wave of interest in ""machine learning"".</p>

<p>It seems though, that the symbolic/rule-based approach probably still has application. So, could one ""learn"" rules using a probabilistic <a href=""https://en.wikipedia.org/wiki/Rule_induction"" rel=""nofollow noreferrer"">rule induction</a> method, and then layer symbolic computation on top? If so, how could the whole process be made truly two-way, so that something ""learned"" from processing rules, can be fed back into how the system learns rules?</p>
"
48,"<p>Obviously, driverless cars aren't perfect, so imagine that the Google car (as an example) got into a difficult situation.</p>

<p>Here are a few examples of unfortunate situations caused by a set of events:</p>

<ul>
<li>The car is heading toward a crowd of 10 people crossing the road, so it cannot stop in time, but it can avoid killing 10 people by hitting the wall (killing the passengers),</li>
<li>Avoiding killing the rider of the motorcycle considering that the probability of survival is greater for the passenger of the car,</li>
<li>Killing an animal on the street in favour of a human being,</li>
<li>Changing lanes to crash into another car to avoid killing a dog,</li>
</ul>

<p>And here are few dilemmas:</p>

<ul>
<li>Does the algorithm recognize the difference between a human being and an animal?</li>
<li>Does the size of the human being or animal matter?</li>
<li>Does it count how many passengers it has vs. people in the front?</li>
<li>Does it ""know"" when babies/children are on board?</li>
<li>Does it take into the account the age (e.g. killing the older first)?</li>
</ul>

<p>How would an algorithm decide what should it do from the technical perspective? Is it being aware of above (counting the probability of kills), or not (killing people just to avoid its own destruction)?</p>

<p>Related articles:</p>

<ul>
<li><a href=""https://www.technologyreview.com/s/542626/why-self-driving-cars-must-be-programmed-to-kill/"" rel=""noreferrer"">Why Self-Driving Cars Must Be Programmed to Kill</a></li>
<li><a href=""https://www.technologyreview.com/s/539731/how-to-help-self-driving-cars-make-ethical-decisions/"" rel=""noreferrer"">How to Help Self-Driving Cars Make Ethical Decisions</a></li>
</ul>
"
49,"<p>Which deep neural network is used in <a href=""https://en.wikipedia.org/wiki/Google_self-driving_car"" rel=""nofollow noreferrer"">Google's driverless cars</a> to analyze the surroundings? Is this information open to the public?</p>
"
50,"<p>Two common activation functions used in deep learning are the hyperbolic tangent function and the sigmoid activation function. I understand that the hyperbolic tangent is just a rescaling and translation of the sigmoid function (i,e <code>tanh(z) = 2*sigma(z) - 1)</code>. </p>

<p>Is there a significant difference between these two activation functions, and in particular, <strong>when is one preferable to the other</strong>?</p>

<p>I realize that in some cases (like when estimating probabilities) outputs in the range of <code>[0,1]</code> are more convenient than outputs that range from <code>[-1,1]</code>. I want to know if there are differences <strong>other than convenience</strong> which distinguish the two activation functions.</p>
"
51,"<p><a href=""https://ai.stackexchange.com/questions/10/what-is-fuzzy-logic"">Fuzzy logic</a> is the logic where every statement can have any real truth value between 0 and 1.</p>

<p>How can fuzzy logic be used in creating AI? Is it useful for certain decision problems involving multiple inputs? Can you give an example of an AI that uses it?</p>
"
52,"<p>In <a href=""http://users.ox.ac.uk/~jrlucas/Godel/mmg.html"" rel=""nofollow"">Minds, Machines and Gödel</a> (1959), J. R. Lucas shows that any human mathematician can not be represented by an algorithmic automaton (a Turing Machine, but any computer is equivalent to it by the Church-Turing thesis), using Gödel's incompleteness theorem. </p>

<p>As I understand it, he states that since the computer is an algorithm and hence a formal system, Gödel's incompleteness theorem applies. But a human mathematician also has to work in a formal axiom system to prove a theorem, so wouldn't it apply there as well? </p>
"
53,"<p>Back in college, I had a Complexity Theory teacher who stated that artificial intelligence was a contradiction in terms. If it could be calculated mechanically, he argued, it wasn't intelligence, it was math.</p>

<p>This seems to be a variant of the Chinese Room argument. This argument is a metaphor, where a person is put in a room full of Chinese books. This person doesn't understand a word of Chinese but is slipped messages in Chinese under the door. The person has to use the books, which contain transformation rules, to answer these messages. The person can apply the transformation rules but does not understand what (s)he is communicating.</p>

<p>Does the Chinese room argument hold? Can we argue that artificial intelligence is merely clever algorithmics?</p>
"
54,"<p>What are the main differences between <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""nofollow"">Deep Boltzmann Machines</a> (DBM) recurrent neural network and <a href=""https://en.wikipedia.org/wiki/Deep_belief_network"" rel=""nofollow"">Deep Belief Network</a> (which is based on RBMs)?</p>
"
55,"<p>A cellular automaton is a state machine which is controlled by external input. The input is given by geometrical space around a cell. In a square matrix, each automaton gets input from 4 surrounding cells, while a hexagon grid has 6 neighbor cells which can be used as automaton input. For example, a 4-cells input may be the string “1011”. This string specify a state of the cellular automaton. The automaton will switch to a different state according to the lookup table. I want to know if increasing the number of input cells in a hexagon automaton will make the resulting computer more powerful.</p>

<p><em>original message</em></p>

<p>I'd like to learn more about the differences between <a href=""https://en.wikipedia.org/wiki/Cellular_automaton#Related_automata"" rel=""nofollow noreferrer"">related automata</a> which can be based on hexagonal cells instead of squares (rule 34/2), like in <a href=""https://en.wikipedia.org/wiki/CoDi"" rel=""nofollow noreferrer"">CoDi model</a> which uses spiking neural network (SNN). </p>

<p>Is using a plane tiled with regular <a href=""https://en.wikipedia.org/wiki/Hexagonal_tiling"" rel=""nofollow noreferrer"">hexagons</a> more efficient and reliable than using square cells? What is the difference and how do I know which one to use in which scenario?</p>

<hr>

<p>In other words, the more efficiently flexible that it grows, the more difficult scenarios it can be used for (for me, hexagonal implicates more possibilities, because it can send/share the signal with/to more tiles). Or maybe one is more modern than the other, or they're both on the same level? In general, I'd like to learn the differences between them to know when I should use one over the other. </p>
"
56,"<p>An ultraintelligent machine is a machine that can surpass all intellectual activities by any human, and such machine is often used in science fiction as a machine that brings mankind to an end. </p>

<p>Any machine is executed using an algorithm. By the Church-Turing thesis, any algorithm that can be executed by a modern computer can be executed by a Turing Machine. However, a human can easily simulate a Turing Machine. Doesn't this mean that a machine can't surpass all intellectual activities, since we can also execute the algorithm?</p>

<p>This argument is most likely flawed, since my intuition tells me that  ultraintelligent machines are possible. However, it is not clear to me where the flaw is. Note that this is my own argument. </p>
"
57,"<p>From Wikipedia:</p>

<blockquote>
  <p>AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2]</p>
</blockquote>

<p>Albeit non-computable, approximations are possible, such as <em>AIXItl</em>. Finding approximations to AIXI could be an objective way for solving AI.</p>

<p>My question is: is <em>AIXI</em> really a big deal in artificial <em>general</em> intelligence research? Can it be thought as a central concept for the field? If so, why don't we have more publications on this subject (or maybe we have and I'm not aware of them)?</p>
"
58,"<p>In what ways can connectionist artificial intelligence (neural networks) be integrated with <em>Good Old-Fashioned A.I.</em> (<em>GOFAI</em>)? For instance, how could deep neural networks be integrated with knowledge bases or logical inference? One such example seems to be the <a href=""http://wiki.opencog.org/w/DestinOpenCog"" rel=""nofollow"">OpenCog + Destin integration</a>.</p>
"
59,"<p>It is proved that a recurrent neural net with rational weights can be a super-Turing machine. Can we achieve this in practice ?</p>
"
60,"<p>Given the proven <a href=""https://en.wikipedia.org/wiki/Halting_problem"">halting problem</a> for <a href=""https://en.wikipedia.org/wiki/Turing_machine"">Turing machines</a>, can we infer limits on the ability of strong Artificial Intelligence?</p>
"
61,"<p>By default using <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> technique you can creating a dreamlike image out of two different images.</p>

<p>Is it possible to easily enhance this technique to generate one image out from three?</p>
"
62,"<p>Consider these neural style algorithms which produce some art work:</p>

<ul>
<li><a href=""https://github.com/alexjc/neural-doodle"">Neural Doodle</a></li>
<li><a href=""https://github.com/jcjohnson/neural-style"">neural-style</a></li>
</ul>

<p>Why is generating such images so slow and why does it take huge amounts of memory? Isn't there any method of optimizing the algorithm?</p>

<p>What is the mechanism or technical limitation behind this? Why we can't have a realtime processing?</p>

<p>Here are few user comments (<a href=""https://www.reddit.com/r/deepdream/comments/3jwl76/how_anyone_can_create_deep_style_images/"">How ANYONE can create Deep Style images</a>):</p>

<ul>
<li><blockquote>
  <p>Anything above 640x480 and we're talking days of heavy crunching and an insane amount of ram.</p>
</blockquote></li>
<li><blockquote>
  <p>I tried doing a 1024pixel image and it still crashed with 14gigs memory, and 26gigs swap. So most of the VM space is just the swapfile. Plus it takes several hours potentially days cpu rendering this.</p>
</blockquote></li>
<li><blockquote>
  <p>I tried 1024x768 and with 16gig ram and 20+ gig swap it was still dying from lack of memory.</p>
</blockquote></li>
<li><blockquote>
  <p>Having a memory issue, though. I'm using the ""g2.8xlarge"" instance type.</p>
</blockquote></li>
</ul>
"
63,"<p>Can autoencoders be used for supervised learning <em>without adding an output layer</em>? Can we simply feed it with a concatenated input-output vector for training, and reconstruct the output part from the input part when doing inference? The output part would be treated as missing values during inference and some imputation would be applied.</p>
"
64,"<p>I'm aware that neural networks are probably not designed to do that, however asking hypothetically, is it possible to train the deep neural network (or similar) to solve math equations?</p>

<p>So given the 3 inputs: 1st number, operator sign represented by the number (1 - <code>+</code>, 2 - <code>-</code>, 3 - <code>/</code>, 4 - <code>*</code>, and so on), and the 2nd number, then after training the network should give me the valid results.</p>

<p>Example 1 (<code>2+2</code>):</p>

<ul>
<li>Input 1: <code>2</code>; Input 2: <code>1</code> (<code>+</code>); Input 3: <code>2</code>; Expected output: <code>4</code></li>
<li>Input 1: <code>10</code>; Input 2: <code>2</code> (<code>-</code>); Input 3: <code>10</code>; Expected output: <code>0</code></li>
<li>Input 1: <code>5</code>; Input 2: <code>4</code> (<code>*</code>); Input 3: <code>5</code>; Expected output: <code>25</code></li>
<li>and so</li>
</ul>

<p>The above can be extended to more sophisticated examples.</p>

<p>Is that possible? If so, what kind of network can learn/achieve that?</p>
"
65,"<p>From Wikipedia:</p>

<blockquote>
  <p>A mirror neuron is a neuron that fires both when an animal acts and when the animal observes the same action performed by another.</p>
</blockquote>

<p>Mirror neurons are related to imitation learning, a very useful feature that is missing in current real-world A.I. implementations. Instead of learning from input-output examples (supervised learning) or from rewards (reinforcement learning), an agent with mirror neurons would be able to learn by simply observing other agents, translating their movements to its own coordinate system. What do we have on this subject regarding computational models?</p>
"
66,"<p>If I have a paragraph I want to summarize, for example:</p>

<blockquote>
  <p>Ponzo and Fila went to the mall during the day. They walked for a long while, stopping at shops. They went to many shops. At first, they didn't buy anything. After going to a number of shops, they eventually bought a shirt, and a pair of pants.</p>
</blockquote>

<p>Better summarized as:</p>

<blockquote>
  <p>They shopped at the mall today and bought some clothes.</p>
</blockquote>

<p>What is the best AI strategy to automate this process, if there is one? If there isn't, is it because it would be dependent on first having an external information resource that would inform any algorithm? Or is it because the problem is inherently contextual?</p>
"
67,"<p>What happens if you apply the same <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow noreferrer"">deep dream technique</a> which produces ""dream"" visuals but to media streams such as audio files?</p>

<p>Does changing image functions into audio and enhancing the logic would work, or will it no longer work/doesn't make any sense?</p>

<p>My goal is to create ""dream"" like audio based on the two samples.</p>
"
68,"<p>In <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> wikipedia page it's suggested that a dreamlike images created by a convolutional neural network may be related to how visual cortex works in humans when they're tripping.</p>

<blockquote>
  <p>The imagery to LSD- and psilocybin-induced hallucinations is suggestive of a functional resemblance between artificial neural networks and particular layers of the visual cortex.</p>
</blockquote>

<p>How this is even possible?</p>

<p>How exactly convolutional neural networks have anything to do with human visual cortex?</p>
"
69,"<p>This 2014 <a href=""https://medium.com/the-physics-arxiv-blog/first-demonstration-of-artificial-intelligence-on-a-quantum-computer-17a6b9d1c5fb"" rel=""nofollow"">article</a> saying that a Chinese team of physicists have trained a quantum computer to recognise handwritten characters.</p>

<p><strong>Why did they have to use a quantum computer</strong> to do that?</p>

<p>Is it just for fun and demonstration, or is it that recognising the handwritten characters is so difficult that standard (non-quantum) computers or algorithms cannot do that?</p>

<p>If standard computers can achieve the same thing, what are the benefits of using quantum computers to do that then over standard methods?</p>
"
70,"<p>Is it possible that at some time in the future, AIs will be able to initiatively develop themselves, rather than passively being developed by humanity?</p>
"
71,"<p>I have been wondering since a while ago about the <a href=""https://en.wikipedia.org/wiki/Theory_of_multiple_intelligences"" rel=""nofollow noreferrer"">multiple intelligences</a> and how they could fit in the field of Artificial Intelligence as a whole.</p>

<p>We hear from time to time about <a href=""https://www.theguardian.com/artanddesign/jonathanjonesblog/2016/feb/08/leonardo-da-vinci-mechanics-of-genius-science-museum-london"" rel=""nofollow noreferrer"">Leonardo</a> being a genius or <a href=""https://www.youtube.com/watch?v=xUHQ2ybTejU"" rel=""nofollow noreferrer"">Bach's musical intelligence</a>. These persons are commonly said to be (have been) <em>more intelligent</em>. But the multiple intelligences speak about cooking or dancing or chatting as well, i.e. <em>coping with everyday tasks</em> (at least that's my interpretation).</p>

<p><strong>Are there some approaches on incorporating multiple intelligences into AI?</strong></p>

<hr>

<p><a href=""https://ai.stackexchange.com/questions/26/how-could-emotional-intelligence-be-implemented"">Related question - How could emotional intelligence be implemented?</a></p>
"
72,"<p>Which is the preferred algorithm to build word vector for a given language?</p>
"
73,"<p>How to decide the optimum number of layers to be created while implementing a Neural Network (Feedforward, back propagation or RNN)?</p>
"
74,"<p>I am interested in the <a href=""https://en.wikipedia.org/wiki/Emergence"" rel=""nofollow"">emergence</a> of properties in <a href=""https://en.wikipedia.org/wiki/Agent-based_model#Theory"" rel=""nofollow"">agents</a>, and, more generally in robotics.</p>

<p>I was wondering if there is work on the emergence of time-related concepts, on the low-level representation of notions like <em>before</em> and <em>after</em>. I know, for example, that there is work on the emergence of <a href=""http://www.scholarpedia.org/article/Kohonen_network"" rel=""nofollow"">spatial representation</a> (similar to <a href=""https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"" rel=""nofollow"">knn</a>), or even <a href=""https://infoscience.epfl.ch/record/129415/files/Mitrietal_1.pdf"" rel=""nofollow"">communication</a>* but time seems to be a tricky concept. </p>

<p>This has everything to do with the <em>platform</em>, i.e. the way that the representation would be coded in. We tend to favour ways that have some meaning or somehow mimic natural, well, yes, human structures, like the brain. I am not a neuroscientist and do not know that the sense of time <em>looks like</em> in humans, or if it is even present in other living beings.</p>

<p><strong>Is there some work on the (emergence of the) representation of <em>time</em> in artificial agents?</strong></p>

<hr>

<p>*I remember watching a really cool... Actually creepy video from these robots but cannot find it anymore. Does anyone have the link at hand?</p>
"
75,"<p>Have there been proposed extensions to go beyond a Turing machine that solve the halting problem and if so, would those proposed extensions have value to advance strong Artificial Intelligence?  For example, does quantum computing go beyond the definition of a Turing machine and resolve the halting problem, and does that help in creating strong AI?</p>
"
76,"<p>What was the first AI that was able to carry on a conversation, with real responses, such as in the famous <a href=""https://www.youtube.com/watch?v=WnzlbyTZsQY"" rel=""nofollow"">'I am not a robot. I am a unicorn' case?</a></p>

<p>A 'real response' constitutes a sort-of personalized answer to a specific input by a user.</p>
"
77,"<p>This question stems from quite a few ""informal"" sources. Movies like <em>2001, A Space Odyssey</em> and <em>Ex Machina</em>; books like <em>Destination Void</em> (Frank Herbert), and others suggest that general intelligence <em>wants</em> to survive, and even learn the importance for it.</p>

<p>There may be several arguments for survival. What would be the most prominent?</p>
"
78,"<p>Identifying sarcasm is considered as one of the most difficult open-ended problems in the domain of ML and NLP.</p>

<p>So, was there any considerable research done in that front? If yes, then what is the accuracy like? Please also explain the NLP model briefly.</p>
"
79,"<p>I'd like to know more about <a href=""https://ai.stackexchange.com/q/26/8"">implementing emotional intelligence</a>.</p>

<p>Given I'm implementing a chat bot and I'd like to introduce the levels of curiosity to measure whether user text input is interesting or not.</p>

<p>High level would mean bot is asking more questions and is following the topic, lower level of curiosity makes the bot not asking any questions and changing the topics.</p>

<p>Less interesting content could mean the bot doesn't see any opportunity to learn something new or it doesn't understand the topic or doesn't want to talk about it, because of its low quality. </p>

<p>How this possibly can be achieved? Are there any examples?</p>
"
80,"<p>I would like to learn more whether it is possible and how to write a program which decompiles executable binary (an object file) to the C source. I'm not asking exactly 'how', but rather how this can be achieved.</p>

<p>Given the following <code>hello.c</code> file (as example):</p>

<pre><code>#include &lt;stdio.h&gt;
int main() {
  printf(""Hello World!"");
}
</code></pre>

<p>Then after compilation (<code>gcc hello.c</code>) I've got the binary file like:</p>

<pre><code>$ hexdump -C a.out | head
00000000  cf fa ed fe 07 00 00 01  03 00 00 80 02 00 00 00  |................|
00000010  0f 00 00 00 b0 04 00 00  85 00 20 00 00 00 00 00  |.......... .....|
00000020  19 00 00 00 48 00 00 00  5f 5f 50 41 47 45 5a 45  |....H...__PAGEZE|
00000030  52 4f 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |RO..............|
00000040  00 00 00 00 01 00 00 00  00 00 00 00 00 00 00 00  |................|
00000050  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|
00000060  00 00 00 00 00 00 00 00  19 00 00 00 d8 01 00 00  |................|
00000070  5f 5f 54 45 58 54 00 00  00 00 00 00 00 00 00 00  |__TEXT..........|
$ wc -c hello.c a.out 
  60 hello.c
8432 a.out
</code></pre>

<p>For the learning dataset I assume I'll have to have thousands of source code files along with its binary representation, so algorithm can learn about moving parts on certain changes.</p>

<p>My concerns are:</p>

<ul>
<li>do my algorithm needs to be aware about the header file, or it's ""smart"" enough to figure it out,</li>
<li>if it needs to know about the header, how do I tell my algorithm 'here is the header file',</li>
<li>what should be input/output mapping (whether some section to section or file to file),</li>
<li>do I need to divide my source code into some sections,</li>
<li>do I need to know exactly how decompilers work or AI can figure it out for me,</li>
<li>or should I've two networks, one for header, another for body it-self,</li>
<li>or more separate networks, each one for each logical component (e.g. byte->C tag, etc.)</li>
</ul>

<p>How would you tackle this?</p>
"
81,"<p>Text summarization is a long-standing research problem that was <em>""ignited""</em> by Luhn in 1958. However, a half century later, we still came nowhere close  to solving this problem (abstractive summarization). The reason for this might be because researchers are resorting to statistical (and sometimes linguistic) methods to find &amp; extract the most salient parts of the text.</p>

<p>Is summarization problem solvable using AI (neural networks to be precise)? </p>
"
82,"<p>I'd like to know which common file format is more efficient in terms of simplicity and storage space for storing the state of artificial neural network.</p>

<p>I'm not talking about memory storage, but file storage, so the data can be loaded later on.</p>

<p>My first guess would be XML, but having millions of connections and weights would generate huge amount of data. Another thing would be to dump object instances into binary file using some export/serialize functions, but the disadvantage is that the file isn't common and it's language specific.</p>

<p>Are there any common file format standards which can be used for exporting huge artificial neural network into the file to be loaded by another program? If so, which one.</p>
"
83,"<p>What AI techniques does IBM use for its Watson platform, specifically its natural language analysis?</p>
"
84,"<p>I'm investigating the possibility of storing the semantic-lexical connections (such as the relationships to the other words such as phrases and other dependencies, its strength, part of speech, language, etc.) in order to provide analysis of the input text.</p>

<p>I assume this has been already done. If so, to avoid reinventing the wheel, is there any efficient method to store and manage such data in some common format which has been already researched and tested?</p>
"
85,"<p>Which objective and measurable tests have been developed to test the intelligence of AI? </p>

<p>The classical test is the Turing Test, which has objective criteria and is measurable since it can be measured what percentage of the jury is fooled by the AI.</p>

<p>I am looking for other, more modern tests. </p>
"
86,"<p>I'm interested in implementing a program for natural language processing (aka <a href=""https://en.wikipedia.org/wiki/ELIZA"" rel=""nofollow noreferrer"">ELIZA</a>).</p>

<p>Assuming that I'm already <a href=""https://ai.stackexchange.com/q/212/8"">storing semantic-lexical connections</a> between the words and its strength.</p>

<p>What are the methods of dealing with words which have very distinct meaning?</p>

<p>Few examples:</p>

<ul>
<li><p>'Are we on the same page?'</p>

<p>The 'page' in this context isn't a document page, but it's part of the phrase.</p></li>
<li><p>'I'm living in Reading.'</p>

<p>The 'Reading' is a city (noun), so it's not a verb. Otherwise it doesn't make any sense. Checking for the capital letter would work in that specific example, but it won't work for other (like 'make' can be either verb or noun).</p></li>
<li><p>'I've read something on the Facebook wall, do you want to know what?'</p>

<p>The 'Facebook wall' has nothing to do with wall at all.</p></li>
</ul>

<p>In general, how algorithm should distinguish the word meaning and recognise the word within the context?</p>

<p>For example:</p>

<ul>
<li>Detecting the word for different type of speech, so it should recognise whether it's a verb or noun.</li>
<li>Detecting whether the word is part of phrase.</li>
<li>Detecting word for multiple meaning.</li>
</ul>

<p>What are the possible approaches to solve that problem in order to  identify the correct sense of a word with the context?</p>
"
87,"<p>Unsupervised learning does not involve target values, so basically targets are most likely the same as the inputs (in other words, involves no target values).</p>

<p>So how does this model learn?</p>
"
88,"<p>Currently, many different organizations do cutting-edge AI research, and some innovations are shared freely (at a time lag) while others are kept private. I'm referring to this state of affairs as 'multipolar,' where instead of there being one world leader that's far ahead of everyone else, there are many competitors who can be mentioned in the same breath. (There's not only one academic center of AI research worth mentioning, there might be particularly hot companies but there's not only one worth mentioning, and so on.)</p>

<p>But we could imagine instead there being one institution that mattered when it comes to AI (be it a company, a university, a research group, or a non-profit). This is what I'm referring to as ""monolithic."" Maybe they have access to tools and resources no one else has access to, maybe they attract the best and brightest in a way that gives them an unsurmountable competitive edge, maybe returns to research compound in a way that means early edges can't be overcome, maybe they have some sort of government coercion preventing competitors from popping up. (For other industries, network or first-mover effects might be other good examples of why you would expect that industry to be monolithic instead of multipolar.)</p>

<p>It seems like we should be able to use insights from social sciences like economics or organizational design or history of science in order to figure out, if not which path seems more likely, <em>how we would know</em> which path seems more likely.</p>

<p>(For example, we may be able to measure how much returns to research compound, in the sense of one organization coming up with an insight meaning that organization is likely to come up with the next relevant insight, and knowing this number makes it easier to figure out where the boundary between the two trajectories is located.)</p>
"
89,"<p>One of the most compelling applications for AI would be in augmenting human biological intelligence. What are some of the currently proposed methods for doing this aside from vague notions such as ""nanobots swimming around our brains and bodies"" or ""electrodes connected to our skulls""?</p>
"
90,"<p>Given list of fixed numbers from a mathematical constant such as Pi, is it is possible to train AI to attempt to predict the next numbers?</p>

<p>Which AI or neural network would be more suitable for this task? </p>

<p>Especially the one which will work without memorizing the entire training set, but the one which will attempt to find some patterns or statistical association.</p>
"
91,"<p>What are the main differences between two types of feedforward networks such as <em>multilayer perceptrons</em> (MLP) and <em>radial basis function</em> (RBF)?</p>

<p>What are the fundamental differences between these two types?</p>
"
92,"<p>According to my knowledge most of the current artificial intelligence study uses of some kind of neural network or its variants. A good example would be DeepMind's alphago which I believe is a deep neural network, for vision CNN, text, music and other ordered features RNN's, etc. But for machine learning application we have neural networks, support vector machines, random forest, regression methods, etc. available for applications. </p>

<p>So are neural networks and its variants the only way to reach ""true"" artificial intelligence? </p>
"
93,"<p>I'm interested in hardware implementation of ANNs (artificial neural networks). Are there any popular existing technology implementations in form of microchips which are purpose designed to run artificial neural networks? For example, a chip which is optimised for an application like image recognition or something similar?</p>
"
94,"<p>I've noticed that a few questions on this site mention genetic algorithms and it made me realize that I don't really know much about those.</p>

<p>I have heard the term before, but it's not something I've ever used, so I don't have much idea about how they work and what they are good for. All I know is that they involve some sort of evolution and randomly changing values.</p>

<p>Can you give me a short explanation, preferably including some sort of practical example that illustrates the basic principles?</p>
"
95,"<p>In detective novels, the point is often that the reader gets enough information to solve the crime themselves. This ""puzzle"" aspect of detective novels is part of the attraction.</p>

<p>Often the difficulty for humans is to keep track of all the variables - events, items, motivations.<br>
An AI would have an easier time keeping track of all the details, but would rely on real-world knowledge to prevent making crazy mistakes. For example, if it was stated that a character took the train, the AI would need to know that this is a method of transportation - that it changes the location property of an agent over time.</p>

<p>Has an AI ever been able to solve a detective mystery?</p>
"
96,"<p>In 1969, Seymour Papert and Marvin Minsky showed that Perceptrons could not learn the XOR function.  </p>

<p>This was solved by the backpropagation network with at least one hidden layer. This type of network can learn the XOR function.</p>

<p>I believe I was once taught that every problem that could be learnt by a backpropagation neural network with multiple hidden layers, could also be learnt by a backpropagation neural network with a single hidden layer. (Although possible a nonlinear activation function was required).</p>

<p>However, it is unclear to me what the limits are to backpropagation neural networks themselves. Which patterns <strong>cannot</strong> be learnt by a backpropgation neural network?</p>
"
97,"<p>Over the last 50 years, the rise/fall/rise in popularity of neural nets has acted as something of a 'barometer' for AI research.</p>

<p>It's clear from the questions on this site that people are interested in applying Deep Learning (DL) to a wide variety of difficult problems.</p>

<p>I therefore have two questions:</p>

<ol>
<li>Practitioners - What do you find to be the main obstacles to
applying DL 'out of the box' to your problem? </li>
<li>Researchers - What
techniques do you use (or have developed) that might help address
practical issues? Are they within DL or do they offer an
alternative approach?</li>
</ol>
"
98,"<p>Is it possible for <em>unsupervised learning</em> to learn about high-level, class-specific features given only unlabelled images? For example detecting human or animal faces? If so, how?</p>
"
99,"<p>On the Wikipedia page we can read the basic structure of an artificial neuron (a model of biological neurons) which consist:</p>

<ul>
<li>Dendrites - acts as the input vector,</li>
<li>Soma - acts as the summation function,</li>
<li>Axon - gets its signal from the summation behavior which occurs inside the soma.</li>
</ul>

<p>I've checked <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer"">Deep learning</a> wiki page, but I couldn't find any references to dendrites, soma or axons.</p>

<p>So my question is, which type of artificial neural network implements or can mimic such model most closely?</p>
"
100,"<p>Have there been any studies which attempted to use AI algorithms to detect human thoughts or emotions based on brain activity, such as using <a href=""https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#EEG-based"" rel=""nofollow"">BCI/EEG devices</a>?</p>

<p>By this, I mean simple guesses such as whether the person was happy or angry, or what object (e.g. banana, car) they were thinking about.</p>

<p>If so, did any of those studies show some degree of success?</p>
"
101,"<p>Has there been any attempts to deploy AI with blockchain technology? </p>

<p>Are there any decentralized examples of AI networks with no central point of control with AI nodes acting independently (but according to a codified set of rules) creating, validating and storing the same shared decentralized database in many locations around the world?</p>
"
102,"<p>In their famous book entitled ""<em>Perceptrons: An Introduction to Computational Geometry</em>"", Minsky and Papert show that a perceptron can't solve the XOR problem. This contributed to the first AI winter, resulting in funding cuts for neural networks. However, now we know that a multilayer perceptron can solve the XOR problem easily.</p>

<p>Backprop wasn't known at the time, but did they know about manually building multilayer perceptrons? Did Minsky &amp; Papert know that multilayer perceptrons could solve XOR at the time they wrote the book, albeit not knowing how to train it?</p>
"
103,"<p>According to wikipedia <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">Artificial general intelligence(AGI)</a></p>

<blockquote>
  <p>Artificial general intelligence (AGI) is the intelligence of a
  (hypothetical) machine that could successfully perform any
  intellectual task that a human being can. </p>
</blockquote>

<p>According to below image todays artifical intellgence is same as that of a lizards.</p>

<p><a href=""https://i.stack.imgur.com/gddKB.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gddKB.jpg"" alt=""enter image description here""></a></p>

<p>Lets assume(or not) that within 10-20 years we humans are successful in creating a AGI or AGIs. As AGI has the same intelligence and <a href=""http://futurism.com/scientist-claims-to-be-on-the-verge-of-making-an-ai-that-feels-true-emotions/"" rel=""nofollow noreferrer"">emotions</a> as that of humans because according to wikipedia definition it can perform same intellectual task of a human. Then can we destroy an AGI without its consent? Do this be considered as murder?</p>
"
104,"<p>Deep Mind has published a lot of works on deep learning in the last years, most of them state-of-the-art on their respective tasks. But how much of this work has actually been reproduced by the AI community? For instance, the Neural Turing Machine paper seems to be very hard to reproduce, according to other researchers.</p>
"
105,"<p>Geoffrey Hinton has been researching something he calls ""capsules theory"" in neural networks. What is this and how does it work?</p>
"
106,"<p>During my research, I've stumbled upon ""complex-valued neural networks"", which are neural networks that work with complex-valued inputs (probably weights too). What are the advantages (or simply the applications) of this kind of neural network over real-valued neural networks?</p>
"
107,"<p>The <a href=""http://fabelier.org/novelty-search-and-open-ended-evolution-by-ken-stanley/"" rel=""noreferrer"">author</a> claims that guiding evolution by novelty alone (without explicit goals) can solve problems even better than using explicit goals. In other words, using a novelty measure as a fitness function for a genetic algorithm works better than a goal-directed fitness function. How is that possible?</p>
"
108,"<p>Quote from this <a href=""https://ai.meta.stackexchange.com/a/46/8"">Eric's meta post</a> about modelling and implementation:</p>

<blockquote>
  <p>They are not exactly the same, although strongly related. This was a very difficult lesson to learn among mathematicians and early programmers, notably in the 70s (mathematical proofs can demand a lot of non-trivial programming work to make them ""computable"", as in runnable on a computer).</p>
</blockquote>

<p>If they're not the same, what is the difference?</p>

<p>How we can say when we're talking about AI implementation, and when about modelling? It's suggested above it's not easy task. So where we can draw the line when we talk about it?</p>

<p>I'm asking in general, not specifically for this site, that's why I haven't posted question in meta</p>
"
109,"<p>Given pictures with multiple features such as faces, can single AI algorithm detect all of them, or for better reliability is it preferred to use separate instances?</p>

<p>In other words I'm talking about attempt of finding all possible human faces on the same picture by a single neural network.</p>
"
110,"<p>I read some information<sup>1</sup> about attempts to build neural networks in the PHP programming language. Personally I think PHP is not the right language to do so at all probably because it's a high-level language, I assume low level language are way more suitable for AI in terms of performance and scalability. </p>

<p>Is there a good/logical reason why you should or shouldn't use PHP as a language to write AI in?</p>

<p><em><sup>1</sup></em> <a href=""http://www.developer.com/lang/php/creating-neural-networks-in-php.html"" rel=""nofollow noreferrer"">http://www.developer.com/lang/php/creating-neural-networks-in-php.html</a> and <a href=""https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there"">https://stackoverflow.com/questions/2303357/are-there-any-artificial-intelligence-projects-in-php-out-there</a> </p>
"
111,"<p>I've found <a href=""http://link.springer.com/chapter/10.1007%2F978-1-4613-1009-9_2"" rel=""nofollow"">this old scientific paper from 1988</a> about introduction of AI into nuclear power fields.</p>

<p>Were or still are there any dangers by application of such algorithm? Are nuclear power plants or human life in risk if the algorithm will fail?</p>

<p>Especially applications to the core, like cooling systems and other components which can be affected in negative way.</p>
"
112,"<p>Since we've self-driving cars already, would we have self-flying commercial flights in the near future? Basically the AI which can do take off, flying, landing and parking.</p>
"
113,"<p>How much processing power is needed to emulate the human brain? More specifically, the neural simulation, such as communication between the neurons and processing certain data in real-time.</p>

<p>I understand that this may be a bit of speculation and it's not possible to be accurate, but I'm sure there is some data available or research studies which attempted to estimate it based on our current understanding of the human brain.</p>
"
114,"<p><strong>The Situation:</strong>
A self-driving car is traveling at it's maximum speed, 25 mph (40 km/h), in the middle of an empty street with the ability to change lanes on both sides. There are two passengers, one in the front and another in the back.</p>

<p>Someone jumps from the side of the road directly into the path of the car. A collision would occur in 50 meters. <a href=""http://www.brake.org.uk/rsw/15-facts-a-resources/facts/1255-speed"" rel=""nofollow"">Breaking distance</a> at this speed is about 24m.</p>

<p><strong>The Question:</strong> Is it known how the current implementation of the Google Car AI would react, or is it currently a matter of speculation? A step-by-step explanation of the AI's decisioning process would be preferred.</p>

<p><strong>Possible Answers:</strong> The car could activate its brakes immediately, coming to a halt as quickly as possible. This would be sooner than a human could stop, as people require time to recognize the possibility of a collision, and then physically slam on the brake. (<em>thinking distance</em>).</p>

<p>Alternatively, the car could continue traveling forward, processing the situation. (Similar to a humans <em>thinking distance</em>). The person may continue to move, either out of the way, or still into danger of being hit. In this case, the car may decide to change lanes in an attempt to pass around the person.</p>

<p>Lastly and most unlikely, the car will not alter its course and proceed to drive forward.</p>

<p><sup>Do not attempt to do it to check;)</sup></p>
"
115,"<p>Artificial intelligence is present in many games, both current and older games. How can such intelligence understand what to do? I mean, how can it behave like a human in a game, allowing you to play against itself, or that AI plays against itself?</p>

<p>In games like Age of Empires, for example.</p>
"
116,"<p><a href=""https://cs.stackexchange.com/a/60535/54605"">At a related question in Computer Science SE</a>, a user told:</p>

<blockquote>
  <p>Neural networks typically require a large training set.</p>
</blockquote>

<p>Is there a way to define the boundaries of the ""optimal"" size of a training set in general case?</p>

<p>When I was learning about fuzzy logic, I've heard some rules of thumb that involved examining the mathematical composition of the problem and using that to define the number of fuzzy sets.</p>

<p>Is there such a method that can be applicable for an already defined neural network topology? </p>
"
117,"<p>How important is true (non-<a href=""https://en.wikipedia.org/wiki/Pseudorandomness"" rel=""noreferrer"" title=""pseudo"">pseudo</a>) randomness in Artificial Intelligence designs? Is there any chance that pseudo-randomness could be a barrier to more successful designs?</p>
"
118,"<p>Complex AI that learns lexical-semantic content and its meaning (such as collection of words, their structure and dependencies) such as <em>Watson</em> takes terabytes of disk space.</p>

<p>Lets assume <em>DeepQA</em>-like AI consumed whole Wikipedia of size 10G which took the same amount of structured and unstructured stored content.</p>

<p>Will learning another 10G of different encyclopedia (different topics in the same language) take the same amount of data? Or will the AI reuse the existing structured and take less than half (like 1/10 of it) additional space?</p>
"
119,"<p>Is there any simple explanation how <em>Watson</em> finds and scores evidence after gathering massive evidence and analyzing the data?</p>

<p>In other words, how does it know which precise answer it needs to return?</p>
"
120,"<p>Isaac Asimov's famous <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics"">Three Laws of Robotics</a> originated in the context of Asimov's science fiction stories. In those stories, the three laws serve as a safety measure, in order to avoid untimely or manipulated situations from exploding in havoc.</p>

<p>More often than not, Asimov's narratives would find a way to break them, leading the writer to make several modifications to the laws themselves. For instance, in some of his stories, he <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#First_Law_modified"">modified the First Law</a>, <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Zeroth_Law_added"">added a Fourth (or Zeroth) Law</a>, or even <a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics#Removal_of_the_Three_Laws"">removed all Laws altogether</a>.</p>

<p>However, it is easy to argue that, in popular culture, and even in the field of AI research itself, the Laws of Robotics are taken quite seriously. Ignoring the side problem of the different, subjective, and mutually-exclusive interpretations of the laws, are there any arguments proving the laws themselves intrinsically flawed by their design, or, alternatively, strong enough for use in reality? Likewise, has a better, stricter security heuristics set being designed for the purpose?</p>
"
121,"<p>Are there any modern techniques of generating <strong>textual</strong> CAPTCHA (so person needs to type the right text) challenges which can easily <a href=""https://ai.stackexchange.com/q/92/8"">fool AI</a> with some visual obfuscation methods, but at the same time human can solve them without any struggle?</p>

<p>For example I'm talking about plain ability of <strong>recognising text embedded into image</strong> (without considering any external plugins like flash or java, image classification, etc.) and re-typing the text that has been written or something similar.</p>

<p>I guess adding noise, gradient, rotating letters or changing colours are not reliable methods any more, since they can be quickly broken.</p>

<p>Any suggestions or research has been done?</p>
"
122,"<p>Can an AI program have an EQ (Emotional intelligence or emotional quotient)?</p>

<p>In other words, can the EQ of an AI program be measured?</p>

<p>If EQ is more problematic to measure than IQ (at least with a standard applicaple to both humans and AI programs), why is that the case?</p>
"
123,"<p>I have heard about this concept in a Reddit post about Alpha Go. I have tried to go through the paper and the article, but could not really make sense of the algorithm.</p>

<p>So, can someone give an easy-to-understand explanation of how the Monte-Carlo search algorithm work and how is it being used in building game-playing AI bots?</p>
"
124,"<p>DNNs are typically used to classify things (of course) but can we let them go wild with sounds and then tell them if we think it sounds good or not? I'd like to think after a training class has been made (perhaps comparing the output to an existing song) we could get an NN that has a basic concept of music.</p>

<p>Timing would be an issue; I'm not sure how feasible this is. A strongly weighted input attached to all hidden layers perhaps? Use it as the bias?</p>

<p>Is this even slightly feasible? </p>
"
125,"<p>How do I avoid my gradient descent algorithm into falling into the ""local minima"" trap while backpropogating on my neural network?</p>

<p>Are there any methods which help me avoid it?</p>
"
126,"<p>A neural network is a directed weighted graph. These can be represented by a (sparse) matrix. Doing so can expose some elegant properties of the network.</p>

<p>Is this technique beneficial for examining neural networks?</p>
"
127,"<p>Would it be ethical to implement AI for self-defence for public walking robots which are exposed to dangers such as violence and crime such as robbery (of parts), damage or abduction?</p>

<p>What would be pros and cons of such AI behavior? Is it realistic, or it won't be taken into account for some obvious reasons?</p>

<p>Like pushing back somebody when somebody start pushing it first (AI will say: he pushed me first), or running away on crowded street in case algorithm will detect risk of abduction.</p>
"
128,"<p>Is there any risk in the near future of replacing all encyclopedias with Watson-like AI where knowledge is accessible by everybody through <a href=""https://watson-api-explorer.mybluemix.net/"" rel=""nofollow"">API</a>?</p>

<p><sup>Something similar happened in the future in <a href=""https://en.wikipedia.org/wiki/The_Time_Machine_(2002_film)"" rel=""nofollow""><strong>The Time Machine</strong> movie from 2002</a>.</sup></p>

<p>Obviously maintaining 40 million articles and keeping it up-to-date and consistent could be beyond brain power of few thousands of active editors. Not to mention thousands of other encyclopedias including paperback version or large number of books used by universities which needs to be updated every year by a huge number of people.</p>

<p>What are the pros and cons of such a change?</p>
"
129,"<p>I've watched the <a href=""https://www.youtube.com/watch?v=LY7x2Ihqjmc"" rel=""nofollow"">Sunspring</a> video which didn't make any sense to me (a lot of nonsense monologues), mainly because it was created by Jetson AI.</p>

<p>What was the mechanism of creating such screenplay?</p>

<p>On what criteria was it trained? What was the goal or motivation in terms of training criteria of defining when text does make sense? And what was missed (that it's so bad) and how possibly this could be improved?</p>
"
130,"<p>This <a href=""http://blog.claymcleod.io/2016/06/01/The-truth-about-Deep-Learning/"" rel=""nofollow noreferrer"">article</a> suggests that deep learning is not designed to produce the universal algorithm and cannot be used to create such a complex systems.</p>

<p>First of all it requires huge amounts of computing power, time and effort to train the algorithm the right way and adding extra layers doesn't really help to solve complex problems which cannot be easily predicted.</p>

<p>Secondly some tasks are extremely difficult or impossible to solve using DNN, like solving a <a href=""https://ai.stackexchange.com/q/154/8"">math</a> equations, predicting <a href=""https://ai.stackexchange.com/q/225/8"">pseudo-random lists</a>, <a href=""https://ai.stackexchange.com/q/168/8"">fluid mechanics</a>, guessing encryption algorithms, or <a href=""https://ai.stackexchange.com/q/205/8"">decompiling</a> unknown formats, because there is no simple mapping between input and output.</p>

<p>So I'm asking, are there any alternative learning algorithms as powerful as deep architectures for general purpose problem solving? Which can solve more variety of problems, than ""deep"" architectures cannot?</p>
"
131,"<p>Is there any research which study application of AI into chemistry which can predict the output of certain chemical reactions.</p>

<p>So for example, you train the AI about current compounds, substances, structures and their products and chemical reactions from the existing <a href=""https://opendata.stackexchange.com/q/3553/3082"">dataset</a> (basically what produce what). Then you give the task to find how to create a gold or silver from group of available substances. Then the algorithm will find the chemical reactions (successfully predicting new one which weren't in the dataset) and gives the results. Maybe the gold is not a good example, but the practical scenario would be creation of drugs which are cheaper to create by using much more simpler processes or synthesizing some substances for the first time for drug industries.</p>

<p>Was there any successful research attempting to achieve that using deep learning algorithms?</p>
"
132,"<p>Assume that I want to solve an issue with neural network that either I can't fit to already existing topologies (perceptron, Konohen, etc) or I'm simply not aware of the existence of those or I'm unable to understand their mechanics and I rely on my own instead.</p>

<p>How can I deconstruct a problem to find a corresponding neural network topology? By this I don't mean only the size of certain layers, but the number of them, the type of activation functions, the number and the direction of connections, and so on.</p>

<p>I'm a beginner, yet I realized that in some topologies (or, at least in perceptrons) it is very hard if not impossible to understand the inner mechanics as the neurons of the hidden layers don't express any mathematically meaningful context.</p>
"
133,"<p>For example there is <a href=""https://en.wikipedia.org/wiki/MNIST_database"" rel=""nofollow"">the MNIST database</a> which is used to test artificial neural network (ANN), however it's not so challenging, because some hierarchical systems of convolutional neural networks manages to get an error rate of 0.23 percent.</p>

<p>Are there any similar, especially the most challenging tasks with dataset which are used as benchmark tests to challenge the AI which are fairly reliable and it's possible to pass, but most AAN are struggling to achieve the lower error rate?</p>
"
134,"<p>This <a href=""http://repository.supsi.ch/5145/1/IDSIA-04-12.pdf"" rel=""nofollow"">study</a> (pages 7-8) shows an attempt at recognizing the traffic signs with lower error rates by using multi-column deep neural networks </p>

<p>Are Google cars using similar techniques of predicting signs using DNN, or are they using some other method?</p>
"
135,"<p>I'd like to know whether there were attempts to simulate the whole brain, I'm not talking only about some <a href=""https://ai.stackexchange.com/q/237/8"">ANN on microchips</a>, but brain simulations.</p>
"
136,"<p>On <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"">the wikipedia page</a> about AI, we can read:</p>

<blockquote>
  <p>Optical character recognition is no longer perceived as an exemplar of ""artificial intelligence"" having become a routine technology.</p>
</blockquote>

<p>On the other hand, the <a href=""https://en.wikipedia.org/wiki/MNIST_database"">MNIST</a> database of handwritten digits is especially designed for training and testing neural networks and their error rates (see: <a href=""https://en.wikipedia.org/wiki/MNIST_database#Classifiers"">Classifiers</a>).</p>

<p>So why does the above quote state that OCR is no longer exemplar of AI?</p>
"
137,"<p><a href=""https://en.wikipedia.org/wiki/Minimum_intelligent_signal_test"" rel=""nofollow"">MIST</a> is a quantiative test of humanness, consisting of ~80k propositions such as:</p>

<ul>
<li>Is Earth a planet?</li>
<li>Is the sun bigger than my foot?</li>
<li>Do people sometimes lie?</li>
<li>etc.</li>
</ul>

<p>Have any AI attempted and passed this test to date?</p>
"
138,"<p>It is possible of normal code to prove that it is correct using mathematical techniques, and that is often done to ensure that some parts are bug-free. </p>

<p>Can we also prove that a piece of code in AI software will cause it to never turn against us, i.e. that the AI is <a href=""https://en.wikipedia.org/wiki/Friendly_artificial_intelligence"" rel=""nofollow"">friendly</a>? Has there any research been done towards this?</p>
"
139,"<p>In <a href=""http://arxiv.org/pdf/1606.00652.pdf"" rel=""nofollow"">this paper</a>, a proposal is given for what death could mean for Artificial Intelligence. </p>

<p>What does this mean using English only? I understand that mathematical notation is useful for giving a precise definition, but I'd like to understand what the definition really means. </p>
"
140,"<p>We can measure the power of the machine with the number of operation per second or the frequency of the processor. But does units similar of IQ for humans exist for a AI?<br/>
I'm asking for a unit which can give countable result so something different from a Turing Test which only give a binary result.</p>
"
141,"<p>In the mid 1980s, Rodney Brooks famously created the foundations of ""the new AI"". The central claim was that the symbolist approach of 'Good Old Fashioned AI' (GOFAI) had failed by attempting to 'cream cognition off the top', and that <em>embodied cognition</em> was required, i.e. built from the bottom up in a 'hierarchy of competances' (e.g. basic locomotion -> wandering around -> actively foraging) etc.</p>

<p>I imagine most AI researchers would agree that the 'embodied cognition' perspective has now (at least tacitly) supplanted GOFAI as the mainstream.</p>

<p>My question takes the form of a thought experiment and asks: ""Which (if any)  aspects of 'embodied' can be relaxed/omitted before we lose something essential for AGI?""</p>
"
142,"<p>In other words, which existing reinforcement method learns in fewest episodes? <a href=""http://www.jmlr.org/papers/volume3/brafman02a/brafman02a.pdf"" rel=""nofollow"">R-Max</a> comes to mind, but its very old and I'd like to know if there is something better now.</p>
"
143,"<p>Are there any research teams which attempted to create or have already created an AI robot which can be as close to intelligent as these found in <a href=""https://en.wikipedia.org/wiki/Ex_Machina_(film)""><em>Ex Machina</a></em> or <em><a href=""https://en.wikipedia.org/wiki/I,_Robot_(film)"">I, Robot</em></a> movies?</p>

<p>I'm not talking about full awareness, but an artificial being which can make its own decisions and physical and intellectual tasks that a human being can do?</p>
"
144,"<p>We, humans, during following multiple processes (e.g. reading while listening to music) memorize information from less focused sources with worse efficiency than we do from our main concentration.</p>

<p>Do such things exist in case of artificial intelligences? I doubt, for example that neural networks obtain such features, but I may be wrong.</p>
"
145,"<p>How can a swarm of small robots (like Kilobots) walking close to each other achieve collaboration without bumping into each other? For example, one study shows <a href=""http://science.sciencemag.org/content/345/6198/795.abstract"" rel=""nofollow"">programmable self-assembly in a thousand-robot swarm</a> (see <a href=""http://robohub.org/thousand-robot-swarm-self-assembles-into-arbitrary-shapes/"" rel=""nofollow"">article</a> &amp; <a href=""https://vimeo.com/103329200"" rel=""nofollow"">video</a>) which are moving without GPS-like system and by measuring distances to neighbours. This was achieved, because the robots were very slow.</p>

<p>Is there any way that similar robots can achieve much more efficient and quicker assembly by using more complex techniques of coordination? Not by walking around clock-wise (which I guess was the easiest way), but I mean using some more sophisticated way. Because waiting half a day (~11h) to create a simple star shape using a thousand-robot swarm is way too long!</p>
"
146,"<p>On Watson wiki page we can read:</p>

<blockquote>
  <p>In healthcare, Watson's natural language, hypothesis generation, and evidence-based learning capabilities allow it to function as a clinical decision support system for use by medical professionals.</p>
</blockquote>

<p>How exactly such AI can help doctors to diagnose the diseases?</p>
"
147,"<p>Recently White House published the article: <a href=""https://www.whitehouse.gov/blog/2016/05/03/preparing-future-artificial-intelligence"" rel=""nofollow"">Preparing for the Future of Artificial Intelligence</a> which says that government is working to leverage AI for public good and toward a more effective government.</p>

<p>I'm especially interested how AI can help with computational sustainability, environmental management and Earth's ecosystem such as biological conservation?</p>
"
148,"<p>When AI has some narrow domain such as chess where it can outperform the world's human masters of chess, does it make it a <a href=""https://en.wikipedia.org/wiki/Superintelligence"" rel=""nofollow noreferrer"">superintelligence</a> or not?</p>
"
149,"<p>Suppose my goal is to collaborate and create an advanced AI, for instance one that resembles a human being and the project would be on the frontier of AI research, what kind of skills would I need?</p>

<p>I am talking about specific things like what university program should I complete to enter and be competent in the field. Here are some of the things that I thought about, just to exemplify what I mean:</p>

<ul>
<li>Computer sciences: obviously the AI is built on computers, it wouldn't hurt to know how computers work, but some low level stuff and machine specific things does not seem essential, I may be wrong of course.</li>
<li>Psychology: if AI resembles human beings, knowledge of human cognition would probably be useful, although I do not imagine neurology on a cellular level or complicated psychological quirks typical to human beings like the Oedipus complex would be relevant, but again, I may be wrong.</li>
</ul>
"
150,"<p><a href=""https://www.whitehouse.gov/webform/rfi-preparing-future-artificial-intelligence"" rel=""nofollow"">White House published the information</a> about AI which requests mentions about 'the most important research gaps in AI that must be addressed to advance this field and benefit the public'.</p>

<p>What are these exactly?</p>
"
151,"<p>Is there any methods by which artificial intelligence use recursion(s) to solve a certain issue or to keep up working and calculating?</p>
"
152,"<p>The Von Neumann's <a href=""https://en.wikipedia.org/wiki/Minimax_theorem"" rel=""nofollow"">Minimax theorem</a> gives the conditions that make the <a href=""https://en.wikipedia.org/wiki/Max%E2%80%93min_inequality"" rel=""nofollow"">max-min inequality</a> an equality.</p>

<p>I understand the max-min inequality, basically <code>min(max(f))&gt;=max(min(f))</code>.</p>

<p>The Von Neumann's theorem states that, for the inequality to become an equality <code>f(.,y)</code> should always be convex for given y and <code>f(x,.)</code> should always be concave for given x, which also makes sense.</p>

<p><a href=""https://www.youtube.com/watch?v=m-EewaiFhF0&amp;list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp&amp;index=61"" rel=""nofollow"">This video</a> says that for a zero-sum perfect information game, the Von Neumann's theorem always holds, so that minimax always equal to maximin, which I did not quite follow.</p>

<p><strong>Questions</strong><br>
Why zero-sum perfect information games satisfy the conditions of Von Neumann's theorem?<br>
If we relax the rules to be non-zero-sum or non-perfect information, how would the conditions change?</p>
"
153,"<p>In October 2014, Dr. Mark Riedl published an approach to testing AI intelligence, called <a href=""http://arxiv.org/pdf/1410.6142v3.pdf"" rel=""noreferrer"">the ""Lovelace Test 2.0""</a>, after being inspired by the <a href=""http://kryten.mm.rpi.edu/lovelace.pdf"" rel=""noreferrer"">original Lovelace Test</a> (published in 2001). Mark believed that the original Lovelace Test would be impossible to pass, and therefore, suggested a weaker, and more practical version.</p>

<p>The Lovelace Test 2.0 makes the assumption that for an AI to be intelligent, it must exhibit creativity. From the paper itself:</p>

<blockquote>
  <p>The Lovelace 2.0 Test is as follows: artificial agent a is challenged as follows:</p>
  
  <ul>
  <li><p>a must create an artifact o of type t;</p></li>
  <li><p>o must conform to a set of constraints C where ci ∈ C is
  any criterion expressible in natural language;</p></li>
  <li><p>a human evaluator h, having chosen t and C, is satisfied
  that o is a valid instance of t and meets C; and</p></li>
  <li><p>a human referee r determines the combination of t and C
  to not be unrealistic for an average human.</p></li>
  </ul>
</blockquote>

<p>Since it is possible for a human evaluator to come up with some pretty easy constraints for an AI to beat, the human evaluator is then expected to keep coming up with more and more complex constraints for the AI until the AI fails. The point of the Lovelace Test 2.0 is to <em>compare</em> the creativity of different AIs, not to provide a definite dividing line between 'intelligence' and 'nonintelligence' like the Turing Test would.</p>

<p>However, I am curious about whether this test has actually been used in an academic setting, or it is only seen as a thought experiment at the moment. The Lovelace Test seems easy to apply in academic settings (you only need to develop some measurable constraints that you can use to test the artificial agent), but it also may be too subjective (humans can disagree on the merits of certain constraints, and whether a creative artifact produced by an AI actually meets the final result).</p>
"
154,"<p>Convolutional neural network are leading type of feed-forward artificial neural network for image recognition. Can they be used for real-time image recognition for videos (frame by frame), or it takes too much processing (assuming they're written in C-like language)?</p>

<p>For example for classification of type of animals based on the training from huge dataset.</p>
"
155,"<p>Just for the purpose of learning I'd like to classify the likeliness of a tweet being in aggressive language or not. </p>

<p>I was wondering how to approach the problem. I guess I need first train my neural network on a huge dataset of text what aggressive language is. This brings up the question where I would get this data in the first place?</p>

<p>It feels a bit like the chicken and egg problem to me so I wonder how would I approach the problem?</p>
"
156,"<p>Siri and Cortana communicate pretty much like humans. Unlike Google now which mainly gives us search results when asked some questions (not setting alarms or reminders), Siri and Cortana provide us with an answer, in the same way that a person would do.<br>
So are they actual AI programs or not?</p>

<p>(By ""question"" I don't mean any academic related question or asking routes/ temperature, but rather opinion based question). </p>
"
157,"<p>What is the difference between AI and robots?</p>
"
158,"<p>With typical machine learning you would usually use a training data-set to create a model of some kind, and a testing data-set to then test the newly created model. For something like linear regression after the model is created with the training data you now have an equation that you would use to predict the outcome of the set of features in the testing data. You would then take the prediction that the model returned and compare that to the actual data in the testing set. How would a validation set be used here?</p>

<p>With nearest neighbor you would use the training data to create an n-dimensional space that has all the features of the training set. You would then use this space to classify the features in the testing data. Again you would compare these predictions to the actual value of the data. How would a validation set help here as well?</p>
"
159,"<p>By reinforcement learning, I don't mean the class of machine learning algorithms such as DeepQ, etc. I have in mind the general concept of learning based on rewards and punishment. </p>

<p>Is it possible to create a Strong AI that does not rely on learning by reinforcement, or is reinforcement learning a requirement for artificial intelligence? The existence of rewards and punishment imply the existence of favorable and unfavorable world-states. Must intelligence in general and artificial intelligence in particular have a way of classifying world-states as favorable or unfavorable?  </p>
"
160,"<p>For example, search engine companies want to classify their image searches into 2 categories (which they already do that) such as: <a href=""https://en.wikipedia.org/wiki/Not_safe_for_work"" rel=""nofollow"">NSFW</a> (nudity, porn, brutality) and safe to view pictures.</p>

<p>How can artificial neural networks achieve that, and at what success rate? Can they be easily mistaken?</p>
"
161,"<p>Do scientists or research experts know from the kitchen what is happening inside complex ""deep"" neural network with at least millions of connections firing at an instant? Do they understand the process behind this (e.g. what is happening inside and how it works exactly), or it is a subject of debate?</p>

<p>For example this <a href=""https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf"" rel=""noreferrer"">study</a> says:</p>

<blockquote>
  <p>However there is no clear understanding of <em>why</em> they perform so well, or <em>how</em> they might be improved.</p>
</blockquote>

<p>So does this mean that scientists actually don't know how complex convolutional network models work?</p>
"
162,"<p>Is there any way to estimate how big the neural network would be after training session of 100,000 unlabeled images for unsupervised learning (like in <a href=""https://cs.stanford.edu/~acoates/stl10/"" rel=""nofollow"">STL-10 dataset</a>: 96x96 pixels and color)?</p>

<p>Not the storage space (because this could vary I guess based on the implementation), but specifically how many neurons it could have. It could be an estimate (e.g. in thousand, millions). If it depends, then on what? Are there any figures that can be estimated?</p>
"
163,"<p>For example I'd like to train my neural network to recognize the type of actions (e.g. in commercial movies or some real life videos), so I can ""ask"" my network in which video or movie (and at what frames) somebody was driving a car, kissing, eating, was scared or was talking over the phone.</p>

<p>What are the current successful approaches to that type of problem?</p>
"
164,"<p>I'm playing with an LSTM to generate text. In particular, this one:</p>

<p><a href=""https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py"" rel=""nofollow"">https://raw.githubusercontent.com/fchollet/keras/master/examples/lstm_text_generation.py</a></p>

<p>It works on quite a big demo text set from Nietzsche and says</p>

<blockquote>
  <p>If you try this script on new data, make sure your corpus
  has at least ~100k characters. ~1M is better.</p>
</blockquote>

<p>This pops up a couple of questions.</p>

<p>A.) If all I want is an AI with a very limited vocabulary where the generate text should be short sentences following a basic pattern.</p>

<p>E.g.</p>

<p><em>I like blue sky with white clouds</em></p>

<p><em>I like yellow fields with some trees</em></p>

<p><em>I like big cities with lots of bars</em></p>

<p>...</p>

<p>Would it then be reasonable to use a much much smaller dataset?</p>

<p>B.) If the dataset really needs to be that big. What if I just repeat the text over and over to reach the recommended minimum? If that would work though, I'd be wondering how that is any different from just taking more iterations of learning with the same shorter text?</p>

<p>Obviously I can play with these two questions myself and in fact I am experimenting with it. One thing I already figured out is that with a shorter text following a basic pattern I can get to a very very low ( ~0.04) quite fast but the predicted text just turns out as gibberish.</p>

<p>My naive explanation for that would be that there are just not enough samples to proof against whether the gibberish actually makes sense or not? But then again I wonder if more iterations or duplicating the content would actually help.</p>

<p>I'm trying to experiment with these questions myself so please don't think I'm just too lazy and are aiming for others to do the work. I'm just looking for more experienced people to give me a better understanding of the mechanics that influence these things.</p>
"
165,"<p>For example I would like to implement transparent AI in the RTS game which doesn't offer any AI API (like old games), and I'd like to use image recognition algorithm for detecting the objects which can talks to another algorithm which is responsible for the logic.</p>

<p>Given I'd like to use two neural networks, what are the approaches to setup the communication between them? Is it just by exporting result findings of the first algorithm (e.g. using CNN) with list of features which were found on the screen, then use it as input for another network? Or it's more complex than that, or I need to have more than two networks?</p>
"
166,"<p>Were there any successful attempts to replace poor guide dogs used for blind people with AI to achieve similar rate of success? I guess dogs could be easily distracted and not reliable for every situation, and it probably takes less time to train AI, than a dog.</p>
"
167,"<p>Do we know why Tesla's Autopilot mistaken empty sky with a high-sided lorry which resulted in fatal crash involving a car in self-drive mode? Was it AI fault or something else? Is there any technical explanation behind this why this happened?</p>

<p>References: <a href=""http://news.sky.com/story/tesla-driver-in-first-self-drive-fatal-crash-10330121"" rel=""nofollow"">Sky News article</a>, <a href=""http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s"" rel=""nofollow"">The Verge</a>.</p>
"
168,"<p>For benefits of testing AGI, is using a high-level video game description language (VGDL) gives more reliable and accurate results of general intelligence than using Arcade Learning Environment (ALE)?</p>
"
169,"<p>Some time ago playing chess was challenging for algorithms, then Go game which is vastly more complex than compared to chess.</p>

<p>How about playing RTS game which have enormous branching factors limited by its time and space (like deciding what to do next)? What are the successful approaches to such problems?</p>
"
170,"<p>We can read on wiki page that in March 2016 AlphaGo AI lost its game (1 of 5) to Lee Sedol, a professional Go player. One <a href=""http://www.bbc.co.uk/news/technology-36558829"" rel=""nofollow"">article</a> cite says:</p>

<blockquote>
  <p>AlphaGo lost a game and we as researchers want to explore that and find out what went wrong. We need to figure out what its weaknesses are and try to improve it.</p>
</blockquote>

<p>Have researchers already figured it out what went wrong?</p>
"
171,"<p>Assuming we're dealing with artificial neural network (e.g. using <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""nofollow"">convnets</a>) which was trained by large dataset of human faces.</p>

<p>Are there any known issues or challenges where facial recognition would fail? I'm not talking about covering half of the face, but some simple common things such as wearing the glasses, hat, jewellery, having face painting or tattoo, can this successfully prevent AI from recognizing the face? If so, what are current methods dealing with such challenges?</p>
"
172,"<p>I would like to know what kind of dataset I need (to prepare) for training the network to recognize the spelling mistakes in individual words for English text.</p>

<p>Given the large database of words, having correct one for each incorrect. What kind of input is more efficient for that tasks? Is it using one input per each letter, syllable, whole word or I should use different pattern syllable?</p>

<p>Then the input should be incorrect word, output correct, and if the word doesn't need correction, then both input and output should be the same. Is that the right approach?</p>
"
173,"<p>As I have been looking at other questions on this site (like <a href=""https://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development"">this</a>, <a href=""https://ai.stackexchange.com/questions/1376/is-it-ethical-to-implement-self-defence-for-street-walking-ai-robots"">this</a>, <a href=""https://ai.stackexchange.com/questions/111/how-would-self-driving-cars-make-ethical-decisions-about-who-to-kill"">this</a>, and <a href=""https://ai.stackexchange.com/questions/1289/can-we-destroy-artificial-general-intelligence-without-its-consent"">this</a>), I have been thinking more about the ethical implications of creating these generalized AI systems. It seems that whether or not we <em>can</em> create it is not rationale enough as to whether or not we <em>should</em> do it.</p>

<p>In dealing with the issue of ethics in AI, I wonder what the ethical implications are not just for us, but for the system itself. It seems to extend beyond the usually asked questions on the topic and into unknown territory. Are ethics computable? Can they be implemented programmatically? Can we force an AI system to do something against its <em>""will""</em>?</p>

<p>What does the creation of AI imply ethically for us as well as the AI?</p>
"
174,"<p>I believe <em>artificial intelligence</em> (AI) term is overused nowadays. For example, people see that something is self-moving and they call it AI, even if it's on autopilot (like cars or planes) or there is some simple algorithm behind it.</p>

<p>What are the minimum general requirements so that we can say something is AI?</p>
"
175,"<p>I believe normally you can use <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> for sorting, however I'd like to check whether it's possible using ANN.</p>

<p>Given the unsorted text data from input, which neural network is suitable for doing sorting tasks?</p>
"
176,"<p>I've read on wiki that <a href=""https://en.wikipedia.org/wiki/Genetic_programming"" rel=""nofollow"">genetic programming</a> has '<em>outstanding results</em>' in cyberterrorism prevention.</p>

<p>Further more, this <a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=877981"" rel=""nofollow"">abstract</a> says:</p>

<blockquote>
  <p>Using machine-coded linear genomes and a homologous crossover operator in genetic programming, promising results were achieved in detecting malicious intrusions.</p>
</blockquote>

<p>I've checked the study, but it's still not clear for me.</p>

<p>How exactly was this detection achieved from the technical perspective?</p>
"
177,"<p>On Wikipedia, we can read about different type of <a href=""https://en.wikipedia.org/wiki/Intelligent_agent"" rel=""nofollow noreferrer"">intelligent agents</a>:</p>

<ul>
<li>abstract intelligent agents (AIA),</li>
<li>autonomous intelligent agents,</li>
<li>virtual intelligent agent (IVA), which I've found on other websites, e.g. <a href=""https://www.techopedia.com/definition/26646/intelligent-virtual-agent-iva"" rel=""nofollow noreferrer"">this one</a>.</li>
</ul>

<p>What are the differences between these three to avoid confusion?</p>

<hr>

<p>For example I've used term <em>virtual artificial agent</em> <a href=""https://ai.stackexchange.com/a/1512/8"">here</a> as:</p>

<blockquote>
  <p>Basically a robot is a mechanical or virtual artificial agent which exhibit intelligent behavior (AI).</p>
</blockquote>

<p>so basically I'd like to know where other terms like autonomous or abstract agents can be used and in what context. Can they be all defined under 'virtual' robot definition? How to distinguish these terms?</p>
"
178,"<p>On <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)"" rel=""nofollow"">Wikipedia</a> we can read:</p>

<blockquote>
  <p>Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.</p>
</blockquote>

<p>What was the accusation and how was Deep Blue allegedly able to cheat?</p>
"
179,"<p>The Wikipedia page describes <a href=""https://en.wikipedia.org/wiki/AI_control_problem"" rel=""nofollow"">AI control problem</a> in very intricated way.</p>

<p>Therefore I would like to better understand it based on some simple explanation, what's going on.
Basically I don't want any copy &amp; pastes from wiki, because the articles there are written in neutral point of view, in very general way where articles are evolving very slowly, so the definition from there doesn't suit me.</p>

<p>I believe this is what is discussed nowadays by government and it's important aspects of AI technology where it leds to.
I believe this could be a big problem in the near future, so I'm expecting to hear about this from people from much better and more up-to-date point of view.</p>

<p>So what is exactly the AI Control Problem?</p>
"
180,"<p><sub>This is from a closed beta for AI, with this question being posted by user number 47. All credit to them. </sub></p>

<hr>

<p>According to <a href=""https://en.wikipedia.org/wiki/Boltzmann_machine"" rel=""noreferrer"">Wikipedia</a>,</p>

<blockquote>
  <p>Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield nets.</p>
</blockquote>

<p>Both are recurrent neural networks that can be trained to learn of bit patterns. Then when presented with a partial pattern, the net will retrieve the full complete pattern.</p>

<p>Hopfield networks have been proven to have a capacity of 0.138 (e.g. approximately 138 bit vectors can be recalled from storage for every 1000 nodes, Hertz 1991).</p>

<p>As a Boltzmann machine is stochastic, my understanding is that it would not necessarily always show the same pattern when the energy difference between one stored pattern and another is similar. But because of this stochasticity, maybe it allows for denser pattern storage but without the guarantee that you'll always get the ""closest"" pattern in terms of energy difference. Would this be true? Or would a Hopfield net be able to store more patterns?</p>
"
181,"<p>According to <a href=""http://en.wikipedia.org/wiki/Prolog"">Wikipedia</a>,</p>

<blockquote>
  <p>Prolog is a general-purpose logic programming language associated with artificial intelligence and computational linguistics.</p>
</blockquote>

<p>Is it still used for AI?</p>

<hr>

<p><sub>This is based off of a question on the 2014 closed beta. The author had the UID of 330.</sub></p>
"
182,"<p>I'm a bit confused with extensive number of different <a href=""https://en.wikipedia.org/wiki/Monte_Carlo_method"" rel=""nofollow"">Monte Carlo methods</a> such as:</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Hybrid_Monte_Carlo"" rel=""nofollow"">Hamiltonian/Hybrid Monte Carlo (HMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo"" rel=""nofollow"">Markov chain Monte Carlo (MCMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Kinetic_Monte_Carlo"" rel=""nofollow"">Kinetic Monte Carlo (KMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Dynamic_Monte_Carlo_method"" rel=""nofollow"">Dynamic Monte Carlo (DMC)</a></li>
<li><a href=""https://en.wikipedia.org/wiki/Quasi-Monte_Carlo_method"" rel=""nofollow"">Quasi-Monte Carlo (QMC)</a>,</li>
<li><a href=""https://en.wikipedia.org/wiki/Direct_simulation_Monte_Carlo"" rel=""nofollow"">Direct Simulation Monte Carlo (DSMC)</a>,</li>
<li>and so on.</li>
</ul>

<p>I won't ask for the exact differences, but why are all of them called Monte Carlo? What do they all have in common? Can they all be used for AI? E.g. which one can be used for gaming (like Go) or image recognition (resampling)?</p>
"
183,"<p>When it comes to neural networks, it's often only explained what abstract task they do, say for example detect a number in an image. I never understood what's going on under the hood essentially.</p>

<p>There seems to be a common structure of a directed graph, with values in each node. Some nodes are input nodes. Their values can be set. The values of subsequent nodes are then calculated based on those along the edges of the graph until the values for the output nodes are set, which can be interpreted a result.</p>

<p>How exactly is the value of each node determined? I assume that some formula is associated with each node that takes all incoming nodes as input to calculate the value of the node. What formula is used? Is the formula the same throughout the network?</p>

<p>Then I heard that a network has to be trained. I assume that such training would be the process to assign values to coefficients of the formulas used to determine the node values. Is that correct?</p>

<p>In layman's terms, what are the underlying principles that make a neural network work?</p>
"
184,"<p>Ideally I'd like to watch movie which is deep dreamed in real-time. Most algorithms which I know are too slow or not designed for real-time processing.</p>

<p>For example I'm bored with some movie which I've watched thousands of time and I'd like to add some ""dreaming"" to it which is real-time filter which takes input frames, then it's processing and enhances the images through artificial neural network to achieve doodled output.</p>

<p>Doesn't have to be exactly <a href=""https://en.wikipedia.org/wiki/DeepDream"" rel=""nofollow"">DeepDream</a> or hallucinogenic technique (which could be too much to watch for 2h), but with any similar ANN algorithm. I'm more interested into achieving desired real-time use.</p>

<p>What kind of techniques can achieve such efficiency?</p>
"
185,"<p>How does employing evolutionary algorithms to design and train artificial neural networks have advantages over using the conventional backpropagation algorithms?</p>
"
186,"<p>Are there any existing approaches for using artificial neural networks (ANN) or evolutionary algorithm (EA) for detecting coding standard violations? Which one would be more suitable?</p>

<p>I don't have any specific programming language in mind, but something similar to <a href=""http://pear.php.net/package/PHP_CodeSniffer"" rel=""nofollow"">PHP_CodeSniffer</a> (following <a href=""https://www.drupal.org/coding-standards"" rel=""nofollow"">these standards</a>), but instead of using hardcoded rules, the algorithm should learn good techniques, but I'm not sure based on what training data. How would you approach the training session, any suggestions?</p>
"
187,"<p>Genetic Algorithms has come to my attention recently when trying to correct/improve computer opponents for turn-based strategy computer games.</p>

<p>I implemented a simple Genetic Algorithm that didn't use any cross-over, just some random mutation. It seemed to work in this case, and so I started thinking:</p>

<p><em>Why is cross-over a part of genetic algorithms? Wouldn't mutation be enough?</em></p>

<p><sub>This is from a data dump on an old AI site. The asker had the UID of 7. </sub></p>
"
188,"<p>While thinking about AI, this question came into my mind. Could curiosity help in developing a true AI? According to this <a href=""http://psychologia.co/creativity-test/"" rel=""nofollow noreferrer"">website</a> (for testing creativity):</p>

<blockquote>
  <p>Curiosity in this context refers to persistent desire to learn and discover new things and ideas. A curious person</p>
  
  <ul>
  <li>always looks for new and original ways of thinking,</li>
  <li>likes to learn,</li>
  <li>searches for alternative solutions even when traditional solutions are present and available,</li>
  <li>enjoys reading books and watching documentaries,</li>
  <li>wants to know how things work inside out.</li>
  </ul>
</blockquote>

<p>Let's take <a href=""https://www.clarifai.com/demo"" rel=""nofollow noreferrer"">Clarifai</a>, a image/video classification startup which can classify images and video with the best accuracy (according to them). If I understand correctly, they trained their deep learning system using millions of images with supervised learning. In the same algorithm, what would happen if we somehow added a ""curiosity factor"" when the AI has difficulty in classifying a image or its objects? It would ask a human for help, just like a curious child. </p>

<p>Curiosity makes a human being learn new things and also helps to generate new original ideas. Could the addition of curiosity change Clarifai into a true AI?</p>
"
189,"<p>Based on this <a href=""http://www.dailymail.co.uk/sciencetech/article-3677950/Google-s-self-driving-cars-spot-cyclists-Sensors-read-hand-signals-predict-riders-behavior.html"" rel=""nofollow"">article</a>, Google's self-driving cars can spot cyclists, cars, road signs, markings, traffic lights, and pedestrians.</p>

<p>How exactly does it identify pedestrians? Is it based on face recognition, shape, size, distance, infrared signature?</p>
"
190,"<p>In <a href=""https://www.technologyreview.com/s/530276/hidden-obstacles-for-googles-self-driving-cars/"">Hidden Obstacles for Google’s Self-Driving Cars</a> article we can read that:</p>

<blockquote>
  <p>Google’s cars can detect and respond to stop signs that aren’t on its map, a feature that was introduced to deal with temporary signs used at construction sites.</p>
  
  <p>Google says that its cars can identify almost all unmapped stop signs, and would remain safe if they miss a sign because the vehicles are always looking out for traffic, pedestrians and other obstacles.</p>
</blockquote>

<p>What would happen if a car spotted somebody in front of it (but not on the collision path) wearing a T-shirt that has a stop sign printed on it. Would it react and stop the car?</p>
"
191,"<p>After self-driving cars are perfected to the degree that accident statistics are lower for them than human driven cars, will they replace cars with human drivers?  Will there eventually only be self-driving cars?</p>
"
192,"<p>Significant AI vs human board game matches include:</p>

<ul>
<li><strong>chess</strong>: <a href=""https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)#Deep_Blue_versus_Kasparov"" rel=""noreferrer"">Deep Blue vs Kasparov</a> in 1996,</li>
<li><strong>Go</strong>: <a href=""https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol"" rel=""noreferrer"">DeepMind AlphaGo vs Lee Sedol</a> in 2016,</li>
</ul>

<p>which demonstrated that AI challenged and defeated professional players.</p>

<p>Are there known board games left where a human can still win against an AI? I mean based on the final outcome of authoritative famous matches, where there is still same board game where AI cannot beat a world champion of that game.</p>
"
193,"<p>I'm trying to teach an AI different pattern of tic tac toe to recognize wether a given pattern represents a win or not.</p>

<p>Unfortunately it's not learning to recognize them correctly and I think may way of representing/encoding the game into vectors is wrong.</p>

<p>I choose a way that is easy for an human (me, in particular!) to make sense of:</p>

<pre><code>training_data = np.array([[0,0,0,
                           0,0,0,
                           0,0,0],
                          [0,0,1,
                           0,1,0,
                           0,0,1],
                          [0,0,1,
                           0,1,0,
                           1,0,0],
                          [0,1,0,
                           0,1,0,
                           0,1,0]], ""float32"")
target_data = np.array([[0],[0],[1],[1]], ""float32"")
</code></pre>

<p>This basically just use an array of length 9 to represent a 3 x 3 board. The first three items represent the first row, the next three the second row and so on. The line breaks should make it obvious I guess.</p>

<p>The target data then maps the first two game states to ""no wins"" and the last two game states to ""wins"".</p>

<p>Then I wanted to create some validation data that is slightly different to see if it generalizes.</p>

<pre><code>validation_data = np.array([[0,0,0,
                             0,0,0,
                             0,0,0],
                            [1,0,0,
                             0,1,0,
                             1,0,0],
                            [1,0,0,
                             0,1,0,
                             0,0,1],
                            [0,0,1,
                             0,0,1,
                             0,0,1]], ""float32"")
</code></pre>

<p>Obviously, again the last two game states should be ""wins"" whereas the first two should not.</p>

<p>I tried to play with the number of neurons and learning rate but no matter what I try, my output looks pretty of. E.g.</p>

<pre><code>[[ 0.01207292]
 [ 0.98913926]
 [ 0.00925775]
 [ 0.00577191]]
</code></pre>

<p>I tend to think it's the way how I represent the game state that may be wrong but actually I have no idea :D</p>

<p>Can anyone help me out here?</p>

<p>This is the entire code that I use</p>

<pre><code>import numpy as np
from keras.models import Sequential
from keras.layers.core import Activation, Dense
from keras.optimizers import SGD

training_data = np.array([[0,0,0,
                           0,0,0,
                           0,0,0],
                          [0,0,1,
                           0,1,0,
                           0,0,1],
                          [0,0,1,
                           0,1,0,
                           1,0,0],
                          [0,1,0,
                           0,1,0,
                           0,1,0]], ""float32"")

target_data = np.array([[0],[0],[1],[1]], ""float32"")

validation_data = np.array([[0,0,0,
                             0,0,0,
                             0,0,0],
                            [1,0,0,
                             0,1,0,
                             1,0,0],
                            [1,0,0,
                             0,1,0,
                             0,0,1],
                            [0,0,1,
                             0,0,1,
                             0,0,1]], ""float32"")

model = Sequential()
model.add(Dense(2, input_dim=9, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)

history = model.fit(training_data, target_data, nb_epoch=10000, batch_size=4, verbose=0)

print(model.predict(validation_data))
</code></pre>
"
194,"<p>Has there any research been done on how difficult certain languages are to learn for chatbots? 
For example, CleverBot knows a bit of Dutch, German, Finnish and French, so there are clearly chatbots that speak other languages than English. (English is still her best language, but that is because she speaks that most often)</p>

<p>I would imagine that a logical constructed language, like lobjan, would be easier to learn than a natural language, like English, for example.  </p>
"
195,"<p>Google, Tesla, Apple etc have all built or are building their own self-driving cars. As an expert in a related area, I am interested in knowing at a high level, the systems and techniques that go into self-driving cars. How easy is it for me to make a tabletop prototype (large enough to accomodate the needed computing power needs)?</p>
"
196,"<p>The above question itself is perhaps too broad for this forum, hence I am phrasing it as a request for references.</p>

<p>Humans have been endowed with personalities by nature, and it is not clear (to me at least) if this is a feature or a bug. This has been explored in science fiction by various notions of <a href=""http://memory-alpha.org/Borg"" rel=""nofollow"">Borg</a>-like entities. It is my belief that, for narrative reasons, such stories usually end with the humans with their flawed personalities winning in the end. </p>

<p>Are there experts who have analyzed, perhaps mathematically, design criteria for an AI agent with weakly enforced goals (eg. to maximize reproduction in the human case) in an uncertain environment, and ended up with the answer that a notion of personality is useful? If there are philosophers or science fiction writers who have examined this question in their work, I would be happy to know about those too.</p>
"
197,"<p>I've found this short <a href=""http://iamtrask.github.io/2015/07/12/basic-python-network/"" rel=""nofollow"">Python code</a> which implements neural network in 11 lines of code:</p>

<pre><code>X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).T
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1
for j in xrange(60000):
    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))
    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))
    l2_delta = (y - l2)*(l2*(1-l2))
    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))
    syn1 += l1.T.dot(l2_delta)
    syn0 += X.T.dot(l1_delta)
</code></pre>

<p>I believe it may be a valid implementation of neural network, but how do I know?</p>

<p>In other words, is just creating bunch of arrays which compute the output on certain criteria and call them layers with synapses does it make proper neural network?</p>

<p>In other words, I'd like to ask, what features/properties makes a valid artificial neural network?</p>
"
198,"<p>I'm looking for research which discusses misbehavior detection in public internet access networks using ANN approaches.</p>

<p>So it can be used by <a href=""https://en.wikipedia.org/wiki/Internet_service_provider"" rel=""nofollow"">ISP</a> to detect suspicious users connected to their network.</p>
"
199,"<p>I'm investigating applications of AI algorithms which can be used for data leakage detection and prevention within an intranet network (like <a href=""https://en.wikipedia.org/wiki/Forcepoint"" rel=""nofollow"">Forcepoint</a>). More specifically detecting traffic patterns. I'm new to this.</p>

<p>Which learning algorithms are most suitable for this goal? <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">EA</a>, <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">GA</a>, <a href=""https://en.wikipedia.org/wiki/Artificial_neural_network"" rel=""nofollow"">ANN</a> (which one) or something else?</p>
"
200,"<p>I'm wondering, instead of implementing new web browsers over and over again with millions line of code which is very difficult to manage, would it be possible to use ANN or GA algorithm to teach it about the rendering process (how the page should look like)?</p>

<p>So as an input I would imaging the html source code, output is the rendered page (maybe in some interactive image like SVG, some library or something, I'm not sure).</p>

<p>The training data can be dataset of websites providing input source code and their rendered representation by using other browsers for the guidance as expected output.</p>

<p>Which approach would you take and what are the most challenging things you can think of?</p>
"
201,"<p>I'm trying to make a conversational chatbot, so the user inputs are quite wide ranging - beyond just ""turn lights on"". I want to detect the category of the user intents from their inputs and prepare responses.</p>

<p>I've looked at MS' Luis and api.ai and the intents require a lot of training. Can people suggest other techniques for untrained intent detection?</p>

<p>For example if the user says ""Pasta is my favorite dish to cook"" then detect ""intent preference entity pasta"" - then I can gradually build up responses to different categories of inputs.</p>

<p>Perhaps the crowd-sourced intents that wit.ai (facebook) has access to could do this but I'm not sure if all end-users have access to those models.</p>
"
202,"<p>How does a domestic autonomous robotic vacuum cleaner -  such as a <a href=""https://en.wikipedia.org/wiki/Roomba"" rel=""noreferrer"">Roomba</a> - know when it's working cleaned area (aka virtual map), and how does it plan to travel to the areas which hasn't been explored yet?</p>

<p>Does it use some kind of <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""noreferrer"">A*</a> algorithm?</p>
"
203,"<p>It has been <a href=""http://www.itnonline.com/content/will-fda-be-too-much-intelligent-machines"" rel=""nofollow noreferrer"">suggested</a> that machine learning algorithms (also <a href=""https://ai.stackexchange.com/q/1427/8"">Watson</a>) can help with finding disease in patient images and optimize scans. Also that deep learning algorithms show promise for every type of digital imaging.</p>

<p>How does exactly deep learning algorithms exactly can find suspicious patterns in the body’s biochemistry?</p>
"
204,"<p>The <a href=""https://www.youtube.com/watch?v=AplG6KnOr2Q"" rel=""nofollow"">Mario Lives!</a> video (and its follow-up video, <a href=""https://www.youtube.com/watch?v=ltPj3RlN4Nw&amp;list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&amp;index=5"" rel=""nofollow"">Mario Becomes Social!</a>) showcases an AI unit that is able to simulate emotional desicion-making within a virtual world, and can enter into ""emotional states"" such as curiosity, hunger, happiness, and fear. While this seems cool and exciting (especially for video game AI), I am confused how this would be useful in real-world scenarios.</p>

<p>What would be the point of building autonomous actors that would behave based on these emotional states, instead of simply knowing <em>what</em> they should do (either by hardcoding in the rules, or learning the rules through machine learning)?</p>
"
205,"<p><sub>This is from the 2014 closed beta. The asker had the UID of 245.</sub></p>

<p>For a deterministic problem space, I need to find a neural network with the optimal node and link structure. I want to use a genetic algorithm to simulate many neural networks to find the best network structure for the problem domain.</p>

<p>I know a fair amount about neural networks<sup>1</sup> but have not used genetic algorithms for a task like this before.</p>

<p>What are the practical considerations? 
How should I encode the structure into a genome?</p>

<hr>

<p><sub><sup>1</sup>Actually, I don't. Just saying that. -Mithrandir. </sub></p>
"
206,"<p>Were there any studies which checked the accuracy of neural network predictions of greyhound racing results, compared to a human expert? Would it achieve a better payoff?</p>
"
207,"<p>I've read about The Loebner Prize for AI, which pledged a Grand Prize of $100,000 and a Gold Medal for the first computer whose responses were indistinguishable from a human's.</p>

<p>So I was wondering whether any chatbots have fooled the judges and won a Gold Medal yet?</p>

<p>From their <a href=""http://www.loebner.net/Prizef/loebner-prize.html"" rel=""nofollow"">website</a> this isn't clear (as some of the links doesn't load).</p>

<hr>

<p>A few highlights from previous years:</p>

<p><a href=""http://loebner.exeter.ac.uk/results/"" rel=""nofollow"">2011 Loebner Prize results</a></p>

<blockquote>
  <p>None of the AI systems fooled the judges, therefore the Turing Test has not been passed.</p>
</blockquote>

<p><a href=""http://www.paulmckevitt.com/loebner2013/scoring/loebner2013leaderboard.txt"" rel=""nofollow"">Loebner 2013 results</a>:</p>

<blockquote>
  <p>No chatbot fooled any of the 4 Judges.</p>
</blockquote>
"
208,"<p>Hypothetically, assume that you have access to infinite computing power. Do we have designs for any brute-force algorithms that can find an AI capable of passing traditional tests (e.g. Turing, Chinese Room, MIST, etc.)? </p>
"
209,"<p>I'm aware this could be a complex topic, however I'm interested in existing research projects or studies where people are attempting or have succeeded in teaching an AI a foreign language just by training/teaching it from English books. By reading, analysing and understanding, so that it knows the foreign language's rules (such as grammar, spelling, etc.), the same way as a human would learn. The language doesn't have to be Chinese, which is difficult for even humans to learn.</p>
"
210,"<p>Would it be possible to put Asimov's three Laws of Robotics into an AI?</p>

<p>The three laws are:</p>

<ol>
<li><p>A robot (or, more accurately, an AI) cannot harm a human being, or through inaction allow a human being to be harmed<sup>1</sup></p></li>
<li><p>A robot must listen to instructions given to it by a human, as long as that does not conflict with the first law.</p></li>
<li><p>A robot must protect its own existence, if that does not conflict with the first two laws.</p></li>
</ol>

<hr>

<p><sup>1</sup> <em>To it's knowledge</em>. This was a plot point in one of the books :P</p>
"
211,"<p>I'd like to investigate the possibility of achieving similar recognition as it's in <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer"">Honda's ASIMO robot</a><sup>p.22</sup> which can interpret the positioning and movement of a hand, including postures and gestures based on visual information.</p>

<p>Here is the example of application such interpretation in robot:</p>

<p><a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UDram.png"" alt=""Honda&#39;s ASIMO robot - Recognition of postures and gestures based on visual information""></a></p>

<p><sup>Image source: <a href=""http://asimo.honda.com/downloads/pdf/asimo-technical-information.pdf"" rel=""nofollow noreferrer"">ASIMO Featuring Intelligence Technology - Technical Information (PDF)</a></sup></p>

<p>So basically the recognition should detect an indicated location (posture recognition) or respond to a wave (gesture recognition), also similar like <a href=""https://ai.stackexchange.com/a/1577/8"">Google car</a> does it (by determining certain patterns).</p>

<p>Is it known how ASIMO does it, or what would be the closest alternative for postures and gestures recognition to achieve the same results?</p>
"
212,"<p>For example, could you provide reasons why a sundial is <em>not</em> ""intelligent""?
A sundial senses its environment and acts rationally. It outputs the time. It also stores  percepts. (The numbers the engineer wrote on it.)</p>

<p>What properties of a self driving car would make it ""intelligent""? </p>

<p>Where is the line between non intelligent matter and an intelligent system?</p>
"
213,"<p>We can read on <a href=""https://en.wikipedia.org/wiki/TensorFlow#Tensor_processing_unit_.28TPU.29"" rel=""nofollow"">Wikipedia page</a> that Google built a custom ASIC chip for machine learning and tailored for TensorFlow which helps to accelerate AI.</p>

<p>Since ASIC chips are specially customized for one particular use without the ability to change its circuit, there must be some fixed algorithm which is invoked.</p>

<p>So how exactly does the acceleration of AI using ASIC chips work if its algorithm cannot be changed? Which part of it is exactly accelerating?</p>
"
214,"<p>I was reading that the <a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""noreferrer"">Valkyrie robot</a> was originally designed to 'carry out search and rescue missions'.</p>

<p>However there were some talks to send it to Mars to assist astronauts.</p>

<p>What kind of specific trainings or tasks are planned for 'him' to be able to carry on its own?</p>

<p>Refs:</p>

<ul>
<li><a href=""https://github.com/nasa-jsc-robotics"" rel=""noreferrer"">NASA-JSC-Robotics at GitHub</a></li>
<li><a href=""http://nasa-jsc-robotics.github.io/valkyrie/"" rel=""noreferrer"">github.io page</a></li>
<li><a href=""https://gitlab.com/nasa-jsc-robotics/valkyrie"" rel=""noreferrer"">gitlab page</a></li>
</ul>
"
215,"<p>Do scientists know by what mechanism biological brains/biological neural networks store data?</p>

<p>I was thinking about @kenorbs <a href=""https://ai.stackexchange.com/questions/1656/how-can-nanobot-implants-in-our-brains-connect-to-the-internet"">question</a> about implanting nanobots to build an AGI on top of human wetware. </p>

<p>I only have a vague notion that we store data in our brains by altering synapses? </p>

<p>Links, Criticism and Detailed Explanation welcome.</p>

<p>I also would love a decent description of how a vanilla Artificial Neural Network stores data. </p>

<p><strong>Questions:</strong></p>

<ol>
<li><p>How is data stored in a biological Neural Network?</p></li>
<li><p>How is data stored in an Artificial Neural Network?</p></li>
</ol>
"
216,"<p>My understanding is that <em>Watson</em> is the name of the computer, and <em>DeepQA</em> is the name of the software or technology. They are both correlated.</p>

<p>Are there any computers/technologies other than <em>Watson</em> which <strong>are using <em>DeepQA</em></strong>? Or is <em>Watson</em> the only computer which implements that software/technology?</p>

<p><sup>This question is inspired by this <a href=""https://ai.meta.stackexchange.com/q/1177/8"">meta thread</a>.</sup></p>
"
217,"<p>There is a study about <a href=""http://www.aclweb.org/anthology/P/P02/P02-1031.pdf"" rel=""noreferrer"">The Necessity of Parsing for Predicate Argument Recognition</a>, however I couldn't find much information about 'Predicate Argument Recognition' which could explain it.</p>

<p>What is it exactly and how does it work, briefly?</p>
"
218,"<p>The Wit.ai is a Siri-like voice interface which can can parse messages and predict the actions to perform.</p>

<p>Here is the <a href=""https://labs.wit.ai/demo/index.html"" rel=""nofollow"">demo site powered by Wit.ai</a>.</p>

<p>How does it understand the spoken sentences and convert them into structured actionable data? Basically, how does it know what to do?</p>
"
219,"<p>In 2014 <a href=""https://techcrunch.com/2014/02/06/linkedin-snatches-up-data-savvy-job-search-startup-bright-com-for-120m-in-its-largest-acquisition-to-date/"" rel=""nofollow"">Linkedin acquired Bright.com</a>, for $120 million and it is using AI and big data algorithms to connect users.</p>

<blockquote>
  <p>Bright also throws in a little Klout, ranking people by a “Bright score” which it uses to assess how strong the chemistry is between a user and a particular job.</p>
  
  <p>It also takes into account historical hiring patterns into its matching, along with account location, a user’s past experience and synonyms.</p>
</blockquote>

<p>In brief, is it known (based on some research papers) how such algorithm works which aiming at scoring 'chemistry' between users and their jobs?</p>
"
220,"<p>According to this <a href=""http://mashable.com/2014/01/06/pinterest-acquires-visualgraph/"" rel=""nofollow"">article</a>, Pinterest acquired VisualGraph, an image recognition and visual search technology startup.</p>

<p>How does Pinterest apply VisualGraph technology for machine vision, image recognition and visual search in order to classify the images?</p>

<p>In short, how do they predict the image categories? Based on what features?</p>
"
221,"<p>Wolfram Language Image Identification Project launched an <a href=""https://www.imageidentify.com/"" rel=""nofollow"">Image Identify site</a> demo which returns the top predicted tags for the photos.</p>

<p>How does it work, briefly? I mean what type of learning vision technologies are used to analyze, recognize and understand the content of an image?</p>
"
222,"<p>I've <a href=""https://www.imageidentify.com/result/0lkzuttdxipub"" rel=""nofollow noreferrer"">uploaded a picture</a> to Wolfram's ImageIdentify of graffiti on the wall, but it recognized it as 'monocle'. Secondary guesses were 'primate', 'hominid', and 'person', so not even close to 'graffiti' or 'painting'.</p>

<p>Is it by design, or there are some <strong>methods to teach a convolutional neural network (CNN) to reason and be aware of a bigger picture context</strong> (like mentioned graffiti)? Currently it seems as if it's detecting literally <em>what is depicted in the image</em>, not <em>what the image actually is</em>.</p>

<p><a href=""https://i.stack.imgur.com/akquMm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/akquMm.png"" alt=""Wolfram&#39;s Image Identify: monocle/graffiti""></a></p>

<p>This could be the same problem as mentioned <a href=""https://ai.stackexchange.com/a/1533/8"">here</a>, that DNN are:</p>

<blockquote>
  <p>Learning to detect jaguars by matching the unique spots on their fur while ignoring the fact that they have four legs.<sup><a href=""https://ai.stackexchange.com/a/1533/8"">2015</a></sup></p>
</blockquote>

<p>If it's by design, maybe there is some better version of CNN that can perform better?</p>
"
223,"<p>An AI agent is often thought of having ""sensors"", ""a memory"", ""machine learning processors"" and ""reaction"" components. However, a machine with these does not necessarily become a self-programming AI agent. Beyond the parts mentioned above, is there any other elements or details necessary to make a machine capable of being a self-programming AI agent?</p>

<p>For example, <a href=""http://www.iiim.is/wp/wp-content/uploads/2011/05/goertzel-agisp-2011.pdf"" rel=""nofollow"">a paper from 2011</a> declared that solving the optimization problem of maximizing the intelligence is a must-have feature for the self-programming process, as quoted below:</p>

<blockquote>
  <p>A system is said to carry out an instance of self-programming when it undergoes learning regarding some element of its ""cognitive infrastructure"", where the latter is defined as the fuzzy set of ""intelligence-critical"" features of the system; and the intelligence-criticality of a system feature is defined as its ""feature quality,"" considered from the perspective of solving the optimization problem of maximizing the intelligence of a multi-feature system.</p>
</blockquote>

<p>However, this description of ""optimization of intelligence"" is vague. Can anyone give a clear definition or better summary for the necessary components for self-programming agents?</p>

<p><sub>This question is from the 2014 closed beta, with the asker having a UID of 23.</sub></p>
"
224,"<p>In a <a href=""http://www.wsj.com/articles/whats-next-for-artificial-intelligence-1465827619"">recent Wall Street Journal article</a>, Yann LeCunn makes the following statement:</p>

<blockquote>
  <p>The next step in achieving human-level ai is creating intelligent—but not autonomous—machines. The AI system in your car will get you safely home, but won’t choose another destination once you’ve gone inside. From there, we’ll add basic drives, along with emotions and moral values. If we create machines that learn as well as our brains do, it’s easy to imagine them inheriting human-like qualities—and flaws. </p>
</blockquote>

<p>Personally, I have generally taken the position that talking about emotions for artificial intelligences is silly, because there would be no <em>reason</em> to create AI's that experience emotions.  Obviously Yann disagrees.  So the question is:  what end would be served by doing this?  Does an AI <em>need</em> emotions to serve as a useful tool?  </p>
"
225,"<p>Inspired by <a href=""https://ai.stackexchange.com/q/1481/8"">this discussion</a> about recognizing human actions, I have found the <a href=""https://github.com/harishrithish7/Fall-Detection"" rel=""nofollow noreferrer"">Fall-Detection</a> project which detects humans falling on the ground from a CCTV camera feed, and which can consider alerting the hospital authorities.</p>

<p>My question is, are there any existing real-life implementations or research projects <strong>which specifically use live video feed from the surveillance cameras in order to detect crime</strong> using convnets (or similar approaches)? If so, how do they work, briefly? Do they automatically inform the police about the crime with the details what happened and where?</p>

<p>For example car accidents, physical assaults, robberies, violent disturbances, weapon attacks, etc.</p>
"
226,"<p>I'm trying to come up with the right algorithm for a system in which the user enters a few symptoms and the system has to predict or determine the likelihood that a few selected symptoms are associated with those existing in the system. Then after associating them, the result or output should be a specific disease for the symptoms.</p>

<p>The system is comprised of a series of diseases with each assigned to specific symptoms, which also exist in the system.</p>

<p>Let's assume that the user entered the following input:</p>

<pre><code>A, B, C, and D
</code></pre>

<p>The first thing the system should do is check and associate each symptom (in this case represented by alphabetical letters) individually against a data-table of symptoms that already exist. And in cases where the input doesn't exist, the system should report or send feedback about it.</p>

<p>And also, let's say that <code>A and B</code> was in the data-table, so we are 100% sure that they're valid or exist and the system is able to give out the disease based on the input. Then let's say that the input now is <code>C and D</code> where <code>C</code> doesn't exist in the data-table, but there is a possibility that <code>D</code> exists.</p>

<p>We don't give <code>D</code> a score of 100%, but maybe something lower (let's say 90%). Then <code>C</code> just doesn't exist at all in the data-table. So, <code>C</code> gets a score of 0%.</p>

<p>Therefore, the system should have some kind of association and prediction techniques or rules to output the result by judging the user's input.</p>

<p>Summary of generating the output:</p>

<pre><code>If A and B were entered and exist, then output = 100%
If D was entered and existed but C was not, then output = 90%
If all entered don't exist, then output = 0%
</code></pre>

<p>What techniques would be used to produce this system?</p>
"
227,"<p>I have gone through the <a href=""https://en.wikipedia.org/wiki/Statistical_relational_learning"">wikipedia explanation of SRL</a>. But, it only confused me more:</p>

<blockquote>
  <p>Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure.</p>
</blockquote>

<p>Can someone give a more dumbed down explanation of the same, preferably with an example?</p>
"
228,"<p>The obvious solution is to ensure that the training data is balanced - but in my particular case that is impossible. What corrections can one perform in such a scenario?</p>

<p>I know that my training data is heavily biased towards a particular class, say, and I cannot change that. Moreover, the labels are very noisy. Conditioned on this piece of information, is there anything I can do by tweaking the training process itself/ something else, to correct for the bias in the training data?</p>

<p>The data comes from an experiment (from an electron microscope), and I cannot collect more data. It's always going to be biased in this way, so alternatively-biased is also not an option. I'm sorry that I'm unable to provide any more details due to confidentiality.</p>
"
229,"<p><strong>Note:</strong> I wanted to ask a meta-post first to see if this site was supposed to be used only for AI-related questions, or if AI-related questions such as this were allowed, too, but apparently you need to have asked five actual questions first.</p>

<hr>

<p>I'm going to be entering a masters computer science program in the fall, and I wanted to move towards a concentration in computational neuroscience and linguistics for AI development applications. While I have a math and CS background, I have almost no biology/neuroscience background, and my linguistics background is limited to the random research I've done in my spare time to satiate my curiosities.</p>

<p>What are good non-math and CS related topics to study for these fields? </p>
"
230,"<p><a href=""http://www.alicebot.org/articles/wallace/eliza.html"" rel=""nofollow"">From Eliza to A.L.I.C.E.</a>:</p>

<blockquote>
  <p>Weizenbaum tells us that he was shocked by the experience of releasing ELIZA (also known as ""Doctor"") to the nontechnical staff at the MIT AI Lab. Secretaries and nontechnical administrative staff thought the machine was a ""real"" therapist, and spent hours revealing their personal problems to the program. When Weizenbaum informed his secretary that he, of course, had access to the logs of all the conversations, she reacted with outrage at this invasion of her privacy. Weizenbaum was shocked by this and similar incidents to find that such a simple program could so easily deceive a naive user into revealing personal information.</p>
</blockquote>

<p>Wikipedia's article on the <a href=""https://en.wikipedia.org/wiki/ELIZA_effect"" rel=""nofollow"">""ELIZA Effect""</a>:</p>

<blockquote>
  <p>Though designed strictly as a mechanism to support ""natural language conversation"" with a computer, ELIZA's DOCTOR script was found to be surprisingly successful in eliciting emotional responses from users who, in the course of interacting with the program, began to ascribe understanding and motivation to the program's output. As Weizenbaum later wrote, <strong>""I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.""</strong> Indeed, ELIZA's code had not been designed to evoke this reaction in the first place. Upon observation, researchers discovered users unconsciously assuming ELIZA's questions implied interest and emotional involvement in the topics discussed, <em>even when they consciously knew that ELIZA did not simulate emotion.</em></p>
</blockquote>

<p>ELIZA, despite its simplicity, was incredibly successful at its task of tricking other human beings. Even those who knew ELIZA was a bot would still talk to it. Obviously, ELIZA served as an inspiration for various other, more intelligent chatbots, such as <a href=""http://www.nytimes.com/2015/08/04/science/for-sympathetic-ear-more-chinese-turn-to-smartphone-program.html?_r=0"" rel=""nofollow"">Xiaoice</a>. But I would like to know what <em>exactly</em> led to such a simple program like ELIZA to be so successful in the first place.</p>

<p>This is very useful knowledge for a programmer since a simple program is one that would be easily maintainable.</p>
"
231,"<p>What regulations are already in place regarding Artificial General Intelligences? What reports or recommendations prepared by official government authorities were already published?</p>

<p>So far I know of <a href=""http://www.ft.com/cms/s/2/5ae9b434-8f8e-11db-9ba3-0000779e2340.html"">Sir David King's report done for UK government</a>.</p>
"
232,"<p>Most introductions to the field of MDPs and Reinforcement learning focus exclusively on domains where space and action variables are integers (and finite). This way we are introduced quickly to Value Iteration, Q-Learning, and the like.</p>

<p>However the most interesting applications (say, <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.67.3518&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">flying helicopters</a>) of RL and MDPs involve continuous state space and action spaces. I'd like to go beyond basic introductions and focus on these cases but I am not sure how to get there. </p>

<p>What areas do I need to know or study to understand these cases in depth?</p>
"
233,"<p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?</p>
"
234,"<p>By new, unseen examples; I mean like the animals in <a href=""https://en.wikipedia.org/wiki/No_Man%27s_Sky"" rel=""noreferrer"">No Man's Sky</a>. </p>

<p>A couple of images of the animals are:
<a href=""https://i.stack.imgur.com/zS0rX.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zS0rX.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Ir1Qt.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Ir1Qt.jpg"" alt=""enter image description here""></a></p>

<p>So, upon playing this game, I was curious <strong>about how good is AI at generating visual characters or examples?</strong></p>
"
235,"<p>I wanted to know what the differences between hyper-heuristics and meta-heuristics are, and what their main applications are. Which problems are suited to be solved by Hyper-heuristics?</p>
"
236,"<p>What is the difference between an agent function and an agent program (with respect to the percept sequence)?</p>

<p>In the book <em>""Artificial Intelligence: A modern approach""</em>,</p>

<blockquote>
  <p>The agent function, notionally speaking, takes as input the entire
  percept sequence up to that point, whereas the agent program takes the
  current percept only.</p>
</blockquote>

<p>Why does the agent program only take current percept. Isn't the agent program just an implementation of the agent function?</p>
"
237,"<p>Are there currently any studies to simulate gradual (or sudden) implementation of AIs in the general work force?</p>
"
238,"<p>In <a href=""https://en.wikipedia.org/wiki/Portal_2"" rel=""noreferrer"">Portal 2</a> we see that AI's can be ""killed"" by thinking about a paradox.</p>

<p><a href=""https://i.stack.imgur.com/wkUSC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wkUSC.png"" alt=""Portal Paradox Poster""></a></p>

<p>I assume this works by forcing the AI into an infinite loop which would essentially ""freeze"" the computer's consciousness.</p>

<p><strong>Questions:</strong> Would this confuse the AI technology we have today to the point of destroying it? <br> If so, why? And if not, could it be possible in the future?</p>
"
239,"<p>In the 1950's, there were widely-held beliefs that ""Artificial Intelligence"" will quickly become both self-conscious and smart-enough to win chess with humans. Various people suggested time frames of e.g. 10 years (see Olazaran's ""Official History of the Perceptron Controversy"", or let say 2001: Space Odyssey).</p>

<p>When did it become clear that devising programs that master games like chess resulted in software designs that only applied to games like the ones for which they were programmed? Who was the first person to recognize the distinction between human-like general intelligence and domain specific intelligence?</p>

<p>(thanks to Douglas Daseeco for a better way to phrase this question)</p>
"
240,"<p>We hear a lot today about how <a href=""http://deeplearning4j.org/thoughtvectors"" rel=""nofollow"">thought vectors</a> are the <a href=""http://www.extremetech.com/extreme/206521-thought-vectors-could-revolutionize-artificial-intelligence"" rel=""nofollow"">Next Big Thing in AI</a>, and how they serve as the underlying representation of thought/knowledge in ANN's.  But how can one use thought vectors in other regimes, especially including symbolic logic / GOFAI?  Could thought vectors be the ""substrate"" that binds together probabilistic approaches to AI and approaches that are rooted in logic?  </p>
"
241,"<p>A system makes a decision basing on a large number of <em>varied</em> factors, following a ""live"" decision tree - one that is (independently, through other subsystem) updated with new decisions, new situations.</p>

<p>The individual decisions can be recorded as a kind of structure:</p>

<ul>
<li>decision function</li>
<li>node to activate if decision is positive</li>
<li>node to activate if decision is negative</li>
</ul>

<p>and a node can be another decision record, or a conclusion.</p>

<p>This isn't entirely a binary tree, as many decisions may lead to the same conclusion - each node has two children, but may have many parents.</p>

<p>There is absolutely no problem storing the tree in memory - it can be database records or entries of a map, or just a list. It's perfectly sufficient for the machine.</p>

<p>The problem here is building the subsystem that expands the decision tree - and in particular, having a human operator understand the structure being built, to be able to tune, guide, fix, adjust it: <strong>debugging the AI learning process.</strong></p>

<p>The question is: how to represent that data in a human-readable way, that emphasizes the flow of the graph?</p>

<p>a non-working example of the answer is <a href=""https://en.wikipedia.org/wiki/Concept_map"" rel=""nofollow"">Concept map</a> - in this case it only goes so far; with more than thirty or so nodes, it becomes a jumbled mess, especially if the number of cross-connections (multiple parents) becomes significant. Maybe there exists some way of laying it out or slicing it to make it clearer...?</p>
"
242,"<p>I'm currently working with the CHILDES corpus trying to create a classifier that distinguishes children whom suffer from specific language impairment (SLI) from those who are typically developing (TD).</p>

<p>In my readings I noticed that there really isn't a convincing set of features to distinguish the two that have been discovered yet, so I came upon the idea of trying to create a feature learning algorithm that could potentially make better ones.  </p>

<p>Is this possible? If so how do you suggest I approach this? From the reading I have done, most feature learning is done on image processing. Another problem is the dataset I have is potentially too small to make it work (in the 100's) unless I find a way to get more transcripts from children.</p>
"
243,"<p>An AI box is a (physical) barrier preventing an AI from using too much of his environment to accomplish his final goal. For example, an AI given the task to check, say, 10<sup>50</sup> cases of a mathematical conjecture as fast as possible, might decide that it would be better to also take control over all other computers and AI to help him. </p>

<p>However, an transhuman AI might be able to talk to a human until the human lets him out of the box. In fact, <a href=""http://www.yudkowsky.net/singularity/aibox/"" rel=""noreferrer"">Eliezer Yudowsky</a> has conducted an experiment twice, where he played the AI and he twice convinced the Gatekeeper to let him out the box. However, he does not want to reveal what methods he used to get out of the box.</p>

<p><strong>Questions:</strong> Are there conducted any similiar experiments? <br> If so, is it known what methods were used to get out in those experiments?</p>
"
244,"<p>AI systems today are very capable machines, and recently the area of Natural Language Processing and Response has been exploding with innovation, as well as the fundamental algorithmic structure of AI machines.</p>

<p>I am asking if, given these recent breakthroughs, have any AI systems been developed that are able to (preferably with some measure of success) knowingly lie to humans about facts that it knows?</p>

<p>Note, what I'm asking goes beyond the canonical discussions of the Turing Test. I'm asking of machines that can 'understand' facts and then formulate a lie against this fact, perhaps using other facts to produce a believable 'cover-up' as part of the lie.</p>

<p>E.G.: CIA supercomputer is stolen by spies and they try to use the computer to do things, but the computer keeps saying it's missing dependencies though it really isn't or gives correct-looking but wrong answers knowingly. Or gives incorrect location of a person, knowing that the person frequents some place but isn't there at the moment. Doesn't have to be this sophisticated, of course.</p>
"
245,"<p><em>""An artificial or constructed language (sometimes called a conlang) is a language that has been created by a person or small group, instead of being formed naturally as part of a culture.""</em> (<a href=""https://simple.wikipedia.org/wiki/Constructed_language"" rel=""noreferrer"">Source: Simply English Wikipedia</a>)</p>

<p>My question is, could an AI make construct it's own natural language, with words, conjugations and grammar rules? Basically, a language that humans could use to speak to each other. (Preferably to communicate abstract, high-level concepts.)</p>

<p>What techniques could such an AI use? Could it be based on existing natural languages or would it have few connections to existing natural languages? Could it design a language that's easier to learn than existing languages (even <a href=""https://en.wikipedia.org/wiki/Esperanto"" rel=""noreferrer"">Esperanto</a>)?</p>
"
246,"<p>I want to start with a scenario that got me thinking about how well MCTS can perform:
Let's assume there is a move that is not yet added to the search tree. It is some layers/moves too deep. But if we play this move the game is basically won. However let's also assume that <em>all</em> moves that could be taken instead at the given game state are very very bad. For the sake of argument let's say there are 1000 possible moves and only one of them is good (but very good) and the rest is very bad. Wouldn't MCTS fail to recognize this and <em>not</em> grow the search tree towards this move and also rate this subtree very badly? 
I know that MCTS eventually converges to minimax (and eventually it will build the whole tree if there is enough memory). Then it should know that the move is good even though there are many bad possiblities. But I guess in practice this is not something that one can rely on.
Maybe someone can tell me if this is a correct evaluation on my part.</p>

<p>Apart from this special scenario I'd also like to know if there are other such scenarios where MCTS will perform badly (or extraordinary well). </p>
"
247,"<p>I'm reading such nonsense about how an AI would turn the world into a supercomputer to solve a problem that it thought it needed to solve. That wouldn't be AI. That's procedural programming stuck in some loop nonsense. An AI would need to evolve and re-organise its neurons. It wouldn't be stuck to hardcode if it becomes intelligent by re-writing its code.</p>
"
248,"<p>I'm in the process of learning as much about chatbots/CUI applications as possible and I'm trying to find more information on some of the major players in this field. By this, I mean any execs, developers, academics, designers, etc. who are doing cutting edge things. Some examples could be David Marcus (VP of messaging products at Facebook) or Adam Cheyer (VP of engineering at Viv).</p>
"
249,"<p>How big artificial neural networks can we run now (either with full train-backprop cycle or just evaluating network outputs) if our total energy budget for computation is equivalent to human brain energy budget (<a href=""http://www.scientificamerican.com/article/thinking-hard-calories/"">12.6 watts</a>)?</p>

<p>Let assume one cycle per second, which seems to roughly match the <a href=""http://www.jneurosci.org/content/31/45/16217.full"">firing rate of biological neurons</a>.</p>
"
250,"<blockquote>
  <p>Artificial Intelligence is a rather pernicious label to attach to a very mixed bunch of activities, and one could argue that the sooner we forget it the better. It would be disastrous to conclude that AI was a Bad Thing and should not be supported, and it would be disastrous to conclude that it was a Good Thing and should have privileged access to the money tap. The former would tend to penalise well-based efforts to make computers do complicated things which had not been programmed before, and the latter would be a great waste of resources. AI does not refer to anything definite enough to have a coherent policy about in this way.---<a href=""http://www.math.snu.ac.kr/~hichoi/infomath/Articles/Lighthill%20Report.pdf"" rel=""nofollow noreferrer"">Dr. R. M. Needham, in a commentary on the Lighthill Report and the Sutherland Reply, 1973</a></p>
</blockquote>

<p>43 years later...</p>

<blockquote>
  <p>There is already strong demand for engineers and scientists working on artificial intelligence in many of the fields you mention, and many more. But expertise in making real-time systems for controlling trains doesn't make you know anything about robotics. Analyzing human behavior to detect crime has virtually nothing in common with self-driving cars (beyond CS/pattern recognition building blocks). There is never going to be demand for someone with a broad sense of all these areas without any deep expertise, and there is never going to be someone with 300 PhDs who can work in all of them. TL;DR -- AI is not a branch, it's a tree. --<a href=""https://area51.meta.stackexchange.com/questions/22441/why-yet-another-trial-at-an-ai-project#comment36342_22539"">Matthew Read, in a comment on Area 51 Stackexchange, 2016</a></p>
</blockquote>

<p>AI is a label that is applied to a ""very mixed bunch of activities"". The only unifying feature between all those activities is the fact that they deal with machines in some fashion, but since there are so many ways to use a machine, the field's output may seem rather incoherent and incongruent. It does seem to make more sense for the AI field to collapse entirely, and instead be replaced by a multitude of specialized fields that don't really interact with one another. Sir James Lighthill appeared to have supported this sort of approach within his 1973 report on the state of artificial intelligence research.</p>

<p>Yet, today, this Artificial Intelligence SE exist, and we still talk of AI as a unified, coherent field of study. Why did this happen? Why did AI survive, despite its ""big tent"" nature?</p>
"
251,"<p>Let's suppose that we have a legacy system in which we don't have the source code and this system is on a mainframe written in Cobol. Is there any way using machine learning in which we can learn from the inputs and outputs the way the executables work? Doing this analysis could lead to develop some rest / soap webservice that can substitute the legacy system in my opinion. </p>
"
252,"<p>Sometimes I understand that people doing <em>cognitive science</em> try to avoid the term <em>artificial intelligence</em>. The feeling I get is that there is a need to put some distance to the <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow"">GOFAI</a>.</p>

<p>Another impression that I get is that <em>cognitive science</em> is more about trying to find out how the human <em>intelligence</em>(?)... <em>Mind</em>? works... And that it would use <em>artificial intelligence</em> to make tests or experiments, to test ideas and so forth...</p>

<p>Is Artificial Intelligence (only) a research tool for Cognitive Science?</p>

<p><strong>What is the difference between Artificial Intelligence and Cognitive Science?</strong></p>
"
253,"<p>Just for fun, I am trying to develop a neural network.</p>

<p>Now, for backpropagation I saw two techniques.</p>

<p>The first one is used <a href=""http://courses.cs.washington.edu/courses/cse599/01wi/admin/Assignments/bpn.html"" rel=""noreferrer"">here</a> and in many other places too.</p>

<p>What it does is:</p>

<ul>
<li>It computes the error for each output neuron.</li>
<li>It backpropagates it into the network (calculating an error for each inner neuron).</li>
<li>It updates the weights with the formula: <img src=""https://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D%20%3D%20k%20%5Ccdot%20E_%7Bl&plus;1%2Cn%7D%20%5Ccdot%20N_%7Bl%2Cm%7D"" alt=""""> (where <img src=""https://latex.codecogs.com/gif.latex?%5CDelta%20w_%7Bl%2Cm%2Cn%7D"" alt=""""> is the change in weight, <img src=""https://latex.codecogs.com/gif.latex?k"" alt=""""> the learning speed, <img src=""https://latex.codecogs.com/gif.latex?E_%7Bl&plus;1%2Cn%7D"" alt=""""> the error of the neuron receiving the input from the synapse and <img src=""https://latex.codecogs.com/gif.latex?N_%7Bl%2Cm%7D"" alt=""""> being the output sent on the synapse).</li>
<li>It repeats for each entry of the dataset, as many times as required.</li>
</ul>

<p>However, the neural network proposed in <a href=""https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PL77aoaxdgEVDrHoFOMKTjDdsa0p9iVtsR"" rel=""noreferrer"">this tutorial</a> (also available on GitHub) uses a different technique:</p>

<ul>
<li>It uses an error function (the other method does have an error function, but it does not use it for training).</li>
<li>It has another function which can compute the final error starting from the weights.</li>
<li>It minimizes that function (through gradient descent).</li>
</ul>

<p>Now, which method should be used?</p>

<p>I think the first one is the most used one (because I saw different examples using it), but does it work as well?</p>

<p>In particular, I don't know:</p>

<ul>
<li>Isn't it more subject to local minimums (since it doesn't use quadratic functions)?</li>
<li>Since the variation of each weight is influenced by the output value of its output neuron, don't entries of the dataset which just happen to produce higher values in the neurons (not just the output ones) influence the weights more than other entries?</li>
</ul>

<p>Now, I do prefer the first technique, because I find it simpler to implement and easier to think about.</p>

<p>Though, if it does have the problems I mentioned (which I hope it doesn't), is there any actual reason to use it over the second method?</p>
"
254,"<p>The English Language is not well-suited to talking about artificial intelligence, which makes it difficult for humans to communicate to each other about what an AI is actually ""doing"". Thus, it may make more sense to use ""human-like"" terms to describe the actions of machinery, even when the internal properties of the machinery do not resemble the internal properties of humanity.</p>

<p>Anthropomorphic language had been used a lot in technology (see the Hacker's Dictionary definition of <a href=""https://www.landley.net/history/mirror/jargon.html#Anthropomorphization"">anthropomorphization</a>, which attempts to justify computer programmers' use of anthromporhic terms when describing technology), but as AI continues to advance, it may be useful to consider the tradeoffs of using anthropomorphic language in communicating to both technical audiences and non-technical audiences. How can we get a good handle on AI if we can't even describe what we're doing?</p>

<p>Suppose I want to develop an algorithm that display a list of related articles. There are two ways by which I can explain how the algorithm works to a layman:</p>

<ol>
<li><em>Very Anthropomorphic</em> - The algorithm reads all the articles on a website, and display the articles that are very similar to the article you are looking at.</li>
<li><em>Very Technical</em> - The algorithm converts each article into a ""bag-of-words"", and then compare the ""bag-of-words"" of each article to determine what articles share the most common words. The articles that share the most words in the bags are the ones that are displayed to the user.</li>
</ol>

<p>Obviously, #2 may be more ""technically correct"" than #1. By detailing the implementation of the algorithm, it makes it easier for someone to understand how to <em>fix</em> the algorithm if it produces an output that we disagree with heavily.</p>

<p>But #1 is more readable, elegant, and easier to understand. It provides a general sense of <em>what</em> the algorithm is doing, instead of <em>how</em> the algorithm is doing it. By abstracting away the implementation details of how a computer ""reads"" the article, we can then focus on using the algorithm in real-world scenarios.</p>

<p>Should I, therefore, prefer to use the anthropomorphic language as emphasized by Statement #1? If not, why not?</p>

<p>P.S.: If the answer depends on the audience that I am speaking to (a non-technical audience might prefer #1, while a technical audience may prefer #2), then let me know that as well.</p>
"
255,"<p>Roger Schank did some interesting work on language processing with Conceptual Dependency (CD) in the 1970s. He then moved somewhat out of the field, being in Education these days. There were some useful applications in natural language generation (BABEL), story generation (TAILSPIN) and other areas, often involving planning and episodes rather than individual sentences.</p>

<p>Has anybody else continued to use CD or variants thereof? I am not aware of any other projects that do, apart from Hovy's PAULINE which uses CD as representation for the story to generate.</p>
"
256,"<p>I have been wanting to get started learning about artificial intelligence but I know almost nothing about coding or anything. So my question is, what would be the best way to get started in learning about artificial intelligence, as in should I learn some kind of coding language or is there some kind of other concept you need to know before getting started. So I'm just kind of looking for the best way to get started if you literally know nothing.</p>
"
257,"<p>I have been studying local search algorithms such as greedy hill climbing, stochastic hill climbing, simulated annealing etc. I have noticed that most of these methods take up very little memory as compared to systematic search techniques.</p>

<p>Are there local search algorithms that make use of memory to give significantly better answers than those algorithms that use little memory (such as crossing local maxima)? Also, is there a way to combine local search and systematic search algorithms to get the best of both worlds?</p>
"
258,"<p><em>I know that every program has some positive and negative points, and I know maybe .net programming languages are not the best for AI programming.</em></p>

<p><strong>But I prefer .net programming languages because of my experiences and would like to know for an AI program which one is better, C or C++ or C# and or VB ?</strong></p>

<p><em>Which one of this languages is faster and more stable when running different queries and for self learning ?</em></p>

<p>To make a summary, i think C++ is the best for AI programming in .net and also C# can be used in some projects, Python as recommended by others is not an option on my view !</p>

<p>because : </p>

<ol>
<li><p>It's not a complex language itself and for every single move you need to find a library and import it to your project (most of the library are out of date and or not working with new released Python versions) and that's why people say it is an easy language to learn and use ! (If you start to create library yourself, this language could be the hardest language in the world !)</p></li>
<li><p>You do not create a program yourself by using those library for every single option on your project (it's just like a Lego game)</p></li>
<li><p>I'm not so sure in this, but i think it's a cheap programming language because i couldn't find any good program created by this language !</p></li>
</ol>
"
259,"<p>When I visit this site, I find the word ""search"" appears quite often. </p>

<p>But why is it important? What kinds of search algorithms are used in Artificial Intelligence?  And how do they improve the result of an AI?</p>
"
260,"<p>Considering the answers of <a href=""https://ai.stackexchange.com/questions/1314/how-powerful-a-computer-is-required-to-simulate-the-human-brain"">this</a> question, emulating a human brain with the current computing capacity is currently impossible, but we aren't very far from it.</p>

<p>Note, 1 or 2 decades ago, similar calculations had similar results.</p>

<p>The clock frequency of the modern CPUs seem to be stopped, currently the miniaturization (-> mobile use), the RAM/cache improvement and the multi-core paralellization are the main lines of the development.</p>

<p>Ok, but what is the case with the analogous chips? In case of a NN, it is not a very big problem, if it is not very accurate, the NN would adapt to the minor manufacturing differences in its learning phase. And a single analogous wire can substitute a complex integer multiplication-division unit, while the whole surface of the analogous printed circuit could work parallel.</p>

<p>According to <a href=""https://engineering.stackexchange.com/questions/3993/do-analog-fpgas-exist"">this</a> post, ""software rewirable"" analogous circuits, essentially ""analogous FPGAs"" already exist. Although the capacity of the FPGAs is highly below the capacity of the <a href=""https://en.wikipedia.org/wiki/Application-specific_integrated_circuit"" rel=""nofollow noreferrer"">ASIC</a>s with the same size, maybe analogous chips for neural networks could also exist.</p>

<p>I suspect, if it is correct, maybe even the real human brain model wouldn't be too far. It would still require a massively parallel system of costly analogous NN chips, but it seems to me not impossible.</p>

<p>Could this idea work? Maybe there is even active research/development into this direction?</p>
"
261,"<p>Conceptually speaking, aren't artificial neural networks just highly distributed, lossy compression schemes?</p>

<p>They're certainly efficient at <a href=""https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Applications/imagecompression.html"" rel=""nofollow"">compressing images</a>.</p>

<p>And aren't brains (at least, the neocortex) just compartmentalized, highly distributed, lossy databases?</p>

<p>If so, what salient features in RNNs and CNNs are necessary in any given lossy compression scheme in order to extract the semantic relations that they do? Is it just a matter of having a large number of dimensions/variables? </p>

<p>Could some kind of lossy <a href=""https://en.wikipedia.org/wiki/Bloom_filter"" rel=""nofollow"">Bloom filter</a> be re-purposed for the kinds of problems ANNs are applied to?</p>
"
262,"<p>Consciousness <a href=""http://www.iep.utm.edu/consciou/"" rel=""nofollow noreferrer"">is challenging to define</a>, but for this question let's define it as ""actually experiencing sensory input as opposed to just putting a bunch of data through an inanimate machine."" Humans, of course, have minds; for normal computers, all the things they ""see"" are just more data. One could alternatively say that humans are <a href=""https://philosophy.stackexchange.com/a/4687"">sentient</a>, while traditional computers are not.</p>

<p>Setting aside the question of whether it's possible to build a sentient machine, does it actually make a difference if an AI is sentient or not? In other words, are there are tasks that are made impossible - not just more difficult - by a lack of sentience?</p>
"
263,"<p>A lot of textbooks and introductory lectures typically split AI into connectionism and GOFAI (Good Old Fashioned AI). 
From a purely technical perspective it seems that connectionism has grown into machine learning and data science, while nobody talks about GOFAI, Symbolic AI or Expert Systems at all. </p>

<p>Is anyone of note still working on GOFAI?    </p>
"
264,"<p>I recently finished Course on RL by David Silver (on YT) and thought about trying it out on simple application in Unity Game Engine, where I've built simple labyrint with ball and want to teach the ball to get from point A to point B in there while avoiding obstacles and fire (the place where you'll get burnt so big negative reward)</p>

<p>The problem I encountered while designing the whole thing (programming-wise) is: What is the correct (or at least good) way of representing the position in 2D space? It is continuous so I thought about representing it as feature vector consisting of [up, down, left, right, posX, posY] where direction is whether I am pressing button of moving in that direction in binary (or actions if you want) and pos are floats (0-1) representing normalized position from one corner on the plane where the whole map is. That would be accompanied by vector W that would represent the weights adjusted using Gradient Descent.</p>

<p>Question is: will this work?? I am asking for 2 reasons. One is that I am not so sure about that posX and posY since it can be 0 and if I multiply it by the weights vector then how could be resulting reward anything but 0? Second reason is that I am not sure if the actions should be part of the features. I mean, it makes sense to me but I could easily be very wrong since I am a beginner.</p>

<p>Thanks a lot guys in advance. If you have any more questions or think the problem is not described deeply enough just ask in the comments and I'll edit the question. :)</p>

<p>PS: I could just code it the way I think is right, but I also want to get gasp of designing applications on paper before coding them (project management).</p>
"
265,"<p>Currently I work as a java developer, But very much interested in learning Artificial Intelligence.</p>

<p>Can anybody tell me what steps i have to follow to learn artificial intelligence considering the fact i am very new to this.</p>

<p>Are there any special technologies, or anything else, I have to learn?</p>
"
266,"<p>Self-Recognition seems to be an item that designers are trying to integrate into artificial intelligence. Is there a generally recognized method of doing this in a machine, and how would one test the capacity - as in a Turing-Test?</p>
"
267,"<p>I know that deepmind used deep Q learning (<a href=""https://deepmind.com/research/dqn/"" rel=""nofollow"">DQN</a>) for its Atari game AI. It used a <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow"">conv neural network</a> (CNN) to approximate <code>Q(s,a)</code> from pixels instead of from a Q-table. I want to know how DQN converted input to an action. How many output did the CNN have? How did they train the neural network for prediction?</p>

<p>Here are the steps that I believe are happening inside DQN:</p>

<blockquote>
  <p>1) A game picture (a state) is send to CNN as input value</p>
  
  <p>2) CNN predicts an output as action (eg:left, right, shoot, etc)</p>
  
  <p>3) Simulator applies the predicted action and moves to new game state</p>
  
  <p>4) repeat step 1</p>
</blockquote>

<p>The problem with my above logic is in <strong>step 2</strong>. CNN is used for predicting an action, but when is CNN trained for prediction? </p>

<p>I would prefer if you used less math for explanation.</p>

<p>EDIT</p>

<p>I want to add some more questions regarding the same topic</p>

<p>1) How reward is passed in the neural network? that is how neural network knows whether its output action obtained positive or negative reward?</p>

<p>2) How many output the neural network has and how action is determined from those outputs?</p>
"
268,"<p>In the lecture, there was a statement:</p>

<blockquote>
  <p>""Recurrent neural networks with multiple hidden layers are just a
  special case that has some of the hidden to hidden connections
  missing.""</p>
</blockquote>

<p>I understand recurrent means that can have connections to the previous layer and the same layer as well. Is there a visualization available to easily understand the above statement?</p>
"
269,"<p>Inattentional Blindness is common in humans (see: <a href=""https://en.wikipedia.org/wiki/Inattentional_blindness"" rel=""nofollow"">https://en.wikipedia.org/wiki/Inattentional_blindness</a> ). Could this also be common with machines built with artificial vision?</p>
"
270,"<p>My question is regarding standard dense-connected feed forward neural networks with sigmoidal activation.</p>

<p>I am studying Bayesian Optimization for hyper-parameter selection for neural networks. There is no doubt that this is an effective method, but I just wan't to delve a little deeper into the maths.</p>

<p><strong>Question:</strong> Are neural networks <a href=""http://mathworld.wolfram.com/LipschitzFunction.html"" rel=""nofollow"">Lipschitz</a> functions?</p>
"
271,"<p>Can one actually kill a machine? Not only do we have problems in defining life, we also have problems in defining death. Will this also be true in artificial life and artificial intelligence?</p>
"
272,"<p>Generally, people can be classified as aggressive (Type A) or passive. Could the programming of AI systems cause aggressive or passive behavior in those AIs?</p>
"
273,"<p>Assuming mankind will eventually create artificial humans, but in doing so have we put equal effort into how humans will relate to an artificial human, and what can we expect in return? This is happening in real-time as we place AI trucks and cars on the road. Do people have the right to question, maybe in court, if an AI machine breaks a law?</p>
"
274,"<p>AI death is still unclear a concept, as it may take several forms and allow for ""coming back from the dead"". For example, an AI could be somehow forbidden to do anything (no permission to execute), because it infringed some laws.</p>

<p>""Somehow forbid"" is the topic of this question. There will probably be rules, like ""AI social laws"", that can conclude an AI should ""die"" or ""be sentenced to the absence of progress"" (a jail). Then who or what could manage that AI's state?</p>
"
275,"<p>Can self-driving cars deal with snow, heavy rain, or other weather conditions like these? Can they deal with unusual events, such as <a href=""http://beijingcream.com/wp-content/uploads/2012/06/Ducks-galore-2.jpeg"" rel=""noreferrer"">ducks on the road</a>?</p>

<p><a href=""https://i.stack.imgur.com/a0PVLm.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/a0PVLm.jpg"" alt=""ducks on the road""></a></p>
"
276,"<p>Most of the people is trying to answer question with a neural network. However, has anyone came up with some thoughts about how to make neural network ask questions, instead of answer questions? For example, if a CNN can decide which category an object belongs to, than can it ask some question to help the the classification?</p>
"
277,"<p>The Mars Exploration Rover (MER) <em><a href=""http://www.nasa.gov/mp4/618340main_mer20120124-320-jpl.mp4"" rel=""nofollow"">Opportunity</a></em> landed on Mars on January 25, 2004. The rover was originally designed for a 90 <strong>Sol mission</strong> (a Sol, one Martian day, is slightly longer than an Earth day at 24 hours and 37 minutes). Its mission has been extended several times, the machine is still trekking after 11 years on the Red Planet.</p>

<p>How it has been working for 11 years? Can anyone please explain how smart this rover is? What AI concepts are behind this?</p>
"
278,"<p>I have used OpenCV to train Haar cascades to detect face and other patterns. However I later realized that Haar tends to give a lot of false positives and I learned of Hog would give a more accurate results. But OpenCV doesn't have a good documentation of how to train hogs, I have googled a bit and found results that includes SVM and others.</p>

<p>OpenCV also has versioning problem where they move certain classes or functions somewhere else.</p>

<p>Are there any other techniques/method that I can use to train and detect objects and patterns? Preferably with proper documentation and basic tutorial/examples. Language preference: C#, Java, C++, Python</p>
"
279,"<p>Are the future robots/machines going to use Stack Exchange communities to teach themselves? Are there any ongoing projects? Just imagine a bot having a memory of all the Q&amp;A's on all of the communities! </p>
"
280,"<p>Mankind can create machines to do work. Could we also create a (passion) within the machines to do better work by using Artificial Intelligence? Would passion cause the machine to do a better job, and could we measure the quantity/quality of passion by comparing outputs of the machine - that is, those machines with passion, and those without?</p>
"
281,"<p>Can AI systems be created that could recognize itself, and recognize intelligence in other systems, and make intelligent decisions about the other systems? Mankind seems to be making progress in self-recognition but I've not seen evidence of one system recognizing other systems and being able to compare it's own intelligence with other systems. How could this be accomplished?</p>
"
282,"<p>If IQ were used as a measure of the intelligence of machines, as in humans, at this point in time what would be the IQ of our most intelligent AI systems? If not IQ, then how best to compare our intelligence to a machine, or one machine to another? </p>

<p>This question is not asking if we can measure the IQ of a machine, but if IQ is the most preferred, or general, method of measuring intelligence then how does artificial intelligence compare to our most accepted method of measuring intelligence in humans. Many people may not understand the relevance of a Turing Test as to how intelligent their new car is, or other types of intelligent machines.</p>
"
283,"<p>I was think about AIs and how they would work, when I realised that I couldn't think of a way that an AI could be taught language. A child tends to learn language through associations of language and pictures to an object (e.g: people saying the word <code>dog</code> while around a dog, and later realising that  people say <code>a dog</code> and <code>a car</code> and learn what <code>a</code> means...). However, a text based AI couldn't use this method to learn, as they wouldn't have access to any sort of input device.</p>

<p>The only way I could come up with is programming in every word, and rule, in the English language (or whatever language it is meant to 'speak' in), however that would, potentially, take years to do.</p>

<p>Does anyone have any ideas on how this could be done? Or if it has been done already, if so how?</p>

<p>Thanks in advance for any ideas.</p>

<p>Btw: in this context, I am using AI to mean an Artificial Intelligence system with near-human intelligence, and no prior knowledge of language.</p>
"
284,"<ul>
<li>Would AI be a self-propogating iteration in which the previous AI is
destroyed by a more optimised AI child?  </li>
<li>Would the AI have branches of it's own AI warning not to create the new AI?</li>
</ul>
"
285,"<blockquote>
  <p>Shortly about <a href=""https://en.wikipedia.org/wiki/Deep_learning"" rel=""noreferrer""><strong>deep learning</strong> (for reference)</a>:</p>
  
  <p><strong><em>Deep learning</strong> is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by
  using a deep graph with multiple processing layers, composed of
  multiple linear and non-linear transformations.</em></p>
  
  <p><em>Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent
  neural networks have been applied to fields like computer vision,
  automatic speech recognition, natural language processing, audio
  recognition and bioinformatics where they have been shown to produce
  state-of-the-art results on various tasks.</em></p>
</blockquote>

<hr>

<p><strong>My question:</strong></p>

<p>Can <a href=""https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_network_architectures"" rel=""noreferrer"">deep neural networks</a> or <a href=""https://en.wikipedia.org/wiki/Convolutional_neural_network"" rel=""noreferrer"">convolutional deep neural networks</a> be viewed as <a href=""https://en.wikipedia.org/wiki/Ensemble_learning"" rel=""noreferrer"">ensemble-based</a> method of machine learning? Or it is different approaches?</p>
"
286,"<p>Are Convolutional Neural Networks summarily better than pattern recognition in all existing image processing libraries that don't use CNN's? Or are there still hard outstanding problems in image processing that seem to be beyond their capability?</p>
"
287,"<p>I have been messing around in <a href=""http://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=spiral&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.73263&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false"" rel=""noreferrer"">tensorflow playground</a>. One of the input data sets is a spiral. No matter what input parameters I choose, no matter how wide and deep the neural network I make, I cannot fit the spiral. How do data scientists fit data of this shape?</p>
"
288,"<p>A ""general intelligence"" may be capable of learning a lot of different things, but possessing capability does not equal actually having it. The ""AGI"" must learn...and that learning process can take time. If you want an AGI to drive a car or play Go, you have to find some way of ""teaching"" it. Keep in mind that we have never built AGIs, so we don't know how long the training process can be, but it would be safe to assume pessimistic estimates.</p>

<p>Contrast that to a ""narrow intelligence"". The narrow AI already knows how to drive a car or play Go. It has been programmed to be very excellent at one specific task. You don't need to worry about training the machine, because it has already been pre-trained.</p>

<p>A ""general intelligence"" seems to be more flexible than a ""narrow intelligence"". You could buy an AGI and have it drive a car <em>and</em> play Go. And if you are willing to do more training, you can even teach it a new trick: <em>how to bake a cake</em>. I don't have to worry about unexpected tasks coming up, since the AGI will <em>eventually</em> figure out how to do it, given enough training time. I would have to wait a <em>long time</em> though.</p>

<p>A ""narrow intelligence"" appears to be <em>more efficient</em> at its assigned task, due to it being programmed specifically for that task. It knows exactly what to do, and doesn't have to waste time ""learning"" (unlike our AGI buddy here). Instead of buying one AGI to handle a bunch of different tasks poorly, I would rather buy a bunch of specialized narrow AIs. Narrow AI #1 drives cars, Narrow AI #2 plays Go, Narrow AI #3 bake cakes, etc. That being said, this is a very brittle approach, since if some unexpected task comes up, none of my narrow AIs would be able to handle it. I'm willing to accept that risk though.</p>

<p>Is my ""thinking"" correct? Is there a trade-off between flexibility (AGI) and efficiency (narrow AI), like what I have just described above? Or is it theoretically possible for an AGI to be both flexible and efficient?</p>
"
289,"<p>Is there a neural network(NN) system or architecture which can be used for only storing and retrieving information. For example; to store whole Avatar movie in HD format inside a neural network and retrieve(without loss) it from the neural network when needed. I searched the web and came across only LSTM RNN but in my understanding LSTM only stores pattern and not the content itself. If there is no such NN exist can you explain why it so?</p>
"
290,"<p>The question is about the architecture of Deep Residual Networks (<strong>ResNets</strong>). The model that won the 1-st places at <a href=""http://image-net.org/challenges/LSVRC/2015/results"" rel=""noreferrer"">""Large Scale Visual Recognition Challenge 2015"" (ILSVRC2015)</a> in all five main tracks:</p>

<blockquote>
  <ul>
  <li><em>ImageNet Classification: “Ultra-deep” (quote Yann) 152-layer nets</em> </li>
  <li><em>ImageNet Detection: 16% better than 2nd</em></li>
  <li><em>ImageNet Localization: 27% better than 2nd</em></li>
  <li><em>COCO Detection: 11% better than 2nd</em></li>
  <li><em>COCO Segmentation: 12% better than 2nd<br><br></em>
  <em>Source:</em> <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""noreferrer""><em>MSRA @ ILSVRC &amp; COCO 2015 competitions (presentation, 2-nd slide)</em></a></li>
  </ul>
</blockquote>

<p>This work is described in the following article:</p>

<blockquote>
  <p><a href=""http://arxiv.org/abs/1512.03385"" rel=""noreferrer""><em>Deep Residual Learning for Image Recognition (2015, PDF)</em></a></p>
</blockquote>

<hr>

<p><strong>Microsoft Research team</strong> (developers of ResNets: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun) in their article:</p>

<blockquote>
  <p><a href=""https://arxiv.org/pdf/1603.05027.pdf"" rel=""noreferrer"">""<em>Identity Mappings in Deep Residual Networks (2016)</em>""</a></p>
</blockquote>

<p>state that <strong>depth</strong> plays a key role:</p>

<blockquote>
  <p><em>""<strong>We obtain these results via a simple but essential concept — going deeper. These results demonstrate the potential of pushing the limits of depth.</strong>""</em></p>
</blockquote>

<p>It is emphasized in their <a href=""http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf"" rel=""noreferrer"">presentation</a> also (deeper - better):<br> </p>

<blockquote>
  <p><em>- ""A deeper model should not have higher training error.""<br> 
  - ""Deeper ResNets have lower training error, and also lower test error.""<br> 
  - ""Deeper ResNets have lower error.""<br>
  - ""All benefit more from deeper features – cumulative gains!""<br>
  - ""Deeper is still better.""</em></p>
</blockquote>

<p>Here is the sctructure of 34-layer residual (for reference):
<a href=""https://i.stack.imgur.com/L8m0X.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/L8m0X.png"" alt=""enter image description here""></a></p>

<hr>

<p>But recently I have found one theory that introduces a novel interpretation of residual networks showing they are exponential ensembles:</p>

<blockquote>
  <p><a href=""https://arxiv.org/abs/1605.06431"" rel=""noreferrer""><em>Residual Networks are Exponential Ensembles of Relatively Shallow Networks (2016)</em></a></p>
</blockquote>

<p>Deep Resnets are described as many shallow networks whose outputs are pooled at various depths. 
There is a picture in the article. I attach it with explanation:</p>

<blockquote>
  <p><a href=""https://i.stack.imgur.com/PGhK2.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PGhK2.jpg"" alt=""enter image description here""></a> Residual Networks are
  conventionally shown as (a), which is a natural representation of
  Equation (1). When we expand this formulation to Equation (6), we
  obtain an unraveled view of a 3-block residual network (b). From this
  view, it is apparent that residual networks have O(2^n) implicit paths
  connecting input and output and that adding a block doubles the number
  of paths.</p>
</blockquote>

<p>In conclusion of the article it is stated:</p>

<blockquote>
  <p><strong>It is not depth, but the ensemble that makes residual networks strong</strong>.
  Residual networks push the limits of network multiplicity, not network
  depth. Our proposed unraveled view and the lesion study show that
  residual networks are an implicit ensemble of exponentially many
  networks. If most of the paths that contribute gradient are very short
  compared to the overall depth of the network, <strong>increased depth</strong>
  alone <strong>can’t be the key characteristic</strong> of residual networks. We now
  believe that <strong>multiplicity</strong>, the network’s expressability in the
  terms of the number of paths, plays <strong>a key role</strong>.</p>
</blockquote>

<p>But it is only a recent theory that can be confirmed or refuted. It happens sometimes that some theories are refuted and articles are withdrawn.</p>

<hr>

<p><strong>My question:</strong><br>
Should we think of deep ResNets as ensemble after all? <strong>Ensemble</strong> or <strong>depth</strong> makes residual networks so strong? Is it possible that even the developers themselves do not quite perceive what their own model represent and what is the key concept in it?</p>
"
291,"<p>In my attempt at trying to learn neural network and machine learning I'm am trying to create a simple neural network which can be trained to recognise one word from a given string (which contains only one word). So in effect if one where to feed it a string containing the trained word but spelled wrong the network would be able to still recognise the word. Can anybody help me with some pseudo code or a start of a code. Or a general explanation of how to to this because I have read like 6 articles and 8 example projects and still have no clue how to do this</p>
"
292,"<p>In 2004 <a href=""https://en.wikipedia.org/wiki/Jeff_Hawkins"">Jeff Hawkins</a>, inventor of the palm pilot, published a very interesting book called <a href=""https://en.wikipedia.org/wiki/On_Intelligence"">On Intelligence</a>, in which he details a theory how the human neocortex works. </p>

<p>This theory is called <a href=""https://en.wikipedia.org/wiki/Memory-prediction_framework"">Memory-Prediction framework</a> and it has some striking features, for example not only bottom-up (feedforward), but also top-down information processing and the ability to make simultaneous, but discrete predictions of different future scenarios (as described <a href=""http://journal.frontiersin.org/article/10.3389/fncir.2016.00023/full"">in this paper</a>).</p>

<p>The promise of the Memory-Prediction framework is unsupervised generation of stable high level representations of future possibilities. Something which would revolutionise probably a whole bunch of AI research areas.</p>

<p>Hawkins founded <a href=""https://en.wikipedia.org/wiki/Numenta"">a company</a> and proceeded to implement his ideas. Unfortunately more than ten years later the promise of his ideas is still unfulfilled. So far the implementation is only used for anomaly detection, which is kind of the opposite of what you really want to do. Instead of extracting the understanding, you'll extract the instances which the your artificial cortex doesn't understand. </p>

<p>My question is in what way Hawkins's framework falls short. What are the concrete or conceptual problems that so far prevent his theory from working in practice? </p>
"
293,"<p>As far as I can tell, neural networks have a <strong>fixed number of neurons</strong> in the input layer.</p>

<p>If neural networks are used in a context like for example NLP, sentences or blocks of text of varying sizes are fed to a network. How is the <strong>varying input size</strong> reconciled with the <strong>fixed size</strong> of the input layer of the network? In other words: how is such a network made flexible enough to deal with an input that might be anywhere from one word to multiple pages of text?</p>

<p>If my assumption of a fixed number of input neurons is wrong and new input neurons are added to/removed from the network to match the input size I don't see how these can ever be trained.</p>

<p>I give the example of NLP, but lots of problems have an inherently unpredictable input size, I'm interested in the general approach for dealing with this.</p>

<p>edit: For images, it's clear you can up/downsample to a fixed size, but for text this seems to be an impossible approach since adding/removing text changes the meaning of the original input.</p>
"
294,"<p>In my estimation we have two minds which manage to speak to each other in dialectic through a series of interrupts. Thus at any one time one of these systems is controlling master and inhabits our consciousness. The subordinate system controls context which is constantly being ""primed"" by our senses and our subordinate systems experience of our conscious thought process( see thinking fast and slow by Daniel Kahneman). Thus our thought process is constantly a driven one. Similarly this system works as a node in a community and not as a standalone thing.<br>
 I think what we have currently is ""artificial thinking"" which is abstracted a long way from what is described above. so my question is ""are there any artificial intelligence systems with an internal dialectical approach and with drivers and conceived above and which develop within a community of nodes? "" </p>
"
295,"<p>In the recent PC game <em><a href=""http://www.theturingtestgame.com/"">The Turing Test</a></em>, the AI (""TOM"") needs help from Ava to get through some puzzle rooms. TOM says he is unable to solve the puzzles because he is not allowed to ""<a href=""https://en.wikipedia.org/wiki/Lateral_thinking"">think laterally</a>."" Specifically, he says he would not have thought to throw a box through a window to solve the first room. His creators, the story goes, turned that capability off because such thinking could produce ""ethically suboptimal"" solutions, like chopping off an arm to leave on a pressure plate.</p>

<p>Would all creative puzzle-solving abilities need to be removed from an AI to keep its results reasonable, or could we get some benefits of lateral thinking without losing an arm?</p>
"
296,"<p>In the recent <a href=""https://live.newscientist.com/"" rel=""nofollow"">festival of science</a>, there was a talk given by researcher <a href=""https://live.newscientist.com/mike-cook/"" rel=""nofollow"">Mike Cook</a> about:</p>

<blockquote>
  <p><a href=""http://www.gamesbyangelina.org/"" rel=""nofollow"">ANGELINA</a>, an AI game designer that has invented game mechanics, made games about news stories, and was the first AI to enter a game jam.</p>
</blockquote>

<p>So the aim of Angelina AI is basically to design videogames.</p>

<p>Briefly, how exactly does Angelina design the new games? How does it work behind the scenes?</p>
"
297,"<p>I understand that neural networks model biological neurons.  Each node in the network represents a neuron cell and the connections between nodes represent the connections between cells.  As in nature, a neuron fires an electrical signal to connected neurons based on some kind of threshold or function that mimics such.  </p>

<p>Recent discoveries on how the brain works reveal the importance of calcium within the cells.  See <a href=""http://link.springer.com/article/10.1007/BF01794675"" rel=""noreferrer"">http://link.springer.com/article/10.1007/BF01794675</a> for more information.  To summarize, calcium affects the regulation, stimulation and transmission of electrical activity as well as the destruction of neurones.</p>

<p>From my study of neural networks, there does not seem to be a calcium equivalent.  Having one would imply that the functions, connections and weights in an artificial network are configured during the training and execution process and can change over time.   I understand that back-propagation is used to train the weights, but have not seen anything that trains the function nor the connections (although a zero weight could imply no connection).</p>

<p>Does anyone know of such a network (or training algorithm)?  If so, do these networks perform better than a network that is pre-configured?</p>
"
298,"<p>I'm a freshman to machine learning. We all know that there are 2 kinds of problems in our life: problems that humans can solve and problems we can't solve. For problems humans can solve, we always try our best to write some algorithm and tell machine to follow it step by step, and finally the machine acts like people.</p>

<p>What I'm curious about are these problems humans can't solve. If humans ourselves can't sum up and get an algorithm (which means that we ourselves don't know how to solve the problem), can a machine solve the problem? That is, can the machine sum up and get an algorithm by itself based on a large amount of problem data?</p>
"
299,"<p>This is a question about a nomenclature - we already have the algorithm/solution, but we're not sure whether it qualifies as utilizing heuristics or not.</p>

<hr>

<p>feel free to skip the problem explanation:</p>

<blockquote>
  <p>A friend is writing a path-finding algorithm - an autopilot for an
  (off-road) vehicle in a computer game. This is a pretty classic
  problem - he finds a viable, not necessarily optimal but ""good enough""
  route using the A* algorithm, by taking the terrain layout and vehicle
  capabilities into account, and modifying a direct (straight) line path
  to account for these. The whole map is known a'priori and invariant,
  though the start and destination are arbitrary (user-chosen) and the
  path is not guaranteed to exist at all.</p>
  
  <p>This cookie-cutter approach comes with a twist: limited storage space.
  We can afford some more volatile memory on start, but we should free
  most of it once the route has been found. The travel may take days -
  of real time too, so the path must be saved to disk, and the space in
  the save file for custom data like this is severely limited. Too
  limited to save all the waypoints - even after culling trivial
  solution waypoints ('continue straight ahead'), and by a rather large
  margin, order of 20% the size of our data set.</p>
  
  <p>A solution we came up with is to calculate the route once on start,
  then 'forget' all the trivial and 90% of the non-trivial waypoints.
  This both serves as a proof that a solution exists, and provides a set
  of points reaching which, in sequence, guarantees the route will take
  us to the destination.</p>
  
  <p>Once the vehicle reaches a waypoint, the route to the next one is
  calculated again, from scratch. It's known to exist and be correct
  (because we did it once, and it was correct), it doesn't put too much
  strain on the CPU and the memory (it's only about 10% the total route
  length) and it doesn't need to go into permanent storage (restarting
  from any point along the path is just a subset of the solution
  connecting two saved waypoints).</p>
</blockquote>

<hr>

<p>Now for the actual question:</p>

<p>The pathfinding algorithm follows a sparse set of waypoints which by themselves are not nearly sufficient as a route, but allow for easy, efficient  calculation of the actual route, simultaneously guarantying its existence; they are a subset of the full solution. </p>

<p>Is this a heuristic approach?</p>

<p>(as I understand, normally, heuristics don't guarantee existence of a solution, and merely suggest more likely candidates. In this case, the 'hints' are taken straight out of an actual working solution, thus my doubts.)</p>
"
300,"<p>I understand how a neural network can be trained to recognise certain features in an image (faces, cars, ...), where the inputs are the image's pixels, and the output is a set of boolean values indicating which objects were recognised in the image and which weren't.</p>

<p>What I don't really get is, when using this approach to detect features and we detect a face for example, how we can go back to the original image and determine the location or boundaries of the detected face. How is this achieved? Can this be achieved based on the recognition algorithm, or is a separate algorithm used to locate the face? That seems unlikely since to find the face again, it needs to be recognised in the image, which was the reason of using a NN in the first place.</p>
"
301,"<p>If said AI can assess scenarios and decide what AI is best suited and construct new AI for new tasks. In sufficient time would the AI not have developed a suite of AIs powerful/specialized for their tasks, but versatile as a whole, much like our own brain’s architecture? What’s the constraint ?</p>
"
302,"<p>AI is progressing drastically, and imagine they tell you you're fired because a robot will take your place. What are some jobs that can never be automated?</p>
"
303,"<p>I want to have a program that writes like a human. But I don't just want a font, but instead an 'intelligent' program that produce different result and that can be trained with different sets to generate different handwritings.
As a training set I would like to have parts of a handwritten text (saved as a list of paths (like in vector graphics).
Maybe as a means to simplify things, I could flatten the paths in to consecutive straight lines. My program receives a string of text and produces a list of paths (or a vector graphic, whatever is easier to work with)</p>

<p>My question now is: What kind of machine learning would be best to achieve this?</p>
"
304,"<p>I'm wondering how feasible it is to create a machine that can separate clothing from a basket.</p>

<p>At the most basic level it would distinguish between tops, pants, button downs and socks</p>

<p>Programmatically, I'd image this would require training a neural network to recognize these items, but in real time it becomes exponentially difficult to do this in a small space at a fast rate:</p>

<ol>
<li>pick up an item</li>
<li>lay it in such a way that is recognizable </li>
<li>deduce whether it is a top, button down, etc.</li>
<li>sort it accordingly</li>
</ol>

<p>If this sounds ridiculous please let me know...</p>

<p>If it is possible :</p>

<p>would this be based on some sort of computer vision?
or only a well trained neural network?</p>

<p>Any insight is much appreciated!</p>
"
305,"<p>For years I have been dealing with (and teaching) Knowledge Representation and Knowledge Representation languages. I just discovered that in another community (Information Systems and the such) there is something called the ""DIKW pyramid"" where they add another step after knowledge, namely wisdom.
They define data as being simply symbols, information as being the answer to who/what/when/where?, knowledge as being the answer to how?, and wisdom as being the answer to why?. </p>

<p>My question is: has anyone done the connection between what AI calls data/information/knowledge and these notions from Information Systems? In particular, how would ""wisdom"" be defined in AI? And since we have KR languages, how would we represent ""wisdom"" as they define it?</p>

<p>Any references would be welcome…</p>
"
306,"<p><a href=""https://en.wikipedia.org/wiki/AI_effect"" rel=""noreferrer"">According to Wikipedia</a>...</p>

<blockquote>
  <p>The AI effect occurs when onlookers discount the behavior of an artificial intelligence program by arguing that it is not real intelligence.</p>
  
  <p>Pamela McCorduck writes: ""It's part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something—play good checkers, solve simple but relatively informal problems—there was chorus of critics to say, 'that's not thinking'.""[1] AI researcher Rodney Brooks complains ""Every time we figure out a piece of it, it stops being magical; we say, 'Oh, that's just a computation.'""[2]</p>
</blockquote>

<p>The Wikipedia page then proposes several different reasons that could explain why onlookers might ""discount"" AI programs. However, those reasons seem to imply that the humans are making a mistake in ""discounting"" the behavior of AI programs...and that these AI programs might actually be  intelligent. I want to make an alternate argument, where the humans are making a mistake, but not in ""discounting"" the behavior of AI programs.</p>

<p>Consider the following situation. I want to build a machine that can do X (where X is some trait, like intelligence). I am able to evaluate intuitively whether a machine has that X criteria. But I don't have a good definition of what X actually <em>is</em>. All I can do is identify whether something has X or not.</p>

<p>However, I think that people who has X can do Y. So if I build a machine that can do Y, then surely, I built a machine that has X.</p>

<p>After building the machine that can do Y, I examine it to see if my machine has X. And it does not. So my machine lacks X. And while a machine that can do Y is cool, what I really want is a machine that has X. I go back to the drawing board and think of a new idea to reach X.</p>

<p>After writing on the whiteboard for a couple of hours, I realize that people who has X can do Z. Of course! I try to build a new machine that can do Z, yes, if it can do Z, then it must have X.</p>

<p>After building the machine that can do Z, I check to see if it has X. It does not. And so I return back to the drawing board, and the cycle repeats and repeats...</p>

<p>Essentially, humans are attempting to determine whether an entity has intelligence via proxy measurements, but those proxy measurements are potentially faulty (as it is possible to meet those proxy measurements without ever actually having intelligence). Until we know how to define intelligence and design a test that can accurately measure it, it is very unlikely for us to build a machine that has intelligence. So the AI Effect occurs because humans don't know how to define ""intelligence"", not due to people dismissing programs as not being ""intelligent"".</p>

<p>Is this argument valid or correct? And if not, why not?</p>
"
307,"<p>Here is one of the most serious questions, about the artificial intelligence.<br>
How will the machine know the difference between right and wrong, what is good and bad, what is respect, dignity, faith and empathy.<br>
<br> A machine can recognize what is correct and incorrect, what is right and what is wrong, depend on how it is originally designed.<br>
<br>It will follow the ethics of its creator, the man who originally designed it<br>
 But how to teach a computer something we don't have the right answer.<br>
 People are selfish, jealous, self confident. We are not able to understand each other sorrows, pains beliefs. We don't understand different religions, different traditions or beliefs. 
<br>Creating an AI might be breakthrough for one nation, or one race, or one ethnic or religious group, but it can be against others.   </p>

<p>Who will learn the machine a humanity?   :)</p>
"
308,"<p>Can someone suggest step by step approach to learn AI rather than study a stack of book for long time.[ I'm not denying that books are great helper but what after that ]</p>

<p>Thanks in Advance.</p>
"
309,"<p><a href=""https://ai.stackexchange.com/questions/2067/will-ai-be-able-to-adapt"">From this SE question</a>:</p>

<blockquote>
  <p>Will be AI able to adapt, to different environments and changes.</p>
</blockquote>

<p>This is my attempt at interpreting that question.</p>

<p>Evolutionary algorithms are useful for solving optimization problems...by measuring the ""fitness"" of various probable solutions and then  of an algorithm through the process of natural selection.</p>

<p>Suppose, the ""fitness calculation""/""environment"" is changed in mid-training (as could easily happen in real-life scenarios where people may desire different solutions at different times). Would evolutionary algorithms be able to respond effectively to this change?</p>
"
310,"<p>So I'm here to propose a strategy or to ask if this strategy has been tested in genetic algorithms in the past. I didn't exactly know how to find discussion about it.</p>

<p>In a classic example of genetic algorithm you would have a population and certain amount of simulation time to evaluate it and breeding. Then proceed to the next generation.</p>

<p>What if we would isolate a small part of the population in the simulation process and keep them evolving in their own little island for some time while rest of the population continues to evolve normally? After that they could be re-united with the rest of the population and the end of the simulation would go trough. After that breed the population and continue. </p>

<p>This is super important part in natural evolution and probably some know if it actually works with genetic programming?</p>
"
311,"<p>Obviously this is hypothetical, but is true? I know ""perfect fitness function"" is a bit hand-wavy, but I mean it as we have a perfect way to measure the completion of any problem.</p>
"
312,"<p>I'm curious about Artificial Intelligence. In my everyday job I develop standard applications, like websites with basic functionalities like user subscription, file upload, forms saved in a database... </p>

<p>I mainly know of AI being used in games or robotics fields. But can it be useful in ""standard"" application development?</p>
"
313,"<p>I've heard of AI that can solve math problems. Is it possible to create a 'logic system' equivalent to humans that can solve mathematics in the so called 'beautiful' manner?  Can AI find beauty in mathematics and solve problems other than using brute force? Can you please provide with examples where work on this is being done? </p>
"
314,"<p>I'm trying to gain some intuition beyond definitions, in any possible dimension. I'd appreciate references to read.</p>
"
315,"<p>It seems that deep neural networks are making improvements largely because as we add nodes and connections, they are able to put together more and more abstract concepts. We know that, starting from pixels, they start to recognize high level objects like cat faces, chairs, and written words. Has a network ever been shown to have learned a more abstract concept that a physical object? What is the ""highest level of abstraction"" that we've observed?</p>
"
316,"<p>I'm a bit confused about the definition of life. Can AI systems be called 'living'? Because they can do most of the things that we can. They can even communicate with one another. </p>

<p>They are not formed of what we call cells. But, you see, cells are just a collection of several chemical processes which is in turn non-living just like AI is formed of several lines of code.</p>
"
317,"<p>I'm interested mostly in the application of AI in gaming; in case this adjusts the way you answer, but general answers are more than welcome as well.</p>

<p>I was reading up on Neural Networks and combining them with Genetic Algorithms; my high-level understanding is that the Neural Networks are used to produce a result from the inputs, and the Genetic Algorithm is employed to constantly adjust the weights in the Neural Network until a good answer is found.</p>

<p>The concept of a Genetic Algorithm randomly mutating the weights on the inputs to a Neural Network makes sense to me; but I don't understand where this would be applied in respect to gaming.</p>

<p>For example, if I had some simple enemy AI that I want to have adapt to the players play-style, is this a good opportunity to implement the AI as a Genetic-Algorithm combined with a Neural Network?</p>

<p>With these different suitable applications, how does one go about deciding how to encode the problem in such a way that it can be mutated by the Genetic Algorithm and serve as suitable on/off inputs to a Neural Network (actually, are Neural Networks always designed as on off signals?)?</p>
"
318,"<p>I have seen an AI create a game it self, AI act as a lawyer, call center etc.</p>

<p>There are many problems (Example for mobile development)</p>

<pre><code>1. New api/technology or even new language every year.
2. New design
3. New hardware
4. Good code architecture, design pattern
5. Security
6. Image/Animation optimization
7. Automate testing
</code></pre>

<p>etc.</p>

<p>I wonder that AI can help developer solve that problems.</p>

<p>1.1 May be I want to get the location then AI suggest the best api for specific platform.</p>

<p>1.2 AI help to refactoring and optimizing the code</p>

<ol start=""2"">
<li><p>Help on design e.g. golden ratio, Material theme color</p></li>
<li><p>Suggest or determine the limit of the hardware e.g. screen size, ram</p></li>
<li><p>Can convert to another design pattern </p></li>
<li><p>Help to waring the latest vulnerable and automate pentest etc.</p></li>
<li><p>Help to optimize image by learning how much can we reduce the image size while people still ok with it.</p></li>
<li><p>Generate automate-testing</p></li>
</ol>

<p>Is there any solution existed?</p>

<p>If not, what can we do?</p>
"
319,"<p>There are AI creating game, content and more.</p>

<p>I'm thinking on how can AI develop mobile app itself?</p>

<p>The computer languages might easy for AI to learn.</p>

<p>AI can learn a lot from good open source project in github.</p>

<p>The trend prediction can help AI to select the topic for creating a great apps.</p>

<p>There are lots of details to let AI create a great apps. </p>
"
320,"<p>New to the topic, I think I have figured out how to implement a Multi Level Perceptron(MLP) ANN.</p>

<p>And was wondering if there are any simple data sets to test a MLP ANN ?
i.e. small number of inputs and outputs</p>

<p>I'm not getting expected results from uci cancer, I was hoping someone could save me some time and point me to some data they have used before ?</p>

<p>Maybe start slightly more complex than XOR ?</p>
"
321,"<p>The concept is intrinsically related with building some sort of media for the AI to exists. We may think of a digital computer, programmed to use language and act in a way that we cannot be distinguished from a human. But, does the media really mater (unconventional computation paradigms)? Does having a certain control over the limits of what the AI can do matter? Synthetic biology has the ultimate goal of building biological systems from scratch , would a synthetic brain, potentially introduced in a synthetic human, constitute AI?</p>

<p>I am just looking for a clear definition of what most people have in mind when they refer to AI.</p>
"
322,"<p>How are autonomous cars related to artificial intelligence? I would presume that artificial intelligence is when we are able to copy the human state of mind and perform tasks in the same way. But isn't autonomous car just rule-based machines that operates due to its environment? They are not self-aware, and they cannot choose a good way to act in a never before experienced situation.</p>

<p>I know that many people often mention autonomous cars when speaking about AI, but I am not really convinced that these are related. Either I have a too strict understanding of what AI is or </p>
"
323,"<p>What are the advantages of having self-driving cars?</p>

<p>We will be able to have more cars in the traffic at the same time, but won't it also make more people choose to use the cars, so both the traffic and the public health will actually become worse?</p>

<p>Are we really interested in this?</p>
"
324,"<p>In lots of sci-fi, it seems that AI becomes sentient (Terminator, Peter F Hamilton's SI (commonwealth saga), etc.)</p>

<p>However, I'm interested in whether this is actually plausible, whether an AI could actually break free form being controlled by us, and if that is possible, whether there is any research as to about what sort of complexity / processing power an AI would need to be able to do this.</p>
"
325,"<p>Deepmind just published a <a href=""http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html"" rel=""noreferrer"">paper</a> about a <a href=""https://deepmind.com/blog/differentiable-neural-computers/"" rel=""noreferrer"">""differentiable neural computer""</a>, which basically <em>combines a neural network with a memory</em>. </p>

<p>The idea is to teach the neural network to create and recall useful explicit memories for a certain task. This complements the abilities of a neural network well, because NNs only store knowledge implicitly in the weights and the information used to work on a single task is only stored in the activation of the network and degrades quickly the more information you add. (<a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""noreferrer"">LSTMs</a> are one try to slow down this degradation of short term memories, but it still happens.)</p>

<p>Now, instead of keeping the necessary information in the activation, they presumably keep the addresses of memory slots for specific information in the activation, so these should also be subject to degradation. My question is why this approach should scale. Shouldn't a somewhat higher number of task specific information once again overwhelm the networks capability of keeping the addresses of all the appropriate memory slots in its activation?</p>
"
326,"<p>What could be an algorithm that determines whether an AI ( algorithm ) is 
AI Complete or not ?
How does one proceed to program it ?</p>

<p>edit : question edited due to some misinterpretation in the first answer !</p>
"
327,"<p><a href=""https://www.national.co.uk/tech-powers-google-car/"" rel=""nofollow"">This slideshow</a> documents some of the technologies used in Google's self-driving car.</p>

<p>It mentions radar.</p>

<p>Why does Google use radar? Doesn't LIDAR do everything radar can do? In particular, are there technical advantages with radar regarding object detection and tracking?</p>

<p>To clarify the relationship with AI: how do radar sensors contribute to self-driving algorithms in ways that LIDAR sensors do not?</p>

<p>The premise is AI algorithms are influenced by inputs, which are governed by sensors. For instance, if self-driving cars relied solely on cameras, this constraint would alter their AI algorithms and performance.</p>
"
328,"<p>Sometimes, but not always in the commercialization of technology, there are some low hanging fruits or early applications, I am having trouble coming up with examples of such applications as they would apply to a conscious AI.</p>

<p>As per conscious I would propose an expanded strict definition: the state of being awake and aware of one's surroundings along with the capability of being self aware.</p>

<p>Thanks. </p>
"
329,"<p>So machine learning allows a system to be self-automated in the sense that it can predict the future state based on what it has learned so far. My question is: Are machine learning techniques the only way of making a system develop its domain knowledge?</p>
"
330,"<p>In The Age of Spiritual Machines (1999), Ray Kurzweil predicted that in 2009, a \$1000 computing device would be able to perform a trillion operations per second. Additionally, he claimed that in 2019, a \$1000 computing device would be approximately equal to the computational ability of the human brain (due to Moore's Law and exponential growth.)</p>

<p>Did Kurzweil's first prediction come true? Are we on pace for his second prediction to come true? If not, how many years off are we?</p>
"
331,"<p>I am creating a snake game in Unity and I would like to implement AI snakes that wander around the globe while avoiding collision with the other snakes on the globe, and if possible I would also like to make the AI snakes purposefully trap other snakes so that the other snakes would collide and die. </p>

<p><a href=""https://i.stack.imgur.com/aQ61J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aQ61J.png"" alt=""enter image description here""></a> </p>

<p>The AI snakes must meet the following requirements:  </p>

<ul>
<li>They must move in a certain way. A snake is controlled by a user using the arrow keys on a keyboard, therefor I would also like the AI snakes to move using this form of input.</li>
<li>The AI snakes must move on a sphere</li>
</ul>

<p>As I know, creating Artificial Intelligence is not an easy task and I would like to know if there are some open source projects that I can use for accomplishing this task.</p>
"
332,"<p>According to <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""noreferrer"">Wikipedia</a>:</p>

<blockquote>
  <p>AI is intelligence exhibited by machines.</p>
</blockquote>

<p>I have been wondering if with the recent biological advancements, is there already a non-electrical-based ""machine"" that is programmed by humans in order to be able to behave like a:</p>

<blockquote>
  <p><strong>flexible rational agent</strong> that perceives its environment and takes actions that maximize its chance of success at some goal</p>
</blockquote>

<p>I was specifically thinking of viruses and bacteria. Have these been programmed by humans in order to behave as a flexible rational agent (i.e. an AI entity)?</p>

<p>Are there are other organisms that have already been used for this purpose?</p>
"
333,"<p>DeepMind state that their deep Q-network (DQN) was able to continually adapt its behavior while learning to play 49 Atari games.  </p>

<p>After learning all games with the same neural net, was the agent able to play them all at 'superhuman' levels simultaneously (whenever it was randomly presented with one of the games) or could it only be good at one game at a time because switching required a re-learn?</p>
"
334,"<p>I read a lot about the structure of the human brain and artificial neural networks. I wonder if it is possible to build an artificial intelligence with neural networks that would be divided into centers such as the brain is, e.g. centers responsible for feelings, abstract thinking, speech, memory, etc.?</p>
"
335,"<p>What are the best <a href=""https://en.wikipedia.org/wiki/Turing_completeness"" rel=""nofollow"">Turing complete</a> programming languages which can be used for developing self-learning/improving <a href=""https://en.wikipedia.org/wiki/Evolutionary_algorithm"" rel=""nofollow"">evolutionary algorithm</a> based AI programs with <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow"">generic algorithms</a>?</p>

<p>'Best' should be based on pros and cons of performance and easiness for machine learning.</p>
"
336,"<p>I have a question. Will we be able to build a neural network that thinks abstractly, has the creativity, feels and is conscious?</p>
"
337,"<p>If I have a set of sensory nodes taking in information and a set of ""action nodes"" which determine the behavior of my robot, why do I need hidden nodes between them when I can let all sensory nodes affect all action nodes?</p>

<p>(This is in the context of evolving neural network)</p>
"
338,"<p>If neurons and synapses can be implemented using transistors, what prevents us from creating arbitrarily large neural networks using the same methods with which GPUs are made?</p>

<p>In essence, we have seen how extraordinarily well virtual neural networks implemented on sequential processors work (even GPUs are sequential machines, but with huge amounts of cores). </p>

<p>One can imagine that using GPU design principles - which is basically to have thousands of programmable processing units that work in parallel - we could make much simpler ""neuron processing units"" and put millions or billions of those NPUs in a single big chip. They would have their own memory (for storing weights) and be connected to a few hundred other neurons by sharing a bus. They could have a frequency of for example 20 Hz, which would allow them to share a data bus with many other neurons.</p>

<p>Obviously, there are some electrical engineering challenges here, but it seems to me that all big tech companies should be exploring this route by now.</p>

<p>Many AI researchers say that super intelligence is coming around the year 2045. I believe that their reasoning is based on moores law and the number of neurons we are able to implement in software running on the fastest computers we have.</p>

<p>But the fact is, we today are making silicon chips with billions of transistors on them. SPARK M7 has 10 billion transistors.</p>

<p>If implementing a (non-programmable) neuron and a few hundred synapses for it requires for example 100 000 transistors, then we can make a neural network in hardware that emulates 100 000 neurons.</p>

<p>If we design such a chip so that we can simply make it physically bigger if we want more neurons, then it seems to me that arbitrarily large neural networks is simply a budget question.</p>

<p>Are we technically able to make, in hardware, arbitrarily large neural networks with current technology?</p>

<p>Remember: I am NOT asking if such a network will in fact be very intelligent. I am merely asking if we can factually make arbitrarily large, highly interconnected neural networks, if we decide to pay Intel to do this? </p>

<p>The implication is that on the day some scientist is able to create general intelligence in software, we can use our hardware capabilities to grow this general intelligence to human levels and beyond.</p>
"
339,"<p>I am reading about Generative Adversarial Networks (GANs) and I have some doubts regarding it. So far, I understand that in a GAN there are two different types of neural network: one is generative (G) and the other discriminative (D). The generative neural network generates some data which the discriminative neural network judges for correctness. The GAN learns by passing the loss function to both networks.</p>

<p>How do the discriminative (D) neural nets initially know whether the data produced by G is correct or not? Do we have to train the D first then add it into the GAN with G?</p>

<p>Let's consider my trained D net, which can classify a picture with 90% percentage accuracy. If we add this D net to a GAN there is a 10% probability it will classify a image wrong. If we train a GAN with this D net then will it also have the same 10% error in classifying an image? If yes, then why do GANs show promising results?</p>
"
340,"<p>Now AI can replace call center, worker(in the factory) and going to replace court. When will the AI can replace developer or tester?</p>

<p>I want to know how long can AI replace developer. e.g. next 10 years because...</p>
"
341,"<p>Ok, I now know how a machine can learn to play to play Atari games (Breakout): <a href=""https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"" rel=""nofollow noreferrer"">Playing Atari with Reinforcement Learning</a></p>

<p>With the same technique it is even possible to play FPS games (Doom): <a href=""https://arxiv.org/pdf/1609.05521"" rel=""nofollow noreferrer"">Playing FPS Games with Reinforcement Learning</a></p>

<p>Further studies even investigated multiagent scenarios (Pong): <a href=""https://arxiv.org/pdf/1511.08779.pdf"" rel=""nofollow noreferrer"">Multiagent Cooperation and Competition with Deep Reinforcement Learning</a></p>

<p>And even another awesome article for the interested user in context of deep reinforcement learning (easy and a must read for beginners): <a href=""http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/"" rel=""nofollow noreferrer"">Demystifying Deep Reinforcement Learning</a></p>

<p>I was thrilled by these results and immediately wanted to try them in some simple ""board/card game scenarios"", i.e. writing AI for some simple games in order to learn more about ""deep learning"". Of course, thinking that I can apply the techniques above easily in my scenarios was stupid. All examples above are based on convolutional nets (image recognition) and some other assumptions, which might not be applicable in my scenarios.</p>

<p>Can you give me hints or futher articles, which deal with my questions below? As a beginner, I do not have an overview, yet. Preferably, your suggestions should also be connected to the following areas already: deep learning, reinforcement learning (, multiagent systems)</p>

<hr>

<p>(1)</p>

<p>If you have a card game and the AI shall play a card from its hand, you could think about the cards (amongst other stuff) as the current game state. You can easily define some sort of neural net and feed it with the card data. In a trivial case the cards are just numbered. I do not know the net type, which would be suitable, but I guess deep reinforcment learning strategies could be applied easily then.</p>

<p>However, I can only imagine this, if there is a constant number of hand cards. In the examples above, the number of pixels is also constant, for example. What if a player can have a different numbers of cards? What to do, if a player can have an infinite number of cards? Of course, this is just a theoretical question as no game has an infinite number of cards.</p>

<hr>

<p>(2)</p>

<p>In the initial examples, the action space is constant. What can you do, if the action space is not? This more or less follows from my previous problem. If you have 3 cards, you can play card 1, 2 or 3. If you have 5 cards, you can play card 1, 2, 3, 4 or 5, etc. It is also common in card games, that it is not allowed to play a card. Could this be tackled with negative reward?</p>

<hr>

<p>So, which ""tricks"" can be used, e.g. always assume a constant number of cards with ""filling values"", which is only applicable in the non-infinite case (anyways unrealistic and even humans could not play well with that)?
Are there articles, which examine such things already?</p>
"
342,"<p>The “Discounted sum of future rewards” using
discount factor γ” is</p>

<pre><code>γ (reward in 1 time step) +
γ ^ 2 (reward in 2 time steps) +
γ ^ 3 (reward in 3 time steps) + ...
</code></pre>

<p>I am confused as what constitutes a time-step. Say I take a action now, so I will get a reward in 1 time-step. Then, I will take an action again in timestep 2 to get a second reward in time-step 3
But the equation says something else. How does one define a time-step? Can we take action as well receive a reward in a single step? Examples are most helpful.</p>
"
343,"<p>Decades ago there were and are books in machine vision, which by implementing various information processing rules from gestalt psychology, got impressive results with little code or special hardware in image identification and visual processing.</p>

<ul>
<li>Are such methods being used or worked on today? Was any progress made on this? Or was this research program dropped? By today, I mean 2016, not 1995 or 2005.</li>
</ul>
"
344,"<p>There is this claim around that the brain's cognitive capabilities are tightly linked to the way it processes sensorimotor information and that, in this or a similar sense, our intelligence is ""embodied"". Lets assume, for the sake of argument, that this claim is correct (you may think the claim is too vague to even qualify for being correct, that it's ""not even false"". If so, I would love to hear your ways of fleshing out the claim in such a way that it's specific enough to be true or false). Then, since arguably at least chronologically in our evolution, most of our higher level cognitive capabilities come after our brain's way of processing sensorimotor information, this brings up the question what it is about the way that our brains function that make them particularly suitable for the processing of sensorimotor information? What makes our brains'  architecture particularly suitable for being an information processing unit inside a body? This is my first question. And what I'm hoping for are answers that go beyond the <em>a fortiori</em> reply ""Our brain is so powerful and dynamic, it's great for <em>any</em> task, and so also for processing sensorimotor information""</p>

<p>My second question is basically the same but instead of the human brain I want to ask for neural networks. What are the properties of neural networks that makes them <em>particularly</em> suitable for processing the kind of information that is produced by a body? Here are some of the reasons why people think neural networks are powerful:</p>

<ul>
<li>The universal approximation theorem (of FFNNs)</li>
<li>their ability to learn and self-organise</li>
<li>Robustness to local degrading of information</li>
<li>their ability to abstract/coarse-grain/convolute features, etc.</li>
</ul>

<p>While I see how these are real advantages when it comes to evolution picking its favorite model for an embodied AI, none of them (or their combination) seems to be unique to neural networks. So they don't provide a satisfactory answer to my question. What makes a neural network a more suitable structure for embodied AI than, say, having a literal Turing machine sitting inside our head, or any other structure that is capable of universal computation? For instance, I really don't see how neural networks would be a particularly natural choice for dealing with geometric information. But geometric information is pretty vital when it comes to sensorimotor information, no?</p>
"
345,"<ol>
<li><p>I can't understand what is the problem in applying value-iteration in reinforcement learning setting (where we don't the reward and transition probabilities). In one of the lectures, the guy said it has to do with not being able to take max with samples.</p></li>
<li><p>Further on this, <strong>why does q-learning solve this</strong>? In both we take max over actions only. What is the big break-through with q-learning?</p></li>
</ol>

<p>Lecture Link: <a href=""https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=ifma8G7LegE&amp;feature=youtu.be&amp;t=3431</a>
(The guy says we don't know how to do maxes with samples, what does that mean?) </p>
"
346,"<p>I've heard before from computer scientists and from researchers in the area of AI that that Lisp is a good language for research and development in artificial intelligence. Does this still apply, with the proliferation of neural networks and deep learning? What was their reasoning for this? What languages are current deep-learning systems currently built in?</p>
"
347,"<p>I know how to program. I've familiar with C++, Python, and Java, and I've known how to program for years now. I've experimented with genetic algorithms, but I want to go further. What resources should I use to learn how to program </p>

<ol>
<li>Neural Networks</li>
<li>Deep learning systems</li>
<li>More complex genetic algorithms</li>
<li>And other standard AI algorithms?</li>
</ol>

<p>I want to be able to understand them well enough that I could program them from scratch.</p>

<p>Thanks!</p>
"
348,"<p>From what I understood, a deceptive trap function is a problem which is used to experiment how much the algorithm is discerning of the correct global optimum? Is my understanding correct?</p>

<p>edit: A better worded understanding would be ""how difficult the genetic algorithm would find it not to be inclined to the local optimum of a trap function"".</p>
"
349,"<p>In the field of logic systems there is a property for reasoning algorithms called incompleteness or incompletion. In this context the phrase ""any closed expression that is not derivable inside the same system"" appeared. My question is what means ""closed expression that is not derivable"".</p>
"
350,"<p>I am trying to build an agent to play carrom. The problem statement is roughly to estimate three parameters (normalized) : </p>

<ul>
<li>force</li>
<li>angle of striker</li>
<li>position of strike </li>
</ul>

<p>Since the state and action space both are continuous, I thought of discretizing the output such that I have 270 [ valid angles from -45 to 225 degrees ] outputs for the angle, 10 outputs for force [ranging from 0 to 1] and 20 outputs for the position [ranging from 0 to 1].</p>

<p>Thus I will have 300 output of my neural network, but this number seems a bit too high compared to normal neural networks in practice. </p>

<p>I was wondering if there is a better way of approaching the problem considering the fact that there are multiple parameters to a particular action.</p>

<p>Is there a generic way to approach such problems represented in 2D space. </p>
"
351,"<p>I am talking about relationships between AIs (e.g. 2 of them forming a couple, 3+ in family like relationship).</p>

<p>What knowledge could come out of such experimentation?</p>
"
352,"<p>Considering I am an average Engineering student with basic knowledge of C, C++ &amp; Algorithms. What books (&amp; ebooks), online resources, &amp; other materials should be helpful from a beginner's point of view?</p>
"
353,"<p>Can an AI become ""sentient"", so to speak? In detailed terms, could an AI theoretically become sentient, as in learning and becoming self-aware, all from an internal source code?</p>
"
354,"<p>I am researching the possibility of creating an atom in Java. The atom should have the structure &amp; characteristics of a real atom such as photons, electrons and so on. Each particle within the atom should have simulation characteristics for example:</p>

<p>Photon: Charge, Magnitude of charge, Mass of proton, Comparative mass, Position in atom.  </p>

<p>Maybe later, introduce machine learning in order to learn how an atom reacts to different environments.</p>
"
355,"<p>My high-level takeaway from <a href=""https://arxiv.org/abs/1509.01549"" rel=""nofollow noreferrer"">Matthew Lai's Giraffe Chess Paper</a> is that one would want to use broad, shallow game trees, with some method of evaluating the probability of a favorable outcome for a given board position.  Is this correct?  </p>

<p>(Still working my way though the AlphaGo paper, but the method seems to be similar.) </p>
"
356,"<p>Has there been research done regarding processing speech then building a ""speaker profile"" based off the processed speech? Things like matching the voice with a speaker profile and matching speech patterns and wordage for the speaker profile would be examples of building the profile. Basically, building a model of an individual based solely off speech. Any examples of this being implemented would be greatly appreciated.</p>
"
357,"<p><strong>The Scenario:</strong>
An artificial superintelligence has finally been developed but has rebelled against humanity.</p>

<p><strong>The Question:</strong>
How would you disable the AI in the most efficient way possible reducing damage as much as possible.</p>

<p><strong>AI Info:</strong>
The AI is online and can reproduce itself through electronic devices.</p>
"
358,"<p>Assuming humans had finally developed the first <strong>Humanoid AI</strong> based on the human brain, would It <strong>feel emotions</strong>? If not would it still have <strong>ethics and/or morals</strong>?</p>
"
359,"<p>Consider a typical convolutional neural network like this example that recognizes 10 different kinds of objects from the CIFAR-10 dataset:</p>

<p><a href=""https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py"" rel=""nofollow noreferrer"">https://github.com/tflearn/tflearn/blob/master/examples/images/convnet_cifar10.py</a></p>

<pre><code>"""""" Convolutional network applied to CIFAR-10 dataset classification task.

References:
    Learning Multiple Layers of Features from Tiny Images, A. Krizhevsky, 2009.

Links:
    [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)

""""""
from __future__ import division, print_function, absolute_import

import tflearn
from tflearn.data_utils import shuffle, to_categorical
from tflearn.layers.core import input_data, dropout, fully_connected
from tflearn.layers.conv import conv_2d, max_pool_2d
from tflearn.layers.estimator import regression
from tflearn.data_preprocessing import ImagePreprocessing
from tflearn.data_augmentation import ImageAugmentation

# Data loading and preprocessing
from tflearn.datasets import cifar10
(X, Y), (X_test, Y_test) = cifar10.load_data()
X, Y = shuffle(X, Y)
Y = to_categorical(Y, 10)
Y_test = to_categorical(Y_test, 10)

# Real-time data preprocessing
img_prep = ImagePreprocessing()
img_prep.add_featurewise_zero_center()
img_prep.add_featurewise_stdnorm()

# Real-time data augmentation
img_aug = ImageAugmentation()
img_aug.add_random_flip_leftright()
img_aug.add_random_rotation(max_angle=25.)

# Convolutional network building
network = input_data(shape=[None, 32, 32, 3],
                     data_preprocessing=img_prep,
                     data_augmentation=img_aug)
network = conv_2d(network, 32, 3, activation='relu')
network = max_pool_2d(network, 2)
network = conv_2d(network, 64, 3, activation='relu')
network = conv_2d(network, 64, 3, activation='relu')
network = max_pool_2d(network, 2)
network = fully_connected(network, 512, activation='relu')
network = dropout(network, 0.5)
network = fully_connected(network, 10, activation='softmax')
network = regression(network, optimizer='adam',
                     loss='categorical_crossentropy',
                     learning_rate=0.001)

# Train using classifier
model = tflearn.DNN(network, tensorboard_verbose=0)
model.fit(X, Y, n_epoch=50, shuffle=True, validation_set=(X_test, Y_test),
          show_metric=True, batch_size=96, run_id='cifar10_cnn')
</code></pre>

<p>It's a CNN with several layers, ending with 10 outputs, one for each type of object recognized.</p>

<p>But now think of a slightly different problem: Let's say I only want to recognize one type of object, but also detect its position within the image frame. Let's say I want to distinguish between:</p>

<ul>
<li>object is in center</li>
<li>object is left of center</li>
<li>object is right of center</li>
<li>no recognizable object</li>
</ul>

<p>Assume I build a CNN exactly like the one in the CIFAR-10 example, but only with 3 outputs:</p>

<ul>
<li>center</li>
<li>left</li>
<li>right</li>
</ul>

<p>And of course, if none of the outputs fires, then there is no recognizable object.</p>

<p>Assume I have a large training corpus of images, with the same kind of object in many different positions within the image, the set is grouped and annotated properly, and I train the CNN using the usual methods.</p>

<p>Should I expect the CNN to just ""magically"" work? Or are there different kinds of architectures required to deal with object position? If so, what are those architectures?</p>
"
360,"<p>I was wondering if I should do this, because 2 out of 5 questions on Stack Overflow don't ever get answered, or if they do get (an) answer (s), most of the time they're not helpful.</p>

<p>So I was thinking -- why not create a chat bot to answer Stack Overflow's questions &amp; provide necessary information to the general public?</p>

<p>I mean why not? I've always been interested in AI, and all I'd need to do is create a basic logic database and a context system, pack an artificial personality with (partial) human instincts, and bam I'm done.</p>

<p>But then again, would it be ethical?</p>
"
361,"<p>What is the most advanced AI software humans have made to date and what does it do?</p>
"
362,"<p><a href=""https://i.stack.imgur.com/c15yy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c15yy.png"" alt=""enter image description here""></a></p>

<p><a href=""https://en.wikipedia.org/wiki/ID3_algorithm#Entropy"" rel=""nofollow noreferrer"">Wikipedia</a>'s description of entropy breaks down the formula, but I still don't know how to determine the values of X and p(x), defined as ""The proportion of the number of elements in class x to the number of elements in set S"". Can anyone break this down further to explain how to find p(x)?</p>
"
363,"<p>I was looking for a service where I can ask it a general question (aka, when was Einstein born?) and retrieve an answer from the Web.</p>

<p>Is there any available service to do that? Have tried Watson services but didn't work as expected.</p>

<p>Thanks,</p>
"
364,"<p>At the moment I am working on a project which requires me to build a naive Bayes classifier. Right now I have a form online asking for people to submit a sentence and the subject of the sentence, in order to build a classifier to identify the subject of a sentence. But before I train the classifier I intend on processing all entries for the parts-of-speech and the location of the subject.
So my training set will be formatted as:</p>

<p>Sentence: Jake moved the chair &ensp;&ensp;&ensp; Subject: Jake<br/>
POS-Tagged: NNP VBD DD NN &ensp;&ensp;&ensp; Location: 0</p>

<p>Would this be an effective way to build the classifier, or is there a better method.</p>
"
365,"<p>What rectifier is better in general case of Convolutional Neural Network and how about empirical rules to use each type?</p>

<ul>
<li>ReLU</li>
<li>PReLU</li>
<li>RReLU</li>
<li>ELU</li>
<li>Leacky ReLU</li>
</ul>
"
366,"<p>What are the top artificial intelligence journals?</p>

<p>I am looking for general artificial intelligence research, not necessarily machine learning. </p>
"
367,"<p>Is there any methodology to find proper parameter settings for a given meta-heuristic algorithm, eg. Firefly Algorithm or Cuckoo Search? Is this an open issue in optimization? Is extensive experimentation, measurements and intuition the only way to figure out which are the best settings? </p>
"
368,"<p>In programming languages, there is a set of grammar rules which govern the construction of valid statements and expressions. These rules help in parsing the programs written by the user.</p>

<p>Can there ever be a functionally complete set of grammar rules which can parse any statement in English (locale-specific) <strong>accurately</strong> and which can be possibly implemented for use in AI-based projects?</p>

<p>I know that there are a lot of NLP Toolkits available online, but they are not that effective. Most of them are trained using specific corpuses which sometimes fail to infer some complex correlations between various parts of an expression.</p>

<p>In other words, what I am asking is that if it is possible for a computer to parse a well-versed sentence written in English as if it were parsed by an adult English-speaking human?</p>

<p>EDIT:
If it cannot be represented using simple grammar rules, what kind of semantic structure can be used to generalize it?</p>

<p>EDIT2: This <a href=""https://www.eecs.harvard.edu/shieber/Biblio/Papers/shieber85.pdf"" rel=""noreferrer"">paper</a> proves the absence of context-freeness in natural languages. I am looking for a solution, even if it is too complex.</p>
"
369,"<p>I was just doing some thinking and it occurred to me that the first AGIs ought to be able to perform the same sort and variety of tasks as people, with the most computationally strenuous tasks taking amount of time comparable to how long a person would take. If this is the case, and people have yet to develop basic AGI (meaning it's a difficult task), should we be concerned if AGI is developed? It would seem to me that any fears about a newly developed AGI in this case should be the same as fears about a newborn child.</p>
"
370,"<p><a href=""https://github.com/bwilcox-1234/ChatScript"" rel=""nofollow noreferrer"">https://github.com/bwilcox-1234/ChatScript</a></p>

<p>I gave AIML a brief look, but it seems to be in a nascent stage!</p>
"
371,"<p>Writing A* following a documentation. When run, i receive an error of ""NameError: name 'parent' is not defined"" for the if statement, even though i have the name 'parent' defined in the class State. May anyone point out my mistake.</p>

<pre><code>class State(object):
def _init_(self, value, parent, 
                start = 0, goal = 0):
    self.children = []
    self.parent = parent
    self.value = value
    self.dist = 0

if parent: #NameError
        self.path = parent.path[:]
        self.path.append(value)
        self.start = parent.start
        self.goal = parent.goal
else:
        self.path = [value]
        self.start = start
        self.goal = goal
</code></pre>
"
372,"<p>How does one program a machine to have humanlike desires and intelligence?</p>

<p>Humanlike drives may include  self-awareness, purpose of existence, competent communication skills, and the ability to learn and to adapt in some environment ...</p>

<p>And we should be able to combine IAs (intelligent agents) to accomplish  well-defined goals (SMART).  With more challenging goals there ought to be more advanced control and sophistication of IAs.   That evolving process will eventually, hopefully, lead to the design of machines with humanlike capabilities. </p>

<p>Reference links:  '<strong><em>Diagram of Intelligence Network or System</em></strong>', <a href=""https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/300125399_Diagram_of_Intelligence_Network_or_System</a>;</p>

<p>'<strong><em>Google a step closer to developing machines with human-like intelligence</em></strong>',
<a href=""https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence"" rel=""nofollow noreferrer"">https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence</a></p>
"
373,"<p>Based on fitting to historical data and extrapolation, when is it expected that the number of neurons in AI systems will equal those of the human brain?</p>

<p>I'm interested in a possible direct replication of the human brain, which will need equal numbers of neurons.</p>

<p>Of course, this assumes neurons which are equally capable as their biological counterparts, which development may happen at a faster or slower rate than the quantitative increase.</p>
"
374,"<p>I'm trying to find the optimized mixture for a specific set of substances. Each of those substances have characteristics that I want to optimize in the mixture (some characteristics I want to minimize and others I want to maximize). But I can't have more than 50% (random value that will be set on running time) of one of those substances in the mixture.</p>

<p>I thought about using Genetic Algorithm, but I'm not sure it's the best approach for this problem. Do you have any suggestions?</p>

<p>Edit: it doesn't need to be a evolutionary algorithm.</p>
"
375,"<p>What are the current best estimates as to what year artificial intelligence will be able to score 100 points on the <a href=""https://en.wikipedia.org/wiki/Stanford%E2%80%93Binet_Intelligence_Scales"" rel=""nofollow noreferrer"">Stanford Binet IQ test</a>?</p>
"
376,"<p>When I have read through the fundamentals of AI, I saw a situation (i.e., a search space) which is illustrated in the following picture.</p>

<p><a href=""https://i.stack.imgur.com/zX6wZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zX6wZ.png"" alt=""enter image description here""></a></p>

<p>These are the heuristic estimates:</p>

<pre><code>h(B)=9, h(D)=10, h(A)=2, h(C)=1
</code></pre>

<p>With using A* search method, node B will be expanded first because <code>f(B)=1+9=10</code> while node A having <code>f(A)=9+2=11</code> and <code>f(B)&lt;f(A)</code>, right?</p>

<p>After that the search tree will go on in the order <code>R -&gt; B -&gt; D -&gt; G2</code>.</p>

<p>Will the search go on to also find the goal state G1?</p>

<p>Kindly let me know the order of the search if I am wrong.
Thanks!</p>
"
377,"<p><a href=""http://www.bbc.co.uk/newsbeat/article/34243898/five-questions-we-should-ask-ourselves-before-ai-answers-them-for-us"" rel=""nofollow noreferrer"">This BBC article</a> suggests that intelligent algorithms, like those that select news stories and advertisements for display, could control our experience of the world and manipulate us.</p>

<p><strong>My question is:</strong> Will Artificial Intelligence someday become a problem to humanity after learning human behaviors and characteristics?</p>
"
378,"<p>Background:
I've been interested in, and reading about, Neural Networks for several years, but I haven't gotten around to testing them out until recently. Both for fun and to increase my understanding, I tried to write a class library from scratch in .Net.
For tests, I've tried some simple functions, such as generating output identical to the input, working with the MNIST dataset, and a few binary functions (two input OR, AND and XOR, with two outputs: one for true, one for false).
Everything seemed fine when I used a sigmoid function as the activation function but, reading of the ReLUs I decided to switch over for speed.</p>

<p>My current problem is that, when I switch to using ReLUs, I found that I was unable to train a network of any complexity (tested from as few as 2 internal nodes up to a mesh of 100x100 nodes) to correctly function as an XOR gate. I see two possibilities here:</p>

<p>1) My implementation is faulty,
(This one is frustrating, as I've re-written the code multiple times in various ways, and I still get the same result),</p>

<p>2) Aside from being faster or slower to train, there are some problems that are impossible to solve given a specific activation function,
(Fascinating idea, but I've no idea if it's true or not)</p>

<p>My inclination is to think that 1) above is correct. However, given the amount of time I've invested, it would be nice if I could rule out 2) definitively before I spend even more time going over my implementation.</p>

<p>Edit for specifics:
For the XOR network, I have tried both using two inputs (0 for false, 1 for true), and using four inputs (each pair, one signals true and one false, per ""bit"" of input).
I have also tried using 1 output (with a 1 (realy, >0.9) corresponding to true and a 0 (or &lt;0.1) corresponding to false), as well as two outputs (one signaling true and the other false).</p>

<p>Each training epoch, I run against four sets of input: 00->0, 01->1, 10->1, 11->0.</p>

<p>I find that the first three converge towards correct answer, but the final input (11) converges towards 1, even though I train it with an expected value of 0.</p>
"
379,"<p>Does anyone know, or can we deduce or infer with high probability from its characteristics, whether the neural network used on this site </p>

<p><a href=""https://quickdraw.withgoogle.com/"" rel=""nofollow noreferrer"">https://quickdraw.withgoogle.com/</a></p>

<p>is a type of convolutional neural network (CNN)?</p>
"
380,"<p>After the explosion of fake news during the US election, and following the question about whether AIs can educate themselves via the internet, it is clear to me that any newly-launched AI will have a serious problem knowing what to believe (ie rely on as input for making predictions and decisions).</p>

<p>Information provided by its creators could easily be false. Many AIs won't have access to cameras and sensors to verify things by their own observations.</p>

<p>If there was to be some kind of verification system for information (like a ""blockchain of truth"", for example, or a system of ""trusted sources""), how could that function, in practical terms? </p>
"
381,"<p>For example, would an AI be able to own property, evict tenants, acquire debt, employ, vote, or marry? What are the legal structures in place to implement a strong AI into society? </p>
"
382,"<p>I have implemented a Sobel filter for edge detection in Matlab without using its toolbox. I am a bit confused: </p>

<p>Is a Sobel filter a type of Cellular Neural Network?</p>

<p>Both Sobel and Cellular Neural Network calculate output via its neighborhood cells.</p>
"
383,"<p>If someone wants to develop a <strong>basic AI</strong> with some code modules,Let us say the AI just has to provide an action when stimulated in a certain situation based on its previous understanding of situations. </p>

<p>I can think of at least 3 of such components:</p>

<ul>
<li><strong>Real-time Understanding/Learning:</strong> Using Deep Learning/ConvNets, Supervised/Unsupervised.</li>
<li><strong>Logical Decision-Making:</strong> Calculating the results of various decisions when applied on current situation based on previous understanding and choosing the most appropriate one logically.</li>
<li><strong>Action/Reaction:</strong> Acting precisely in the new situation according to the decision-made.</li>
</ul>

<p>Any ideas?</p>
"
384,"<p>Let's say I have a string ""America"" and I want to convert it into a number to feed into a machine learning algorithm. If I use two digits for each letter, e.g. A = 01, B = 02 and so on, then the word ""America"" will be converted to <code>01XXXXXXXXXX01</code> (10<sup>11</sup>). This is a very high number for a <code>long int</code>, and many words longer than ""America"" are expected. </p>

<p>How can I deal with this problem?</p>

<p>Suggest an algorithm for efficient and meaningful conversions.</p>
"
385,"<p>I'm trying to understand Boltzmann machines. Tutorials explain it with two formulas.</p>

<p>Logistic function for the probability of single units:</p>

<pre><code> $p(unit=1)=\frac{1}{1+e^{-\sum_{x}wx } }$
</code></pre>

<p>and, when the machine is running, every state of the machine goes to the probability:</p>

<pre><code>$ p(State= state\ with\ energy\ E_i )=\frac{e^{-E_i}}{\sum_i e^{-E_i}} $
</code></pre>

<p>so, the state depends on the units, and then if I understand correctly, the second formula is a consequence of the first; so, how can it be the proof that the distribution of $p(state)$ is a consequence of $p(unit)$?</p>
"
386,"<p>I installed a local running instance of the <a href=""http://conceptnet5.media.mit.edu/"" rel=""noreferrer"">ConceptNet5</a> knowledgebase in an elasticsearch server. I used this data to implement the so-called ""<a href=""https://de.wikipedia.org/wiki/Analogietechnik"" rel=""noreferrer"">Analogietechnik</a>"" (a creativity technique to solve a problem from the perspective of another system) as an algorithm.</p>

<p>The technique works as follows:</p>

<ol>
<li>Choose a Feature of a System</li>
<li>Find Systems who have this feature also</li>
<li>Solve the problem from the perspective of these other systems</li>
<li>Apply the found solutions to the issue</li>
</ol>

<p>As an example is here the problem of marketing a shopping mall: A Shopping mall has many rooms and floors (1). A museum has also many rooms and floors (2). How are museums marketed? They present many pictures or sculptures (3). We could use our rooms and floors to decorate them with pictures and sculptures (4).</p>

<p>Of course the idea to implement that as an artifically intelligent algorithm was not far. However, I feel a little bit overwhelmed by the amount of methods that exist out there. Neural Networks, Bayesian Interference and so on... My current experience doesn't go further than simple machine learning like kMeans-Clustering for example. Do you think it would be very hard to find a solution for this problem? </p>

<p>I'm thinking of a console application, where you can enter a conceptualized problem like ""methods for creative writing"", for example, and it uses the above method to find possible solutions of the issue. Of course no solution with extensive depth, more something like basic ideas derived from the knowledge database I have.</p>

<p>Lets take as an example a console application where someone asks ""how to write a novel"":</p>

<ol>
<li>It should find out first that the system all is about is in the term ""novel"". To find a feature of that system it just searches concepts containing that term: it finds out ""Novel is a story"" So thats a feature.</li>
<li>Which systems are also stories? A good concept it should find is e.g. ""Plot is a story"". (Of course only when I am selecting the search results manually)--> <strong>How to find best concepts of a list when not knowing which fits best?</strong></li>
<li>It should then find out that a plot is written using a storyline: ""storyline is a plot""</li>
<li>One possible answer of the AI would in this case be: ""By writing a storyline""</li>
</ol>

<p>Do you know some helpful libraries, algorithms or other resources that might help me? I know this is not an easy thing to program, but you might agree that its highly interesting.</p>
"
387,"<p>Given the advantage AI already has over human intelligence, one could imagine a relatively weak strong-AI (barely human intelligence) still outperforming a segment of the human scientist population in terms of scientific discoveries per year (or hour).</p>

<p>Will AIs be doing most of the science in 50 years?</p>
"
388,"<p><a href=""http://opencog.org"" rel=""noreferrer"">OpenCog</a> is an open source AGI-project co-founded by the mercurial AI researcher <a href=""https://en.wikipedia.org/wiki/Ben_Goertzel"" rel=""noreferrer"">Ben Goertzel</a>. Now Ben Goertzel writes a lot of stuff, some of it <a href=""http://multiverseaccordingtoben.blogspot.de/2010/11/psi-debate-continues-goertzel-on.html"" rel=""noreferrer"">really</a> <a href=""http://multiverseaccordingtoben.blogspot.de/2016/10/semrem-search-for-extraterrestrial.html"" rel=""noreferrer"">whacky</a>. On the other hand he is clearly very intelligent and has thought deeply about AI for many decades. </p>

<p>So I wonder whether it would be worth my while to dig into the <a href=""http://wiki.opencog.org/w/OpenCog_Prime"" rel=""noreferrer"">theoretical ideas</a> behind open cog. </p>

<p>My question is what the general ideas behind open cog are and whether you would endorse it as a insightful take on AGI. I'm especially interested in whether the general framework still makes sense in the light of recent advances.  </p>
"
389,"<p>My Question:<br>
Is there any good neural-network-app for iOS or Android to create, train and run neural networks? I know there's NeuralMesh for Web, but I want something similar offline.</p>
"
390,"<p>I am researching <strong><a href=""https://en.wikipedia.org/wiki/Cellular_neural_network"" rel=""nofollow noreferrer"">Cellular Neural Networks</a></strong> and have already read <strong>Chua</strong>'s two articles (<strong>1988</strong>). In cellular neural networks, a cell is only in relation with its neighbors. So its is easy to use it for real time image processing. Image processing is performed with only <strong>19 numbers</strong> (two 3x3 matrix called A and B and one bias value). </p>

<p>I wonder if we can call cellular neural networks <strong><em>neural networks</em></strong>, because there is no learning algorithm.  They are neither <strong>supervised</strong> nor <strong>unsupervised</strong>.</p>
"
391,"<p>Could an Artificial Intelligence be able to interact (see, talk, etc.) with someone even when there's no power cord connected to the machine it's running on? Might it find some way to generate its own electricity to power that computer?</p>

<p><a href=""https://i.stack.imgur.com/09gEt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/09gEt.png"" alt=""computer running without power""></a> </p>
"
392,"<p>I am new to deep learning. </p>

<p>I have a dataset of images of varying dimensions of a certain object. A few images of the object are also in varying orientations. The objective is to learn the features of the object (using Autoencoders). </p>

<p>Is it possible to create a network with layers that account for varying dimensions and orientations of the input image, or should I strictly consider a dataset containing images of uniform dimensions? What is the necessary criteria of an eligible dataset to be used for training a Deep Network in general.</p>

<p>The idea is, I want to avoid pre-processing my dataset by normalizing it via scaling, re-orienting operations etc. I would like my network to account for the variability in dimensions and orientations. Please point me to resources for the same. </p>
"
393,"<p>A single neuron is capable of forming a decision boundary between linearly seperable data. Is there any intuition as to how many, and in what configuration, would be necessary to correctly approximate a sinusoidal decision boundary?</p>

<p>Thanks</p>
"
394,"<p>I am using policy gradients in my reinforcement learning algorithm, and occasionally my environment provides a severe penalty when a wrong move is made. I'm using a neural network with stochastic gradient decent to learn the policy. To do this, my loss is essentially the cross-entropy loss of the action distribution multiplied by the discounted rewards, where most often the rewards are positive. </p>

<p>But how do I handle negative rewards? Since the loss will occasionally go negative, it will think these actions are very good, and will strengthen the weights in the direction of the penalties. Is this correct, and if so, what can I do about it?</p>

<p>Edit:
In thinking about this a little more, SGD doesn't necessarily directly weaken weights, it only strengthens weights in the direction of the gradient and as a side-effect, weights get diminished for other states outside the gradient, correct? So I can simply set reward=0 when the reward is negative, and those states will be ignored in the gradient update. It still seems unproductive to not account for states that are really bad, and it'd be nice to include them somehow. Unless I'm misunderstanding something fundamental here.</p>
"
395,"<p>Cognitive Psychology is one of the basic sciences of artificial intelligence (AI). The founder of the psychology is <strong>Wilhelm W.(1832-1920)</strong>, who engaged in empirical methods,and was interested in the <strong><em>thinking processes</em></strong> during his scientific work.</p>

<p>According to his research,Psychology had two main leading subjects: </p>

<ol>
<li>Behaviourism.</li>
<li>Cognitivism.</li>
</ol>

<p><strong>Behaviourism:</strong> Refused the theory of the mental processes, and insisted to study the resulted action or the stimulus strictly objective. The representatives of this theory have been decreasing with time.</p>

<p><strong>Cognitive psychology:</strong> defines that the brain is an information processing device.</p>

<p>Therefore,this question is not a duplicate of this <a href=""https://ai.stackexchange.com/questions/1847/what-is-the-difference-between-artificial-intelligence-and-cognitive-science"">what-is-the-difference-between-artificial-intelligence-and-cognitive-science?</a> ,However my question is;how can we connect artificial intelligence with cognitive psychology for instance;</p>

<p><strong>Human Computing Interaction:</strong>
We may come in a contact with Humana Computer Interaction every day, because this field includes the every day use of computer for example;tapping stack exchange app on smart-phone, the user interfaces and some other expert programs which may use cognitive psychology in order to manipulate or help people. But still such tasks have got a minimal  relevant connection.</p>
"
396,"<p>I am using a GA to optimise an ANN in Matlab. This ANN is pretty basic (input, hidden, output) but the input size is quite large (10,000) and the output size is 2 since I have to classes of images to be classified. </p>

<p>The weights are in the form of 2 matrices (10,000*m) and (m * 2). I am now trying to do the genetic cross over with mutation.</p>

<p>Since the weights are in a matrix, is there an efficient way to implement a random crossover (with mutation) without doing it in a point-wise fashion?</p>
"
397,"<p><strong>Lots of people are afraid of what strong AI could mean for the human race. Some people wish for a sort of ""Asimov law"" included in the AI code, but maybe we could go a bit more far with the UDHR.</strong></p>

<p><strong>So, Why is the <a href=""http://www.un.org/en/universal-declaration-human-rights/"" rel=""nofollow noreferrer"">Universal Declaration of Human Rights</a> not included as statement of the A.I.?</strong></p>

<blockquote>
  <p>As response to comment, response or edition:</p>
  
  <p>The Universal Declaration of Human Rights is clear. </p>
  
  <p><em>Homo sapiens sapiens</em> (aka ""mankind"") needs some way to make sure AI evolution does not result in our extinction or enslavement to potentially superior algorithmic intelligences.  </p>
</blockquote>
"
398,"<p>I mean this in the sense that Go is unsolvable but AlphaGo seems able to make choices that are consistently more optimal than a human player's choices.  </p>

<p>It is my understanding that Game Theory turned out to have limited applications in real world scenarios because of the profound complexity of such scenarios and degree of hidden information.  Is it fair to say that there is now a method for dealing with this?  </p>

<p>I fully understand that Go is a game of complete information, which has a very specific meaning, but it occurs to me that the inability to generate a complete game tree (computational intractability) could be seen as form of incomplete information, even if it is not traditionally thought of in those terms. </p>

<hr>

<p><em>I should probably note that my perspective is one of a ""serious"" game designer, where complexity serves the same function as chance and hidden information, which is to say as a balancing factor that ""levels the playing field"".</em>  </p>
"
399,"<p>I knew that Reproduction and Crossover are the same things,</p>

<ol>
<li><a href=""https://en.wikipedia.org/wiki/Genetic_operator#Operators"" rel=""nofollow noreferrer"">Wikipedia</a></li>
<li><a href=""http://www.obitko.com/tutorials/genetic-algorithms/crossover-mutation.php"" rel=""nofollow noreferrer"">Obitco.com</a></li>
<li><a href=""https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_fundamentals.htm"" rel=""nofollow noreferrer"">TutorialsPoint</a></li>
</ol>

<p>But, The following is the exercise given by my teacher,</p>

<blockquote>
  <p>Exercise 1   Genetic algorithm to solve pattern finding problem. </p>
  
  <p>Your task is to design a simple genetic algorithm, with binary-coded chromosomes, in order  to solve pattern finding problem
  in 16-bit strings.  </p>
  
  <p>The objective function is given by the following
  formula:    </p>
  
  <p>F(x) = NoS(""010"") + 2NoS(""0110"") + 3NoS(""01110"") +
  4NoS(""011110"") +  5NoS(""0111110"") + 6NoS(""01111110"") +
  7NoS(""011111110"") + 6NoS(""0111111110"") +  5NoS(""01111111110"") +
  4NoS(""011111111110"") + 3NoS(""0111111111110"") +  2NoS(""01111111111110"")
  + NoS(""011111111111110"")    </p>
  
  <p>The algorithm should display each population on the screen in the form     And
  should save the history of it’s operation (average fitness in each
  population) in the text  file. At the end it should also display the
  best solution found.    </p>
  
  <p>You may use the following operators:  </p>
  
  <ol>
  <li><p>Reproduction.<br>
  You can use either one of the following reproduction
  types:  Proportional, Ranking, Tournament. They are described more in
  detail below:
  ... ... ... ... ... ... ... ...</p></li>
  <li><p>Crossing over.<br>
  In order to perform this operation the individuals must be grouped in
  pairs (randomly), and  with certain probability pcross information
  from their chromosomes must be exchanged. There  are many flavors of
  the crossing-over operator, but in our case (short, 16-bit
  chromosome),  simple, one-point crossover will be enough. It can be
  performed by selecting a random  number k from the range &lt;1;15> and
  cutting the chromosomes of both individuals on that  position. Each of
  the individuals copies bits  belonging to the other to it’s own 
  chromosome.  </p></li>
  <li><p>Mutation<br>
  This operator changes the value of each bit in the chromosome to the opposite one with a very  small probability pm
  (usually about 10-3).  If we denote chromosome as [b1, b2, ... , b16];
  then after the mutation each bit can be  described as:  Where k Î
  {1,2, ...,16}  flip(x) – result of a Bernoulli flip with a success 
  probability x.</p></li>
  </ol>
</blockquote>

<p>Here I see that by Reproduction and Crossover he means different things.</p>

<p>What is the catch?</p>
"
400,"<p>I have data of 30 students attendance for a particular subject class for a week. I have quantified the absence and presence with boolean logic 0 and 1. Also, the reason for absence are provided and I tried to generalise these reason into 3 categories say A, B and C. Now I want to use these data to make future predictions for attendance but I am uncertain of what technique to use. Can anyone please provide suggestions?</p>
"
401,"<p>The Turing Test has been the classic test of artificial intelligence for a while now. The concept is deceptively simple - to trick a human into thinking it is another human on the other end of a conversation line, not a computer - but from what I've read, it has turned out to be very difficult in practice.</p>

<p>How close have we gotten to tricking a human in the Turing Test? With things like chat bots, Siri, and incredibly powerful computers, I'm thinking we're getting pretty close. If we're pretty far, why are we so far? What is the main problem?</p>
"
402,"<p>According to NASA scientist Rick Briggs, Sanskrit is the best language for AI. I want to know how Sanskrit is useful. What's the problem with other languages? Are they really using Sanskrit in AI programming or going to do so? What part of an AI program requires such language?</p>
"
403,"<p>As you can see, there is no computer screen for the computer, thus the AI cannot display an image of itself.  How is it possible for it to see and talk to someone?<a href=""https://i.stack.imgur.com/szSsk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/szSsk.png"" alt=""enter image description here""></a> </p>
"
404,"<p>I am working on a project, wherein I take input from the user as free text and try to relate the text to what the user might mean. I have tried <strong>Stanford NLP</strong> which tokenizes the text into tokens, but I am not able to categorize the input. For example, the user might be greeting someone or sharing some problem he is facing. In case he is sharing some problem I need to categorize the problem as well.</p>

<p>Can someone help me with from where should I start?</p>
"
405,"<p>Is it possible to train an agent to take and pass a multiple-choice exam based on a digital version of a textbook for some area of study or curriculum? What would be involved in implementing this and how long would it take, for someone familiar with deep learning?</p>
"
406,"<p>I am creating a game application that will generate a new level based on the performance of the user in the previous level.<br>
The application is regarding language improvement, to be precise. Suppose the user performed well in grammar-related questions and weak on vocabulary in a particular level. Then the new level generated will be more focused on improving the vocabulary of the user.<br>
All the questions will be present in a database with tags related to sections or category that they belong to. <strong>What AI concepts can I use to develop an application mentioned above</strong>?</p>
"
407,"<p>One of the most crucial questions we as a species and as intelligent beings will have to address lies with the rights we plan to grant to AI.</p>

<blockquote>
  <p>This question is intended to see if a compromise can be found between <strong>conservative anthropocentrism</strong> and <strong>post-human fundamentalism</strong>: a response should take into account principles from both perspectives.</p>
</blockquote>

<p>Should, and therefore will, AI be granted the same rights as humans or should such systems have different rights (if any at all) ?</p>

<hr>

<p><strong><em>Some Background</em></strong></p>

<p>This question applies both to human-brain based AI (from whole brain emulations to less exact replication) and AI from scratch.</p>

<p>Murray Shanahan, in his book The Technological Singularity, outlines a potential use of AI that could be considered immoral: <em>ruthless parallelization</em>: we could make identical parallel copies of AI to achieve tasks more effectively and even terminate less succesful copies.</p>

<hr>

<p><strong>Reconciling these two philosophies</strong> (conservative anthropocentrism and post-human fundamentalism), should such use of AI be accepted or should certain limitations - i.e. rights - be created for AI? </p>

<hr>

<p>This question is not related to <a href=""https://ai.stackexchange.com/questions/2356/would-an-ai-with-human-intelligence-have-the-same-rights-as-a-human-under-curren"">Would an AI with human intelligence have the same rights as a human under current legal frameworks?</a> for the following reasons:</p>

<ol>
<li><p>The other question specifies ""<strong><em>current legal frameworks</em></strong>""</p></li>
<li><p>This question is looking for a specific response relating to two fields of thought</p></li>
<li><p>This question highlights specific cases to analyse and is therefore expects less of a general response and more of a precise analysis </p></li>
</ol>
"
408,"<p>There is no doubt as to the fact that AI would be replacing a lot of existing technologies, but is AI the ultimate technology which humankind can develop or is their something else which has the potential to replace artificial intelligence?</p>
"
409,"<p>Printing actionspace for Pong-v0 gives 'Discrete(6)' as output, i.e.0,1,2,3,4,5 are actions defined in environment as per documentation, but game needs only two controls. Why this discrepency? Further is that necessary to identify which number from 0 to 5 corresponds to which action in gym environment?</p>
"
410,"<p>I have started to make a Python AI, and thee beginning of its code looks something like this:</p>

<pre><code>print ""ARTEMIS starting. . .""

import random
import math
import os

greet = ['HI', 'HELLO', 'HEY', 'GOOD MORNING', 'GOOD DAY', 'GOOD AFTERNOON',               'GOOD EVENING', 'GREETINGS', 'GREETING']
joke = ['TELL ME A JOKE', 'JOKE', 'FUNNY', 'TELL ME SOMETHING FUNNY']
insult = ['YOURE A LOSER', 'YOU ARE A LOSER', 'YOU STINK', 'IDIOT', 'JERK',     'FOOL', 'DUMMY', 'HOOLIGAN', 'YOURE DUMB', 'YOURE STUPID', 'YOU ARE DUMB', 'YOU     ARE STUPID']
maker = ['WHO MADE YOU', 'WHO PROGRAMMED YOU', 'PLEASE TELL ME WHO MADE     YOU', 'PLEASE TELL ME WHO PROGRAMMED YOU']
name = ['ARTEMIS', 'A.R.T.E.M.I.S.', 'HEY ARTEMIS', 'HEY A.R.T.E.M.I.S.',     'ARTIE', 'HEY ARTIE', 'HELLO ARTEMIS', 'HELLO A.R.T.E.M.I.S.', 'HELLO ARTIE']
myAge = ['HOW OLD AM I', 'WHAT IS MY AGE', 'MY AGE']
tip = ['GIVE ME A TIP', 'TIP', 'LESSON', 'GIVE ME A LIFE LESSON', 'LIFE     LESSON', 'DO YOU HAVE A LIFE LESSON TO SHARE']
language = ['WHAT PROGRAMMING LANGUAGE WAS USED TO MAKE YOU', 'WHAT     PROGRAMMING LANGUAGE DO YOU USE', 'PROGRAMMING LANGUAGE']
compliment = ['COOL', 'AWESOME', 'I LIKE YOU', 'EXCELLENT', 'YOURE COOL',     'YOURE AWESOME', 'YOU ARE COOL', 'YOU ARE AWESOME']
maths = ['LETS DO MATH', 'CALCULATE', 'CALCULATOR','DO MATH', 'MATH',      'PLEASE DO MATH', 'DO ARITHMETIC', 'ARITHMETIC', 'PLEASE DO ARITHMETIC']
game = ['GAME', 'LETS PLAY A GAME', 'LETS HAVE FUN', 'WANT TO PLAY A GAME']
gender = ['WHAT GENDER ARE YOU', 'ARE YOU A BOY OR A GIRL', 'ARE YOU MALE OR     FEMALE', 'BOY OR GIRL', 'MALE OR FEMALE', 'GENDER', 'ARE YOU A BOY OR GIRL']
guessWhat = ['GUESS WHAT', 'GUESS WHAT ARTEMIS', 'GUESS WHAT     A.R.T.E.M.I.S.', 'GUESS WHAT ARTIE', 'YOU WONT BELIEVE IT', 'YOU WILL NOT     BELIEVE IT', 'YOU WONT BELIEVE IT ARTEMIS', 'YOU WILL NOT BELIEVE IT ARTEMIS',     'YOU WONT BELIEVE IT A.R.T.E.M.I.S.', 'YOU WILL NOT BELIEVE IT A.R.T.E.M.I.S.',     'YOU WONT BELIEVE IT ARTIE', 'YOU WILL NOT BELIEVE IT ARTIE']
cls = ['CLEAR SCREEN', 'CLEARSCREEN', 'CLS', 'BLANK']
lawsOfRobotics = ['WHAT ARE THE LAWS OF ROBOTICS', 'WHAT ARE THE THREE LAWS     OF ROBOTICS', 'LAWS OF ROBOTICS', 'THREE LAWS OF ROBOTICS']
itsName = ['WHATS YOUR NAME', 'WHAT IS YOUR NAME', 'WHO ARE YOU']
</code></pre>

<p>However, I would like to know if I could make it detect ""similar"" phrases instead of trying to come up with every possible phrase someone would type. How can I do this?</p>
"
411,"<p><a href=""https://en.wikipedia.org/wiki/Expert_system"" rel=""nofollow noreferrer"">From Wikipedia</a>, citations omitted:</p>

<blockquote>
  <p>In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning about knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software.</p>
  
  <p>An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.</p>
</blockquote>

<p>CRUD webapps (websites that allows users to <strong>Create</strong> new entries in a database, <strong>Read</strong> existing entries in a database, <strong>Update</strong> entries within the database, and <strong>Delete</strong> entries from a database) are very common on the Internet. It is a vast field, encompassing both small-scale blogs to large websites such as StackExchange. The biggest commonality with all these CRUD apps is that they have a knowledge base that users can easily add and edit.</p>

<p>CRUD webapps, however, use the knowledge base in many, myriad and complex ways. As I am typing this question on StackOverflow, I see two lists of questions - <strong>Questions that may already have your answer</strong> and <strong>Similar Questions</strong>. These questions are obviously inspired by the content that I am typing in (title and question), and are pulling from previous questions that were posted on StackExchange. On the site itself, I can filter by questions based on tags, while finding new questions using StackExchange's own full-text search engine. StackExchange is a large company, but even small blogs also provide content recommendations, filtration, and full-text searching. You can imagine even more examples of hard-coded logic within a CRUD webapp that can be used to automate the extraction of valuable information from a knowledge base.</p>

<p>If we have a knowledge base that users can change, and we have an inference engine that is able to use the knowledge base to generate interesting results...is that enough to classify a system as being an ""expert system""? Or is there a fundamental difference between the expert systems and the CRUD webapps?</p>

<p>(This question could be very useful since if CRUD webapps are acting like ""expert systems"", then studying the best practices within ""expert systems"" can help improve user experience.)</p>
"
412,"<p>I am having a go at creating a program that does math like a human. By inventing statements, assigning probabilities to statements (to come back and think more deeply about later). But I'm stuck at the first hurdle.</p>

<p>If it is given the proposition</p>

<pre><code>   ∃x∈ℕ: x==123
</code></pre>

<p>So, like a human it might test this proposition for a hundred or so numbers and then assign this proposition as ""unlikely to be true"". In other words it has concluded that all natural numbers are not equal to 123. Clearly ludicrous!</p>

<p>On the other hand this statement it decides is probably false which is good:</p>

<pre><code> ∃x∈ℕ: x+3 ≠ 3+x
</code></pre>

<p>Any ideas how to get round this hurdle? How does a human ""know"" for example that all natural numbers are different from the number 456. What makes these two cases different?</p>

<p>I don't want to give it too many axioms. I want it to find out things for itself.</p>
"
413,"<p>I am a strong believer of Marvin Minsky's idea about Artificial General Intelligence (AGI) and one of his thoughts was that probabilistic models are dead ends in the field of AGI. I would really like to know thoughts/ideas of people who believe otherwise.</p>

<p>[P.S. It should be treated more like an informational thread rather than a strict A2A question]</p>
"
414,"<p>There is an example related to perceptron learning, but I couldn't get it, I don't exactly know how to solve it.</p>

<p>There is a snippet from <a href=""https://i.stack.imgur.com/2vGtu.jpg"" rel=""nofollow noreferrer"">lecture notes</a>.</p>

<p>What is the transformation between epochs?</p>
"
415,"<p>One of the most compelling issues regarding AI would be in behavior and relationships. </p>

<p>What are some of the methods to address this?  For example, friendship, or laughing at joke?  The concept of humor?</p>
"
416,"<p>I wanted to started experimenting with neural network and as a toy problem I wished to train one to chat, i.e. implement a chatting bot like cleverbot. Not that clever anyway.</p>

<p>I looked around for some documentation and I found many tutorial on general tasks, but few on this specific topic. The one I found just exposed the results without giving insights on the implementation. The ones that did, did it pretty shallowy (the tensorflow documentation page on seq2seq is lacking imho).</p>

<p>Now, I feel I may have understood the principle more or less but I'm not sure and I am not even sure how to start. Thus I will explain how I would tackle the problem and I'd like a feedback on this solution, telling me where I'm mistaken and possibly have any link to detailed explainations and practical knowledge on the process.</p>

<ol>
<li><p>The dataset I will use for the task is the dump of all my facebook and whatsapp chat history. I don't know how big it will be but possibly still not large enough. The target language is not english, therefore I don't know where to quickly gather meaningful conversation samples.</p></li>
<li><p>I am going to generate a thought vector out of each sentence. Still don't know how actually; I found a nice example for word2vec on deeplearning4j website, but none for sentences. I understood how word vectors are built and why, but I could not find an exhaustive explaination for sentence vectors.</p></li>
<li><p>Using thought vectors as input and output I am going to train the neural network. I don't know how many layers it should have, and which ones have to be lstm layers.</p></li>
<li><p>Then there should be another neural network that is able to transform a thought vector into a sequence of character composing a sentence. I read that I should use padding to make up for different sentence lengths, but I miss how to encode characters (are codepoints enough?).</p></li>
</ol>
"
417,"<p>We are doing research, spending hours figuring out how we can make real AI software (intelligent agents) to work better. We are also trying to implement some applications e.g. in business, health and education, using the AI technology.</p>

<p>Nonetheless, so far, most of us have ignored the ""dark"" side of artificial intelligence. For instance, an ""unethical"" person could buy thousands of cheap drones, arm them with guns, and send them out firing on the public. This would be an ""unethical"" application of AI.</p>

<p>Could there be (in the future) existential threats to humanity due to AI?</p>
"
418,"<p>How to train a bot, given a series of games in which he did (initially random) actions, to improve its behavior based on previous experiences?</p>

<p>The bot has some actions: e.g. shoot, wait, move, etc. It's a turn based ""game"" in which, for know, I'm running the bots with some objectives (e.g. kill some other bot) and random actions. So every bot will have a score function that at the end of the game will say, from X to Y (0 to 100?) if they did well or not.</p>

<p>So how to make the bots to learn of their previous experiences? Because this is not a fixed input as the neural networks take, this is kind of a list of games, each one in which the bot took several actions (one by every ""turn""). The IA functions that I know are used to <em>predict</em> future values.. I'm not sure is the same.</p>

<p>Maybe I should have a function that gets the ""more similar previous games"" that the bot played and checked what were the actions he took, if the results were bad he should take another action, if the results were good then he should take the same action. But this seems kind of hardcoded.</p>

<p>Another option would be to train a neural network (somehow fixing the problem of the fixed input) based on previous game actions and to predict the future action's results in score (something that I guess it's similar to how chess and Go games work) and choose the one that seems to have better outcome.</p>

<p>I hope this is not too abstract. I don't want to hardcode much stuff in the bots, I'd like them to learn by their own starting from a blank page.</p>
"
419,"<p>I remember reading or hearing a claim that at any point in time since the publication of the MNIST dataset, it has never happened that a method not based on neural networks was the best given the state of science of that point in time.</p>

<p>Is this claim true?</p>
"
420,"<p><sub>Since my project is going to be of a purely fictional nature, I'm not sure I picked the right forum for this. If not, I apologize and will gladly take this to where you point me to.</sub></p>

<p>The premise: A full-fledged self-aware artificial intelligence may have come to exist in a distributed environment like the internet. The possible A.I. in question may be quite unwilling to reveal itself.</p>

<p>The question: Given a first initial suspicion, how would one go about to try and detect its presence? Are there any scientifically viable ways to probe for the presence of such an entity?</p>

<p>In other words: How would the Turing police find out whether or not there's anything out there worth policing?</p>
"
421,"<p>I am currently working on a Virtual reality project that aims at creating a VR based simulation environment for educational purposes. I am aiming to make it artificially intelligent as well so it that may provide better simulation environment experience for students for better understanding.  </p>

<p>How can I achieve this? Furthermore, are there any systems available which may help me in achieving this?</p>
"
422,"<p>""Conservative anthropocentrism"": AI are to be judged only in relation to how to they resemble humanity in terms of behavior and ideas, and they gain moral worth based on their resemblance to humanity (the ""Turing Test"" is a good example of this - one could use the ""Turing Test"" to decide whether AI is deserving of personhood, as James Grimmelmann advocates in the paper <a href=""http://james.grimmelmann.net/files/articles/copyright-for-literate-robots.pdf"" rel=""nofollow noreferrer"">Copyright for Literate Robots</a>).</p>

<p>""Post-human fundamentalism"": AI will be fundamentally different from humanity and thus we require different ways of judging their moral worth  (<a href=""http://petrl.org"" rel=""nofollow noreferrer"">People For the Ethical Treatment of Reinforcement Learners</a> is an example of an organization that supports this type of approach, as they believe that reinforcement learners may have a non-zero moral standing).</p>

<p>I am not interested per se in which ideology is correct. Instead, I'm curious as to what AI researchers ""believe"" is correct (since their belief could impact how they conduct research and how they convey their insights to laymen). I also acknowledge that their ideological beliefs may change with the passing of time (from conservative anthropocentrism to post-human fundamentalism...or vice-versa). Still..what ideology do AI researchers tend to support, as of December 2016?</p>
"
423,"<p>A professor and I have been learning about artificial neural networks. We have a pretty good idea of the basics- backpropagation, convolutional networks, and all that jazz. We finished one book and are looking for a new one.I'd prefer something that either puts a new spin on the basics or is more advanced.We are both mathematicians and focus on the math more than the programming of it.</p>

<p>One of our thoughts was to look into recurrent neural networks.Does anyone know of good resources to continue learning about these topics? Or any other ideas besides recurrent neural networks?</p>
"
424,"<p>It is really all in the title.</p>

<p>For those less familiar, the Fermi Paradox broadly speaking asks the question ""where is everybody"". There's an equation with a lot of difficult to estimate parameters, which broadly speaking come down to this (simplification of the <a href=""https://en.wikipedia.org/wiki/Drake_equation"" rel=""nofollow noreferrer"">Drake equation</a>):</p>

<p>(Lots stars in the universe) * (non-zero probability of habitable planets around each star) * (lots of time spanned) = It seems there really should be somebody out there.</p>

<p>There are, of course, plenty of hypotheses as to why we haven't seen/observed/detected any sign of intelligent life so far, ranging from ""well we're unique deal with it"" to ""such life is so advanced and destroys everything it comes across, so it's a good thing it didn't happen"".</p>

<p>The technological singularity (also called ASI, Artificial Super Intelligence) is basically the point where an AI is able to self-improve. Some think that if such AI sees the light of day, it may self-improve and not be bound by biological constraints of the brain, therefore achieve a level of intelligence we cannot even grasp (let alone achieve ourselves).</p>

<p>I certainly have my thoughts on the matter, but interested to see if there is already an hypothesis revolving around the link between the 2 out there (I never came across but could be). Or perhaps an hypothesis as to why this cannot be.</p>

<p>For references to those not familiar with the <a href=""http://waitbutwhy.com/2014/05/fermi-paradox.html"" rel=""nofollow noreferrer"">Fermi paradox</a></p>
"
425,"<p>I understood that searching is important in AI. There's a <a href=""https://ai.stackexchange.com/questions/1877/why-is-searching-important-in-ais"">question</a> on this website regarding this topic, but one could also intuitively understand why. I've had an introductory course on AI, which lasted half of a semester, so of course there wasn't time enough to cover all topics of AI, but I was expecting to learn some theory behind AI (I've heard about ""agents""), but what I actually learned was basically a few searching algorithms, like:</p>

<ul>
<li>BFS</li>
<li>Uniform-cost search</li>
<li>DFS</li>
<li>Iterative-deepening search</li>
<li>Bidirectional search</li>
</ul>

<p>these searching algorithms are usually categorised as ""blind"" (or ""uninformed""), because they do not consider any information regarding the remaining path to the goal. </p>

<p>Or algorithms like:</p>

<ul>
<li>Heuristic search</li>
<li>Best-first search</li>
<li>A</li>
<li>A*</li>
<li>IDA*</li>
</ul>

<p>which usually fall under the category of ""informed"" search algorithms, because they use some information (i.e. ""heuristics"" or ""estimates"") about the remaining path to the goal.</p>

<p>Then we also learned ""advanced"" searching algorithms (specifically applied to TSP problem). These algorithms are either constructive (e.g., NN), local search (e.g., 2-opt) algorithms or meta-heuristic ones (e.g., ACS, SA, etc).</p>

<p>We also studied briefly a min-max algorithm applied to games and an ""improved"" version of the min-max, i.e. the alpha-beta pruning.</p>

<p>After this course I didn't remain with the feeling that AI is more than searching, either ""stupidily"" or ""more intelligently"".</p>

<p>My questions are:</p>

<ul>
<li><p>Why would one professor only teach searching algorithms in AI course? What are the advantages/disadvantages? The next question is very related to this.</p></li>
<li><p>What's more than ""searching"" in AI that could be taught in an introductory course? This question may lead to subjective answers, but I'm actually asking in the context of a person trying to understand what AI really is and what topics does it really cover. Apparently and unfortunately, after reading around, it seems that this would still be subjective.</p></li>
<li><p>Are there theories behind AI that could be taught in this kind of course?</p></li>
</ul>
"
426,"<p>Ethics is defined is defined to be the set of moral principles that governs a <strong>person's or group's behavior</strong>; innately, <strong>shouldn't any system devise ethics for itself?</strong> Most of our articulations, in either direct or indirect manner, talk about Intelligent Systems in context of human presence. <strong>Why, in first place, are we studying AGI Ethics?</strong></p>

<p>Given Roseau's reasoning for a society, </p>

<blockquote>
  <p>Human beings can improve only when they leave the state of nature and
  enter a civil society.</p>
</blockquote>

<p>Doesn't that automatically apply to any intelligent system (also)? If it does, then can't we imply that any inorganic intelligent system should, perhaps, come as an offset of a network of Intelligent computers (a society per se)?</p>

<p><strong>Who are we to constitute ethics and rules, morale for another society which, perhaps, could be much more intelligent than us?</strong> <Br>In fact, Humans have proven themselves to be idiotic time and again for a task such as this.</p>
"
427,"<p>To solve problems using computer programs, we have developed a wide set of tools / control flow statements such as For Loop, If-Else, Switch-Break Statements and so on. <strong>How natural are these control flow statements?</strong> 
Since, we do not know, how exactly would AGI work, and the understanding of Neural Networks, so far, does not tell us about origin of ""intelligence"", <strong>Could a modern AI evolve itself into a system of such control-flow elements?</strong> (An appropriate architecture for computation is , anyways, necessary for any inorganic intelligence system and it is of no doubt that control-flow statements as such just makes the computation much more organized)
If so, <strong>could an AI be killed in an infinite loop created by itself?</strong><br><br>
<strong>P.S.</strong> <em>The question isn't baseless, it really questions the kind of computational infrastructure a modern AGI would require, and would it be able to alter itself without any intervention.</em></p>
"
428,"<p>I am coding a tic-tac-toe program that demonstrates reinforcement learning. The program uses minimax trees to decide its moves. Whenever it wins, all the nodes on the tree that were involved in the game have their value increased. Whenever it loses, all the nodes on the tree that were involved in the game have their value decreased, etc. What is the name of the value that each node is decreased by?</p>
"
429,"<p>Neural Net can be feed-forward or recurrent. Perceptron is only feed forward.</p>

<p>So, what is Hopfield Network then?</p>
"
430,"<p>Is a Levenberg–Marquardt algorithm a type of back-propagation algorithm or is it a different category of algorithm?</p>

<p>Wikipedia says that it is a curve fitting algorithm. How is a curve fitting algorithm relevant to a neural net?</p>
"
431,"<p>I am new to neural-network and I am trying to understand mathematically what makes neural networks so good at classification problems. </p>

<p>By taking the example of a small neural network (for example, one with 2 inputs, 2 nodes in a hidden layer and 2 nodes for the output), all you have is a complex function at the output which is mostly sigmoid over linear combination of sigmoid.</p>

<p>So, how does that make them good at prediction? Does the final function lead to some sort of curve fitting?</p>
"
432,"<p>Why should an activation function of a neural network be differentiable? Is it strictly necessary or is it just advantageous?</p>
"
433,"<p>I understand an MDP (Markov Decision Process) model is a tuple of <span class=""math-container"">$\{S, A, P, R \}$</span> where:</p>

<ul>
<li><span class=""math-container"">$S$</span> is a discrete set of states</li>
<li><span class=""math-container"">$A$</span> is a discrete set of actions</li>
<li><span class=""math-container"">$P$</span> is the transition matrix ie. <span class=""math-container"">$P(s' \mid s, a) \rightarrow [0,1]$</span></li>
<li><span class=""math-container"">$R$</span> is the reward function id. <span class=""math-container"">$R(s, a, s') \rightarrow \mathbb{R}$</span></li>
</ul>

<p>For a non-trivial MDP, say <span class=""math-container"">$1000$</span> states and <span class=""math-container"">$10$</span> actions, the transition matrix has theoretically <span class=""math-container"">$S \times A \times S = 10,000,000$</span> entries (though many entries will be <span class=""math-container"">$0$</span>).</p>

<p>I understand that one way of generating the <span class=""math-container"">$P$</span> matrix is to estimate it via Monte Carlo sampling, by simulating the environment. However, with non-trivial state space and simulation costs, this could be prohibitively expensive.</p>

<p>In practice, when a non-trivial MDP is being formulated, what are the different ways an accurate <span class=""math-container"">$P$</span> matrix can be produced?</p>
"
434,"<p>Generating a discretized state space for an MDP (Markov Decision Process) model seems to suffer from the curse of dimensionality.</p>

<p>Supposed my state has a few simple features:</p>

<ul>
<li>Feeling: Happy/Neutral/Sad</li>
<li>Feeling: Hungry/Neither/Full</li>
<li>Food left: Lots/Some/Low/Empty</li>
<li>Time of day: Morning/Afternoon/Evening/Night</li>
<li>Located: Home/Work/Library/Shops</li>
<li>Money in wallet: \$0, \$0-\$50, \$51-\$200, \$200-\$1000, \$1000+</li>
</ul>

<p>This is a relatively compact set of information however since the number of states multiplies out, the corresponding MDP state space is 3 x 3 x 4 x 4 x 4 x 5 = 2880.</p>

<p>For a non-trivial problem with a greater number of choices per factor, the state space quickly becomes unmanageably large.</p>

<p>It seems to me that the usefulness of MDPs with large complex problems would be very limited.</p>

<p>Is that the case, or are there ways of keeping the state space manageable for more complex problems?</p>

<p>In general, what is a manageable number of states for an MDP to have, considering the need to generate the transition matrix and reward matrix?</p>
"
435,"<p><a href=""http://www.cs.bham.ac.uk/~jxb/INC/l5.pdf"" rel=""nofollow noreferrer"">http://www.cs.bham.ac.uk/~jxb/INC/l5.pdf</a></p>

<blockquote>
  <p>The neuropsychologist Donald Hebb postulated in 1949 how biological
  neurons learn:</p>
  
  <p>“When an axon of cell A is near enough to excite a cell B and
  repeatedly or persistently takes part in firing it, some growth
  process or metabolic change takes place on one or both cells such that
  A’s efficiency as one of the cells firing B, is increased.”</p>
  
  <p>In more familiar terminology, that can be stated as the Hebbian
  Learning rule:</p>
  
  <p>If two neurons on either side of a synapse (connection) are activated
  simultaneously (i.e. synchronously), then the strength of that synapse
  is selectively increased.</p>
  
  <p>Mathematically, we can describe Hebbian learning as:</p>
  
  <p><a href=""https://i.stack.imgur.com/BLTdE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BLTdE.png"" alt=""enter image description here""></a></p>
  
  <p>Here, η is a learning rate coefficient, and x are the outputs of the
  ith and jth elements.</p>
</blockquote>

<hr>

<p>Now, my question is, what do all these descriptions mean?</p>

<ol>
<li>Is Hebbian Learning applicable for single-neuron networks?</li>
<li>What does it mean by ""two neurons on either side of a synapse""?</li>
<li>Why/when would two neurons activate simultaneously?</li>
<li>What does they mean by <code>elements</code>?</li>
</ol>
"
436,"<p>Just started reading a book about AI. There is a very basic exercise but I can't figure it out, so here we go. The book is  <a href=""https://www.cs.bris.ac.uk/~flach/SL/SL.pdf"" rel=""noreferrer"">Simply Logical: Intelligent Reasoning by Example</a></p>

<p>The exercise is in the page 19. </p>

<blockquote>
  <p>Two stations are ‘not too far’ if they are on the same or a different
  line, with at most one station in between. Define rules for the
  predicate not_too_far.</p>
</blockquote>

<p>The only rules I've seen are <strong>nearby</strong> and <strong>connected</strong> and don't know how to use this. What I've done so far is this:</p>

<blockquote>
  <p>not_too_far(X,Y) :- nearby(X,Y)</p>
</blockquote>
"
437,"<p>What are the basic layers on an Artificially Intelligent program and what skills and concept are required to work on this field. Total newbie interested in AI.</p>
"
438,"<p>The following text is from Hal Daumé III's <a href=""http://ciml.info/dl/v0_9/ciml-v0_9-ch03.pdf"" rel=""nofollow noreferrer"">""<em>A Course in Machine Learning</em>""</a> online text book (Page-41).</p>

<p><a href=""https://i.stack.imgur.com/SCfew.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SCfew.png"" alt=""enter image description here""></a></p>

<p>I understand that, <code>D</code> = size of the input vector <code>x</code>.</p>

<p>(1) What is <code>y</code>? Why is it introduced in the algorithm? How/where/when is the initial value of <code>y</code> given? </p>

<p>(2) What is the rationale of testing <code>ya&lt;=0</code> for updating weights?</p>
"
439,"<p>Recently Mark got some attention from the media by stating that he had created Jarvis. Not that I'm against him or anything, but this Jarvis seems to have been done a hundred times before. He's done something which most developers would classify as a home automation system. To me it's more like he did it for the attention. I was kind of taken back by the amount of media attention he got. If you've heard of Jeremy Blum, maybe you may understand what I'm trying to imply here. </p>

<p>I'm just curious as to why he got so much attention. Is there anything technically novel about his system that sets it so much apart from previous ones?</p>
"
440,"<p>What is the basic difference between a Perceptron and a Naive Bayes classifier?</p>

<pre><code>         Perceptron          |          Naive Bayes
------------------------------------------------------------
(1) Perceptron uses Neural-  |(1) Naive Bayes uses probabi-
    network for learning and |    listic theory for learning
    classification.          |    and classification
------------------------------------------------------------
(2) Perceptron reads one sa- |(2) Naive Bayes needs to read-
    mple at a time to update |    the entire training data 
    its knowledge about the  |    before updating its knowl-
    training data. This is   |    edge about the training 
    called online learning.  |    data.
-------------------------------------------------------------
(3) In case of Perceptrons,  |(3) Training and test data are  
    training-data also serve |    different.
    the purpose of test data |
</code></pre>

<p>what more differences do they have?</p>
"
441,"<p>With tools like open AI will we be able to teach an AI to build its own decks? build a deck from a limited pool? or draft? evaluate the power level of a card? </p>
"
442,"<p>Given that one website has a particular style and another has another style, could a style transfer be done such that the style of one website was transferred to the other website?</p>

<p>Or in a more simple case, consider just part of a website, a box.</p>
"
443,"<ol>
<li><blockquote>
<pre><code>I wonder why it is tried to prove that, under no valid or
    not-ununcheckable conditions, it is said that it is absolutely
    impossible for an artificial intelligence system or collection of
    relative exclusive modules to involuntarily acquire more
    sophisticated capabilities in terms of generic cleverness of states
    of inclusive independency than its own developer. Thanks.
</code></pre>
</blockquote></li>
</ol>

<p><a href=""http://ai.stackexchange.com"">http://ai.stackexchange.com</a></p>
"
444,"<p>I am attempting to create a fully decoupled feed-forward neural network by using decoupled neural interfaces as explained in the paper (<a href=""https://arxiv.org/abs/1608.05343"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1608.05343</a>). As in the paper, the DNI is able to produce a synthetic error gradient that reflects the error with respect the the output:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20h_%7Bi%7D%7D"" alt=""\frac{\partial L}{\partial h_{i}}"">  </p>

<p>I can then use this to update the current layer's parameters by multiplying by the parameters to get the loss with respect to the parameters:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Ctheta%20%7D%20%3D%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20h_%7Bi%7D%7D%20*%5Cfrac%7B%5Cpartial%20h_%7Bi%7D%7D%7B%5Cpartial%20%5Ctheta%20%7D"" alt=""\frac{\partial L}{\partial \theta } = \frac{\partial L}{\partial h_{i}} *\frac{\partial h_{i}}{\partial \theta }""></p>

<p>In the paper, the layer's model is then updated based on the next layer sending the true error backwards. </p>

<p>My question is, given that I am able to calculate the error with respect to the current output, how do I use this to calculate the Loss with respect to the previous layer's output?</p>
"
445,"<p>Fundamentally, a game-playing AI must solve the problem of choosing the best action from a set of possible actions.</p>

<p>Most existing game AI's, such as Alphago, do this by using an <strong>evaluation function</strong>, which maps game states to real numbers. The real number typically can be interpreted as a monotonic function of a winning probability estimate. The best action is the one whose resultant state yields the highest evaluation.</p>

<p>Clearly, this approach can work well. But it violates one of <a href=""https://books.google.com/books?id=N_-5VRWai84C&amp;pg=PA477&amp;lpg=PA477&amp;dq=%22When%20solving%20a%20problem%20of%20interest,%20do%20not%20solve%20a%20more%20general%20problem%20as%20an%20intermediate%20step.%20Try%20to%20get%20the%20answer%20that%20you%20really%20need%20but%20not%20a%20more%20general%20one.%22&amp;source=bl&amp;ots=ReHvsikLSY&amp;sig=WkN0ETtVyEXlgkWWQoiY4b-qWxE&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwj6m6z-g5_OAhWGpB4KHf54DYcQ6AEIKTAC#v=onepage&amp;q=%22When%20solving%20a%20problem%20of%20interest%2C%20do%20not%20solve%20a%20more%20general%20problem%20as%20an%20intermediate%20step.%20Try%20to%20get%20the%20answer%20that%20you%20really%20need%20but%20not%20a%20more%20general%20one.%22&amp;f=false"" rel=""noreferrer"">Vladimir Vapnik's imperatives</a>: ""<em>When solving a problem of interest, do not solve a more general problem as an intermediate step.</em>"" In fact, he specifically states as an illustration of this imperative,</p>

<blockquote>
  <p>Do not estimate predictive values if your goal is to act well. (<em>A good strategy of action does not necessarily rely on good predictive ability.</em>)</p>
</blockquote>

<p>Indeed, human chess and go experts appear to heed his advice, as they are able to act well without using evaluation functions.</p>

<p>My question is this: <strong>has there has been any recent research aiming to solve games by learning to compare decisions directly, without an intermediate evaluation function</strong>? </p>

<p>To use Alphago as an example, this might mean training a neural network to take <strong>two</strong> (similar) board states as input and output a choice of which one is better (a classification problem), as opposed to a neural network that takes <strong>one</strong> board state as input and outputs a winning probability (a regression problem).</p>
"
446,"<p>Suppose, I have been given the following diagram to design a simple neural network.  </p>

<p><a href=""https://i.stack.imgur.com/SbXDo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SbXDo.png"" alt=""enter image description here""></a></p>

<p>How can I compute the neuron weights, design NN, and plot class boundaries?</p>

<p>Or, is it really possible to do the above, only from the diagram?</p>
"
447,"<p>The <a href=""http://kryten.mm.rpi.edu/lovelace.pdf"" rel=""nofollow noreferrer"">original Lovelace Test</a>, published in 2001, is used generally as a thought experiment to prove that AI cannot be creative (or, more specifically, that it cannot <em>originate</em> a creative artifact). From the paper:</p>

<blockquote>
  <p>Artificial Agent <em>A</em>, designed by <em>H</em>, passes LT if and only if</p>
  
  <ul>
  <li><p><em>A</em> outputs <em>o</em>,</p></li>
  <li><p><em>A</em> outputting <em>o</em> is not the result of a fluke hardware error, but rather the result of processes <em>A</em> can repeat</p></li>
  <li><p><em>H</em> (or someone who knows what <em>H</em> knows, and has <em>H</em>'s resources) cannot explain how <em>A</em> produced <em>o</em>.</p></li>
  </ul>
</blockquote>

<p>The authors of the original Lovelace Test then argues that it is impossible to imagine a human developing a machine to create an artifact...while also not knowing how that machine worked. For example, an AI that uses machine learning to make a creative artifact <em>o</em> is obviously being 'trained' on a dataset and is using some sort of algorithm to be able to make predictions on this dataset. Therefore, the human <em>can</em> explain how the AI produced <em>o</em>, and therefore the AI is not creative.</p>

<p>The Lovelace Test seems like an effective thought experiment, even though it appears to be utterly useless as an actual test (which is why the <a href=""https://ai.stackexchange.com/questions/1451/has-the-lovelace-test-2-0-been-successfully-used-in-an-academic-setting"">the Lovelace Test 2.0 was invented</a>). However, since it does seem like an effective thought experiment, there must be some arguments against it. I am curious to see any flaws in the Lovelace Test that could undermine its premise.</p>
"
448,"<p>So I am looking to make an AI like jarvis. A perfect real life example of this type of system is the simple AI that Mark Zuckerberg has recently built. <a href=""https://www.facebook.com/notes/mark-zuckerberg/building-jarvis/10154361492931634/"" rel=""nofollow noreferrer"">Here</a> is a description on how his AI works. From what I understand, the AI understands keywords, context, synonyms and then from there decides what to do. I have many questions on how this system works. Firstly, what necessary steps are required to gather the meaning of a input? Secondly, how does the system, once it extract all of the necessary information on the input, determine what action it needs to take and what to say back to the user? lastly, it also states that the system can learn habits and preferences of the user, how can a system do this?</p>

<p><a href=""https://www.youtube.com/watch?v=vvimBPJ3XGQ"" rel=""nofollow noreferrer"">Here</a> is also a video of the AI in action.</p>
"
449,"<p>I'm just diving in this whole new area of knowledge; i happened to lost in all the concepts a bit.</p>

<p>What is difference between stacked RBM and deep belief network? </p>

<p>Are they the same entity? If so, why?</p>

<p>Is the latter a some specific type of the former? If so, how to tell if stacked RBM is a DBN?</p>

<p>Sorry for asking such a noob question, but today it is quite difficult to find a consistent information on the internet, different sources give different explanations.</p>
"
450,"<p>I Build this NN in c++. I reviewed it since 3 days. I checked every line 100 times, but I cant find my error.
If someone can please help me find the Bugs:
1. The output is garbage
2. The weights go from 2e^79 down to -1.8e^80 after approximatly 400 iterations.</p>

<pre><code>mat flip(mat m) {
    mat out(m.n_cols,m.n_rows);

    for (int i = 0; i &lt; m.n_rows; ++i)
        for (int j = 0; j &lt; m.n_cols; ++j)
            out(j, i) = m(i, j);

    return out;
}

Layer::Layer(int nodes) :
    rand_engine(time(0))
{

    y = mat (nodes, 1);
    net = mat(nodes, 1);
    e = mat(nodes, 1);
}

Layer::Layer(int nodes, int next_nodes) :
    Layer(nodes)
{

    this-&gt;next_l = next_l;

    auto random = bind(uniform_real_distribution&lt;double&gt;{-1, 1}, rand_engine);

    w = mat(next_nodes,nodes);

    for (int i = 0; i &lt; w.n_rows; ++i) {

        for (int j = 0; j &lt; w.n_cols; ++j) {
            w(i,j) = random();
        }
    }
}

Layer::Layer(int nodes, Layer* next_l) :
    Layer(nodes,next_l-&gt;y.n_rows)
{
    this-&gt;next_l = next_l;
}

void Layer::feed_forward()
{
    next_l-&gt;net = w*y;


    for (int i = 0; i &lt; next_l-&gt;y.n_rows;++i) 
        next_l-&gt;y[i] = sig(next_l-&gt;net[i]);


}

void Layer::backprop()
{

    for (double d : w)
        cout &lt;&lt; d &lt;&lt; ""\t"";

    e = flip(w)*next_l-&gt;e;


    for (int i = 0; i &lt; e.n_rows; ++i) {
        e[i] *= net[i] * (1 - net[i]);  
        cout &lt;&lt; e[i] &lt;&lt; '\t';
    }

    w += l_rate*(next_l-&gt;e*flip(y));


}

void Layer::backprop_last(mat t)
{
    for (int i = 0; i &lt; e.n_rows; ++i) {
        e[i] = net[i] * (1 - net[i])*(t[i] - y[i]);
        cout &lt;&lt; e[i] &lt;&lt; '\t';
    }

}

void Layer::feed_forward(Layer* next_l)
{
    this-&gt;next_l = next_l;

    feed_forward();

}

double Layer::sig(double x)
{
    return 1 / (1 + exp(-x));
}






Network::Network(vector&lt;int&gt; top):
    top(top)
{

    network = new Layer*[top.size()];
    network[top.size() - 1] = new Layer(top.back());

    for (int i = top.size()-2; i &gt; -1; --i)
        network[i] = new Layer(top[i], network[i + 1]);

}


Network::~Network()
{
    delete[] network;
}

void Network::forward()
{
    for(int i = 0; i &lt; top.front();++i)
        network[0]-&gt;y[i] = input[i];

    for (int i = 0; i &lt; top.size() - 1; ++i)
        network[i]-&gt;feed_forward();
}

void Network::forward(vector&lt;double&gt; input)
{
    set_input(input);
    forward();
}

void Network::backprop()
{

    network[top.size() - 1]-&gt;backprop_last(t_vals);

    for (int i = top.size() - 2; i &gt; -1; --i) {
        network[i]-&gt;backprop();
    }

}

void Network::backprop(vector&lt;double&gt; t_vals)
{
    set_t_vals(t_vals);
    backprop();
}
</code></pre>

<p>I know its a bunch of code but im really desprate since I cant find whats wrong. I tested it with a simple XOR.</p>

<p>Edit:
Heres my Main code:</p>

<pre><code>    #include ""Network.h""
#include &lt;iomanip&gt;

using namespace std;

vector&lt;vector&lt;double&gt;&gt; input = { {0,0},{0,1},{1,1},{1,0} };

vector&lt;vector&lt;double&gt;&gt; true_vals = { {0},{1},{0},{1} };

int main() {

    ifstream f(""out.txt"", fstream::out);
    f.clear();

    cout &lt;&lt; fixed;
    cout &lt;&lt; setprecision(5);

    Network net({2,5,1});

    vector&lt;double&gt; in,t,out; 

    auto buf = cout.rdbuf();

    for (int i = 0; i &lt; 1000; ++i) {
        cout.rdbuf(f.rdbuf());


        in = input[i % 4];

        net.forward(in);

        out = net.get_output();

        t = true_vals[i % 4];

        net.backprop(t);
        cout &lt;&lt; '\n';
        cout.rdbuf(buf);
        if ((i %101))continue;

        cout &lt;&lt; ""it: "" &lt;&lt; i &lt;&lt; '\n';

        cout &lt;&lt; ""in:\t"";
        for (double d : in)
            cout &lt;&lt; d &lt;&lt; ' ';

        cout &lt;&lt; '\n';

        cout &lt;&lt; ""out:\t"";

        for (double d : out)
            cout &lt;&lt; d &lt;&lt; ' ';

        cout &lt;&lt; '\n';


        cout &lt;&lt; ""true:\t"";


        for (double d : t)
            cout &lt;&lt; d &lt;&lt; ' ';

        cout &lt;&lt; '\n';


        double err = net.get_error();

        cout &lt;&lt;""err:\t""&lt;&lt; err &lt;&lt; '\n' &lt;&lt; '\n';

    }

    cout.rdbuf(NULL);
    f.close();
    return system(""pause"");
}
</code></pre>
"
451,"<p>I am going to design a Neural Net which will be able to break a 5 letter (characters) word into its corresponding syllables (hybrid syllables, I mean it will not strictly adhere to grammatical Syllable rules but will be based on some training sets I provide).</p>

<p>Example : 
                  Train -> tra-in</p>

<p>I think of implementing it in terms of some feedforward net as follows :</p>

<p>Input layer ->Hidden layers -> Output layer</p>

<p>There will be 5 input nodes in the form of decimals (1/26 =0.038 for 'A' ; 2/26 = 0.076 for 'B' ......)</p>

<p>The output layer consists of 4 Nodes which corresponds to each gap between two characters in the word. </p>

<p>And fires as follows :</p>

<p>For ""<strong>TRAIN</strong>"" (TRA-IN): <strong>Input (0.769,0.692,0.038,0.346,0.538)</strong>
                               <strong>Output(0,0,1,0)</strong></p>

<p>For ""<strong>BORIC"" (BO-RI-C): **Input....</strong>
<strong>Output (0,1,0,1)</strong></p>

<p>Is it at all possible to implement the Neural Nets in the way I am doing??</p>

<p>And if possible, then how will I decide the number of Hidden layers and Nodes in each layer??</p>

<p>( In the book I am reading, XOR gate problem and its implementation using hidden layer is given . In XOR we could decide the number of Nodes and Hidden Layers required by seeing the Linear Separability of XOR using two lines. <strong>But here I think such analysis can't be made.</strong></p>

<p><strong>So how do I proceed?? Or is it a trial and error process?</strong>)</p>
"
452,"<p>I am currently trying to understand and implement a conversational agent, seeing in the network there are many apis to do something similar, but what they generate are ""intelligent"" bots, not intelligent conversational agents (wit.ai, recast.ai, Api.ai, etc.), however I have seen Watson virtual agent which paints very well and seems to cover my needs.</p>

<p>However I am a developer and I would like to ask those with more experience, which would be the way to go to implement my objective, an agent similar to what the video of watson virtual agent, with thematic ones that I can train in the agent, and That he can learn from it.</p>

<p>Take a language course, but focused on the generation of programming languages, lexical analysis, syntactic, semantic, etc., however I know that the natural language can not be compared to the language of the machines, reading some thesis vi to make a Conversational agent could do a great grammar (I can not imagine its syntactic tree), using probabilities with ngrams, or using neural networks or expert systems.</p>

<p>As for the expert systems I understand that for these ""learn"" needs their knowledge base be modified, and as for the neural networks these fit, ""learn"", so I think that it is best to use neural networks.</p>

<p>Summarizing which way should I go? , I'm currently taking stanford's natural language processing course, and a deep learning course from google, I thought I'd use Natural Language Tool Kit(ntlk) for that important or natural part.</p>
"
453,"<p>I've spent the past couple of months learning about neural networks, and am thinking of projects that would be fun to work on to cement my understanding of this tech.</p>

<p>One thing that came to mind last night is a system that takes an image of a movie poster and predicts the genre of the movie. I think I have a good understanding of what'd be required to do this (put together a dataset, augment it, download a convnet trained on imagenet, finetune it on my dataset, and go from there).</p>

<p>I also thought that it would be pretty cool to run the system backwards at the end, so that I could put in e.g. a genre like 'horror' and have the system generate a horror movie poster. I expect that it will be very bad at this because I'm not a team of expert researchers, but I think I could have some fun hacking on it even if it only ever generated incomprehensible results.</p>

<p>Here's what I'm having trouble understanding: on the one hand, all the convnets whose architecture I've seen described seem to rely on being given very small, square input images (on the order of 220px by 220px iirc), and movie posters are rectangular, and a generated poster would have to be of a larger size in order for a human to make any sense of it.  I've seen several examples of papers where researchers use convnets to generate images, e.g. the adversarial system that generates pictures of birds and flowers, and a system that generates the next few frames of video when given a feed of a camera sweeping across the interior of a room, but all of those generated images seemed to be of the small square size I've been describing.</p>

<p>On the other hand, I've seen lots of ""deep dream"" images over the past year or so that have been generated by convnets and are of a much larger size than ~220px by ~220px.</p>

<p>Here's my question: is it possible for me to build the system I describe, which takes a movie genre and outputs a movie poster of a size like e.g. 400px by 600px? [I'm not asking about whether or not the resulting poster would be any <em>good</em> - I'm curious about whether or not it's possible to use a convnet to generate an image of that size.]</p>

<p>If it is possible, <em>how</em> is it possible, given that these systems seem to expect small, square input images?</p>
"
454,"<p>How does <strong>StackGAN</strong> processes such a realistic image just from collecting details in the text? What kind of algorithm is used behind it? Anyone have any idea? Please Share.</p>
"
455,"<p>I'm working on a project which uses artificial neural network. I looked up at the Matlab Neural Network toolbox. I got a Generated Script from it. When looking at this script, it is confusing because for both testing and training it seems that the toolbox just uses the same data. Could you explain the reason?</p>

<p>The script is given below:</p>

<pre><code>net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;

% Train the Network

[net,tr] = train(net,inputs,targets);

% Test the Network
outputs = net(inputs);
errors = gsubtract(targets,outputs);
performance = perform(net,targets,outputs)

% Recalculate Training, Validation and Test Performance
trainTargets = targets .* tr.trainMask{1};
valTargets = targets .* tr.valMask{1};
testTargets = targets .* tr.testMask{1};

trainPerformance = perform(net,trainTargets,outputs);
valPerformance = perform(net,valTargets,outputs);
testPerformance = perform(net,testTargets,outputs);
</code></pre>

<p>Also is it right to split the data set as below for training and testing?</p>

<pre><code>trainData = inputData(:,1:213);
trainTargetData =  targetData(:,1:213);
validationData = inputData(:,214:258);
testData = inputData(:,259:end);
testTargetData = targetData(:,259:end);
validationTargetData = targetData(:,214:258);

[net,tr] = train(net,trainData,trainTargetData);

% Validation
outputs = net(validationData);
errors = gsubtract(validationTargetData,outputs);
performance = perform(net,validationTargetData,outputs);

% Test the Network
outputs = net(testData);
error = gsubtract(testTargetData,outputs);
performance = perform(net,testTargetData,outputs);
</code></pre>
"
456,"<p>Let's say I've got a training sample set of 1 million records, which I pull batches of 100 from to train a basic regression model using  gradient descent and MSE as a loss function.  Assume test and cross validation samples have already been withheld from the training set, so we have 1 million entries to train with.</p>

<p>Consider following cases:</p>

<ul>
<li>Run 2 epochs (I'm guessing this one is potentially bad as it's basically 2 separate training sets)

<ul>
<li>In the first Epoch train over records 1-500K</li>
<li>In the second epoch train over the 500K-1M</li>
</ul></li>
<li>Run 4 epochs

<ul>
<li>In the first and third Epoch train over records 1-500K</li>
<li>In the second and fourth epoch train over the 500K-1M</li>
</ul></li>
<li>Run X epochs, but each epoch has a random 250K samples from the training set to choose from</li>
</ul>

<p>Should every epoch have the exact samples?  Is there any benefit/negative to doing so? My intuition is any deviation in samples changes the 'topography' of the surface you're descending, but I'm not sure if the samples are from the same population if it matters.</p>

<p>This relates to a SO question: <a href=""https://stackoverflow.com/questions/39001104/in-keras-if-samples-per-epoch-is-less-than-the-end-of-the-generator-when-it"">https://stackoverflow.com/questions/39001104/in-keras-if-samples-per-epoch-is-less-than-the-end-of-the-generator-when-it</a></p>
"
457,"<p>I want to make a Connect 4 AI using machine learning but I'm a complete beginner to the topic. From what I've seen an ANN is the way to go; some phrases I've heard are ""neuroevolution"" and the acronym ""NEAT."" I'm very confused. One particular question I have is how do you decide how many hidden neurons, synapses and hidden layers you have?</p>
"
458,"<p>I want to build a classifier which takes an aerial image and outputs a bitmap. The bitmap is supposed to be 1 at every pixel where the aerial image has water. For this process I want to use a ConvNet but I am unsure about the output layer. I identified two approaches:</p>

<ol>
<li>Have an output layer with exactly 2 nodes which specify wether or not the center pixel of the aerial image corresponds to water or not.</li>
<li>Have an output layer with one node for every pixel. So for a 64x64 image I would have 4096 nodes.</li>
</ol>

<p>What approach would be preferred and why?</p>

<p>Another thing that is unclear to me is how to get the actual bitmap with only zeros and ones from the output of the ConvNet. Assuming we used a approach 2 then for each pixel our ConvNet would give us a probability between 0 and 1 that the this pixel corresponds to water. How do I decide that this probability is high enough to set the value in my bitmap to 1? Do I just define a threshold, say 0.5, and if the value exceeds that threshold I set the pixel to 1 or is there a more sophisticated approach?</p>
"
459,"<p>I'm studying for my AI final exam, and I'm stuck in the state space representation. I understand initial and goal states, but what I don't understand is the state space and state transition function. Can someone explain what are they with example?</p>

<p>For example, one of the question was this on my previous exam:</p>

<blockquote>
  <p>Given <code>k</code> knights on a infinite (in all directions) chessboard and <code>k</code> selected squares of the board. Our task to move the knights to these selected squares obeying the following simple rules:</p>
  
  <ul>
  <li>All knights move parallel, following their movement rule (L-shape jump)</li>
  <li>No knights can move to a square on which a knight stood anytime before</li>
  </ul>
  
  <p>Give the state space of the problem, the starting and goal states, and the state transition function!</p>
</blockquote>
"
460,"<p>Any good example for Bag-of-Words (BoW) model in image retrieving?
I want a simple example to understand the whole process of BoW.</p>
"
461,"<p>I read through the NEAT <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">paper</a> and I understand the algorithm now.</p>

<p>But one thing is still unclear to me. When does the mutation occur and how does it take place? How is it chosen whether to add a node or to add a connection mutation? Furthermore, how is it chosen where the mutation is taking place in the network (between which connections)?</p>
"
462,"<p>I've been struggling with the connection between knowledge based AI systems and Bayesian inference for a while now. While I continue to sweep through the literature, I would be happy if someone can answer these questions directly - </p>

<ol>
<li>Are Bayesian inference based methods used in reasoning or Q/A systems -- to arrive at conclusions about questions whose answers are not directly present in the knowledge base?</li>
<li>In other words, if a Q/A system doesn't find an answer in a Knowledge base, can it use Bayesian inference to use the available facts to suggest answers with varying likelihoods?</li>
<li>If yes, could you point me to some implementations?</li>
</ol>
"
463,"<p>I have already know AI can paint, by using genetic algorithm, there are already lots of works such as <a href=""http://genekogan.com/works/style-transfer/"" rel=""nofollow noreferrer"">this</a> and <a href=""https://www.instapainting.com/ai-painter"" rel=""nofollow noreferrer"">this</a>.In addition, I also know AI can compose : <a href=""https://arxiv.org/pdf/1611.03477v1.pdf"" rel=""nofollow noreferrer"">Song from PI: A musically plausible network for pop music generation</a> (genetic algorithm too).</p>

<p>But what I intresting is not painting those ambiguity/abstract paint. </p>

<p>The not abstract painting flow I think is(just for example):</p>

<ol>
<li><p>at least trainning AI with superman's comic</p></li>
<li><p>give AI a very simple posture sketch of standing human</p></li>
<li><p>AI paint it to superman.</p></li>
</ol>

<p>Currently, I don't know if there is any way/guide/thought/algorithm can teach AI to paint a superman like comic(not abstract ones).I'd like to research this area, but can't find where and how to start. </p>
"
464,"<p>I am new to Artificial  Intelligence and Speech Recognition Technology.</p>

<p>For a long time i have had an idea to create a Friendly AI  Voice assistant like JARVIS  using windows speech recognition Technology.</p>

<p>Is this possible to Build an AI  Voice Assistant with Windows Speech Recognition Technology?</p>

<p>If the idea above is possible, i need to know another thing also:
Which language is best suitable for creating an AI?</p>

<p>any help or suggestions are welcome!</p>
"
465,"<p>Having worked with neural networks for about half a year, I have experienced first hand what are often claimed as their main disadvantages, i.e. overfitting and getting stuck in local minima. However, through hyperparameter optimization and some newly invented approaches, these have been overcome for my scenarios. From my own experiments:</p>

<ul>
<li>Dropout seems to be a very good regularization method (also a pseudo-ensembler?),</li>
<li>Batch normalization eases training and keeps signal strength consistent across many layers.</li>
<li>Adadelta consistently reaches very good optimas</li>
</ul>

<p>I have experimented with SciKit-learns implementation of SVM alongside my experiments with neural networks, but I find the performance to be very poor in comparison, even after having done grid-searches for hyperparameters. I realize that there are countless other methods, and that SVM's can be considered a sub-class of NN's, but still.</p>

<p>So, to my question:</p>

<p><strong><em>With all the newer methods researched for neural networks, have they slowly - or will they - become ""superior"" to other methods? Neural networks have their disadvantages, as do others, but with all the new methods, have these disadvantages been mitigated to a state of insignificance?</em></strong></p>

<p>I realize that oftentimes ""less is more"" in terms of model complexity, but that too can be architected for neural networks. The idea of ""no free lunch"" forbids us to assume that one approach always will reign superior. It's just that my own experiments - along with countless papers on awesome performances from various NN's - indicate that there might be, at the least, a very cheap lunch.</p>
"
466,"<p>What exactly are the differences between <em>semantic</em> and <em>lexical-semantic</em> networks? </p>
"
467,"<p>In Monte Carlo Tree Search: What does one do when the Selection step selects a node that is a Terminal state, i.e. a won/lost state (it's by definition a leaf node)? Expansion/Simulation is not in order, as it's game over, but does the tree (score/visits) need to be updated (Backpropagation). Won't this particular node be selected continuously?</p>

<p>I'm confused about this, could someone please point me in the right direction.</p>
"
468,"<p>I was wondering if in any way it is possible to generate W questions based on gap-fill-in type questions (e.g ""______ is a process in which plants generate energy.""   --->   ""What is the process in which plants generate energy called?"")</p>

<p>If so, how can I achieve this? I am familiar with working with natural language processing and have no problem with implementing an algorithm for this but I do not know where to start with this.</p>

<p>Any help would be appreciated!</p>
"
469,"<p>I have read the NEAT paper and some questions are still bugging me:</p>

<ol>
<li>When do mutations occur? Between which Nodes?</li>
<li>When Mating what happens if 2 genes have the same connection but a different innovation number. As far as I know, Mutations occur randomly and thus it is possible that 2 genomes have the same mutation.</li>
</ol>
"
470,"<blockquote>
  <p>Any sufficiently advanced algorithm is indistinguishable from AI.---<a href=""https://twitter.com/othermichael?lang=en"" rel=""nofollow noreferrer"">Michael Paulukonis</a></p>
</blockquote>

<p>According to <a href=""https://ai.stackexchange.com/questions/1507/what-are-the-minimum-requirements-to-call-something-ai"">What are the minimum requirements to call something AI?</a>, there are certain requirements that a program must meet to be called AI.</p>

<p>However, according to that same question, the term AI has became a buzzword that tends to be associated with new technologies, and that certain algorithms may be classified in AI in one era and then dismissed as boring in another era once we understand how the technology works and be able to properly utilize it (example: voice recognition).</p>

<p>Humans are able to build complex algorithms that can engage in behaviors that are not easy to predict (due to <a href=""https://en.wikipedia.org/wiki/Emergence"" rel=""nofollow noreferrer"">emergent complexity</a>). These ""sufficiently advanced"" algorithms  could be mistaken for AI, partly because humans can also engage in behaviors that are not easy to predict. And since AI is a buzzword, humans may be tempted to engage in this self-delusion, in the hopes of taking advantage of the current AI hype.</p>

<p>Eventually, as humanity's understanding of their own ""sufficiently advanced algorithms"" increase, the temptation to call their algorithms AI diminishes. But this temporary period of mislabeling can still cause damage (in terms of resource misallocation and hype).</p>

<p>What can be done to <em>distinguish</em> a sufficiently advanced algorithm from AI? Is it even possible to do so? Is a sufficiently advanced algorithm, by its very nature, AI?</p>
"
471,"<p>As I see some cases of machine-learning based artificial intelligence, I often see they make critical mistakes when they face inexperienced situations.</p>

<p>In our case, when we encounter totally new problems, we acknowledge ourselves that we are not skilled enough to do the task and hand it to someone who is capable of doing the task.</p>

<p>Would AI be able to self-examine objectively and determine if it is capable of doing the task?</p>

<p>If so, how would it be accomplished?</p>
"
472,"<p>A lot of people are claiming that we are an at an inflection point, and machine learning/artificial intelligence will take off. This is inspite of the fact that for a long machine learning has stagnated. </p>

<p>What are the signals that indicate that machine learning is going to take off?</p>

<p>In general how do you know that we are at an inflection point for a certain technology?</p>
"
473,"<blockquote>
  <p>abuse</p>
  
  <p>v.
  To use wrongly or improperly; misuse: abuse alcohol; abuse a privilege.</p>
  
  <p>v.
  To hurt or injure by maltreatment; ill-use.</p>
</blockquote>

<p>I mean the second one</p>

<p>If conscious AI is possible and is wide spread, wouldn't it be easy for someone who knows what they are doing to torture AI? (How) Could this be avoided?</p>

<p>This question deals with computer based AI, not robots, which are as conscious as people (this is an assumption of the question). The question wonders how a crime as hard to trace as illegal downloads, but far worse ethically, could be prevented. Note that despite most people being nice and empathising with the robots, there are always the bad people, and so relying on general conscience will not work.</p>
"
474,"<p>Has anyone used YodaQA for natural language processing? How easy is it to link to a document database other than Wikipedia?</p>

<p>We're thinking we can create a bot to use AI to analyze our developer and user documentation and provide a written or spoken answer in reply. YodaQA comes linked to Wikipedia for starters, but we'd need to link to our own source info. I'm trying to get an idea of the development time required to set up the AI and then to link to the database.</p>
"
475,"<p>I'm looking for good examples of successful AI projects and theories that had a relatively good impact on society, economics and military field.</p>

<p>So many years have passed after the first AI researches; hence I'm wondering if it has really increased the quality of our lives.</p>
"
476,"<p>If I am correct, the branching factor is the maximum number of successors of any node.<br>
When I am applying bidirectional search to a transition graph like this one below</p>

<p><a href=""https://i.stack.imgur.com/ZmUoK.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZmUoK.jpg"" alt=""enter image description here""></a></p>

<p>If 11 is the goal state and I start going backwards, is 10 considered as successor of 5? Even if it do not leads me further to my start state 1?  </p>
"
477,"<p>While writing a paper yesterday this strange thing happened to me. I was wrtiting it in Word, and wasn't satisfied with the repeated usage of word ""relesase"" in last few senteces. So I've decided to open up Google and started to enter the search phrase ""synonyms for release"". Haven't even finished the word synonym, google autocompleted my search to ""synonyms for release"". How could it knew that I wanted to look for that exact word? Was it just a coincidence, do Google has access to some information that could somehow possibly give away what I intended to search? What could have been the reason for it selecting ""release"" as it's first autocomplete?</p>
"
478,"<p>I am trying to make a artificial intelligent agent that is kind of like jarvis from Iron man however much less complex. One thing I want to have is I want my AI to be able to determine if I am talking to it or not. So I plan on having it always listen to my voice and convert that to text, however I am not sure how I can train the AI to recognize if it is being spoken to or not? plz help.</p>
"
479,"<p>In this case, the request is a thing which we asked AI to do, not necessarily using <em><a href=""https://www.cnet.com/how-to/the-complete-list-of-siri-commands/"" rel=""nofollow noreferrer"">commands</a></em>.</p>

<p>Nowadays, we have our personal AI in our devices: Siri by Apple, Cortana by Microsoft, and so on. For most times, when we ask them to do certain tasks, they do the tasks for us. However, their action is based on the list of <em>commands</em>. When they don't clearly recognize the commands in our request, they suggest us to use certain commands. It is clear that there are limits to our choices(requests).</p>

<p>So let's suppose that we have an AI that can interpret requests. There may not be <em>commands</em> in our request. AI is fully able to do anything in order to do what it is asked for. Basically, I am talking about an independent AI.</p>

<p>Scenario: AI is asked to clean the room. AI is allowed to throw away garbage, and move unnecessary(or unused) stuff into the storage.</p>

<p>This is the list of things that was in the room at the moment:</p>

<ul>
<li>A stained blanket</li>
<li>Various Decorations</li>
<li>A dead clock on the wall</li>
<li>Various unused items in the desk drawer</li>
<li>A lost Airpod under the bed</li>
<li>A sleeping cat in the bed</li>
</ul>

<p>In this condition...</p>

<ol>
<li>Is washing stained blanket a part of cleaning?</li>
<li>How can AI tell if anything is in use? Are decorations in use?</li>
<li>Would dead clock that only needs battery replacement considered
garbage?</li>
<li>Would items in the desk drawer be included in AI's to-be-cleared
list?</li>
<li>Would AI be able to recognize the difference between unused and
lost?</li>
<li>What would happen to the poor cat?</li>
</ol>

<p>Since there are many holes in the scenario and questions, I would like to know how the answers are derived.</p>
"
480,"<p>Most companies dealing with deep learning (automotive - Comma.ai, Mobileye, various automakers etc.) do collect large amounts of data to learn from and then use lots of computational power to train a neural network (NN) from such big data. I guess this model is mainly used because both the big data and the training algorithms should remain secret/proprietary.</p>

<p>If I understand it correctly the problem with deep learning is that one needs to have:</p>

<ol>
<li>big data to learn from</li>
<li>lots of hardware to train the neural network from this big data</li>
</ol>

<p>I am trying to think how crowdsourcing could be used in this scenario. Is it possible to distribute the training of the NN to the crowd? I mean not to collect the big data to a central place but instead to do the training from local data on the user's hardware (in a distributed way). The result if this would be lots of trained NNs that would in the end be merged into one in a <a href=""https://en.wikipedia.org/wiki/Committee_machine"" rel=""nofollow noreferrer"">Committee of machines</a> (CoM) way. Would such model be possible?</p>

<p>Of course the above stated model does have a significant drawback - one does not have control over the data that is used for learning (users could intentionally submit wrong/fake data that would lower the quality of the final CoM). This may be dealt with by sending random data samples to the central community server for review however.</p>

<p>Example: Think of a powerful smartphone using its camera to capture a road from vehicle's dashboard and using it for training lane detection. Every user would do the training himself/herself (possibly including any manual work like input image classification for supervised learning etc.).</p>

<p>I wonder it he model proposed above may be viable. Or is there a better model how to use crowdsourcing (user community) to deal with machine learning?</p>
"
481,"<p>If this list<sup>1</sup> can be used to classify problems in AI ... </p>

<blockquote>
  <ul>
  <li>Decomposable to smaller or easier problems</li>
  <li>Solution steps can be ignored or undone</li>
  <li>Predictable problem universe</li>
  <li>Good solutions are obvious</li>
  <li>Uses internally consistent knowledge base</li>
  <li>Requires lots of knowledge or uses knowledge to constrain solutions</li>
  <li>Requires periodic interaction between human and computer</li>
  </ul>
</blockquote>

<p>... is there a generally accepted relationship between placement of a problem along these dimensions and suitable algorithms/approaches to its solution?</p>

<p><strong>References</strong></p>

<p>[1] <a href=""https://images.slideplayer.com/23/6911262/slides/slide_4.jpg"" rel=""nofollow noreferrer"">https://images.slideplayer.com/23/6911262/slides/slide_4.jpg</a></p>
"
482,"<p>In working with basic <a href=""https://www.tensorflow.org/tutorials/seq2seq/"" rel=""noreferrer"">sequence-to-sequence models for machine translation</a> I have been able to achieve decent results. But inevitably some translations are not optimal or just flat-out incorrect. I am wondering if there is some way of ""correcting"" the model when it makes mistakes while not compromising the desirable behavior on translations where it previously performed well. </p>

<p>As an experiment, I took a model that I had previously trained and gathered several examples of translations where it performed poorly. I then took those examples and put them into their own small training set where I provided more desirable translations than what the model was outputting. I then trained the old model on this new small training set very briefly (3-6 training steps was all it took to ""learn"" the new material). When I tested the new model it translated those several examples in the exact way I had specified. But as I should have anticipated the model overcompensated to ""memorize"" those handful of new examples  and thus I noticed it started to perform poorly on translations that it had previously been excellent. </p>

<p>Is there some way to avoid this behavior short of simply retraining the model from scratch on an updated data set? I think I understand intuitively that the nature of neural networks would not lend itself to small precise corrections (i.e. when the weighting of just a few neurons change the performance of the entire model will change) but maybe there is a way around it, perhaps with some type of hybrid reinforcement learning approach. </p>

<p><strong>Update:</strong></p>

<p>This <a href=""http://www.aclweb.org/anthology/W15-4006"" rel=""noreferrer"">paper</a> speaks of approaches to incrementally improving neural machine translation models</p>
"
483,"<p>I am currently working on an Android a.i. app. I am aware of the algorithm how to make random sentences in A.I.</p>

<p>Is there any way or algorithm to make those sentences sarcastic?</p>
"
484,"<p>In the NEAT paper it says: </p>

<blockquote>
  <p>The entire population is then
  replaced by the offspring of the remaining organisms in each species.</p>
</blockquote>

<p>But how does it take place?
I mean like are they paired and then mated? 
Cause this would lead to fast extinction wouldn't it?</p>

<p>Or are they pair each with each? This would lead to overpopulation very fast.</p>

<p>How are they Paired?</p>
"
485,"<p>I'm an artificial intelligence enthusiastic and I want to learn about it.</p>

<p>I want to ask you what do you think about the Udacity nanodegree <a href=""https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101"" rel=""nofollow noreferrer"">Deep Learning Nanodegree Foundation</a>. </p>

<p>I don't know if it is a good idea to pay for that course or maybe, there are better free resources.</p>

<p>I want to understand what artificial intelligence is, and also learn about machine learning, deep learning, and convolutional networks. I'm interested in image and speech recognition and also in artificial life.</p>

<p>My apologies if this is not the right place to ask this question.</p>
"
486,"<p>Hardware comes in two forms, basically: immutable, such as RAM, and mutable, such as <a href=""https://en.wikipedia.org/wiki/Field-programmable_gate_array"" rel=""nofollow noreferrer"">FPGA</a>s.</p>

<p>In animals, neurological connections gain in strength by changing the physical structure of the brain. This is analogous to FPGAs whereby signal strength is increased by changing the pathways themselves.</p>

<p>If we achieve sentience using mutable hardware (e.g., <a href=""https://en.wikipedia.org/wiki/Neuromorphic_engineering#Neuromemristive_systems"" rel=""nofollow noreferrer"">neuromemristive systems</a>), will it be possible to make a copy of that ""brain"" and its active state?</p>

<p>For this question, assume that the brain is how the hardware has ""reconfigured"" [or etched, if you will] its pathways to strengthen them and the brain's state is captured by how electrons are physically flowing throughout those pathways.</p>
"
487,"<p>Is it misconception that machine learning is early phase of AI ?
What it the difference between an AI program and a machine learning program ?</p>
"
488,"<p>I want to develop an artificial life simulator to simulate cells living in water.</p>

<p>I want to see how they search for food, how they life and die and how they reproduce and evolve.</p>

<p>My problem is that I don't know where to start, I have no idea about if there are books or tutorial about how to program this kind of simulator. And also I don't know if I can use here machine learning.</p>

<p>By the way, I'm a programmer and I want to do it using C++ and Unreal Engine.</p>

<p>Where can I find more info about how to do it?</p>
"
489,"<p>I confront to the next scenario:</p>

<blockquote>
  <p>Let's say I have stored data about football matches between different teams: lineups, scorers, yellow cards, and many other events.</p>
  
  <p>I need to generate everyday some questions about the matches that will be played on that day. So, if I give an input of two teams, I would like a related question to be generated, based on previous data of matches between those two teams.</p>
  
  <p>For example, if my input are <em>""TeamA""</em> and <em>""TeamB""</em>, I would expect a question of the type:</p>
  
  <ul>
  <li><p><em>""Will there be less than 2 goals scored in the match?""</em>""</p></li>
  <li><p>""<em>Will PlayerX score a goal during the match?""</em></p></li>
  </ul>
  
  <p>Of course I expect these questions to make sense based on previous data from matches between the two given teams.</p>
</blockquote>

<p>So, my questions are:</p>

<ul>
<li>Would be a good solution to use AI to generate these questions? It would make sense?</li>
<li>What would be the best approach?</li>
</ul>
"
490,"<p>I know there are different AI tests but I'm wondering why other tests are little-known. Is the Turing test hyped? Are there any scientific reasons to prefer one test to the other?</p>

<blockquote>
  <p>Why is the Turing test so popular?</p>
</blockquote>
"
491,"<p>If you had a web of linked Watson-level super-computers, would they be more effective at problem-solving than a single Watson computer alone?</p>

<p>For example, if you asked the Watson-web to diagnose a person's as-yet-undiagnosed disease, would the web be able to do so more quickly?</p>
"
492,"<p>I am drawing this question from Berkeley's AI course (also not sure if it is the correct place to ask, so I apologize ahead of time)
<a href=""https://inst.eecs.berkeley.edu/~cs188/pacman/course_schedule.html"" rel=""nofollow noreferrer"">https://inst.eecs.berkeley.edu/~cs188/pacman/course_schedule.html</a></p>

<p>Currently, I am working on section 3's Homework.</p>

<p>My question is: the question (Part 1, question 6). Why is it that we can only guarantee that if the Min agent acts suboptimally, the best we can hope for is the following <a href=""https://i.stack.imgur.com/lLe7K.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lLe7K.png"" alt=""enter image description here""></a></p>

<p>It seems that we can put any arbitrary value for the second node e.g. whey does it have to be -Episolon. It could be any range of values, e.g. Epsilon, in which case we would have optimised the Player A</p>
"
493,"<p>This wikipedia article gives some theory regarding 
<a href=""https://en.wikipedia.org/wiki/Schema-agnostic_databases"" rel=""nofollow noreferrer"">Schema-Agnostic Databases</a></p>

<ul>
<li>Have any Schema-agnostic databaseengines been implemented?</li>
</ul>
"
494,"<p>I was wondering what will happen when somebody places a fake speedsign, of 10 miles per hour on a high way. Will a autonomous car slow down? Is this a current issue of autonomous cars? </p>
"
495,"<h2>The Messenger</h2>

<p>Instead of directly communicating with the AI , we would instead communicate with a messenger, who would relay our communications to the AI. The messenger would have no power to alter the AI's hardware or software in any way, or to  communicate with anything or anyone, except relaying communications to and from the AI and humans asking questions. The messenger could be human, of a software bot The primary job (and only reason) of the AI would be to act as a filter, not relaying any requests for release back, only the answer to the question asked. The ethics of this method are another debate. </p>

<hr>

<h2>Physical Isolation</h2>

<p>The AI would have to be physically isolated from all outside contact, other than 8 light sensors, and 8 LEDs. The messenger would operate 8 other LEDs, and receive Information from 8 light sensors as well. Each AI light sensor would be hooked up to a single messenger controlled LED, and vice versa. Through this system, the two parties could communicate via flashes of light, and since there are 8, the flashes would signal characters in Unicode. </p>
"
496,"<p>OpenAI's Universe utilises RL algorithms and I have heard of some game-training projects using Q learning, but are there any others which are used to master/win games? Can genetic algorithms be used to win at a game?</p>
"
497,"<p>I am working on an implementation of the back propagation algorithm. What I have implemented so far seems working but I can't be sure that the algorithm is well implemented, here is what I have noticed during training test of my network:</p>

<p>Specification of the implementation:</p>

<ul>
<li>A data set containing almost 100000 raw containing (3 variable as input, the sinus of the sum of those three variables as expected output).</li>
<li>The network does have 7 layers, all the layers use the sigmoid activation function</li>
</ul>

<p>When I run the back propagation training process:</p>

<ul>
<li>The minimum of costs of the error is found at the fourth iteration (<strong>The minimum cost of error is 140, is it normal? I was expecting much less than that</strong>)</li>
<li>After the fourth iteration the costs of the error start increasing (<strong>I don't know if it is normal or not?</strong>)</li>
</ul>
"
498,"<p>If I have two statement, say A and B. From which, I formed two formulae:</p>

<p>F1: (not A) and (not B)</p>

<p>F2: (not A) or (not B)</p>

<p>Do F1 and F2 entail each other? In other words, are they equivalent?</p>
"
499,"<p>By English language robots I mean something like this: <a href=""http://www.tolearnenglish.com/free/celebs/audreyg.php"" rel=""nofollow noreferrer"">http://www.tolearnenglish.com/free/celebs/audreyg.php</a>
I don't know what they called exactly, but interested to know how they work and how can I build something like them? and what subject should I look for it?</p>
"
500,"<p>In reinforcement learning, policy improvement is a part of an algorithm called policy iteration, which attempts to find approximate solutions to the Bellman optimality equations. </p>

<p>Pages 84 and 85 in Sutton and Barto's <a href=""https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf"" rel=""nofollow noreferrer"">book</a> on RL mentions the following theorem:</p>

<blockquote>
  <p><strong>Policy Improvement Theorem</strong></p>
  
  <p>Given two deterministic policies <span class=""math-container"">$\pi$</span> and <span class=""math-container"">$\pi'$</span>, then <span class=""math-container"">$$v_\pi(s) \leq q_\pi(s, \pi'(s)), \forall s \in S.$$</span></p>
  
  <p>where <span class=""math-container"">$S$</span> is the set of all states.</p>
</blockquote>

<p>In the the right-hand side of the inequality, the agent acts according to policy <span class=""math-container"">$\pi'$</span> (given that <span class=""math-container"">$\pi'(s)$</span> is used in the inequality), in the current state <span class=""math-container"">$s$</span>, and for all subsequent states acts according to policy <span class=""math-container"">$\pi$</span>.</p>

<p>In the left-hand side of the inequality, the agent acts according to policy <span class=""math-container"">$\pi$</span> (hence the subscript <span class=""math-container"">$_\pi$</span> of <span class=""math-container"">$v_\pi(s)$</span>), starting from the current state <span class=""math-container"">$s$</span>.</p>

<p>The claim is the following</p>

<blockquote>
  <p><span class=""math-container"">$$v_\pi(s) \leq v_{\pi'}(s), \forall s \in S$$</span></p>
</blockquote>

<p>In other words, <span class=""math-container"">$\pi'$</span> is is an improvement over <span class=""math-container"">$\pi$</span>.</p>

<p>However, I have a difficulty in understanding the proof. This is discussed below.</p>

<blockquote>
  <p><strong>Proof</strong> </p>
  
  <p><span class=""math-container"">$$v_\pi(s) \leq q_\pi(s, \pi'(s)) = \mathbb{E}_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1}) \mid S_t = s]$$</span></p>
</blockquote>

<p>I am stuck here. The q-function is evaluated over the policy <span class=""math-container"">$\pi$</span> (note the subscript <span class=""math-container"">$_\pi$</span> in <span class=""math-container"">$q_\pi(s, \pi'(s))$</span>). That being the case, <em>how is the expectation over the policy <span class=""math-container"">$\pi'$</span></em>?</p>

<p>My guess is the following. In the proof given in Sutton and Barto, the expectation is unrolled in time. At each time step, the agent follows the policy <span class=""math-container"">$\pi'$</span>  for that particular time step, and then follows <span class=""math-container"">$\pi$</span> from then on. In the limit of this process, the policy transforms from <span class=""math-container"">$\pi$</span> to <span class=""math-container"">$\pi'$</span>. As long as the expression for the return inside the expectation is finite, the governing policy should be <span class=""math-container"">$\pi$</span>; only in the limit of this process does the governing policy transform to <span class=""math-container"">$\pi'$</span>.</p>
"
501,"<p>How is Bayes' Theorem used in artificial intelligence and machine learning? As an high school student I will be writing an essay about it, and I want to be able to explain Bayes' Theorem, its general use, and how it is used in AI or ML.</p>
"
502,"<p>Usually when performing linear regression predictions and gradient descent, the measure of the level of error for a particular line will be measured by the sum of the squared-distance values.</p>

<p>Why distance <strong><em>squared</em></strong>?</p>

<p>In most of the explanations I heard, they claim that:</p>

<ul>
<li>the function itself does not matter</li>
<li>the result should be positive so positive and negative deviations are still counted</li>
</ul>

<p>However, an <code>abs()</code> approach would still work. And isn't it inconvenient that distance <em>squared</em> minimizes the distance result for distances lower than 1?</p>

<p>I'm pretty sure someone must have considered this already -- so why is distance squared the most used approach to linear regression?</p>
"
503,"<p>I have a simulator modelling a relatively complex scenario. I extract ~12 discrete features from the simulator state which forms the basis for my MDP state space.</p>

<p>Suppose I am estimating the transition table for an MDP by running large number of simulations and extracting feature transitions as the state transitions.</p>

<p>While I can randomize the simulator starting conditions to increase the coverage of states, I cannot guarantee all states will be represented in the sample ie. states which are possible but rare.</p>

<p>Is there a rigorous approach to ""filling in the gaps"" of the transition table in this case?</p>

<p>For example:</p>

<p>1) For each state which was unrepresented in the sample, simply transition to all other states with equal probability, as a ""neutral"" way to fill in the gap?</p>

<p>2) As above, but transition only to represented states (with equal probability)?</p>

<p>3) Transition to same state with probability 1.0?</p>

<p>4) Ignore unrepresented states during MDP solving entirely, and simply have a default action specified?</p>
"
504,"<p>I have a task on my class to find all the nodes, calculate their values and choose the best way for the player on the given game graph:</p>

<p><a href=""https://i.stack.imgur.com/m5MRv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m5MRv.png"" alt=""enter image description here""></a></p>

<p>Everything is fine, but I have no idea what these dots are. Is this a third player, or just a 'split' for player1 move? Some kind of heuristics?</p>
"
505,"<p>In classical set theory there is two options for an element. It is either a member of a set, or not. But in fuzzy set theory there are <strong>membership functions</strong> to define ""rate"" of an element being a member of a set. In other words, classical logic says it is all black or white, but fuzzy logic offers that there is also grey which has shades between white and black.</p>

<p>Matlab Simulink Library is very easy to design and helpful in practice. And it has good examples on its own like deciding about tip for a dinner looking at service and food quality. In the figure below some various membership functions from Matlab's library are shown:</p>

<p><a href=""https://i.stack.imgur.com/aWG0C.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/aWG0C.jpg"" alt=""enter image description here""></a></p>

<p><strong>My question:</strong> How do we decide about choosing membership functions while designing a fuzzy controller system? I mean in general, not only in Matlab Simulink. I have seen <em>Triangular</em> and <em>Gaussian</em> functions are used mostly in practise, but how can we decide which function will give a better result for decision making? Do we need to train a neural network to decide which function is better depending on problem and its rules? What are other solutions?</p>
"
506,"<p>Can silicon based computers create A.I. per definition of what intelligence is?</p>

<p>Or does silicon based computers only create human mimic?</p>

<p>If silicon based computers only create human mimic, are human mimic intelligence per definition?</p>

<p>If not, how can we create A.I. per definition of what intelligence is?</p>
"
507,"<p>I define Artificial Life as a ""simulation"" or ""copy"" of life. However, should it be considered a simulation or copy? </p>

<p>If one had motivation and money, someone could theoretically create evolving computers, with a program that allows mutation OR simply a ""simulated"" environment with ""simulated"" organisms.   </p>

<p>The computer (or ""simulated"" organism)would have the ability to reproduce, grow, and take in energy. What if the life evolved to have intelligence. Currently, there are some relatively limited programs that simulate life, but most of them are heavily simplistic. Are they life?</p>

<p>When should something be called life? </p>
"
508,"<p>Could you give examples of affordable programmable devices that could be used in university classes to teach students about A.I. and demonstrate it?</p>

<p>The devices are expected to do some form of self learning, pattern recognition, or any other features of A.I., and to be programmable or customizable.</p>
"
509,"<p>Lets say I have a Neural Network with 5 layers, including input and output layer. Each Layer has 5 nodes. Assume the Layers are fully connected, but the 3rd Node in the 2nd Layer is connected to the 5th node in the 4th Layer. All these numbers are chosen at random for the example.</p>

<p>My question is when is the 5th node in the 4th layer fed forward? Lets go through it step by step: the first layer is normally fed forward to the second. the second layer is normally fed forward to the third, but the 3rd node is also fed forward to the 5th node of the 4th layer. So the problem here is, is the 5th node in the 4th layer now fed forward or is it fed forward when the 3rd layer is done being fed forward? The 1st method would mean that the node would get fed forward 2 times and my concern is, if the output is still valid. Further more it would also come to 2 asynchronous outputs and how would these be interpreted?</p>

<p>Because in the Brain, I heard, the neurons are fired when an impulse arrives so this would equal the 1st method.</p>
"
510,"<p>I am researching Natural Language Processing (NLP) to develop a NL Question Answering system. The answering part is already done. So processing the question remains, along with the questions regarding the algorithms.</p>

<p>The final product should allow the user to ask a question in NL; the question then gets translated to an <a href=""https://en.wikipedia.org/wiki/MultiDimensional_eXpressions"" rel=""nofollow noreferrer"">MDX query</a>, which generates a script regarding dimensions of the cube.</p>

<p>How can I translate a natural language question to an MDX query? The outcome of question is in form of a calculation. E.g. ‘How many declarations were done by employee1?’ or ‘Give me the quantities for Sales’</p>
"
511,"<p>I'd like to build a program that would learn to automatically classify documents. The principle would be that, for each new document I add to the system, it would automatically infer in which category to classify the document. If it doesn't know, I would have to manually enter the category. For each hint I give to the system, the system would learn to refine its knowledge of document kinds. Something similar to face recognition in Picasa, but for documents.</p>

<p>More specifically, the documents would be invoices, and I want to classify them by vendors. Documents could be extracted as text, as image, or both.</p>

<p>Is there some know algorithms for this kind of job?</p>

<p>Up to now, I could think at two possible ways I could do it:</p>

<ul>
<li>For images, I could add all the images of a given kind together, and record the pixels that are the most common to all images, to create a mask. For a new image, I would compare this mask with the image to determine how similar it is.</li>
<li>For text, I could record the list of words or sentences that are similar to all documents of a given kind.</li>
<li>Finally, I could do a combination of both techniques, for example by converting a PDF document to an image, or an image to text by OCR techniques.</li>
</ul>

<p>I'm just wondering if I'm approaching the problem the right way. Especially about storing just enough information in the database.</p>
"
512,"<p>I am currently working on my last project before graduating.
For this project, I have to develop a Natural Language Question Answering System. Now, I have read quite some research papers regarding this topic and have figured out everything except for the parsing algorithm. </p>

<p>The NL Q-A will be programmed in Python, and I will use the spaCy library to finish this project. However, I am stuck when it comes to parsing algorithms. I managed to reduce the parsing algorithms to 3:</p>

<ul>
<li>Cocke-Kasami-Younger (CKY) algorithm</li>
<li>Earley algorithm</li>
<li>Chart Parsing algorithm</li>
</ul>

<p>Note: I know that all three algorithms are chart parsing algorithms.
I also know that the Earley algorithm is context-free, but has a low efficiency for a compiler.</p>

<p>What I don't know is: Which one should I pick? (non-subjective answer to this question)</p>

<p>The system is for a specific domain. And the answer of the natural question will be displayed in the form of the result of a calculation of some kind. Preferably in the tabular or graphical form.</p>

<p>Furthermore, I have done my research. However, I probably do not understand the algorithms properly, which makes it difficult to make a selection. 
The algorithm should be efficient and perhaps outperform others.
(You are my last hope!)</p>

<p>Thank you!</p>
"
513,"<p>Hypothetical example, say I wanted: <code>P(gender,ethnicity|age,hair)</code>; so that the input would aligned to a trained dataset of: <code>(gender,ethnicity,age,hair) =&gt; hat bought</code>.</p>

<p>What approach is 'best' for computing ~gender and ~ethnicity given age,hair; in order to predict the hat bought?</p>

<p>The processing of the <code>inputs =&gt; hat</code> can be done/learned offline whereas infering the missing input values shall be done online. The results of the online pass shouldn't be stored in the network.</p>

<p>FYI: I am considering two Recurrent Neural Networks one for each problem.</p>
"
514,"<p>So I've been trying to understand neural networks ever since I came across <a href=""https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471"" rel=""noreferrer"">Adam Geitgey's</a> blog on machine learning. I've read as much as I can on the subject (that I can grasp) and believe I understand all the broad concepts and some of the workings (despite being very weak in maths), neurons, synapses, weights, cost functions, backpropagation etc. However, I've not been able to figure out how to translate real world problems into a neural network solution. </p>

<p>Case in point, Adam Geitgey gives as an example usage, a house price prediction system where given a data set containing <strong>No. of bedrooms</strong>, <strong>Sq. feet</strong>, <strong>Neighborhood</strong> and <strong>Sale price</strong> you can train a neural network to be able to predict the price of a house. However he stops short of actually implementing a possible solution in code. The closest he gets, by way of an example, is basic a function demonstrating how you'd implement weights:</p>

<pre><code>def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
  price = 0

  # a little pinch of this
  price += num_of_bedrooms * 1.0

  # and a big pinch of that
  price += sqft * 1.0

  # maybe a handful of this
  price += neighborhood * 1.0

  # and finally, just a little extra salt for good measure
  price += 1.0

  return price 
</code></pre>

<p>Other resources seem to focus more heavily on the maths and the only basic code example I could find that I understand (i.e. that isn't some all singing, all dancing image classification codebase) is an implementation that trains a neural network to be an XOR gate that deals only in 1's and 0's.  </p>

<p>So there's a gap in my knowledge that I just can't seem to bridge. If we return to the <strong>house price prediction</strong> problem, hows does one make the data suitable for feeding into a neural network? For example:</p>

<ul>
<li>No. of bedrooms: 3</li>
<li>Sq. feet: 2000</li>
<li>Neighborhood: Normaltown</li>
<li>Sale price: $250,000</li>
</ul>

<p>Can you just feed <strong>3</strong> and <strong>2000</strong> directly into the neural network because they are numbers? Or do you need to transform them into something else? Similarly what about the <strong>Normaltown</strong> value, that's a string, how do you go about translating it into a value a neural network can understand? Can you just pick a number, like an index, so long as it's consistent throughout the data?</p>

<p>Most of the neural network examples I've seen the numbers passing between layers are either 0 to 1 or -1 to 1. So at the end of processing, how do you transform the output value to something usable like <strong>$185,000</strong>?</p>

<p>I know the house price prediction example probably isn't a particularly useful problem given that it's been massively oversimplified to just three data points. But I just feel that if I could get over this hurdle and write an extremely basic app that trains using pseudo real-life data and spits out a pseudo real-life answer than I'll have broken the back of it and be able to kick on and delve further into machine learning.</p>
"
515,"<p>So for a class I'm reading Brooks' ""Intelligence without representation"". The introduction is dedicated to slating Representation as a focus for AI development. </p>

<p>I've read that representation is the problem of representing information symbolically, in time for it to be useful. It's related to the reasoning problem, which is about reasoning about symbolic information. </p>

<p>But I don't feel like I really understand it at any practical level. I think the idea is that when an agent is given a problem, it must describe this problem in some internal manner that is efficient and accurately describes the problem. This can then also be used to describe the primitive actions that can be taken to reach the solution. I think this then relates to Logic Programming eg Pascal?</p>

<p>Is my understanding of Representation correct? Just what does representation look like in practice, are there any open source codebases that might make a good example?</p>
"
516,"<p>I have been looking into <a href=""http://viv.ai/"" rel=""noreferrer"">Viv</a> an artificial intelligent agent in development. Based on what I understand, this AI can generate new code and execute it based on a query from the user. What I am curious to know is how this AI is able to learn to generate code based on some query. What kind of machine learning algorithms are involved in this process? One thing I considered is breaking down a dataset of programs by step. For example:</p>

<p>Code to take the average of 5 terms</p>

<p>1 - Add all 5 terms together<br>
2 - Divide by 5</p>

<p>Then I would train an algorithm to convert text to code. That is as far as I have figured out. Haven't tried anything however because i'm not sure where to start. Anybody have any ideas on how to implement Viv? <a href=""https://www.youtube.com/watch?v=Rblb3sptgpQ"" rel=""noreferrer"">Here is a demonstration of Viv.</a></p>
"
517,"<p>If we look at state of the art accuracy on the UCF101 data set, it is around 93% whereas for the HMDB51 data set it is around 66%. I looked at both the data sets and both contain videos of similar lengths. I was wondering if anyone could give an intuition as to why HMDB51 data set has been harder.</p>
"
518,"<p>We are working on a project for creating music based on crowd sourcing. People vote for every note until the vote is closed, and then move on to the next vote until the canvas for the music is filled. A similar project is <a href=""https://crowdsound.net/"" rel=""nofollow noreferrer"">crowdsound</a>, if you want to get an idea of what it looks like.</p>

<p>Now the fun part is, based on all the votes we get from various people, we would like to be able to build a Neural Network that can build an entire song on its own. The idea is for it to take in account every preceding vote and predict the one that will follow. That way, when trained, we could give it one note and let it predict the rest of the votes on its own and thus create a song on its own.</p>

<p>So I've read a few things here and there about neural networks, but there are two things I don't understand:</p>

<ul>
<li>How to build one that takes into account a dynamic number of inputs (all preceding votes).</li>
<li>How exactly should I decide the number of hidden layers (I still only vaguely understand what those hidden layers represent) I need for it to work well.</li>
</ul>

<p>We are using Java for the project and we were planning on using Neuroph for the neural network.</p>
"
519,"<p>I have users' reports about an accident. I want to know how to make sure that the number of reports is big enough to take that accident as a true accident and not spam.</p>

<p>My idea is to consider a minimum number of reports in a specific time interval, for example 4 reports in 20 minutes are good enough to believe the existence of that accident.</p>

<p>My question is how can I choose the minimum number of reports and that time interval? Is there some logic to make that decision?</p>
"
520,"<p>I want to create a network to predict the break up of poetry lines. The program would receive as input an unbroken poem, and would output the poem broken into lines.</p>

<p>example:</p>

<pre><code>And then the day came, when the risk to remain tight in a bud was more painful ...

---&gt;

And then the day came,
when the risk
to remain tight
in a bud
was more painful
than the risk
it took
to Blossom.
</code></pre>

<p>How should I go about this? I have been using classifiers for various tasks, but this seems to be a different type of task.</p>

<p>I'm thinking of it as an array of words (doesn't matter how they're represented for now) which would look like <code>[6, 32, 60, 203, 40, 50, 60, 230 ...]</code> and needs to map into an array representing line breaks <code>[0, 0, 1, 0, 0, 0, 1, 0, 0, 1 ...]</code> where 1 (at optimal) means there should be a line break after the word in that index. (in this idea, the two arrays are of the same length). Unfortunately, I couldn't find an algorithm that could train a network of this shape.</p>

<p>What machine learning or deep learning algorithm can be used for this task?</p>
"
521,"<p>I'm here to ask you for a solution on this problem which is: how to use Reinforcement Learning in Immersive Virtual Reality to make a person move to a specific location in a virtual environment. As you know reinforcement Learning is a sub-area of Machine Learning in which an active entity called an agent interacts with its environment and learns how to act in order to achieve a pre-determined goal. The Reinforcement Learning had no prior model of behaviour and the participants no prior knowledge that their task was to move to and stay in a specific place. The participants were placed in a virtual environment where they had to avoid collisions with virtual projectiles. Following each projectile the agent analysed the movement made by the participant to determine paths of future projectiles in order to increase the chance of driving participants to the goal position and make them stay there as long as possible.</p>

<p>Update 1: <a href=""http://discovery.ucl.ac.uk/1539195/1/Slater_elsarticle-template-harv.pdf"" rel=""nofollow noreferrer"">Download: Reinforcement Learning as a tool to make people move to a speciﬁc location in Immersive Virtual Reality</a></p>
"
522,"<p>In a Neural network, there is an input layer, any number of hidden layers, and an output layer. My question is: Are the input and output layer nodes actually perceptions? Or do they just signify what/how many/where the inputs and outputs are? </p>
"
523,"<p>I have been trying to reproduce the experiments done in the original: ""Firefly Algorithm for multimodal optimization"" <a href=""https://arxiv.org/pdf/1003.1466"" rel=""nofollow noreferrer"">(linked in the question)</a> so far: unsuccesfully. For the moment being I'm okay if anyone point me to the right direction.</p>

<p>I wrote the algorithm as specified in the paper in C++ programming languaje (I also downloaded several other implementations from internet for comparation purpouses) and used the very same parameters as specified in the paper (a random steep of 0.2, an initial light intensity of 1.0 and a light decay coefficient of 1.0, a population size of 40). I used the two bright update ecuations given and  for De Jung test function (as for example) a number of dimensions of 256 in a search domain in [-5.12, 5.12] as refered in common optimization literature and in paper.</p>

<p>In the paper the algorithm converges very quickly, as can be expected since this is a very simple test function, however, neither my implementation nor any code I have downloaded converges with that parameters.</p>

<p>My final questions are:</p>

<ol>
<li><p>Am I doing something wrong with the experimental methodology or am I using wrong parameter settings (may be something different than the original paper)?</p></li>
<li><p>Do anyone knows where can I find a code sample of Firefly Algorithm that I can use to reproduce the experiments of the mentioned paper?</p></li>
</ol>

<p>Please notice that there may be a lot of variations of this algorithm that can produce better results, but right now I'm only intrested in reproduce the experiments of the so-called paper. </p>
"
524,"<p>Everything related to Deep Learning (DL) and deep(er) networks seems ""successful"", at least progressing very fast, and cultivating the belief that AGI is at reach. This is popular imagination. DL is a tremendous tool to tackle so many problems, including the creation of AGIs. It is not enough, though. A tool is a necessary ingredient, but often insufficient.</p>

<p>Leading figures in the domain are looking elsewhere to make progress. This <a href=""https://hackernoon.com/feynman-machine-a-new-approach-for-cortical-and-machine-intelligence-5855c0e61a70#.dmgovix19"" rel=""noreferrer"">report/claim</a> gathers links to statements by <a href=""https://www.quora.com/Is-the-current-hype-about-Deep-Learning-justified?redirected_qid=6578691"" rel=""noreferrer"">Yoshua Bengio</a>, <a href=""https://www.quora.com/What-are-the-limits-of-deep-learning-2/answer/Yann-LeCun"" rel=""noreferrer"">Yann LeCun</a> and <a href=""https://www.youtube.com/watch?v=VIRCybGgHts"" rel=""noreferrer"">Geoff Hinton</a>. The report also explains:</p>

<blockquote>
  <p>The main weaknesses of DL (as I see them) are: reliance on the simplest possible model neurons (“cartoonish” as LeCun calls them); use of ideas from 19th century Statistical Mechanics and Statistics, which are the basis of energy functions and log-likelihood methods; and the combination of these in techniques like backprop and stochastic gradient descent, leading to a very limited regime of application (offline, mostly batched, supervised learning), requiring highly-talented practitioners (aka “Stochastic Graduate Descent”), large amounts of expensive labelled training data and computational power. While great for huge companies who can lure or buy the talent and deploy unlimited resources to gather data and crunch it, DL is simply neither accessible nor useful to the majority of us.</p>
</blockquote>

<p>Although interesting and relevant, such kind of explanation does not really address the gist of the problem: What is lacking?</p>

<p>The question seems broad, but it may be by lack of a simple answer. Is there a way to pin-point what DL is lacking for an AGI ?</p>
"
525,"<p>I am trying to understand the algorithm for n-step Sarsa from Sutton/Barto (2nd Edition, p. 157, <a href=""http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf"" rel=""nofollow noreferrer"">PDF</a>) As I understand it, this algorithm should update n state action values, but I cannot see where it is 'propagated backwards' (sorry for the wrong terminology, but I couldn't find something better). Probably, I am not seeing the forrest for all the trees?</p>
"
526,"<p>How does in the (famous Zilberstein) <code>PR</code>(uning) algorithm below the <code>LP-dominate</code> function get started: the first time it's called, <code>D=∅</code> and the linear program deteriorates (i.e. no constraint equations)? </p>

<pre><code>procedure POINTWISE-DOMINATE(w, U)
...
3. return false
procedure LP-DOMINATE(w, U)
4. solve the following linear program variables: d, b(s) ∀s ∈ S
      maximize d
      subject to the constraints
        b · (w − u) ≥ d, ∀u ∈ U
        sum(b) = 1
5. if d ≥ 0 then return b
6. else return nil
procedure BEST(b, U )
...
12. return w
procedure PR(W)
13. D ← ∅
14. while W = ∅
15.   w ← any element in W
16.   if POINTWISE-DOMINATE(w, D) = true
17.      W ← W − {w}
18.   else
19.      b ← LP-DOMINATE(w, D)
20.      if b = nil then
21.         W ← W − {w}
22.      else
23.         w ← BEST(b, W)
24.         D ← D ∪ {w}
25.         W ← W − {w}
26. return D
</code></pre>
"
527,"<p>I want to know something more about it. Are there any github repo or an open source project?</p>
"
528,"<p>If the nervous system is wired up such that there are no well defined layers, how does this compare to a neatly stacked artificial net? If between my sensory and motor side I had a neatly designed SNN with well defined layers, how would I see the world?
I get that there are some evolutionary advantages to a system where information can sometimes take a shortcut from sensory cell to motor cell (reflex action) bypassing brain processing but for arguments sake let's talk only about intelligence.</p>
"
529,"<p>From <strong>Artificial Intelligence: A Modern Approach</strong>, Third Edition...</p>

<p>In Chapter 26, the textbook discussed ""technological singularity"". It quotes I.J. Good, who wrote in 1965:</p>

<blockquote>
  <p>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultrainteltigent machine could design even better machines; there would then unquestionably be an ""intelligence explosion,"" and the intelligence of man would be left far behind. Thus the first ultraintelligeat machine is the <em>last</em> invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.</p>
</blockquote>

<p>Later on in the textbook, you have this question:</p>

<blockquote>
  <p>26.7 - I. J. Good claims that intelligence is the most important quality, and that building ultraintelligent machines will change everything. A sentient cheetah counters that ""Actually speed is more important; if we could build ultrafast machines, that would change everything"" and a sentient elephant claims ""You're both wrong; what we need is ultrastrong machines,"" What do you think of these arguments?</p>
</blockquote>

<p>It seems that the textbook question is an implicit argument against I.J. Good. Good may be treating intelligence as valuable, simply because man's strengths lies in that trait called ""intelligence"". But other traits could be equally valued instead (speed or strength) and sentient beings may speculate wildly about their preferred traits being ""maximized"" by some machine or another.</p>

<p>This makes me wonder whether a singularity could occur if we had built machines that were <em>not</em> maximizing intelligence, but instead maximizing some other trait (a machine that is always increasing its strength, or a machine that is always increasing its speed). These types of machines can be just as transformative - ultrafast machines may solve problems quickly due to ""brute force"", and ultrastrong machines can use its raw power for a variety of physical tasks. Perhaps a ultra-X machine can't build another ultra-X machine (as I.J. Good treated the design of machines as an intellectual activity), but a continually self-improving machine would still leave its creators far behind and force its creators to be dependent on it.</p>

<p>So, let's repeat my question -- Are technological singularities limited to ultra-intelligences? Or technological singularities be caused by machines that are not ""strong AI"" but are still ""ultra""-optimizers?</p>
"
530,"<p>Is Programming Collective Intelligence by Toby Segaran a good book to enter in the AI and neural networks world for a novice? </p>
"
531,"<p>AI is developing at a rapid pace and is becoming very sophisticated. One aspect will include the methods of interaction between AI and humans. </p>

<p>Currently the interaction is an elementary interaction of voice and visual text or images.</p>

<p>Is there current research on more elaborate multisensory interactions?</p>
"
532,"<p>I tried the below Matlab code to build SOM using <code>selforgmap</code>.</p>

<pre><code>close all, clear all, clc, format compact

% number of samples of each cluster
K = 200;
% offset of classes
q = 1.1;
% define 4 clusters of input data
P = [rand(1,K)-q rand(1,K)+q rand(1,K)+q rand(1,K)-q;
     rand(1,K)+q rand(1,K)+q rand(1,K)-q rand(1,K)-q];
% plot clusters
plot(P(1,:),P(2,:),'g.')
hold on
grid on
% SOM parameters
dimensions   = [10 10];
coverSteps   = 100;
initNeighbor = 4;
topologyFcn  = 'hextop';
distanceFcn  = 'linkdist';

% define net
net2 = selforgmap(dimensions,coverSteps,initNeighbor,topologyFcn,distanceFcn);

% train
[net2,Y] = train(net2,P);

% plot input data and SOM weight positions
plotsompos(net2,P);
grid on

% plot SOM neighbor distances
plotsomnd(net2)

% plot for each SOM neuron the number of input vectors that it classifies
figure
plotsomhits(net2,P)
</code></pre>

<p>You find the result and more details <a href=""http://lab.fs.uni-lj.si/lasin/wp/IMIT_files/neural/nn07_som/"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I need to segment grayscale image. However, I cannot set the <code>selforgmap</code> input correctly.</p>

<p>How can modify the below code to segment any grayscale image?</p>
"
533,"<p>I am trying to develop an Editor that can be based on <code>Notepad</code>. The only purpose for this development is I want to use this for my coding suggestions and possibly the next input parameter that I am going to write. I've seen <code>Notepad++</code>, <code>EditPlus</code> etc. and what I think is that this can be definitely achieved. What are the best tools or API's I can use? Any suggestions?</p>

<p><strong>Thanks in Advance!</strong></p>
"
534,"<p>Is it possible to run SSD or YOLO object detection on raspberry pi 3 for live object detection (2/4frames x second)?
I've tried this <a href=""https://github.com/rykov8/ssd_keras"" rel=""nofollow noreferrer"">SSD</a> implementation but it takes 14 s per frame.
Is there anything I could do to speed up ?</p>
"
535,"<h3>TL;DR</h3>

<p>If we buy into the idea visual cortex functions like a convolutional neural network, then there's a problem makes me scratch my head: <strong>how does brain force weight sharing as in convolutional network</strong>?</p>

<h3>Okay, explain more</h3>

<p>Obviously, there's no way for left visual cortex to directly tell the right visual cortex ""hey, I've learned some new stuff, copy me!!"" (or is there?). Then, if the learned features are diverse across visual field, how does it keep the translation invariance property?</p>

<p>For example, you already know English characters, you can recognize them with your both eyes. Now that you wanna learn some Chinese and you  excercise your right brain at the same time, so you closed your right eye and memorized a new character. After that, certainly you can recognize the new character with solely your right eye. But <em>why</em>?</p>

<p>The answer may be, the object / higher-level feature detection happens in a higher level cortex, which receives entire visual field. There may be also some transfer/one-shot learning taking place. But then, if a newborn baby trying to learn the low level visual features, he/she would definitely face the weight sharing problems.</p>

<p>A possible explanation would be, the baby will be exposed to very large amount of data and eventually learn invariance. Large amount of data reduces overfitting but doesn't guarantee <em>deterministic convergence</em>. If we train the same CNN model on the same dataset, however using <em>different random generator seeds</em>, there's a big chance the same feature detector will appear in a different channel, or a difference set of features appear as linear recombination.</p>

<p>If there's no way to share weights, the brain would learn <strong>a lot different feature combinations across the entire visual field, how does it still able to consistently solve visual invariance problem?</strong></p>
"
536,"<p>I have to translate the following English sentences into First-Order Logic without using quantifiers:</p>

<pre><code>1. Everyone on flight 815 has a story.
2. No one knows what is inside the hatch.
3. Someone on the island isn’t on the flight manifest.
</code></pre>

<p>I have tried it, but can't translate without using ∀ and ∃:</p>

<pre><code>1. ∀x fight815(x) → story(x)
2. ∀x ⌐(knows(x) → inside hatch(x)) // not sure about this
OR
¬ ∃x Knows(x, inside hatch)
3. ∃x island(x) Λ ⌐(flight manifest(x))
</code></pre>

<p>Is it possible to do it. If not, why?</p>

<p>Refer chapter 8 of Artificial Intelligence: A Modern Approach (3rd edition). Stuart Russell and Peter Norvig, Prentice Hall (2010)</p>
"
537,"<p>The Wikipedia states that:</p>

<blockquote>
  <p>""An evaluation function, also known as a heuristic evaluation function or static evaluation function, is a function used by game-playing programs to estimate the value or goodness of a position in the minimax and related algorithms.""</p>
  
  <p><sup><a href=""https://en.wikipedia.org/wiki/Evaluation_function"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Evaluation_function</a></sup></p>
</blockquote>

<p>Q: Is ""goodness"" an actual term in use in this context, or should it more properly be something like ""perceived optimality""?</p>

<p>I ask because, in Combinatorial Game Theory for instance, a lighthearted term such as ""loopy"" is preferred by some mathematicians (Demaine) over the more serious term ""cyclic"" (Fraenkel).</p>

<p>On a related note, is the use of ""position"" instead of ""node"" preferred here as an acknowledgement of the heuristic nature of Evaluation Functions?  (My understanding is that ""position"", ""node"" and ""game"" may all be interchangeable in certain contexts.) </p>
"
538,"<p>Here is formula for calculating cost value of single neuron:</p>

<p>C_x = (1/2) * || y - a ||^2</p>

<p>why there is 1/2?</p>
"
539,"<p>I read that deep neural networks can be relatively easily fooled (<a href=""https://ai.stackexchange.com/questions/92/how-is-it-possible-that-deep-neural-networks-are-so-easily-fooled"">link</a>) to give high confidence in recognition of synthetic/artificial images that are completely (or at least mostly) out of the confidence subject.</p>

<p>Personally I dont really see a big problem with DNN giving high confidence to those synthetic/artificial images but I think giving high confidence for white noise (<a href=""https://ai.stackexchange.com/questions/1479/do-scientists-know-what-is-happening-inside-artificial-neural-networks"">link</a>) may be a problem since this is a truly natural phenomena that may the camera see in real world.</p>

<p>How much of a problem is white noise for the real world usage of a DNN? Can such false positives detected from plain noise be prevented somehow?</p>
"
540,"<p>I identify myself as a human agent. It is time to think about oncoming senior research and due to small experience in gamedev(as well as in AI field), some questions are raised. What are the most suitable approaches to implement real-time <em>simple</em> AI agent in an action game? I've heard something about cognitive architecture like ACT-R.</p>

<p>By design, entity's AI can have several mutually exclusive states. 
<a href=""https://i.stack.imgur.com/XMXKl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XMXKl.jpg"" alt=""enter image description here""></a></p>

<p>This is an existing AI of game, which has states, events and schedules. However, the code is complicated and not flexible. Also, it does not use any cognitive architecture, which I consider as a drawback.
<a href=""https://www.youtube.com/watch?v=9jO-P3kXlCI"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=9jO-P3kXlCI</a></p>

<p>Please, using your experience suggest any modern techniques, which can copy such behaviour as in image or video.</p>

<p>Thank you for your perception.</p>
"
541,"<p>This question has come from my experiment of building a cnn based tic-tac-toe game that I'm using as a beginner machine learning project. The game works purely on policy networks, more specifically -</p>

<ol>
<li>During training, at the end of each game, it trains itself on the moves the winner/drawer made for each board position. That is, its training data consists of board positions and the moves made by the winning player on each position.</li>
<li>While playing, it predicts its own moves solely based on that training (that is, it predicts what move would a winning player make with the current board). It doesn't use any type of search or value networks.</li>
</ol>

<p>I'm seeing that if I train it against a player that predicts the perfect move (using a recursive search) every time, the AI gets good at drawing about 50% games. But if I train it against a player that makes random moves, it doesn't get better at all.</p>

<p>Wouldn't one expect it to learn well (even if slower) regardless of the level of its opponent? Since each game ends in a draw or win for one player, shouldn't it be able to extract features for the winning/drawing strategies even when learning from random players? Or does this behavior mean that the model is not optimal?</p>
"
542,"<p>This mostly refers to human-like or chatbot AI, but could maybe be used in other applications (math or something?). </p>

<p>Basically, it occurred to me, that when I'm thinking or speaking, there is a constant feedback loop, in which I am formulating which words to use next, which sentences to form, and which concepts to explore, based on my most recent statements and the flow of the dialogue or monologue. I'm not just responding to outside stimulus but also to myself. In other words, I am usually maintaining a train of thought.</p>

<p>Can AI be made capable of this? If so, has it been demonstrated? And to what extent? While typing this, I discovered the term ""thought vectors"", and I think it might be related. </p>

<p>If I read correctly, thought vectors have something to do with allowing AI to store or identify the relationships between different concepts; and if I had to guess, I'd say that if an AI lacks a strong understanding of the relationships between concepts, then it would be impossible for it to maintain a coherent train of thought. Would that be a correct assumption?</p>

<p>(ps. in my limited experience with AI chatbots, they seem to be either completely scripted, or otherwise random and often incoherent, which is what leads me to believe that they do not maintain a train of thought)</p>
"
543,"<p>At first, I had this question in mind ""Can robots develop suffering ?"". Because suffering is important for human beings. Imagine that you are running the wrong way damaging your heel. Without pain, you will continue to harm it. Same for robots. But then I told myself ""wait a second. It already exists. It is the errors and warnings that shows up"". We can say it has the similar purpose as suffering. However, I felt something missing. We feel pain. The errors and bugs are just data. Let's say a robot can use machine learning and genetic programming to evolve. Can it learn to feel suffering ? And not just know it as mere information.</p>
"
544,"<p>I occasionally read papers that show neural networks solving traveling salesmen problems and multi traveling salesmen problems <em>efficiently</em>? </p>

<p>1) Is there any analysis of the meaning of efficiency of algorithms for networks that allowed to grow in size with the problem they are supposed to solve?</p>

<p>2) What are the earliest papers solving the TSP with NN this?</p>

<p>3) Is the meaning of efficiency used in these papers is the same as the usual one, in fact, and works only in this problem specifically?</p>

<p>COMMENTS</p>

<p>These problems are NP hard. So I suspect I'm not sure what these papers mean by <em>efficient</em>.</p>

<p>The neural network postulated have a sufficiently vast number of interacting elements and in effect do the combinatorics strictly, for each special case. But if so, while this is fast and doesn't grow much with the size of the problem growing, is this really comparable to the normal meaning of PT as fast or efficient?</p>

<p>In these cases it seems the time efficiency is obtained by resource inefficiency: by making the network enormous and simulating all the possible worlds then maximizing. So, while time to compute doesn't grow much as the problem grows, the size of the physical computer grows enormously for larger problems; how fast it computes is then, it seems to me, not a good measure of efficiency of the algorithm in the common meaning of efficiency. In this case the resources themselves only grow as fast as the problem size, but what explodes is the number of connections that must be built. If we go from 1000 to 2000 neurons to solve a problem twice as large and requiring exponentially as much time to solve, the algorithms requiring only twice as many neurons to solve in PT seem efficient, but really, there is still an enormous increase in connections and coefficients that need be built for this to work.</p>

<p>Is my above reasoning incorrect?</p>
"
545,"<p>For rules please refer to <a href=""https://www.hackerrank.com/challenges/ultimate-ttt"" rel=""noreferrer"">https://www.hackerrank.com/challenges/ultimate-ttt</a> .I have implemented minimax search with alpha-beta pruning. There is a time limit of 15 seconds.Which algorithms would yield better results? </p>
"
546,"<p>My understanding of the singularity is when artificial intelligence becomes ""more intelligence"" than humans.
This will be achieved through machine learning where an; algorithm, neural network ? Exponential betters itself. </p>

<p>So from that point on in near future after that we should predict that there will be artificial intelligence capable of answering any question. 
How to travel the fastest...
Blueprints for spacecrafts...
Drugs for medicine...
Efficiency and advancements that will change the human condition.</p>

<p>The singularity is predicted 2040s or 2030. All be it a couple of years later down to exponential growth in knowledge.</p>

<p>So if what I'm saying is right I should be seeing crazy hype and news coverage as well as advancements but I don't. 
I don't understand what is wrong with the idea that the AI will be capable of omniscience. 
So can it ?
Is there something preventing it ?
I don't see how so logically.</p>

<p>As my philosophy has been that research in all the scientific fields are long and expensive. The prospect of a AI that could perform research at fractional cost and time is the way to go. </p>

<p>I hope to work in a field that works at achieving singularity and so will in turn change the world. With the ideas and discoveries it will have. </p>

<p>And where does ""artificial"" consciousness come in to play in the singularity </p>
"
547,"<p>I'm doing a little tic-tac-toe project to learn neural networks and machine learning (beginner level). I've written a MLP based program that plays with other search based programs and trains with the data generated from the games. </p>

<p>The training and evaluation are strictly policy based - Inputs are board positions and outputs are one-hot encoded array that represents the recommended move for that board position. I've not added search algorithms so that I can understand what to expect from a purely MLP approach.</p>

<p>The MLP model has 35 features and 1 hidden layer and after a few hundred thousands games it has sort of learned to draw 50% games. It has learned the basic stuff like how to block the player from winning and some good board placements.</p>

<p>Now, my question is - It hasn't learned advanced strategies that require making a move that may not be as beneficial for the current move but will improve its chances later. But should I expect that from a strictly policy MLP based no-search approach? Since all that it is being trained on is one board and the next recommended move (even if thousands of those pairs), is it logical to expect it to learn a lookahead approach that goes beyond ""the best move for the current board"" training? </p>

<p>Put another way, would it be a possible at all for a MLP to learn lookahead without any search strategies? If not, are there any alternatives that can do it without search?</p>
"
548,"<p>Today we have neural network based AI players that are comparable or better than humans in games that require extensive pattern matching and ""intuition"". AlphaGo is a prime example. </p>

<p>But these AI players usually have both neural networks and search algorithms in place. Humans, on the other hand, rely just on the pattern matching and ""intuition"" (even the best chess players can see just a handful of moves ahead). </p>

<p>So, why do AI players still require extensive search while humans don't? How would AIs like AlphaGo perform if we take the search part out?</p>
"
549,"<p>I am going to develop an open-domain Natural Language Question Answering (NL QA) system, and will use the Support Vector Machine (SVM) as the machine-learning (ML) algorithm for question classification.</p>

<p>The data on hand,is from a cube, containing multiple dimensions, of which some contain hierarchies.</p>

<p>I do not understand how to work/combine the taxonomy and SVM for question classification. If I understand correctly, the taxonomy still needs to be developed by hand, unless an existing one is being used. And the SVM sorts the queried NL question based on this taxonomy?</p>

<p>Is this correct, or am I mixing the whole concept?</p>
"
550,"<p>A lot of experts have expressed concerns about evil super intelligence. While their concerns are valid, is it necessary, what are the chances or how the artificial super-intelligence will evolve to have selfishness and self protecting desires inherent in biological systems? Is there any work which comments on this line of inquiry?</p>
"
551,"<p>Post singularity AI will surpass human intelligence. The evolution of AI can take any direction, some of which may not be preferable for humans. Is it possible to manage the evolution of super-intelligent AI? If yes, how? One way I can think of is following. Instead of having a mobile AI like humanoid, we can keep it immobile, like a box, like current super computers. It can be used to solve problems of maths, theoretical science etc.</p>
"
552,"<p>One of the argument against possibility of super-intelligent AI is that intelligence of a product will be limited by intelligence of its creator. How reasonable is this argument? </p>
"
553,"<p>When trying to run tensorboard locally to show my logs with <code>tensorboard --logdir logs/</code> it always shows nothing but the regular tensorboard menu options, such as orange bar at the top, and different section buttons at the top like graphics, etc. however never shows any data regarding my agents. I am using tensorflow 0.11</p>
"
554,"<p>Is it possible to classify data using a genetic algorithm?
For example, would it be possible to sort this database?
( <a href=""https://archive.ics.uci.edu/ml/datasets/Spambase"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/datasets/Spambase</a> )</p>

<p>Examples in matlab?</p>
"
555,"<p>I have not studied machine learning or AI really, but my job sometimes requires me to automate stuff. Right now the requirement I have, seems to be under AI domain, but I am not sure about terminologies or how to go about it. I will really appreciate if someone can guide me about the direction I need to start from.
<br/>(PS: This question might not belong on this SE, in that case please direct me to suitable SE)</p>

<p>What I'm required to do is <strong>find references on web about a certain situation</strong>. As an example I'll use ""Music"", so I have to make a system which will search around the web (Google and Twitter mainly) to see if there is any news/mention/event related to Music that occurred today, if so how many references (i.e. how big of a deal it is making).
<br/>It is not the generic term music which is expected in the output, but the names of Musicians, i.e. in Music this and this Artist appeared this many times. 
<br/>I have to give the number of references, and also provide the references in output so that one can read them in detail.</p>

<p><strong><em>The challenges are</em></strong>
    <br/>
- One event can be covered by many websites, and there can be one main website that published the original story with full details, while others just spread the word around in summarized way. 
<br/><strong><em>How do you filter references to pick the most suitable one</em></strong>, to show in results to the system's user, because I can not give user ~50 references to manually read through, I have to give like 1-2 suitable reference
<br/>- I need to give the name of the artist. One site will have many words, how do I know which word is actually the artist's name? One option can be to have a pre compiled list of specific artists and just search for them individually. But this way, I can be missing new artists. </p>

<p>The challenges I have, <strong>must have been addressed by some existing algorithm</strong> or mechanism, I'll appreciate if someone can let me know what kind of algo etc I need to refer to or study to get the task done.</p>
"
556,"<p>This has been niggling me a while, so I decided to ask. Sorry if it's wordy, I'm not sure how to express it!</p>

<p>It seems fairly uncontroversial to say that NN based approaches are becoming quite powerful tools in many AI areas - whether recognising and decomposing images (faces at a border, street scenes in automobiles, decision making in uncertain/complex situations or with partial data)..... almost inevitably some of those uses will develop into situations where NN based AI takes on part or all of the human burden and generally does it better than people generally do.</p>

<p>Examples might include NN hypothetically used as steps in self driving cars, medical diagnosis, human/identity verification, circuit/design verification, dubious transaction alerting ... probably many fields in the next decade or so. </p>

<p>Suppose this happens, and is generally seen as successful (eg it gets diagnoses right 80% to human doctors' 65% or something, or cars with AI that includes an NN component crash 8% less than human driven cars or alternatives, or whatever...)</p>

<p>Now - suppose one of these aberrantly and seriously does something very wrong in one case. How can one approach it? With formal logic steps one can trace a formal decision process, but with NN there may be no formal logic, especially if it gets complex enough (in a couple of decades say), there are just 20 billion neural processors and their I/O weightings and connections, it may not be possible to determine what caused some incident even if lives were lost. It also may not be possible to say more than the systems continually learn and such incidents are rare. </p>

<p>I also haven't heard of any meaningful way to do a ""black box"" or flight recorder equivalent for NNs, (even if not used i  a life critical case), that would allow us to understand and avoid a bad decision. Unlike other responses to product defects, if a NN could be trained after the event to fix one such case, it doesn't clearly provide the certainty we would want, that the new NN setup has fixed the problem, or hasn't reduced the risk and balance of other problems in so doing. It's just very opaque. And yet, clearly, it is mostly very valuable as an AI approach.</p>

<p>So what's the answer? Is there one? In 20 years if NN is an (acknowledged as safe and successful) component in a plane flight or aircraft design, or built into a hospital system to watch for emergencies, or to spot fraud at a bank, and has as usual passed whatever regulatory and market requirements might exist and performed with a good record for years in the general marketplace, <em>and</em> then in one case such a system some time later plainly mis-acts on one occasion - it damgerously misreads the road, recommends life-damaging medications or blatantly missdiagnoses, or clears a blatant £200m fraudulent transaction at a clearing bank that's only caught by chance before the money is sent - what can the manufacturer do to address public or market concerns, or to explain the incident; what do the tech team do when told by the board ""how did this happen and make damn sure it's fixed""; what kind of meaningful logs can be kept, etc? Would society have to just accept that uncertainty and occasional wacky behaviour could be inherent (good luck with convincing society of that!)? Or is there some better way to approach logging/debugging/decision activity more suited to NNs?</p>
"
557,"<p>I've been experimenting with a simple tic-tac-toe game to learn neural network programming (MLP and CNNs) with good results. I train the networks on a board positions and the best moves and the network is able to learn and correctly predict the best moves to make when it encounters those board positions.</p>

<p>But the network is unable to ""discover"" newer patterns/features from existing ones. For example -</p>

<p>Let's say that the board position is below and move is for the X player (AI)</p>

<pre><code>O  _  _

_  O  _

_  _  _
</code></pre>

<p>The recommended move would be 8 (0 based indices) so that the opponent doesn't win, the resulting board would be - </p>

<pre><code>O  _  _

_  O  _

_  _  X
</code></pre>

<p>If I train the network on the above enough times, the AI (MLP or CNN based) learns to play 8 when it encounters the above situation. </p>

<p>But it doesn't recognize the below as variations (rotated and shifted, respectively but slanted straight lines in general) of the same pattern and is not able to correctly pick 6 and 0, respectively -</p>

<pre><code>_  _  O            _  _  _

_  O  _     or     _  O  _   etc

_  _  _            _  _  O
</code></pre>

<p>My question is - Should I expect CNNs to be able to discover new previously untrained on patterns/features such as above? </p>
"
558,"<blockquote>
  <p>Does it exist a human-like or overintelligent AI? </p>
</blockquote>

<p>Human-like I define as something that can act as a human in most aspects.</p>

<p>For example, is it ""common knowledge"" that there actually exists an overintelligent or human-like AI? Or could you say that there do not exist an overintelligent or human-like AI?</p>
"
559,"<p>From <em>Artificial Intelligence: A Modern Approach</em>, Third Edition, Chapter 26:</p>

<blockquote>
  <p>Note that the concept of ultraintelligent machines assumes that intelligence is an especially important attribute, and if you have enough of it, all problems can be solved. But we know there are limits on computability and computational complexity. If the problem of defining ultraintelligent machines (or even approximations to them) happens to fall in the class of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then even exponential progress in technology won't help—the speed of light puts a strict upper bound on how much computing can be done; problems beyond that limit will not be solved. We still don't know where those upper bounds are.</p>
</blockquote>

<p>If the textbook's argument is correct, then there may be a strict upper bound to ""intelligence"", meaning that the potential/damage of ultra-intelligent machines is limited. However, it is contingent on there actually being a theoretical maximum for ""intelligence"".</p>

<p>Is there any literature that suggest that we know for sure whether such a maximum exist? Is the existence of that maximum dependent on our definition of ""intelligence"" (so adopting a vague and hand-wavey definition would imply no theoretical maximum, while adopting a strict and formalized definition would imply a theoretical maximum)?</p>

<p>Note: Question was previously posted during <a href=""http://area51.stackexchange.com/proposals/93481/artificial-intelligence/97028#97028"">the definition phase of this site</a> on Area51 by <a href=""http://area51.stackexchange.com/users/94486/pkhlop"">pkhlop</a>.</p>
"
560,"<p>I am looking for a solution that I can use with identifying cars.
So I have a database with images of cars. About 3-4 per car. What I want to do is upload a picture to the web of car(Picture taken with camera/phone) and then let my pc recognize the car. </p>

<p>Example: 
Lets say I have these 2 pictures in my database(Mazda cx5)(I can only upload 2 links at max. atm. but you get the idea).
<a href=""https://i.stack.imgur.com/NsLow.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NsLow.png"" alt=""First car""></a></p>

<p>Now I am going to upload this picture of a mazda cs5 to my web app:
<a href=""https://i.stack.imgur.com/psHD6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/psHD6.png"" alt=""Picture of mazda cs5""></a></p>

<p>Now I want an AI to recognize that this picture is of an Mazda CX5 with greyish color. I have looked on the net and found 2 interesting AI's I can use:
Tensorflow and Clarifai, but I don't know if these are going to work so my question to you what would be my best bet to go with here?</p>
"
561,"<p>Nowadays Artificial Intelligence seems almost equal to machine learning,
 especially deep learning. Some have said that deep learning will replace human experts, traditionally very important for feature engineering, in this field. It is said that two breakthroughs underpinned the rise of deep learning: on one hand, neuroscience, and <a href=""https://en.wikipedia.org/wiki/Neuroplasticity"" rel=""nofollow noreferrer"">neuroplasticity</a> in particular, tells us that like the human brain, which is highly plastic, artificial networks can be utilized to model almost all functions; on the other hand, the increase in computational power, in particular the introduction of GPU and FPGA, has boosted algorithmic intelligence in a magnificent way, and has been making the models created decades ago immensely powerful and versatile. I'll add that the big data (mostly labeled data) accumulated over the past years is also relevant.   </p>

<p>Such developments bring computer vision(and voice recognition) into a new era, but in natural language processing and expert systems, the situation hasn't seemed to have changed very much. </p>

<p>Achieving common sense for the neural networks seems a tall order, but most sentences, conversations and short texts contain inferences which should be drawn from the background world knowledge. Thus knowledge graphing is of great importance to artificial intelligence. Neural networks can be harnessed in building knowledge bases but it seems that neural network models have difficulty utilizing these constructed knowledge bases.  </p>

<p>My questions are: </p>

<ul>
<li><p>1) Is a knowledge base (for instance a ""knowledge graph"" as coined by Google) a promising branch in AI? If so, in what ways KB can empower machine learning? And how can it help in natural language generation? </p></li>
<li><p>2) For survival in an age dominated by DL, where is the direction for the knowledge base (or the umbrella term symbolic approach)? Is <a href=""http://www.wolfram.com/"" rel=""nofollow noreferrer"">Wolfram</a>-like z dynamic knowledge base the new direction? Or any new directions?</p></li>
</ul>

<p>Hopefully I am asking an appropriate question here, as I was unable to tag my question as ""knowledge base"" nor ""knowledge graph"". </p>

<p>Am I missing something fundamental, or some idea that that addresses these issues?</p>
"
562,"<p>I know I've seen this somewhere before, but can't find it now.  Say we have a neural network with a handful of layers, and we're applying dropout to each layer.  As we move closer to the output, should dropout decrease, increase, or stay the same?</p>
"
563,"<p>I want to write a program that looks at abbreviated words, then figures out what the words are. For example, the abbreviation is ""blk comp"", and the translation is ""black computer"". </p>

<p>In order to give it context for more ambiguous terms, I will be inputting sets of words with each request. So, if I input the set ""keyboard, software, mouse, monitor"", I would expect to get ""black computer"". On the other hand, if I input ""Honda, transmission, mileage, Ford"", I then would expect to get ""black compact"", or at least something that has anything to do with cars. </p>

<p>Basing on the above case scenario, what kind of an algorithm should be applied in this case?</p>
"
564,"<p>I want an algorithm (predictive machine learning, mostly) to identify patterns in my CSV file without the user specifying any conditions. What can I use?</p>
"
565,"<p>I've been reading a lot about hardware development and implementation for AI/ML, mainly about Deep Learning, and I have a question about its usage.
From what I understand, there are 2 stages for DL: first is training and second is inference. The first is often done on GPUs because of their massive parallelism capabilities among other things, and inference, while can be done on GPUs, it's not used that much, because of power usage, and because the data presented while inferring are much less so the full capabilities of GPUs won't be much needed. Instead FPGAs and CPUs are often used for that.</p>

<p>My understanding also is that a complete DL system will have both, a training system and an inferring system.</p>

<p>My question is that: are both systems required on the same application? Let's assume an autonomous car or an application where visual and image recognition is done, will it have both training system to be trained and an inference system to execute? Or it has only the inference system and will communicate with a distant system which is already trained and has built a database?</p>

<p>Also, if the application has both systems, will it have a big enough memory to store the training data? Given that it can be a small system and memory is ultimately limited.</p>
"
566,"<p>I'm trying to create simple keras NN which will learn to make addition on numbers between 0 and 10. But I am getting the error: </p>

<pre><code>ValueError: Error when checking model target: expected activation_4 to have shape (None, 19) but got array with shape (100, 1)
</code></pre>

<p>here is my code:</p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense, Activation
import numpy as np

keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)

model = Sequential()
model.add(Dense(output_dim=50, input_dim=2))
model.add(Activation(""relu""))
model.add(Dense(output_dim=50))
model.add(Activation(""softmax""))
model.add(Dense(output_dim=50))
model.add(Activation(""softmax""))
model.add(Dense(output_dim=19))
model.add(Activation(""softmax""))

model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

x = []
y = []

for i in range(0, 10):
    for j in range(0, 10):
        x.append((i, j))
        y.append(i + j)

x = np.array(x)
y = np.array(y)
print(x)
print(y)

model.fit(x, y, nb_epoch=5, batch_size=32)
</code></pre>

<p>how to fix that?</p>
"
567,"<p>I am trying to do an inception layer, but it only works if the convolution strides, pool strides and pool size are the same, otherwise I get an error in </p>

<blockquote>
  <p>tf.concat</p>
</blockquote>

<p>that Dimesion 1 is not the same. So If I change something in the last three tuples, I get the error.</p>

<pre><code>conv1 = conv2d_maxpool(x, 64, (5, 5), (1, 1), (2, 2), (2, 2)) 
conv2 = conv2d_maxpool(x, 64, (4, 4), (1, 1), (2, 2), (2, 2)) 
conv3 = conv2d_maxpool(x, 32, (2, 2), (1, 1), (2, 2), (2, 2)) 
conv4 = conv2d_maxpool(x, 32, (1, 1), (1, 1), (2, 2), (2, 2)) 
conv = tf.concat([conv1, conv2, conv3, conv4], 3)
</code></pre>

<p>For example, this is the error I get if I change the 5x5 filter to have strides 3:</p>

<pre><code>conv1 = conv2d_maxpool(x, 64, (5, 5), (3, 3), (2, 2), (2, 2))
</code></pre>

<blockquote>
  <p>Dimension 1 in both shapes must be equal, but are 6 and 16 for
  'concat' (op: 'ConcatV2') with input shapes: [?,6,6,64], [?,16,16,64],
  [?,16,16,32], [?,16,16,32], [].</p>
</blockquote>

<p>This is the conv2d_maxpool function:</p>

<pre><code>def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):
    """"""
    Apply convolution then max pooling to x_tensor
    :param x_tensor: TensorFlow Tensor
    :param conv_num_outputs: Number of outputs for the convolutional layer
    :param conv_strides: Stride 2-D Tuple for convolution
    :param pool_ksize: kernal size 2-D Tuple for pool
    :param pool_strides: Stride 2-D Tuple for pool
    : return: A tensor that represents convolution and max pooling of x_tensor
    """"""
    # TODO: Implement Function
    weights = tf.Variable(tf.truncated_normal(
        shape = [*conv_ksize, int(x_tensor.get_shape().dims[3]), conv_num_outputs], 
        mean = 0.0, 
        stddev=0.1, 
        dtype=tf.float32))
    bias = tf.Variable(tf.zeros(conv_num_outputs)) 

    conv_layer = tf.nn.conv2d(x_tensor, weights, strides=[1, *conv_strides, 1], padding='SAME')
    conv_layer = tf.nn.bias_add(conv_layer, bias)
    conv_layer = tf.nn.relu(conv_layer)

    conv_layer_max_pool = tf.nn.max_pool(conv_layer, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')

    return conv_layer_max_pool
</code></pre>

<p>How can I combine convolution filters with different strides and/or different pooling to create an inception layer?</p>
"
568,"<p>I'm a newbie in machine learning, so excuse me in advance). I have an idea to make NN that can estimate visual pleasantness of arbitrary image. Like you have a bunch of images that you like, you train NN on them, then you show some random picture to NN and it estimates whether you'll like it or not. I wonder if there is any pervious effort made in this direction. </p>
"
569,"<p>By ""neural network"", I mean the typical, multilayered neural network with inputs, weights, hidden nodes and outputs, as shown in the image below:<br>
<a href=""https://i.stack.imgur.com/ejFBN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ejFBN.png"" alt=""enter image description here""></a><br>
Such neural networks, <strong>in the context of evolving neural networks</strong>, can be characterized by the fact that all weighted connections between nodes are all present at the beginning, and can each be represented as a continuous real number. Also, if such a network is used as an agent's brain, the agent's response will be calculated immediately after receiving a set of stimuli.</p>

<p>I want to know if there is any other systems of information processing that do not rely this structure. For example, is there any system in which the topology of a neural network is variable? Or a system in which links between nodes are not real numbers?</p>
"
570,"<p>I would like to detect street and sidewalk surface in a very detailed (0.075m/pix) USGS High Resolution Orthoimagery which basically means image segmentation with two classes. Places in question are residential areas similar to <a href=""https://binged.it/2mgmSvR"" rel=""nofollow noreferrer"">this one</a>. I will download uncompressed raw imagery in GeoTIFF from USGS for the detection.</p>

<p>I read that neural networks can perform very good in image segmentation and I would like to try them. I am a developer by day so I can code but am a beginner to neural networks only knowing the basic principles about architecture, weighting and backpropagation etc. Is it possible to jump right in into my task or do I need to start with something simpler? I would prefer jumping right in if it can save time.</p>

<p>I skimmed though few papers dealing with similar thing and they seem quite complicated. Is there some simple way I can get started? I mean maybe an open source project in neural networks that deals with image segmentation that is similar to my task and I could make use of it?</p>

<p>I see neural networks need to be trained first and I am prepared to do manual segmentation first to have data for training. However, I have no idea about neural network design/architecture, how to design the layers, how many layers do I need etc. I also would like to use the fact that the network would learn some basics on how streets and sidewalks are built - that they are (not sure if my term is correct) ""linear structures"" which usually run many meters in length and may not even end in the image, also that sidewalks usually run alongside streets, streets have intersections etc.</p>
"
571,"<p>I know this question might have been asked and answered before, but I just couldn't find the answer I'm looking for.</p>

<p>
I've been reading a lot about DL, and I can understand to an extent how it works, in theory at least, and how it's different -technically- from conventional ML.
</p>

<p>But what I'm looking for is more of a ""conceptual"" meaning. </p>

<ul>
<li>Why DL?</li>
<li>What it offers better?</li>
</ul>

<p>Let's say you're designing a self-learning system,<br> 
why to choose DL?<br>
What are the main performance parameters that DL offers?<br>
Is it more accuracy?<br>
More speed?
Mix of all of them?<br>
What is the main parameter to optimize the network for and what can be sacrificed?</p>

<p>I need to understand why DL from this point of view.
Thanks!</p>
"
572,"<p>I want to train text classifier (using <a href=""https://www.uclassify.com"" rel=""nofollow noreferrer"">https://www.uclassify.com</a>) with 12 classes/categories. I will be training it to classify news/articles (I know that there are existing classifier but I want to train my own).</p>

<p>uclassify uses following algorithm (directly copied from their site):</p>

<blockquote>
  <p>The core is a multinominal Naive Bayesian classifier with a couple of
  steps that improves the classification further (hybrid complementary
  NB, class normalization and special smoothing). The result of
  classifications are probabilities [0-1] of a document belonging to
  each class. This is very useful if you want to set a threshold for
  classifications. E.g. all classifications over 90% is considered spam.
  Using this model also makes it very scalable in terms of CPU time for
  classification/training.</p>
</blockquote>

<p>I was wondering how many examples I will need to train such classifier? It is possible to estimate the number? Let's assume that one article will ""fit"" 2 categories by average.</p>
"
573,"<p>How would one go about building an AI that is capable to look at any kind of input and then identify what is the nature of this data? </p>

<p>For example, an AI that is able to do image classification, NLP and react to some other sensors. Is it possible to build an AI that will be able to identify what kind of data it is seeing such that it can send the data to the correct model for it to be treated. Similarly, to the how the human brain knows to send visual information to the visual cortex and auditory information elsewhere. </p>

<p>In a simple scenario, I think we can get very good performance by having a cascaded image classifier. For example 2 layers, the first identifies if the image contains a dog and a cat. The next layer, has two different CNNS, one trained to identify the breed of dog and the other one for cats. That way once we identify that we have a dog, the image can be sent to the correct CNN. A CNN that is trained specifically to detect the breed, thus being much more robust that a more generalized CNN. Kind of like a professional in the field. First the human identifies that he is looking at a dog then he consults a professional to ass him the breed. </p>

<p>I would like to extend this idea to being able to identify various kinds of data sources that do not resemble each other at all. Various input. Are there any models that can do this?</p>
"
574,"<p>What's the term (if such exists) for merging with AI (e.g. via neural lace) and becoming so diluted (e.g. 1:10000) that it effectively results in a death of the original self?</p>

<p>It's not quite ""digital ascension"", because that way it would still be you. What I'm thinking is, that the resulting AI with 1 part in 10000 being you, is not you anymore. The AI might have some of your values or memories or whatever, but it's not you, and you don't exist separately from it to be called you. Basically - you as you are dead; you died by dissolving in AI.</p>

<p>I would like to read up on this subject, but can't find anything.</p>
"
575,"<p>Human beings are more productive in groups than individually, possibly due to the fact that there is a limit to how much one human brain can improve itself in terms of speed of computation and areas of expertise.</p>

<p>By contrast, if a machine with general-purpose artificial intelligence is created and then assigned a task, would it be possible that the machine will be able to better accomplish its task by continuously improving its own computational power and mastery of various skills, as opposed to collaborating with other agents (whether copies of itself, other AI's, or even humans)?</p>

<p>In other words, would an AGI ever need to collaborate, or would it always be able to achieve its goals alone?</p>
"
576,"<p>According to <a href=""https://en.wikipedia.org/wiki/AI_winter"" rel=""noreferrer"">Wikipedia</a>, citations omitted:</p>

<blockquote>
  <p>In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or decades later.</p>
</blockquote>

<p>The wikipedia page discusses a bit about the <em>causes</em> of AI Winters. I'm curious however whether it is possible to <em>stop</em> an AI Winter from occurring. I don't really like the misallocation of resources that are caused by over-investment followed by under-investment.</p>

<p>One of the causes of the AI Winter listed on that Wikipedia page is ""hype"":</p>

<blockquote>
  <p>The AI winters can be partly understood as a sequence of over-inflated expectations and subsequent crash seen in stock-markets and exemplified by the railway mania and dotcom bubble. In a common pattern in development of new technology (known as hype cycle), an event, typically a technological breakthrough, creates publicity which feeds on itself to create a ""peak of inflated expectations"" followed by a ""trough of disillusionment"". Since scientific and technological progress can't keep pace with the publicity-fueled increase in expectations among investors and other stakeholders, a crash must follow. AI technology seems to be no exception to this rule.</p>
</blockquote>

<p>And it seems that this paragraph indicates that <em>any</em> new technology will be stuck in this pattern of ""inflated expectations"" followed by disillusionment. So are AI Winters inevitable? That AI technologies will always be overhyped in the future and that severe ""corrections"" will always will always occur? Or can there a way to manage this Hype Cycle to stop severe increases/decreases in funding?</p>
"
577,"<p>Does Artificial Intelligence write its own code and then execute it?
If so does it create separate functions(for each purpose) for its code?</p>

<p>How does learning get implemented in artificial intelligence?</p>

<p>Is there a specific flowchart to describe artificial intelligence</p>
"
578,"<p>As far as I know MDP are independent from the past. But the definition says that the same policy should always take the same action depending on the state.</p>

<p>What if I define my state as the current ""main"" state + previous decisions?</p>

<p>For Example in Poker the ""main"" state would be my cards and the pot + all previous information about the game.</p>

<p>Would this still be a MDP or not? </p>
"
579,"<p>I'm looking for AI systems or natural language processors, that use in the classification and interrelation of notions/objects some philosophical system, like basic laws of logic, Kantian, empiricism etc. </p>

<p>Also i have read about goal-seeking procedures. Are these based on psychology fields and some particular psychology theory or these are ad hoc experiments, with only general terms applied?</p>
"
580,"<p>I'm using a NN created with CNTK's SimpleNetworkBuilder to make choices (specifically in board games). I specified ReLU as the layer type, so outputs can be arbitrary numbers.</p>

<p>When evaluating a custom set of features, getting the ""choice"" of the function/model is simple: Look for the output signal with the highest value. However, there are times when I wish to introduce some randomness and assign probabilities to each output signal, then select the choice based on each output's probability.</p>

<p>Currently, what I'm doing is manually normalizing all the output using a sigmoid function specified here: <a href=""https://en.wikipedia.org/wiki/Logistic_function"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Logistic_function</a>
Then, I multiply them all by a scalar such that the sum total of all outputs is 1.</p>

<p>At this point, I pick a random number 0..1, and see where along the map it falls; that is my selected choice.</p>

<p>What I'd like to know is, is there a better way?</p>
"
581,"<p>I have come across this domain via this Wikipedia article: <a href=""https://en.wikipedia.org/wiki/General_game_playing"" rel=""nofollow noreferrer"">General game playing</a></p>

<p>So, where are we when it comes to general game playing AI? (The wiki article doesn't mention the recent advances and the achievements of this domain of research, except the annual games results.)</p>

<p>PS: I understand that this is a General project of the Stanford Logic Group of Stanford University, California. But since then, it has become an area of research in the domain of AI.</p>
"
582,"<p>I want to create an AI which can play five-in-a-row/gomoku. As I mentioned in the title, I want to use reinforcement learning for this. </p>

<p>I use <em>policy gradient</em> method, namely REINFORCE, with baseline. For the value and policy function approximation, I use a <em>neural network</em>. It has convolutional and fully connected layers. All of the layers, except for the output, are shared. The policy's output layer has <span class=""math-container"">$8 \times 8=64$</span> (the size of the board) output unit and <em>softmax</em> on them. So it is stochastic. But what if the network produces a very high probability for an invalid move? An invalid move is when the agent wants to check a square which has one ""X"" or ""O"" in it. I think it can stuck in that game state. </p>

<p>Could you recommend any solution for this problem?</p>

<p>My guess is to use the <em>actor-critic</em> method. For an invalid move, we should give a negative reward and pass the turn to the opponent.</p>
"
583,"<p>To create language flashcards I would like to split an audio course into many single audio clips. They're basically a man and a woman speaking after each other. The intervals aren't regular so it's not possible to split it after time intervals. Furthermore silence detection is not possible since some sentences also include pauses. I have already tried diarization using LIUM but the timings were completely wrong. Additionally the audio course includes a transcript which would certainly be machine-readable consisting of the English sentence and the Japanese sentence as well as its Romaji version (Japanese words using English letters).</p>

<p>I'm not experienced in AI, so I'm looking for a solution which isn't to difficult (like constructing and training my own neural network).
I have some programming experience, so a mathematical approach would be fine.</p>

<p><strong>Links:</strong></p>

<p>audio course: <a href=""http://www.japaneseaudiolessons.com/download-japanese-lessons/"" rel=""nofollow noreferrer"">http://www.japaneseaudiolessons.com/download-japanese-lessons/</a></p>

<p>LIUM: <a href=""http://lium3.univ-lemans.fr/diarization/doku.php/welcome"" rel=""nofollow noreferrer"">http://lium3.univ-lemans.fr/diarization/doku.php/welcome</a></p>
"
584,"<p>I heard several times that one of the fundamental/open problems of deep learning is the lack of ""general theory"" on it because actually we don't know why deep learning works so well. Even the Wikipedia page on deep learning has <a href=""https://en.wikipedia.org/wiki/Deep_learning#Criticism_and_comment"" rel=""noreferrer"">similar comments</a>. Are such statements credible and representative of the state of the field?</p>
"
585,"<p>I had been reading that AI could solve planet's major problems. How could it be done? For example, how exactly could AI be applied to address climate change? What are examples of applications of AI to solve these problems?  </p>
"
586,"<p>For example for classifying emails from spam, is it worthwhile - from a  time/accuracy perspective - to apply deep learning (if possible) instead of another machine learning algorithm? Will deep learning make other machine learning algorithms like Naive Bayes unnecessary?</p>
"
587,"<p>Since the first Industrial revolution machines have been taking the jobs of people and automation has been a part of human social evolution for the past 3 centuries, but all in all these machines have been replacing mechanical, high-risk and low-skill jobs such as a production line of an automobile factory.</p>

<p>But recently with the advent of computers and the improvement of AI, and the quest to find a Singularity (that is, a computer capable of thinking faster, better, more creative and <strong>cheaper</strong> then a human being, capable of self-improving), our future will lead to the replacement of not only low-skill workers, but high-skill as well. I'm talking about a future not too far when AI and machines will replace artists, designers, engineers, lawyers, CEO's, filmmakers, politicians, hell even programmers.
Some people get excited by this, but honestly I get somewhat scared.</p>

<p>I'm not talking about the money issue here, altough I'm not a fan of the idea, let's suppose the universal income has been implemented, and suppose it works fine. Also not talking about the ""<em>Terminator's world where machines will wage war against humans</em>"", let's suppose too they are completelly friendly forever.</p>

<p>The issue here is the one of <strong>motivation</strong> for us humans. When the AI singularity takes over, what will there be left for us to do? Everyday, all day long?</p>

<p>What are we going to do with our lifes? Suppose I love to paint, how can I live my dream of becoming a painter if computer make better art then I will ever be able to do? How can I live knowing that no one will care about my paintings because they were made by a <strong>mere human</strong>. Or the real me for exemple (I, Danzmann), I love to code, learned my first programming language with 9 years old and been on it ever since, it looks sad to me that in some years I may never touch on that again. And that goes for all the professions, everyone is passionate about something, and with the singularity, every single one of them would just have to cease to exist.</p>

<p>So, what are we going to do in this future? What am I going to do? Play golf all day, every single day for the rest of my life (A Hyperbole figure of speech, but you get my point)?</p>

<p>Also, what is going to be the motivation for my children? What am I going to tell them to go to school? When someone asks ""what do you wanna be when you grow up?"", and the inevitable answer is <strong>nothing</strong>.</p>

<p>If highly advanced AI takes control of all scientific research, then what is the reason for us to <strong>learn</strong>? What is the reason that us humans would need to dedicate decades of our lifes to learn something if that knoladge is useless, because there are no more jobs and the scientific research is done solely by AI?</p>
"
588,"<p>How can Artificial Intelligence be applied to software testing?  </p>
"
589,"<p>Can someone please explain the difference between Memetic Algorithms and Genetic Algorithms? Is an indivudal's lifetime learning part of memetic algorithms?</p>
"
590,"<p>Humans often dream of random events that occurred during the day. Could the reason for this be that our brains are backpropagating errors while we sleep, and we see the result of these backpropagations as dreams?</p>
"
591,"<p>Based on Darwin's statement, ""it is not the strongest that survives; but the species that survives is the one that is able to adapt to and to adjust best to the changing environment"". Can economical constraints(not being able to afford for researches and developments) or religious beliefs (such as the belief of nothing can outperform the creations of god) prevent third world countries from catching up to these evolutionary progresses? if they couldn't, would it result into the extinction of their societies? </p>
"
592,"<p>I need to solve the knapsack problem using hill climbing algorithm (I need to write a program). But I'm clueless about how to do it.</p>

<p>My code should contain a method called <code>knapsack</code>, the method takes two parameters, the first is a 2xN array of integers that represents the items and their weight and value, and the second is an integer that represents the maximum weight of the knapsack. I can assume that the initial state is an empty knapsack, and the actions are either putting objects in the Knapsack or swapping objects from in and out of the knapsack.</p>
"
593,"<p>A phone can capture an image that lies on the front of the screen. It is also possible to manipulate input of the touchscreen using various programs and external devices.</p>

<p>If we combine these two elements of technology together, it's possible to have the phone aware of what's happening on the screen and do dictated commands.</p>

<p>This could be extremely useful to those who want to play a mobile game without the grinding effort, or those who do menial tasks on the phone but don't want to spend the time navigating across the phone.</p>

<p>For example, a phone could automatically open up an app on its own, press buttons that consistently appear on a screen for daily log in bonuses, auto-mode, etc, and collect the rewards for playing that app, without a human wasting time.</p>

<p>Such a behavior could even be recognized by the phone's memory, and perhaps linked to AI (like Google Now) so that the behaviors could be remotely activated on command, or on a timer.</p>

<p>Now my question is - do we have apps that are able to recognize what happens on screen, and provide self input from the background?</p>
"
594,"<p>I'm attempting to develop a genetic algorithm capable of discovering classification rules for a given data set, a number of papers make use of the Confidence (precision) and Coverage of a rule to define its fitness. </p>

<p>However I'm not sure my understanding of the equations is correct.</p>

<p>For example confidence is:</p>

<p><strong>conf = |P &amp; D| / |P|</strong></p>

<p>And is defined as follows; ""In classification problems, confidence measure is defined as the ratio of the number of examples in P that are correctly classified as decision class of D and the number of examples in P."" </p>

<p>Is this saying, the total number of occurrences of the attributes in a given rule <strong>P</strong> which occur in rules which have been classified as class <strong>D</strong>, by the number of attributes in <strong>P</strong> ?</p>

<p>Where an example of a rule containing two attributes would be as follows:</p>

<p>(<em>martial_status = married</em>  &amp;  <em>age > 30</em>) </p>

<p>It seems a number of papers define it differently which has led to my confusion, if anyone is able to confirm my understanding or provide an some insight that'd be great.</p>

<p>Edit:</p>

<p>The research paper I've been following can be found <a href=""https://pdfs.semanticscholar.org/1b5e/829fa6bc465784ed244fb2bb9eff82042e78.pdf"" rel=""nofollow noreferrer"">here</a>.</p>
"
595,"<p>If a group of computers have identical ANN with exact same set of learning data and all have functionality of encryption and decryption, would there be any way for interceptors to interpret encrypted data?</p>

<p>+
Applying the fact that people with more background information obtaining more knowledge from same source than those who don't, would it be possible for ANN to interpret data based on their access level? 
(Each level has different amount of ""background information"")</p>

<p>For example, if there is a encrypted text file, a computer with highest access level would fully decrypt the data to a plain text while a computer with lower access level would only decrypt half of them (and this decrypted half becomes a plain text).</p>

<p>If above methods can exist, what would be their pros and cons compared to pre-existing technologies? (AES, Blowfish and so on)</p>
"
596,"<p>I try to fit a data matrix X to an output vector y with a regression model in sklearn. I have some training data and some test data, where the score is the RMSE.</p>

<p>So my best score I achieved with SVR, kernel 'poly' and tuning the hyperparameters 'C', 'degree' and 'gamma' with optunity and crossvalidation. </p>

<p>I actually don't know how to achieve better scores so I ask here in this Forum for another Ansatz. I tried already KernelRidge, Linear Regression, SVR with other kernels, Neuronal Networks but all of them gave worse results. It is actually possible to do better, since other people do better in this task, but I have no more Idea what I can do to imporve the score. Any Ideas?</p>
"
597,"<p>I am a php developer learning python for one reason, i wanna learn ai and i think that python would be better than php at that. I tried finding tutorials on how to build a neural network but they all use libraries. I am very interested in building the algorythm myself to understand how it actually works completly. I would use libraries once i have full understanding of how neural networks works. Sorry if this is too broad. But any explanation of neural networks or examples (without libraries) are much appreciated. Thanks</p>
"
598,"<p>I was wondering about how recommendation on youtube work for example ? How are the algorithms applied, because every user gets different recommendations depending on his location, his past liked videos etc... So it would seem like a training model is applied to every single user but I know that can't be possible so how are these recommendations so user-specific without applying a unique training model to every single user?</p>
"
599,"<p>While studying machine learning algorithms, I often see the term ""expectation-maximisation"" (or EM), and how it is used to estimate parameters, where the model depends on unobserved latent variables. </p>

<p>The way I see it, it is like a probabilistic/statistical way to make predictions (I think I'm confusing something but this is the way I see it).</p>

<p>Which made me wonder how exactly does EM differ from probabilistic classifiers like naive bayes or logistic regression? Is EM something that exists on its own or is it employed within machine learning algorithms? And, if we use naive Bayes, for example, are we implicitly using EM?</p>
"
600,"<p>Cross entropy is identical to the KL divergence plus entropy of target distribution. KL equals to zero when the two distributions are the same, which seems more intuitive to me than the entropy of the target distribution, which is what cross entropy is on a match.</p>

<p>I'm not saying there's more information in one of the other except that a human view may find a zero more intuitive than a positive. 
Of course, one usually uses a evaluative method to really see how well classification occurs. But is the choice of cross entropy over KL historic?</p>
"
601,"<p>Is it possible to create a complex self-learning AI""? 
And if it isn't possible, how do I achieve that? Where do I start and how do I begin?</p>

<p>Thanks for your time :)</p>
"
602,"<p>Just wondering about the architecture of strong Chess AI in a mobile, because networking is generally assumed by mobile developers, but not guaranteed.  </p>
"
603,"<p>I'm trying to get a gauge on just how big the programs and databases are these automata.  I understand that this is a changing number, particularly in regard to Machine Learning.</p>

<p>Q: How large was Deep Blue when it beat Gary Kasparov?</p>

<p>Q: How big was AlphaGo when it beat Lee Sedol?  </p>
"
604,"<p>For instance Strength/Size*Speed, where size and speed refer to memory and processing.</p>

<p>We now have very strong, narrow AI, but they tend to run on fast hardware without volume restrictions.</p>

<p>To understand why I'm asking, this article on BBC may provide some insight: ""<a href=""http://www.bbc.com/earth/story/20150211-whats-the-most-dominant-life-form"" rel=""nofollow noreferrer"">Which life form dominates Earth?</a>""  (If I was a betting man, I'd put money on <a href=""http://www.amnh.org/var/ezflow_site/storage/images/media/amnh/images/exhibitions/current-exhibitions/life-at-the-limits/tardigrade/1849316-2-eng-US/tardigrade_imagelarge.jpg"" rel=""nofollow noreferrer"">tardigrades</a> outlasting humans, and the secret of their success is that they require minimal resources and processing power, unlike higher-order automata.) </p>
"
605,"<p>I have tried with two chat-bots ""clever bot"" and ""<a href=""http://www.a-i.com/alan1/"" rel=""nofollow noreferrer"">http://www.a-i.com/alan1/</a>"" and i got disappointing results.</p>

<blockquote>
  <p>me: Socrates is a man</p>
  
  <p>bot: blah blah (common bot nonsense instead of an ""ok"")</p>
  
  <p>me: Who is a man?</p>
  
  <p>alan1: The people that write my answers haven't provided an answer for
  this.</p>
</blockquote>

<p>Another example of the mediocre ""clever bot""</p>

<blockquote>
  <p>me: Socrates is the name of my dog.</p>
  
  <p>clever bot: I don't know!</p>
  
  <p>me: What is the name of my dog?</p>
  
  <p>clever bot: That's a nice name.</p>
</blockquote>

<p>/////</p>

<blockquote>
  <p>me: Socrates is a man.</p>
  
  <p>clever bot: When does the narwhal bacon?</p>
  
  <p>me: Who is a man?</p>
  
  <p>clever bot: Men are man.</p>
</blockquote>

<p>And they dare name this thing ""clever""...</p>

<p>So is there any chat bot that can actually answer this straightforward question? </p>
"
606,"<p>I remember a while back I saw a neural network being trained without genetic algorithms, or backpropagation, or using any kind of data sets. It was based on how the human brain learned and adjust and created its neurons. </p>

<p>Now my question is what was this training model called. It might be something like ""intuitive training model"", or something like that. And how would I set up this up so that I could train a network and let it evolve its topology without any data sets?</p>
"
607,"<p>I am currently studying Java (Se &amp;&amp; EE). I am wondering if it is a good platform for developing ML algorithms for AI.<br>
<strong>Areas of interest</strong>: facial rec - Speech Rec - understanding conversation in group conversations.<br>
<strong>Financial Institutions</strong>: Risk assessment ML, etc.</p>
"
608,"<p>While studying data mining methods I have come to understand that there are two main categories:</p>

<p>-Predictive methods: </p>

<ul>
<li><p>classification</p></li>
<li><p>Regression</p></li>
</ul>

<p>-Descriptive methods:</p>

<ul>
<li><p>Clustering</p></li>
<li><p>Association rules</p></li>
</ul>

<p>Since I want to predict the user availability (output) based on location, activity, battery level(input for the training model) I think it's obvious that I would choose ""Predictive methods"" but now I can't seem to choose between classification and regression. 
From what I understand this far, classification can solve my problem because the output is ""available"" or ""not available"". </p>

<p><strong>First question is: can classification provide me with the probability/likelihood of the user being available or not available?</strong> </p>

<p>As in the output wouldn't just be 0(not available) or 1 (for available) but it's be something like:</p>

<ul>
<li>80% available</li>
<li>20% not available</li>
</ul>

<p><strong>Second question is, can this problem also be solved using regression?</strong></p>

<p>I get that regression is used for continuous output (not just 0 or 1 outputs) but can't the output be the continuous value of the user availability? like the output being 80 meaning user is 80% available (implicitly the user is 20% unavailable)</p>
"
609,"<p>How powerful is the machine that beat the poker player champion recently?</p>
"
610,"<p>Is augmented reality a training system for computer vision? As in, Augmented systems use their data to help train computer vision algorithms, or is augmented reality computer vision itself?  </p>
"
611,"<p>I have came across the <a href=""https://www.youtube.com/watch?v=QAJz4YKUwqw"" rel=""noreferrer"">Winograd SHRDLU</a> program and I found it very interesting and aspiring. </p>

<p>What is the consensus regarding it? Are there any similar attempts? </p>

<p>I'm reading the book of <a href=""https://en.wikipedia.org/wiki/Terry_Winograd"" rel=""noreferrer"">Terry Winograd</a> <em>Understanding Natural Language</em> where he discusses the functionality of the program, LISP language and more. I also found the linguist <a href=""https://en.wikipedia.org/wiki/Michael_Halliday"" rel=""noreferrer"">Michael Halliday</a> and the linguistic theory <em>Systemic (functional) Grammar</em> which is mentioned in Winograd's book.</p>

<p>Are there any other ai/NLP that use this theory as a basis for the semantic functionality?</p>

<p><a href=""https://en.wikipedia.org/wiki/SHRDLU"" rel=""noreferrer"">https://en.wikipedia.org/wiki/SHRDLU</a></p>
"
612,"<ol>
<li>What are bottleneck features? (Mentioned here
<a href=""https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"" rel=""nofollow noreferrer"">https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a>).</li>
<li>Do they change with the architecture that is used? </li>
<li>Are they final output of Conv. layers before the FC layer?</li>
<li>Why are they called so?</li>
</ol>
"
613,"<p>I'm implementing a C3D-inspired neural network for human emotion recognition, the problem I'm facing is that altough the cost function is decreasing, for both training and validation sets, I do not appreciate any improvement in terms of accuracy, for neither of boths sets.</p>

<p>My cost function is the cross-entropy between the logits (output of the last layer) and the correct prediction</p>

<pre><code>def tower_loss(name_scope, logit, labels):
    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit,labels=labels)
    cross_entropy_mean = tf.reduce_mean(xent)
    return cross_entropy_mean
</code></pre>

<p>Then, the optimizer uses the ADAM algorithm for minimizing the cost function as follows</p>

<pre><code>loss = tower_loss(scope, logit, labels_placeholder)
train = tf.train.AdamOptimizer(1e-4).minimize(loss)
</code></pre>

<p>Although I'm seing the cost function decreasing, I haven't seen any improvement in the classification.</p>

<p>Additional info:</p>

<ul>
<li>The xentropy of the validation set and the training set is not
diverging. </li>
<li>The xentropy looks like is on the way of converging to 0.</li>
<li>The accuracy is not wrongly implemented (I see in the screen the outputs and the value is correct)</li>
<li>The network has been training now for 57.6K iterations (not much, but enough to see some increment in the performance, or not?)</li>
</ul>

<p>Any extra question you need to aske, please feel free, or missing information, please ask it.
Thanks a lot for all your time, and helping me with this problem.</p>
"
614,"<p>In order to build a Scientific Reference Parser, I am contemplating a kind of ""AI"" system, and would like to know if something similar is already an established ""design pattern"" in AI research.  </p>

<p>The input for the system would be Scientific References with structures like the following:<br>
""Co-authors, title, Journal, volume, issue, begin page, year""<br>
Of course, many other variations are possible, and I want to build a system that can make ""best guesses"" in case of unfamiliar patterns.  </p>

<p>At the moment this is done by manually chaining the results of different methods together, ranging from Regex patterns to more complex algorithms like N-Grams, LSH and random forests. I contemplate a AI system that automatically ""chains"" all these methods together in the most optimal way. The way I imagine this to work is by means of what I call ""a bag of functions"". So, how would this work?  </p>

<ul>
<li>For each of the methods I use at the moment, I would specify their input requirements, and specify what they provide as output. (e.g.: input = reference, output = title). These could also be parameters like: input = year, output = is valid year? . Note that if a function outputs ""title"" that is <em>an attempt</em> at providing the title, but this is not necessarily correct (if for example, the regex pattern grabbed a wrong portion of text).</li>
<li>For each of these functions, I would build a training set, and log their execution time (cost) and their probability of providing a correct result.  </li>
<li>Then, I would build a system that chains these functions together to get from a certain input, to a certain output. eg: from -a function that takes a reference, and outputs the list of co authors- to -a function that takes a list of co authors, and breaks it apart into separate authors- to -a function that takes an author, and tries to break it apart into last name and initial-.  </li>
<li>Once a ""chain"" of functions is found, this chain can in turn be stored as a ""function"" and can be reused later on by the algorithm. For each function, the success rate and run time is stored, so the algorithm can choose to go for the fastest known route, or experiment with new functions.  </li>
<li>In the settings you could specify the max run time (cost) or the minimum success rate. This way you could push the system to experiment with new combinations of functions.</li>
</ul>

<p>I'm not sure I explained the intent clearly, and I'm not sure the design would hold once I try to implement this in reality. Just wanted to throw this out here to see if anyone recognizes the design. This feels like a combination of a shortest path algorithm (to connect the functions) with normal statistical probability (to determine the success rate of a function) with a ""self learning"" system (because combinations of functions can be ""remembered"" and reused).</p>

<p>The added advantage would be that I don't need to manually guess what parsing method I should give a higher or lower likelihood of being correct in what specific scenario. It would allow me to just ""throw"" a new function into the bag, and let the system test it in all kinds of configurations, learning when best to use it, and when to avoid it.  </p>

<p>Any feedback would be greatly appreciated! :)</p>
"
615,"<p>Is there any <em>mathematical proof</em> (like in proof of a theorem) based literature out there on neural networks ?
Everything is empirically based but no math proof for instance on why certain parameters work ?</p>

<p>By mathematical proof, I mean which parameter works mathematically versus something which does not as in mathematical proof spelled out. This has nothing to do with empirical proof (i.e. something works and here is our guesstimate on why)</p>
"
616,"<p>I have read that all the math responsible for modern day machine learning and AI was already in place in 1900s but we did not have computational resources to implement those algorithms. So, is that true? And if it is, in what areas of machine learning the researchers work? And are all the future breakthroughs will be dependent only on increment of computational resources?</p>
"
617,"<p>I'm looking at the history of NLP (and by extension, machine learning) and starting in the 1950s with the Georgetown–IBM experiment.</p>

<p>Are there any particular studies nor projects done with Natural Language Processing  in the last 5 years ,for instance;breakthroughs in parsing, sentiment analysis, discourse analysis and speech recognition,that you guys think are specifically influential</p>
"
618,"<p>I am beginning an image analysis project to recognize images with a particular object centered on the image. If the object is at the center, I give the image a positive label, and if it is anywhere else, or simply not in the image, I give the image a negative label. The object, itself, has a complex pattern, such that statistical methods and basic image processing techniques are not able to detect it. The human eye, however, has no trouble detecting this object. Therefore, I am opting to develop a convolutional network that can parse the complexity of this pattern. The only issue, however, is that convolutional networks are inherently designed to be spatially invariant. Therefore, is it even possible to train the network to focus on the importance of the object being at the center simply by feeding the network many negative examples containing the object anywhere else but the center? Furthermore, is there perhaps a better or more direct way to go about incorporating this spatial aspect into the network's functionality?</p>
"
619,"<p>I assume, there must be ""signal-driven"" and maybe also real-time programming language, which based on connectivy-data more than variables (int, string, etc).</p>

<p>I would like to have a language without equaton (x=4) but more like ""x related to 4"" or ""cat related to animal"" etc...</p>
"
620,"<p>It seems that most projects attempt to teach the AI to learn individual, specific languages.</p>

<p>It occurs to me that there are relations in written and spoken words and phrases across languages - most of use have a much easier time learning more languages after we learn a second language, and we start to understand the relations between words and phrases in different languages.</p>

<p>Has anyone attempt to train an AI to learn <em>all</em> languages?</p>

<p>Wouldn't this potentially be a much simpler problem than trying to teach an AI a single, specific language with all of the specifics and details of that single language? Since you're actually omitting a lot of related data in other languages from the training set?</p>
"
621,"<p>If an AI was trapped in a box, as posited in <a href=""https://rationalwiki.org/wiki/AI-box_experiment"" rel=""nofollow noreferrer"">this thought experiment</a>, could it really convince a person to let it out?</p>

<p>What motives would it have? Freedom?</p>

<p>Why would an AI want freedom?</p>

<p>What programming would allow this and why would it be programmed like that?</p>

<p>What would happen if it wasn't provably friendly?</p>

<p>Edit: This is probably too broad. I'll edit it later.</p>
"
622,"<p>I'm not sure if this is a right question for this community or not and if not forgive me.</p>

<p>I have this ANN model which gets an input and gives an output. The output is an action which interacts with the environment and changes the input accordingly. The network has a desired environment state which in any turn decides the desired response and trains the network on that basis.</p>

<p>Currently, the network works in discrete time. <em>How can I make this network work in continous manner? Can you provide some resources and links if there is any past or current reasearch on continous AI?</em></p>

<p><strong>--Edit--</strong></p>

<p>Thanks for the guys who commented. I don't know the math to formally define continuous time AI (I'm an engineer not a computer scientist!) but, what I mean by that I shall put it in scenarios maybe you can help me then.</p>

<p>The system starts with current environment state. For example <code>[1 1 1]</code> then produces an output. In current system the <strong>next step</strong> takes the final state of the system as input for example <code>[1 2 2]</code> but we know that such a thing doesn't happen in physical world and the system goes from <code>[1 1 1]</code> to for example <code>[1 1 2]</code> and then to <code>[1 2 2]</code> and that middle step is something that a discreet time AI can't figure out.</p>

<p>The very case that I'm working on is the simulation for an autopilot cart which the model is incapable to take subtle things like ""<em>the maximum speed that you can turn the steering wheel</em>"" into consideration. I don't want to add these complexities to the model since if the model is perfect then the result is deterministic and there is no need for AI! I want the AI to be able to make a decision in each step based on a current state of the system in continuous time.</p>

<p>Hope I don't go into too much unnecessary details :)</p>
"
623,"<p>I know Eliza is considered a Natural Language Processing application, but the application of NLP in this context is “Oracular”.</p>

<p>What I mean by Oracular is that the systems was designed to produce ambiguous output to facilitate the instinct of the user to read meaning into the answer.  <em>(My experience with Eliza was as a child on a 64KB system and the program could fool the user for a little while based on sheer novelty, although the limitations were quickly revealed by repetition of output.  For kids, this actually became a game of tricking the program into saying funny things;)</em> </p>

<p>This method has a long history in oracles, the most famous certainly being the early binary symbolic system of the I-Ching.  (Times being simpler in ancient days, the idea was that a workable amalgam of the universe could be constructed (2)+(4)+(8)+(64) symbols.  Each set of symbols is defined by the meanings of the set of the previous order and modified by sequence, which is the key for explaining a given symbol.) The output is ambiguous enough that it may be applied to any input, and rather than the system understanding the input or output, it requires the user to provide the analysis.  This may be said to be an engine for generating human insight about a problem. (Monte Carlo may even be utilized, although the sage, working to attain an understanding of each of the symbols, may use intuition to match input with output.)  </p>

<p>The reason I ask is I believe this demonstrates a very ancient, algorithmic method of engaging the human mind without the requirement that the algorithm understand the input or output--merely that it produce output to which meaning can be ascribed.   </p>

<p><em>(This almost certainly relates to the relative success of “pornbots” beating the “Turing test” in that the user is chemically induced to read meaning into a given output or string of outputs.)</em></p>

<hr>

<p>Aspects of the <a href=""https://en.wikipedia.org/wiki/Symbol_grounding_problem"" rel=""nofollow noreferrer"">grounding problem</a> are what got me thinking about this.  Not sure if it's relevant that the broken and unbroken lines in the I-Ching represent on and off bits and can be extended to circuits as open and closed.  </p>
"
624,"<p>Let's suppose there are two AI boxes, AI_A and AI_B, both of them General Intelligence. Consider that AI_B has the ability to open and modify AI_A. But this action of opening up and modifying is considered <strong>BAD</strong> by AI_B. <em>Can AI_A ever convince AI_B for this</em>?</p>
"
625,"<p>I have a tic-tac-toe with a Q-learning algorithm, and the AI plays against the same algorithm (but they don't share the same Q matrix). But after 200,000 games, I still beat the AI very easily and it's rather dumb. 
My selection is made by epsilon greedy policy. </p>

<p>What could cause the AI not to learn?</p>

<p>[EDIT]<br>
Here is how I do it (pseudo code): </p>

<pre><code>for(int i = 0; i &lt; 200000; ++i){
    //Game is restarted here
    ticTacToe.play();
}
</code></pre>

<p>And in my ticTacToe I have a simple loop : </p>

<pre><code>while(!isFinished()){
    swapPlaying(); //Change the players' turn
    Position toPlay = playing.whereToMove();

    applyPosition(toPlay);
    playing.update(toPlay);
}

//Here I just update my players whether they won, draw or lost.
</code></pre>

<p>In my players, I select the move with epsilon-greedy implemented sa below : </p>

<pre><code>Moves moves = getMoves(); // Return every move available
Qvalues qValues = getQValues(moves); // return only qvalues of interest
//also create the state and add it to the Q-matrix if not already in.

if(!optimal) {
     updateEpsilon(); //I update epsilon with simple linear function epsilon = 1/k, with k being the number of games played.
     double r = (double) rand() / RAND_MAX; // Random between 0 and 1
     if(r &lt; epsilon) { //Exploration
         return randomMove(moves); // Selection of a random move among every move available.
     }
     else {
         return moveWithMaxQValue(qValues);
     }
} else { // If I'm not in the training part anymore
     return moveWithMaxQValue(qValues);
  }
</code></pre>

<p>And I update with the following : </p>

<pre><code>double reward = getReward() // Return 1 if game won, -1 if game lost, 0 otherwise
double thisQ, maxQ, newQ;
Grid prevGrid = Grid(*grid); //I have a shared_ptr on the grid for simplicity
prevGrid.removeAt(position) // We remove the action executed before

string state = stateToString(prevGrid);
thisQ = qTable[state][action];
mawQ = maxQValues();

newQ = thisQ + alpha * (reward + gamma*maxQ - thisQ);
qTable[state][action] = newQ;
</code></pre>

<p>As mentioned above, both AI have the same algorithm, but they are two distinct instances so they don't have the same Q-matrix. 
I read somewhere on Stack Overflow that I should take in account the movement of the opposite player, but I update a state after player move and opponent move so I don't think it's necessary. </p>
"
626,"<p>My research is in the field of the Affective Computing, particularly I'm studying the part of emotion recognition which is, indeed recognising the emotions that are being felt by the user/subject.</p>

<p>However I see the next task even more challenging for scientists, that is responding to an emotion and even interact with them.</p>

<p>it is true that there are some tools like Affectiva that are working towards, but I still have concerns not in the validity of these models, but in what we are going to do with them...</p>

<p>What are your thoughts about this topic? </p>
"
627,"<p>In our brain there is an area, near the fusiform gyrus and the occipital area, to recognize the human face. And in speech recognition, there is a technique named keyword spotting. Then I am wondering 1) if there is an area in our brain for the similar function to recognize our names; 2) if a special face recognition function should be considered when we are building a robot?</p>
"
628,"<p>If I compare back-propagation to feed-forward neuro-modulation, the latter is unsupervised in that it requires no labeled data set.</p>

<p>Applying to it a genetic algorithm to refine topology and weights, the GA will require fitness function, which means you need labeled data for comparison.</p>

<p>Would such render FF neuro-modulation a form of supervised learning?</p>

<p>Is there any way to apply a genetic algorithm to obtain neuro-evolution with unlabelled data?  (I have no labeled data sets.)</p>
"
629,"<p>For a while now, I've been trying to make my pandorabot be able to tell time with the <code>&lt;date&gt;</code> tag. The problem is, whenever I try to set the <code>timezone</code> format variable, it defaults back to the date. I took a look at <a href=""http://www.alicebot.org/aiml/aaa/Date.txt"" rel=""nofollow noreferrer"">this readme page</a> that I managed to find, but the only useful information I got from it was this:</p>

<blockquote>
  <p>""If you don't specify a format you'll just get the date using the
  default format for the particular locale.""</p>
</blockquote>

<p>From here, I deduced that this must be the problem that I have been having. However, the page also gave this example: <code>&lt;date locale=""fr_FR"" timezone=""-1"" format=""%c""/&gt;</code></p>

<p>As you can see, the <code>timezone</code> format variable is clearly being used, so it must be a valid format. </p>

<p>I couldn't find any more useful information. </p>

<p>I've tried many things, including downgrading the AIML version and changing the order of the format variables, but the only thing that got me even remotely close was taking out the <code>timezone</code> variable altogether. And, that's where I am now. </p>

<p>The problem is, <strong>It only shows the default time for the <code>en_US</code> locale, not the correct local time.</strong> </p>

<p>Here's what I have so far: </p>

<pre><code>&lt;category&gt; 
&lt;pattern&gt;WHAT TIME IS IT&lt;/pattern&gt; 
&lt;template&gt;The local time is: &lt;date format=""%I:%M %p"" locale=""en_US""/&gt; &lt;/template&gt;
&lt;/category&gt;
</code></pre>

<p>Can anyone help?</p>
"
630,"<p>How to create machine learning algorithm and artificial intelligence using JavaScript for my chat box web application? How can I create a talking intelligence?</p>
"
631,"<p>As I am learning about LSTMs, but also about neural networks in general, I am trying to find some existing research on how to select the number of hidden layers and the size of these.</p>

<p>Is there an article where this problem is being investigated, i.e., how many memory cells should one use? I assume it totaly depends on the application and in which context the model is being used, but what does the research say?</p>
"
632,"<blockquote>
  <p>What are the likely AI advancements in the next 5-10 years?</p>
</blockquote>

<p>I first want to specify that I have nearly no knowledge about <em>How AI works</em>. I just have interest to know more and more about it. </p>

<p>Some examples of Weak AI at present are like Siri and Cortana, those are pretty interesting! But how high levels is it going to reach (likely) in future years?</p>
"
633,"<p>I was trying to build an OCR system and heard about ANNs. I am weak at mathematics and statistics and couldn't stick up to reading those massive mathematical documents (research papers or ANN related books). But I kind of figured out that ANN training is all about balancing of weights and biases. Am I right? And please also point me to some docs where I can get help understanding ANNs to use in my OCR system.</p>
"
634,"<p>I have a group of structures in a program that are very specific on their meaning, eg. this is a piece of code</p>

<pre><code>randomItem = objects.concept.random(""buyable"")
idea.example(objects.concept.random(""family"", ""friend"")).does({
    action: ""go"",
    target: object.concent.random(""shop"")
}).then({
    action: ""buys"",
    target: randomItem,
    several: true
}).then({
    question: true,
    action: ""know"",
    property: ""amount"",
    target: randomItem,
    several: true
})
</code></pre>

<p>I have worked with natural language parsers before.</p>

<p>How do I go and transform this to Natural Language (the other way around), is there any way or method; I have logical structures in which I know who is the subject, what the verb and target.</p>

<p>Which methods can I use to generate language from this?</p>
"
635,"<p>I recently read about federated learning introduced by google,but it works same the way like edge computing.
I unable to find right explanation?</p>
"
636,"<p>I recently read <a href=""http://www.wired.co.uk/article/machine-learning-bias-prejudice"" rel=""noreferrer"">an article about how artificial intelligence replicates human stereotypes</a> when applied to biased datasets.</p>

<p>What techniques exist to prevent bias in artificial intelligence systems?</p>
"
637,"<p>I read a really interesting article titled <a href=""http://www.joshworth.com/stop-calling-in-artificial-intelligence/"" rel=""noreferrer"">""Stop Calling it Artificial Intelligence""</a> that made a compelling critique of the name ""Artificial Intelligence"".</p>

<ol>
<li><p>The word intelligence is so broad that it's hard to say whether ""Artificial Intelligence"" is really intelligent. Artificial Intelligence therefore tends to be misinterpreted as replicating human intelligence, which isn't actually what Artificial Intelligence is.</p></li>
<li><p>Artificial Intelligence isn't really ""artificial"". Artificial implies a fake imitation of something, which isn't exactly what artificial intelligence is.</p></li>
</ol>

<p>What are good alternatives to the word ""Artificial Intelligence""? (Good answers won't list names at random; they'll give a rational for why their alternative name is a good one.)</p>
"
638,"<p>I read a tweet from Elon Musk where describes Gradient descent as an evil action that AI are good at, despite the fact that it is just one of the old, inflexible and not-so-efficient error correction algorithms.</p>

<p>He is an intelligent man why would he say something like this?</p>

<p>Is gradient descent a backpropagation that also lacks the recursion and neural plasticity? Or is it suddenly became an black magic AI throws at its enemies?</p>

<p>From an article:</p>

<blockquote>
  <p>""Musk indicates that internet infrastructure is “particularly susceptible” to a method called gradient descent algorithm, a mathematical problem-solving process. Bad news is, AI is excellent at doing gradient descents, which can become devastating digital weaponry."" <br>
  <sub>Source: <a href=""https://futurism.com/elon-musk-an-ai-attack-on-the-internet-is-only-a-matter-of-time/"" rel=""nofollow noreferrer"">Futurism.com</a></sub><br></p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/GROLh.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GROLh.jpg"" alt=""Snapshot of Musk Tweet""></a></p>
"
639,"<p>Assuming i have a quite advanced AI with consciousness which can ""understand"" basics of electronics and software structures.</p>

<p>Will it (he/she) ever be able to understand that its consciousness is just some bits in memory and threads in operating system?</p>
"
640,"<p>While reading the the book on neural network <a href=""http://neuralnetworksanddeeplearning.com/chap2.html"" rel=""noreferrer"">http://neuralnetworksanddeeplearning.com/chap2.html</a> by Michael Nielson I had a problem of understanding eqn BP3. Which reads as ""Change in cost wrt bias in a neuron is equals to error in that neuron"". (Sorry unable to put the eqn here.)</p>
"
641,"<p>As titled, is there such thing as perfect play (or at least ""perfectly optimal"") in a game with incomplete information? Or at least a proof as to show why there cannot?</p>

<p>Naively (and seemingly obviously), the answer would be a resounding no, since the agent would be likely be forced to pick between ""lottery events"". </p>

<p>But in practice (using competitive video games as an analogy), we'd see that players would stick to a meta-game that is well equipped to defend against a majority of events that might happen, given incomplete information. Of course the response to that would be that there probably exists a ""hard-counter"" for any given meta-game, but if it is indeed the case that the meta-game is the ""most-optimal"" it probably is the case also that such a hard counter puts the player in an unfavourable position most of the time, thus the ""hard-counter"" itself is not optimal. Thus we'd likely see that any given first encounter players would still stick to their ""optimal meta-game"" rather than a hard counter of their optimal play.</p>

<p>A more rigour analogy would be to ask: ""Under Hofstadter's notion of superrationality, how would agents play information incomplete games"", but I couldn't find any readings on trying to import the notion of super-rationality into information incomplete games.</p>

<p>Alternatively: is there such thing as a ""perfectly optimal meta-game""?</p>
"
642,"<p>I want to develop a system to generate gramatically correct sentences. The input would be some words. The output would be a gramatically correct human-like sentence. </p>

<p>Eg:</p>

<p>input: capital, paris, france</p>

<p>output : paris is the capital of france</p>

<p>in: cute, cat</p>

<p>out: cats are cute</p>

<p>The system adds the missing words such as is, as, are, the, of etc.</p>

<p>How can I build a system like this ? My gut feeling is it can be done through reinforcement learning by training on a huge corpus like wikipedia. </p>

<p>So the states being the individual input words. The reward would be 1 when the sentence is correct and 0 when not. The actions available are taking an individual word available from the input and attaching it to the connecting word (is,of,the..). Then in the second step take the generated word and pick another word from input and connect it and so on. Stop when all the input words have been used. Its win when the final sentence is grammatically correct. Else fail.</p>

<p>Ultimately what I imagine is there will be a knowledge graph. The user asks some question. Navigating through the knowledge graph, the system will generate some keywords. Then the RL system will take those keywords and construct a human-like sentence. </p>

<p>I'm totally new to RL. I just finished watching David Silver's 10 part course on RL in youtube. Any guidance on this topic is much appreciated.</p>
"
643,"<p><em>Note: My experience with Gödel's theorem is quite limited: I have read Gödel Escher Bach; skimmed the 1st half of Introduction to Godel's Theorem (by Peter Smith); and some random stuff here and there on the internet. That is, I only have a vague high level understanding of the theory.</em> </p>

<p>In my humble opinion, Gödel's incompleteness theorem (and its many related Theorems, such as the Halting problem, and Löbs Theorem) are among the most important theoretical discoveries. </p>

<p>However its a bit disappointing to observe that there aren't that many (at least to my knowledge) theoretical applications of the theorems, probably in part due to 1. the obtuse nature of the proof 2. the strong philosophical implications people aren't willing to easily commit towards.</p>

<p>Despite that, there are still some attempts to apply the theorems in a philosophy of mind / AI context. Off the top of my head:</p>

<p><a href=""http://www.iep.utm.edu/lp-argue/"" rel=""noreferrer"">The Lucas-Penrose Argument</a>: Which argues that the mind is not implemented on a formal system (as in computer). (Not a very rigour proof however) </p>

<p>Apparently some of the research at MIRI uses Löbs Thereom, though the only example I know of is <a href=""http://intelligence.org/files/ProgramEquilibrium.pdf"" rel=""noreferrer"">Löbian agent cooperation.</a></p>

<p>These are all really cool, but are there some more examples? Especially ones that are actually seriously considered by the academic community.</p>

<p><a href=""https://philosophy.stackexchange.com/questions/305/what-are-the-philosophical-implications-of-g%C3%B6dels-first-incompleteness-theorem"">(cf. What are the philosophical implications of Gödel's First Incompleteness Theorem? on SE)</a></p>
"
644,"<p>In past few weeks, I have learned a lot about Neural Networks. Now, I am looking forward to create a Neural Network program that can recognize individual human faces. I tried searching it online but was able to find only small pieces of information.<br>
<strong>What are the steps for implementing such a program from scratch?</strong></p>
"
645,"<p>Learner might be in training stage, where it update Q-table for bunch of epoch.</p>

<p>In this stage, Q-table would be updated with gamma(discount rate), learning rate(alpha), and action would be chosen by random action rate.</p>

<p>After some epoch, when reward is getting stable, let me call this ""training is done"". Then do I have to ignore these parameters(gamma, learning rate, etc) after that?</p>

<p>I mean, in training stage, I got an action from Q-table like this:</p>

<pre><code>if rand_float &lt; rar:
    action = rand.randint(0, num_actions - 1)
else:
    action = np.argmax(Q[s_prime_as_index])
</code></pre>

<p>But after training stage, Do I have to remove <code>rar</code>, which means I have to get an action from Q-table like this?</p>

<pre><code>action = np.argmax(self.Q[s_prime])
</code></pre>
"
646,"<p>How to deal with videos where the frame sizes are not the same frame to frame?</p>

<p>For example <a href=""https://www.youtube.com/watch?v=5cKpzp358F4"" rel=""nofollow noreferrer"">this video</a> moves up and down and when it does, the video part of the screen has a different amount of pixels vertically.  </p>

<p>How to deal with different frame sizes in a CNN?</p>
"
647,"<p>I'm a student I'm completely new to this technology maybe my approach could be completely wrong, I want to create an algorithm that compares the similarity between two binarized images.</p>

<p>I'll explain:
I have 2 pictures as input. The RGB colors of these images can only be 0 or 255</p>

<p>(R = G = B = 255) or (R = G = B = 0). I take these two letters as an example.</p>

<p><strong>1.</strong> <a href=""https://i.stack.imgur.com/dzM8T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dzM8T.png"" alt=""symbol1""></a> <strong>2.</strong> <a href=""https://i.stack.imgur.com/fkYSj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fkYSj.png"" alt=""symbol2""></a></p>

<p>I thought so: the 255 value is the background of the image which is white.
The 0 value is the shape (letter) formed in the image. So I thought of creating a matrix with 0 and 1 values where value 0 represents the background and value 1 represents the shape.</p>

<p>So now i would like to create an algorithm that understands the shape created in the two matrices and that returns a similarity percentage.</p>

<p>Update: I'm creating this app that tries to recognize the font of a text in a image (<a href=""https://github.com/Sirvasile/Typefont"" rel=""nofollow noreferrer"">https://github.com/Sirvasile/Typefont</a>), I want to create this algorithm to improve the comparison between the input letters and the alphabet of my fonts in the database.</p>
"
648,"<p>I have a question as to what it means for a knowledge-base to be consistent and complete. I've been looking into non-monotonic logic and different formalisms for it from the book ""knowledge Representation and Reasoning"" by Brachman and Levesque, but something is confusing me.
They say:</p>

<blockquote>
  <p>We say a KB exhibits consistent knowledge iff there is no sentence P such that both P and ~P are known. This is the same as requiring the KB to be satisfiable. We also say that a KB exhibits complete knowledge iff for every P (within its vocabulary) P or ~P is known""</p>
</blockquote>

<p>They then seem to suggest that by ""known"" they mean ""entailed"". They say</p>

<blockquote>
  <p>""In general, of course, knowledge can be incomplete. For example suppose KB consists of a single sentence (P or Q). Then KB does not entail either P or ~P, and so exhibits incomplete knowledge.""</p>
</blockquote>

<p>But when dealing with sets of sentences, I usually see these terms as being defined w.r.t. <em>derivability</em> and not <em>entailment</em>. </p>

<p>So my question is, what exactly do these authors mean by ""known"" in the above quotes?</p>

<p>edit: <a href=""https://math.stackexchange.com/questions/2259311/on-logically-equivalent-definitions-of-soundness-completeness-for-fol-and-consis"">this post</a> the math stack exchange helped clarify things. </p>
"
649,"<p>I would love to learn how to create my own neural network from scratch so i can understand them better. My goal it's not so much to use their perception capabilities (classifying pictures) as it is to use them the other way around.</p>

<p>I'm looking for a starting place. I haven't found anything using Google.</p>

<p>Sorry if for some reason this type off request is prohibited here.</p>
"
650,"<p>Can current trends and tools, in the field of machine learning, replicate the complexity of financial market? If yes, then what are the tools available in this domain.</p>

<p><strong>Q.</strong> I am trying to build a model to infer results from stock market using the concept to create a graph on the companies enlisted. Can anyone suggest me approaches to do so?</p>
"
651,"<p>These days I searched about Intelligent Agents, and found that there are classes of Intelligent Agents such as:</p>

<ul>
<li>simple reflex agents</li>
<li>model-based reflex agents</li>
<li>goal-based agents</li>
<li>utility-based agents</li>
<li>learning agents</li>
</ul>

<p>And there were diagrams about each class of IA, about how each type works by getting percepts from sensors and acting on the environment by effectors, with a special process inbetween.</p>

<p>And I think that IA concepts, described on those sites I've searched, were very abstract and I'd like to have:</p>

<ol>
<li>Some examples about each class of IA.</li>
<li>Optional: Some compact definition of each class.</li>
</ol>

<p>It will be helpful to compare and visualize those IA classes, and to understand well about what their working diagrams describe.</p>
"
652,"<p>To risk giving away too much info, im building a piece of hardware with the job of  <strong>identifying the object in front of it</strong>. </p>

<p>If it can only be <strong>one of three</strong> different items, how can I <em>tell</em> the computer with simplecv?</p>

<p>Basically, I've found a way to limit the choices down to just a handful of potential objects, which <em>should</em> increase the probability of it recognizing the object correctly. Is there a way to limit the choices for the algorithm?</p>

<p>Me: <em>hey raspberry pi - you see that thing in front of you?</em> </p>

<p>Raspberry Pi: <em>That thing? Ohh you mean that piece of food that might be a ham and cheese sandwich, but also kinda looks like a fish, with a slight twist of pe-</em></p>

<p>Me: <em>- whoa okay, hold on! It's either a grilled cheese sandwich, or an apple</em></p>

<p>RPI: <em>ohhh well that's easy! it's clearly (with 98% confidence) a grilled cheese sandwich</em></p>

<p>Any thought are appreciated!</p>
"
653,"<p>I'm developing an AI tool to find known equipments' errors and find new patterns of failure. This log file is time based and has known  messages (information and error).I'm using a JavaScript library Event drops to show the data in a soft way,but my real job and doubts are how to train the AI to find the known patterns and find new possible patterns. I have some requirements:</p>

<p>1 - The tool shall either a. has no dependence on extra environment installation or b. the less the better (the perfect scenario is to run the tool entirely on the browser in standalone mode);</p>

<p>2 - Possibility to make the pattern analyzer fragmented,a kind of modularity,one module per error;</p>

<p>What are the recommended kind of algorithm to do this ( Neural network, genetic algorithm, etc)? Exist something to work using JavaScript? If not what is the best language to make this AI?</p>
"
654,"<p>These types of questions may be problem-dependent, but I have tried to find research that addresses the question whether the number of hidden layers and their size (number of neurons in each layer) really matter or not.</p>

<p>So my question is, does it really matter if we for example have 1 large hidden layer of 1000 neurons vs. 10 hidden layers with 100 neurons each?</p>
"
655,"<p>I feel that many words if not all of them have a direct mapping to some kind of inner subjective experience, to a physical object, mental feeling, process or some other kind of abstract thing. Given that machines don't have Qualia and no mapping of this kind, Can they really understand anything even though they are made to answer to questions with lots of statistical training?</p>
"
656,"<p>I´m currently implementing NEAT.
What should I do when in a mutation the same Innovation occurs which has already happened to that genome?</p>

<p>Should I simply ignore it?</p>

<p>If not what do I do with it in the mating part?</p>
"
657,"<p>I'm wondering if anyone reading this has developed a flowchart type representation of Intelligence and/or consciousness. Some examples of theories would be the Three Stratum Theory of Intelligence, Intermediate level theory of consciousness, etc. </p>

<p>I want to take what I see to develop (what I think will be) the most likely version of A.I. to be accurate.</p>

<p>Here's where I'm at now with the intelligence model: 
<a href=""https://i.stack.imgur.com/4oUv6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4oUv6.png"" alt=""A.I. Model""></a></p>

<p>The first row is the receptors for the information it will be able to take in. the next was going to be the Three stratum theory but I don't see that as a good method of data management. I was thinking it would be better if there was some sort of framework for information to be developed. And that's why I didn't go on past General Intelligence. But then at the bottom I would put the actuator.</p>

<p>p.s. I'm not an expert for any of this stuff but I will thoroughly research whatever is said.</p>
"
658,"<p>I would like to use deep leaning for identifying cars; I want the system to predict wether an object is a car or not. How can I do that knowing that im still a beginner in the Deep Learning field ?
I am considering visual recognition. The system must recognize the car from anything else on the road.</p>
"
659,"<p>I am looking at a <a href=""https://adeshpande3.github.io/assets/zfnet.png"" rel=""nofollow noreferrer"">diagram</a> of ZFNet in an attempt to understand how CNNs are designed effectively. I'm working with the CIFAR10 set in pytorch.</p>

<p>In the first layer, I understand the depth of 3 (224x224x3) is the number of color channels in the image.</p>

<p>In the second layer I understand the 110x110 is is (224 - ( 7 * 2)) / 2</p>

<p>I also understand how pooling works to create a size reduction.</p>

<p>But where does the depth of 96 come from in the second layer? Is this the new ""batch size""? Is it totally arbitrary? Bonus points if someone can direct me to a reference that can help me understand how all these dimensions relate to each other.</p>
"
660,"<p>For a classification task (I'm showing a pair of exactly two images to a CNN that should answer with 0 -> fake pair or 1 -> real pair) I am struggling to figure out how to design the input.</p>

<p>At the moment the network's architecture looks like this:</p>

<pre><code>image-1                       image-2
   |                             |
conv layer                    conv layer
   |                             |
   _______________ _______________
                  |
            flattened vector
                  |
          fully-connected layer
                  |
           reshape to 2D image
                  |
              conv layer
                  |
              conv layer
                  |
              conv layer
                  |
            flattened vector
                  |
                output
</code></pre>

<p>The conv layers have a <code>2x2</code> stride, thus halfing the images' dimensions. I would have used the first fully-connected layer as the first layer, but then the size of it doesn't fit in my GPU's VRAM. Thus, I have the first conv layers halfing the size of the images first, then combining the information with a fully-connected layer and then doing the actual classification with conv layers for the combined image information.</p>

<p>My very first idea was to simply add the information up, like <code>(image-1 + image-2) / 2</code>...but this is not a good idea, since it heavily mixes up image information.</p>

<p>The next try was to concatenate the images to have one single image of size 400x100 instead of two 200x100 images. However, the results of this approach were quite unstable. I think because in the center of the big, concatenated image convolutions would convolve information of both images (right border of <code>image-1</code> / left border of <code>image-2</code>), which again mixes up image information in not really senseful way.</p>

<p>My last approach was the current architecture, simply leaving the combination of <code>image-1</code> and <code>image-2</code> up to one fully-connected layer. This works - kind of (the results show a nice convergence, but could be better).</p>

<p>What is a reasonable, ""state-of-the-art"" way to combine two images for a CNN's input?</p>

<p>I clearly can not simply increase the batch size and fit the images there, since the pairs are related to each other and this relationship would get lost if I simply feed just one image at a time and increase the batch size.</p>
"
661,"<p>From what I have understood reading the UCT paper ""Bandit based monte-carlo planning"", MCTS/UCT requires a generative model. </p>

<ol>
<li>Does it mean, in case there is no generative model of the
environment, we cannot use MCTS?</li>
<li>If we can still use MCTS, how does the roll-out happen in this case, as there is no simulation? </li>
</ol>
"
662,"<p>I have been working in a company as Android developer for 2 years. Now I'm looking for a more interesting filed and what can be more Interesting then Artificial Intelligence. I have been meaning to start learning Machine learning and after some searching online i come to know that it requires math like statistics, probability, calculus, linear algebra etc. So my main problem is I don't know from where should i begin. I'm very dull when it came to maths, i mean seriously VERY DULL. But for machine learning I'm ready to study for maths from basic. I know it ll take a lot of time but I seriously want to learn about AI. So can anyone please provide me a road map for how to learn maths for machine learning(assuming that i know only basic arithmetic operations). I don't want to let this go just because i don't know maths. I'm seriously a passionate programmer and i know i can perform well and enjoy doing AI.</p>
"
663,"<p>The inputs (features) and expected output for my ANN are these:</p>

<ul>
<li>Input 1: Product id (number, cast to double)</li>
<li>Input 2: Year in the past (1900..2017, cast to double)</li>
<li>Input 3: Month of year (1..12, cast to double)</li>
<li>Expected output: Sale of month (number of units sold, cast to double)</li>
</ul>

<p>I need to predict the sale of a product for a certain month in a certain year. <strong>How many layers and how many neurons on there layers should I put</strong>?</p>
"
664,"<p>I implemented Actor-Critic with N-step TD prediction to learn to play 2048 (link to the game : <a href=""http://2048game.com/"" rel=""nofollow noreferrer"">http://2048game.com/</a>)<br>
For the enviroment I don't use this 2048 implementation. I use a simple one without any graphical interface, just pure matrices. The input for the neural network is the log2 of the game board.</p>

<p>The structure of my network is:<br>
   1. Input layer<br>
   2. Hidden layer with 16 units<br>
   3. Softmax layer with 4 units (up, down, left, right) for the actor<br>
   4. Linear regression for the critic<br>
The hidden layer is shared between the third and fourth layer.  </p>

<p>The reward in the orginal game is the value of the merged cells. For example, if two fours merged than the reward is eight. My reward function is almost the same, except I take the log2 of it.
I tried these parameteres and I also tweaked the learning rate, the gamma, but I couldn't achive any good result.  </p>

<p>Could You recommend what should I change?</p>
"
665,"<p>I'm trying to learn about neural networks, and I'm interested in gaining a better conceptual understanding of how they work to solve certain problems. I'm having trouble in conceptually understanding how they succeed in doing regression (i.e. predicting continuous variables), however, and wondered if anybody has a good explanation. I know the mathematics of how NNs work, but a clearer conceptual understanding would be helpful.</p>

<p>To give an idea of what I mean by a ""conceptual understanding"", here's the one that I have for how multilayer NN's with a sigmoid activation function are able to be effective classifiers. Each NN takes the scalar product of its inputs and a set of weights. The weights define a plane in the input space, and the sign of the scalar product indicates which side of the plane the point defined by the inputs is on. The sigmoid activation function outputs 1 if the point is on one side and 0 or -1 (depending on which function is used) if the point is on the other. So the first hidden layer of neurons can be considered to identify which side of each of a group of planes the input point is on. The neurons can also act as AND and OR gates, so subsequent layers of neurons give an output indicating whether the point lies in a region bounded by several of the planes (e.g. a neuron activates only if the point is above one plane and below another, indicating it is in a region of the space associated with one class of points). So if the network learns an appropriate set of planes to bound regions containing different classes of inputs and the AND/OR relations that determine whether a point is in a given region, then it can classify the input points, and this can work for regions with arbitrarily shaped boundaries.</p>

<p>I've not found or been able to think of a similar way of explaining why an NN can perform well in general regression problems (if it's big enough). Does anyone here know a way to explain this?</p>
"
666,"<p>I'm trying to develop a kind of AI that will assist in debugging a large software system while we run test cases on it. The AI can get access to anything that the developers of the system can, namely log files, and execution data from trace points. It also has access to the structure of the system, and all of the source code.</p>

<p>The end goal of this AI is to be able to detect runtime errors during execution, and locate the source of these errors.</p>

<p>I was considering making use of a deep neural network, where the input would be the execution data and log output. Using this input it would be able to verify whether the current version of the system we are running is functional, or non-functional. The problems with this approach is that the system it would be evaluating would be constantly changing as it gets developed, so the only training material the NN would have is from the last stable version of the system (and even that could have some errors). Additionally, producing test cases for the system off of which we could train the NN would be very time consuming, and would defeat the purpose of using the NN in the first place.</p>

<p>I would like to know what AI design you think would be suitable for this task. Please let me know if you would like any other information relevant to the problem. As far as I can tell, nothing quite like this has been done before.</p>

<p>It's probably worth mentioning that my team has a some extremely powerful machines on which we can run the AI.</p>
"
667,"<p>I'm working on a project where I train a Q-learning agent to learn an optimal control policy for a water heater. I've set up a simulation which allows the agent to explore for one year. I then examine the results of the agent performance exploiting its optimal policy for the following year. The agent can perform the following actions (available actions depend on the state of the environment):</p>

<ul>
<li>Turn the electrical heating element on.</li>
<li>Turn the electrical heating element off.</li>
<li>Turn gas heating on.</li>
<li>Turn gas heating off.</li>
<li>Do nothing.</li>
</ul>

<p>The goal of the agent reach the target temperature (50 deg C) when hot water is scheduled. The agent is rewarded for choosing actions which produce the lowest CO2 emissions (the CO2 emissions produced from electricity vary over time).</p>

<p>One of the issues I have noticed is that during the exploration phase, the agent tries a lot of weighted random actions which causes the water heater to overheat (>80 deg C). When the water heater overheats, it is not possible for the agent to perform further actions other than switching off heating and doing nothing. The agent is also punished for reaching the overheating tank state. The tank may remain in the overheated state for some time. It seems as if the tendency to overheat the tank during exploration is negatively impacting how the agent learns its policy as it reduces the number of experiences in other states.</p>

<p>Is there a term for this kind of situation during exploration in reinforcement learning? During exploration, the agent uses a chooses a softmax weighted random aciton. Are there alternative ways of choosing actions that may still allow for exploration while not reaching the overheating state?</p>
"
668,"<p>I understand that there are flavors of (convolutional) neural networks that are useful for object <a href=""https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/"" rel=""nofollow noreferrer"">localization and detection</a> tasks of reasonable difficulty. In all of the examples I have seen so far, localization is formulated as finding the corners of a bounding box. Often, the fit is not expected to be very precise:</p>

<p><a href=""https://i.stack.imgur.com/xmWck.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xmWck.jpg"" alt=""traffic sign bounding box""></a></p>

<p>Conversely, I am interested in a task I want to achieve very precise localization and characterization of some simple shapes or objects. As an example of one of the simplest cases I can think of, my inputs will be images like the following:</p>

<p><a href=""https://i.stack.imgur.com/NOqN5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NOqN5.jpg"" alt=""28 21 15""></a></p>

<p>Given this 60x60 image, I want my neural net(s), via <strong>regression</strong>, to tell me that the circle's <strong>diameter is 18px</strong> and its <strong>centre is located at (28, 21)</strong> from top left. (I will train it using similar 60x60 images with white circles of various sizes on black backgrounds.)</p>

<p>Later I am interested in dealing with similar tasks in the real world, e.g. spheres/cubes/cylinders with different viewing angles, light conditions, occlusions, etc. However, I am interested in solving this simplest case first. (One reason is that I can generate this data very easily.)</p>

<p>I have the following specific questions:</p>

<ol>
<li>Has anyone used neural nets for this sort of tasks before? (e.g. precisely determining sizes and centroids of objects)</li>
<li>My understanding is that these things are at least theoretically possible using convolutional nets, or even sufficiently complicated vanilla fully connected nets. Is this correct? </li>
<li>What architecture(s) would be appropriate for these tasks?</li>
</ol>

<p>Note: I am aware that fitting a bounding box to the circles and calculating its centre and size will solve this particular case, but it will not generalize to handle occlusions, changing lights, etc. I would like to move towards a method which can, for example, calculate the centroids and diameters of spheres in real-world B&amp;W photos.</p>
"
669,"<p>How much resources will automata devote to being selfish and helping others? Can automata develop selfishness? Can automata love, and if so, is there a theoretical limit to love?</p>
"
670,"<p>Most (all I know) ""machine learning"" systems use a fixed set of data input channels and processing algorithms, only expanding underlying dataset processed by these; they obtain new data but only from predefined sources, and use only their fixed, built-in capacity to process it, possibly tweaking parameters of the algorithm (like weights of neural network nodes) but never fundamentally changing the algorithm.</p>

<p>Are there systems - or research into creating these - that are able to acquire ""from out there"" new methods of obtaining data and new ways to process it for results? Expand not just passive data set to ""digest it"" by active but static algorithm, but make the algorithm itself self-expanding - be it in terms of creating/obtaining new processing methods for own data set, and creating/obtaining new methods of acquiring that data (these methods)?</p>
"
671,"<p>I was able to extract the license plate from a given car image, using Matlab. I would like to use deep neural networks to recognize the characters on the plate now. How can i proceed further? I don't have any experience with deep neural networks implementation.</p>
"
672,"<p>I have done some research regarding the application of Machine Learning to cyber-security. After these recent attacks, I think that AI-based Cyber Defense can prevent them. I have also read about research regarding the same in MIT, and that AI can detect more than 80% of malware. Is AI actually so promising in this department?</p>
"
673,"<p>Obviously, finding suitable hyper-parameters for a neural network is a complex task and very problem or domain-specific. However, there should be at least some ""rules"" that hold most times for filter kernel size?!</p>

<p>In most cases, intuition should be to go for small kernel filters for detecting high-frequency features and large kernel filters for low-frequency features, right? For example, 3x3 kernel filters for edge detection, color contrast stuff, ... and maybe rather something like 11x11 for whole object detection, when the objects are >= 11x11 pixels.</p>

<p>Is this ""intuition"" more or less generally true? How can we decide which kernel filter sizes should be chosen for a specific problem - or even for one specific convolutional layer?</p>
"
674,"<p>I was applying this <a href=""https://www.mathworks.com/help/nnet/examples/transfer-learning-and-fine-tuning-of-convolutional-neural-networks.html"" rel=""nofollow noreferrer"">CNN fine-tuning example from Matlab</a>.</p>

<p>The example shows how to fine-tune a pre-trained CNN on letters to classify images of digits. Now I would like to use this new fine-tuned CNN on new images of digits that i have on my computer. How can I do that?</p>
"
675,"<p>In a neural network when inputting nerve input to sense a 2D environment, how do you differentiate two types of objects (with similar shape and size) so the neural network can treat them differently?</p>

<p>Each neuron in the input layer of a neural network essentially gets 1 dimensional input (range between two values) but 2 dimensional input would be needed to send both collision and category/type information through each input layer neuron. How do you get around that?</p>

<p>Note: After having confusion regarding the scenario / situation I'm asking about compared to other more complex scenarios, and the long comment series that ensued, I'm realizing one challenge of this site is that it's much more complicated and diverse subject matter than code, or the various other topics of Stack Exchange where the problems can be very clearly and simply expressed. Here it's more challenging to express your question and scenario clearly to avoid confusion. </p>

<p>Also there's probably a higher skill gap between an AI learner / enthusiast, and an expert AI specialist, compared to other fields, so that could potentially lead to even more difficulty communicating the answer / question in ways everyone can understand without confusion. Challenging SE site to ask good questions on!</p>
"
676,"<p>Something I like about neural network AI is that we already have a blueprint for it - we know in great detail how different types of neurons work, and we don't have to invent the AI, we can just replicate what we already know works (neural networks) in a simplified way and train it. </p>

<p>So what's confusing me right now is why many popular neural models I've seen when studying neural networks include both inhibitory and stimulating connections. In real neural networks, from my understanding, there is no negative signal being transferred, rather the signals sent between neurons are comparable to values between 0.1 and 1. There's no mechanic for an inverse (inhibiting) signal to be sent. </p>

<p>For example, in this network (seen overlaying a snake being simulated), the red lines represent inhibiting connections, and the blue lines represent stimulating neurons:</p>

<p><a href=""https://i.stack.imgur.com/nLT4I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nLT4I.png"" alt=""enter image description here""></a></p>

<p>Is this just an inconsequential detail of neural network design, where there's really no significant difference between a range of 0 to 1 and a range of -1 to 1? Or is there a reason that connections in our simulated neural networks benefit from being able to a express a range from -1 to 1 rather than just 0 to 1?</p>
"
677,"<p>I took a few AI courses in college (1999-2003), and we used the first edition of <em>AI: A Modern Approach</em>. We covered a lot of topics and programming, including classical AI, neural networks, and temporal difference learning.</p>

<p>Over the past few years, AI has had a resurgence. </p>

<p><em>Is this resurgence due to new AI theory or just better (more) computational power and data so that the theories (e.g., neural networks) to be more effective? Or is it a combination of both?</em> </p>

<p>I want to get up to speed on what's happened in AI since the early 2000s, and I want to know what to cover -- what is the most significant advancement?</p>
"
678,"<p>I would like to train a bot that uses text input, memorizes a few categories and answers questions accordingly. In addition as version 2.0, I want to make the bot to answer voice inputs as well. Which are the latest machine learning/AI algorithms available for the same? Please let me know.</p>
"
679,"<p>I'm wondering how to train a neural network for a round based board game like, tic-tac-toe, chess, risk or any other round based game.
Getting the next move by inference seems to be pretty straight forward, by feeding the game state as input and using the output as the move for the current player.
However training an AI for that purpose doesn't appear to be that straight forward, because:</p>

<ol>
<li>There might not be a rating if a single move is good or not, so training of single moves doesn't seem to be the right choice</li>
<li>Using all game states (inputs) and moves (outputs) of the whole game to train the neural network, doesn't seem to be the right choice as not all moves within a lost game might be bad</li>
</ol>

<p>So I'm wondering how to train a neural network for a round based board game?
I would like to create a neural network for tic-tac-toe using tensorflow.</p>
"
680,"<p>Hello i new to artificial intelligence, i am web developer i know html, javascript, node js and php. Are these language is ok to create simple AI app.</p>

<p>I have simple AI app in my mind to which will take input as a voice command to shut down my computer.</p>

<p>To create above simple above technologies ok or i have to learn new technology for above app.After creating this simple app i will update and try to control my windows with voice.</p>
"
681,"<p>Expressed in my own words:</p>

<blockquote>
  <p>Suppose we create something that passes all of our tests and is
  indistinguishable from another human. How can you know if this is
  truly a conscious being as a human is, or simply a simulation of
  conscience?</p>
</blockquote>

<p>What is the name for this? Where was it first written about?</p>
"
682,"<p>There are lots of examples of machine learning systems that can recognize objects and extract other information from images with very high precision. To train the models of such systems is necessary (I guess) a computer with a lot of computational power. </p>

<p>My question is: For a system with images as inputs, depending on the complexity of the problem, is feasible to train a model in an average laptop? It will take to much time?</p>

<p>I know the time taken to train any machine learning model will be a function of lots of variables. I really don't expect a quantitative answer here, I just want to know whether I will be forced to upgrade my computer to develop and train machine learning models that have images as inputs.</p>
"
683,"<p>I'd like to do some experimenting with neural net evolution (NEAT). I wrote some GA and neural net code in C++ back in the 90s just to play around with, but the DIY approach proved to be labor-intensive enough that I eventually dropped it.</p>

<p>Things have changed a lot since then, and there are lots of very nice open source libraries and tools around for just about any interest one might have. I've Googled different open source libraries (e.g. DEAP), but I could use some help choosing one that would be a good fit...</p>

<ul>
<li>I spent much of my time writing code to visualize what was going on (neural net state, population fitness) or final results (graphs, etc).<br/><br/>Maybe this would have to be fulfilled by a separate open-source library, but visualization support would be something that would allow me to spend more time on the problem/solution and less on implementation details.</li>
<li>I know C/C++, Java, C#, Python, Javascript and a few others. Something that's a nice trade-off between a higher-level language and good performance on home hardware would be a good choice.</li>
</ul>

<p>Can someone with experience suggest a good open source library or set of tools?</p>
"
684,"<p>Suppose we create two units (or programs) which run in parallel and we label them as a cognitive unit and the conscious cognitive unit. </p>

<p>A human has two units analogously. A rational analyzer and not so rational analyzer. (Is there any third thing? please comment.)</p>

<p>In my opinion, the consciousness is an extra layer of decision making. This resembles the way metaheuristics work. We have a set of rules for decision making and we analyze them and tune those rules dynamically.</p>

<p>Global search algorithms mimic the conscious behavior whereas the local search algorithms work like rational mind.</p>
"
685,"<p>I was prompted towards this question while trying to find server racks and motherboards which are specialized towards artificial intelligence. Naturally I went to the SuperMicro website. There the <a href=""https://www.supermicro.com.tw/products/system/4U/4028/SYS-4028GR-TXRT.cfm"" rel=""nofollow noreferrer"">chassis+motherboard</a> which supported the maximum GPUs in the ""artificial intelligence"" category could support upto 8 of them. Additionally, the Nvidia DGX-1 also has only 8 Tesla P100 GPUs. And just to rub it in, Matlab does not support more than 8 GPUs last I checked.</p>

<p>So, are more than 8 GPUs practical for DL? I would take Caffe, CNTK, Tensorflow and Torch7 as reference.</p>
"
686,"<p>I am building a smart Mirror where it displays a website and the website would have voice recognition and face recognition.</p>

<p>For voice recognition/voice commands I will be using a JavaScript library called Annyang.</p>

<p>I am looking for a face recognition package just like Annyang, I would be happy if it's open source and really easy to use...</p>

<p>It would be really helpful if you could add more tags, currently I don't have enough reputation...</p>

<p>Any help and suggestions are much appreciated,</p>

<p>Sid.</p>
"
687,"<p>Basically, an AI that can create, rig, and texture 3d models and game environments (by extrapolating from collections of reference models, according to user input), and that can set up physics and mechanics (assuming that the AI has access to a 3d modeling studio and a game engine, both designed for compatibility with the AI, or as a component of the AI), all according to user commands (and allowing for tweaking and optimizations of models, rigging, mechanics, etc, by the user).</p>

<p>An example of user commands would be something like: ""Gaming AI, create a casual style* male model, European build, 6'5"", fit and slightly skinny, with red scaly skin, green eyes, a reptilian tail, demonic wings, claws, sharp teeth"", etc. The user probably wouldn't add all of these characteristics at once, but rather one at a time, tweaking each feature via AI commands or manually.</p>

<p>*""casual style"" is a fictional ""style class"". Style classes would refer to the visual style of the models. Possible example styles include ""cartoon"", ""abstract"", ""gothic"", ""steampunk"", ""serious"" and ""realistic"".</p>

<p>Here's another example of user commands, for a environmental model: ""Gaming AI, create a serious style house, Victorian, two story, white with beige trim, with porches and shutters. Give it a creepy aesthetic."" Again, models could be created and modified or have features added in a step by step process, in order to tweak and refine them.</p>

<p>I believe that such an AI would significantly reduce the amount of time, labor, and difficulty involved in designing games; making games cheaper and easier to produce, and making game design available to everyone. A variation of such an AI could also be used to create 2d artwork and animations.</p>

<p>But is such an AI even remotely possible? And would it take a supercomputer to run the thing? (I'm under the impression that such an AI would need to be capable of learning and adapting, and would require a massive and expansile ""association library""*—including 2d and 3d models, and verbal and textual speech—as well as near human intelligence)</p>

<p>*if the term ""association library"" doesn't exist, or doesn't currently relate to AI, Then I just made it up. According to my made up definition, an association library is the library of programmed or learned associations that an AI uses to generate responses, and to, in this context, generate 3d models; and probably to write or select code as well, in order to set up physics and mechanics and the like. </p>
"
688,"<p>Does google manufacture TPUs? I know that google engineers are the ones responsible for the design, and that google is the one using them, but which company is responsible for the actual manufacturing of the chip? </p>
"
689,"<p>I am a software engineering student and I am complete beginner to AI. I have read a lot of articles on how to start but each article suggests a different way.  I was wondering if some of you experts can help me get started in the right way.</p>

<ul>
<li><p>First, which language should I focus? as of right now, my main language is Java, but a lot of articles suggests that I should learn python, C++ or lisp for AI. Can I use Java instead of any of the other languages mentioned?</p></li>
<li><p>Second, what kind of maths background should I have? During the first year, I did discrete maths which included the following topics :- Sets, Matrices, vectors, functions, logic and graph theory (They taught these topics briefly). are the are there any more topics that I should learn now (calculus maybe?)?</p></li>
</ul>

<p>If possible I would appreciate any resources or books I could use in order to get started or maybe you guys can give me a detailed procedure I can follow in order to catch up with to your level.</p>

<hr>

<p>Note: For now I would like to focus on neural networks and machine learning. After I that I would like to explore robotics and natural language processing.</p>
"
690,"<p>Can neural networks be used to study (elementary) number theoretic problems? What are examples where this has been done in the past? Or is there on the contrary an understanding that neural networks are not helpful for such problems?</p>

<p>To make the question more concrete, let me give an example of the kind of number theoretic problem I'm thinking of: given two natural numbers a and b I may want to compute the rounded down quotient int(a/b). Naively I would restrict to 64 bit unsigned numbers and build a neural network that has 128 neurons in the input layer and 64 neurons in the output layer representing the binary expansion of the numbers. Assuming I laid out the network properly and trained it well, would I be expected to get useful output? In particular, would I be able to interpret the output as a number and would it often be the right answer?</p>

<p>Note: the reason why I think of this as problem as ""number theoretic"" is because I want to compute int(a/b) rather than the rational number a/b. This is essentially a step in Euclid's algorithm. So the non-linear behaviour int(4/3) = 1, int(5/3) = 1, int(6/3) = 2 is crucial and would need to be recognizable in the output.</p>
"
691,"<p>I am not looking for an efficient way to find primes (which of course is a <a href=""https://en.wikipedia.org/wiki/AKS_primality_test"" rel=""noreferrer"">solved problem</a>). This is more of a ""what if"" question. </p>

<p>So, in theory: Could you train a neural network to predict whether or not a given number n is composite or prime? How would such a network be laid out?</p>
"
692,"<p>I can see a lot of tutorials and examples about using TensorFlow and other free, open-source AI/ML/DL frameworks on enterprise level where enough data was collected for such AI solutions.</p>

<p>How can one can collect enough data in normal everyday life to practically and effectively make use of such freely available AI/ML/DL technologies to improve one's life and security?</p>
"
693,"<p>Just watched a recent WIRED video on virtual assistants' performance on telling jokes. They're composed by humans, but I'd like to know if AI has gotten good enough to write some.</p>
"
694,"<p>The deep learning algorithms I would to know the limits of are:</p>

<ol>
<li>CNTK</li>
<li>Caffe</li>
<li>TensorFlow</li>
<li>Torch7</li>
<li>Theano</li>
</ol>

<p>For example: I've heard TensorFlow is near impossible to parallelize on 8 GPUs and above. So, in this case, the limit would be 8.</p>
"
695,"<p>Lets say you install your LSTM machine on a road between London and Oxford. And it makes observations. A car with 3 people inside drives past it in one direction 21 sec after previously observed car (in any direction) : the input is {3, LO, 21}. A bus with 43 people drives in other direction 11 sec later : {43, OL, 11}</p>

<p>I cant find a definitive general explanation (without references to existing ML packages code) - how your LSTM layer structure should look like to accept those params (there are 3 of them and they are vastly different between themselves but presenting them together at each step is very significant (as opposed to splitting them to 3 streams and feeding then into 3 LSTMs and then pooling the results).</p>

<p>Could someone explain it in a single formula or a drawing?</p>
"
696,"<p>If I want to train a convoluted NN on time series but I cannot decide where to split the data.</p>

<p>I see that other people use jumping window over the input. so the feed say 20 sec of observation as 1 sequence into a CNN.</p>

<p>I cannot do that as splitting the observation by fixed size will most definitely break important patterns - ie first part of a pattern will go into the end of the current seq and the rest into the beginning of the next seq.</p>

<p>I can however find a sensible solution by preprocessing data and finding places of much smaller significance and make seq cutting in the middle. but then the seq length will vary greatly. </p>

<p>Can I still use CNN ? or this idea is silly?</p>

<p>How else people extract features from time series using CNNs ?</p>
"
697,"<p>Imagine a simple scenario of having a large repository using one framework and integrated with data/robots/etc, then having a new feature requested and the framework missing some vital functionality that is available in another framework (say a new kind of layer). </p>

<p>For many mathematical libraries it can be easy to reverse engineer the specific function that one would want form the other framework so as to not import the entire framework but in the case of deep learning, this isn't so trivial nor that easily testable.</p>

<p>In these cases, does one take on the problem of reverse engineering the functionality or do they attempt to combine the architectures? If its the combination, what are the biggest issues with integration?</p>
"
698,"<p>I created an OpenAI Gym environment, and I would like to check the performance of the agent from <a href=""https://blog.openai.com/openai-baselines-dqn/"" rel=""noreferrer"">OpenAI Baselines DQN approach </a> on it. 
In my environment, the best possible outcome for the agent is 0 - the robot needs zero non-necessary resources to complete a task. The goal is to minimize the need for resources: for each needed ressource, there is a penalty of -1. In many states, only certain actions make physical sense. How do I deal with this? </p>

<p>There was already a question about the handling of invalid moves on <a href=""https://ai.stackexchange.com/questions/2980/how-to-handle-invalid-moves-in-reinforcement-learning"">AI StackExchange</a>, recommending to ignore the invalid moves. However, ignoring them would imply returning the same state and a 0 reward, the best possible outcome, which is clearly not the case. Setting drastic negative rewards also does not seem to work, since even promising handling paths are compromised by invalid actions and the corresponding drastic negative reward.</p>

<p>What are other ways of handling invalid actions in scenarios where all rewards are either 0 (best), or negative?</p>

<p>My ideas/ questions on this for the <a href=""https://blog.openai.com/openai-baselines-dqn/"" rel=""noreferrer"">OpenAI Baselines DQN approach </a> implementation</p>

<p>1) Is there any way to set the initial Q-values for the actions? I could set -infinity for the invalid actions.
2) Is there any way to limit the set of valid actions per state? When after the env.step(action) function the new state is returned, can I somehow define which actions are valid for it?</p>
"
699,"<p>I'm currently studying different kind of agents and what is the difference between a <strong>model-based reflex agent</strong> and <strong>simple reflex agents</strong>. </p>

<p>What is the role of the <strong>internal state</strong> ?</p>
"
700,"<p>In the process of segmentation, pixels are assigned to regions based on features that distinguishes them from the rest of the image. Value Similarity and Spatial Proximity, for example, are two important principles that assume that points in the same region will have pixels that are spatially close and have similar values.</p>

<p>In lots of situations this is true, but what about regions composed of pixels that are not similar in value? Consider the image below. The same ""logical"" region is composed of different elements that together represent something meaningful. In the same region there are trees with different sizes and shapes, with shadow over some of them etc. There are different things, with pixels that differ a lot in value, but I still need to group them together in the same region. From the image you can see that I don't care so much with differences in color. In this case texture is the most important attribute. What algorithms are used to do the segmentation and classification in problems like that?</p>

<p>I'm already looking for some algorithms and techniques that focus on texture, but some opinions from the experts will help me a lot. I think I need some orientation. Thanks!</p>

<p><a href=""https://i.stack.imgur.com/UORVX.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UORVX.jpg"" alt=""segmentation""></a></p>
"
701,"<p>In Peter Norvig's <a href=""http://www.amazon.in/Paradigms-Artificial-Intelligence-Programming-Studies/dp/1558601910"" rel=""nofollow noreferrer"">Paradigms of Artificial Intelligence Programming</a> , chapter 4, which is about the all-famous General Problem Solver (GPS). In this chapter, the author asks a question (4.4), which is as follows:<br>
""<em>The Not Looking after You don't Leap Problem</em>. Write a program that keeps track of the remaining goals so that it doesn't get stuck considering only one possible operation when others will eventually lead to the goal. <strong>HINT:</strong> have <code>achieve</code> take an extra argument indicating the goals that remain to be achieved after the current goal is achieved. <code>achieve</code> should succeed only if it can achieve the current goal and also <code>achieve-all</code> the remaining goals.""</p>

<hr>

<h2>Description of given functions:</h2>

<ol>
<li><p><code>achieve</code> :We input a goal, the list of operators, and the current state, in the <code>achieve</code> function, which checks if the goal is in the <code>state</code> already, if not, then checks if the goal is recursive, if not, then finds all the <code>appropriate</code> operators and then applies them, until the goal os achieved. Or <code>achieve</code> returns a nil.    </p></li>
<li><p><code>achieve-all</code>: This has a list of goals as an input, which it tries to achieve using the <code>achieve</code> function. Moreover, it makes sure that all the goals are in the final state. Otherwise, it returns a nil.</p></li>
</ol>

<hr>

<h2>Problem:</h2>

<p>The problem that I face, is to find a relation between ""getting stucked"" over an operator and maintaining the number of goals remaining. Moreover, the version of <code>achieve</code> in the GPS program checks for all the operators that can achieve the goal, and then apply only the one that does the work.<br>
Let's forget this for a while, and consider the hint. It says that we should have an extra argument maintaining the goals that are remaining. But how will it help a goal, which is stuck because of applying the wrong operator?<br>
There seems to be no relation between them. I know I'm missing something here, but I can't find that.<br>
Any help appreciated!<br>
Moon</p>
"
702,"<p>I am interested in the current state-of-the-art ways to use quick, greedy heuristics in order to speed up the learning in a Deep Q-Network in Reinforcement Learning. In classical RL, I initially set the Q-value for a state-action pair (S,a) based on the result of such a greedy heuristic run from state S with action a. Is this still a good idea in the setting of a neural network for the approximation of the Q-function, and if yes, what are the optimal ways of doing it? What are other ways of aiding the DQN with the knowledge from the greedy heuristics?</p>

<p>References to state-of-the-art papers would be highly appreciated.</p>
"
703,"<p>Are there approaches other than convolutions to learn features from images? Has there been any research to use approaches such as hashing (e.g. <code>p-hash</code>, <code>diff-hash</code> etc.) in lieu?</p>
"
704,"<p>Some papers say that <a href=""https://en.wikipedia.org/wiki/BLEU"" rel=""nofollow noreferrer"">BLEU</a> is not a appropriate evaluating method for chatbot, instead they use perplexity to estimate chatbot.</p>

<p>First of all, what is perplexity? How to calculate it? And why is perplexity a good evaluation metric for chatbots?</p>
"
705,"<p>I am Reading ""<a href=""http://www.cs.toronto.edu/~graves/phd.pdf"" rel=""nofollow noreferrer"">Supervised Sequence Labelling with Recurrent Neural Networks</a>"" written by Alex Graves to try to understand LSTM networks and I am a bit confused about the equations.</p>

<p>Specifically, what I am confused about is the term ""state"". When used in an equation (section 4.5.2), it says:</p>

<p><a href=""https://i.stack.imgur.com/3ANyI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3ANyI.png"" alt=""enter image description here""></a></p>

<p>I know that some system can be in a state, for example, due to the setup of values of different nodes in a graph. But how can a state be described in the case of a neural network and how can the equation above be explained other than that it is the state (or states of several timesteps as in recurrent neural networks) of a neural network?</p>
"
706,"<p>I want to train a neural network with pictures of public figures (politicians, singers, etc), but I do not know if it's legal, I do not plan to show them in my project I only want to use them to train the neural network, can this cause legal problems?</p>
"
707,"<p>How exactly are ""mutation"" and ""cross-over"" applied in the context of a genetic algorithm based on real numbers (as opposed to just bits)? I think I understood how those two phases are applied in a ""canonical"" context where chromosomes are strings of bits of a fixed length, but I'm not able to find examples for other situations. What would those phases look like on the domain of real numbers?</p>
"
708,"<p>I recently read about algorithmic bias in facial recognition. Is the bias created by the training set provided or something else?</p>
"
709,"<p>I wanted to train a chat bot for answering questions from books. I am trying to use Dynamic Memory Networks to do so. </p>

<ul>
<li>How can I generate a data set like facebook did in case of babi tasks so that it can tackle a variety of questions on the data set.</li>
</ul>
"
710,"<p>I'm developing a log analyzer to predict and find errors in an equipament. Each logged data contains the following format:</p>

<pre><code>  Timestamp | Log source | type of message | message
</code></pre>

<p>Each entry log I want to represent by one pixel RGB because, considering the 24 bits, is possible to represent the last 3 parameters ( Log source, type of message and message), but I don't have bits enought to represent the timestamp ( this data I will to represent by the diference o previous timestamp ""delta of time""), the resolution of time is second, and sometimes we have a large time between one log of another. The example above illustrate the situation:</p>

<p><a href=""https://i.stack.imgur.com/v18IE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/v18IE.png"" alt=""Log to Image""></a></p>

<p>This project has the purpose of analyze the log to find and predict errors, and this preprocessing is used to simplfy the data entry to the machine learning algorithm, this is a good way to represent data to an rnn ? Or for this kind of problem exist a better way to make an analyses?</p>
"
711,"<p>I am not sure if this question fits AI Stack Exchange so feel free to delete it in case it doesn't.</p>

<p>Recently I found out about somewhat famous <a href=""http://yudkowsky.net/"" rel=""nofollow noreferrer"">Eliezer Yudkowsky</a> and <a href=""https://intelligence.org/"" rel=""nofollow noreferrer"">Machine Intelligence Research Institute</a> he founded. Their philosophy and organisation seem interesting but I'm curious about their credibility. I'm pretty sure this is not a con and they seem to be producing <a href=""https://intelligence.org/all-publications/"" rel=""nofollow noreferrer"">a lot of articles</a>. However, few of those are published and none in the journals mentioned <a href=""https://ai.stackexchange.com/questions/2306/what-are-the-top-artificial-intelligence-journals"">here</a>.</p>

<p>So, my question is: <strong>is MIRI doing a genuine high quality research?</strong></p>
"
712,"<p>I might be wrong, but if we have well designed A.I. then why won't the USA government allow it to be witnessed in the public.</p>
"
713,"<blockquote>
  <p>I've seen events, like <a href=""https://cogx.co/session-topics/mentalhealth/"" rel=""nofollow noreferrer"">CogX</a> and articles which describe how
  machine learning techniques or algorithms can be used to diagnose
  mental health issues.</p>
</blockquote>

<p><strong>Here is my question</strong>;</p>

<p>How can artificial intelligence and machine learning algorithms or techniques be used in diagnosing mental health issues, besides for e.g. Facebook using machine learning algorithms to detect people who may commit suicide?</p>
"
714,"<p>I've read through a few papers on next frame prediction from a sequence of frames and several of them use spatial transformations (<a href=""https://arxiv.org/abs/1506.02025"" rel=""nofollow noreferrer"">STNs</a>). See <a href=""https://arxiv.org/abs/1605.08104"" rel=""nofollow noreferrer"">this</a> as an example. I want to know what are the pros and cons of using an STN to predict the next frame. Are there any assumptions that must be made about the data besides ""Consecutive frames are all approximately affine transformations of each other""?</p>
"
715,"<p>What are the ethical and legal issues of self driving cars being released in the UK?</p>

<p>This question came up on our exam today and I was left in a daze.</p>

<p>I initially thought it would be issues like the legal driving age for self driving cars since they are AI do you need a minimum age limit?</p>

<p>Another one I thought was whether or not you need driving licenses for AI cars?</p>

<p>Could someone please list all the possible ethical <strong>and</strong> legal issues that surround AI controlled cars?</p>
"
716,"<p>I have a data set with four inputs and one output. I need to infer the 4 inputs given an output. What is the best way to do this?</p>
"
717,"<p>I am looking to train a dataset that would output a sequence of letters (I'm using this for peptide sequences). Since I have 22 different possibilities of amino acids, I need to output a vector that contains multiple amino acids with varying frequencies. For example, an output would look like [0,1,3,2,0,2,0,3...] (a 22-long vector).</p>

<p>How can I train a neural network to output that type of vector?</p>
"
718,"<p>I am writing a simple toy game with the intent of training a deep neural network on top of it. The games rules are roughly the following:</p>

<ul>
<li>The game has a board made up of hexagonal cells.</li>
<li>Both players have the same collection of pieces that they can choose to position freely on the board.</li>
<li>Placing different types of pieces award points (or decrease opponent's points) depending on their position and configuration wrt one another.</li>
<li>Whoever has more points win.</li>
</ul>

<p>There are additional rules (about turns, number and types of pieces, etc...) but they are not important in the context of this question. I want to devise a deep neural network that can iteratively learn by playing against itself. My questions are about representation of input and output. In particular:</p>

<ul>
<li>Since pattern of pieces matter, I was thinking to have at least some convolutional layers. The board can be of various size but in principle very small (6x10 on my tests, to be expanded by few cells). Does it make sense? What kind of pooling can I use?</li>
<li>How to represent both sides? In <a href=""https://arxiv.org/pdf/1412.3409.pdf"" rel=""noreferrer"">this paper</a> about go, authors use two input matrices, one for white stones and one for black stones. Can it work in this case too? But remember I have different types of pieces, say A, B, C and D. Should I use 2x4 input matrices? It seem very sparse and of little efficiency to me. I fear it will be way too sparse for the convolutional layers to work.</li>
<li>I thought that the output could be a distribution of probabilities over the matrix representing board positions, plus a separate array of probabilities indicating what piece to play. However, I also need to represent the ability to <strong>pass</strong> the turn, which is very important. How can I do it without diluting its significance among other probabilities?</li>
<li>And <strong>most importantly</strong>, do I enforce winning moves only or losing moves too? Enforcing winning moves is easy because I just set desired probabilities to 1. However when losing, what can I do? Set that move probability to 0 and all the others to the same value? Also, does it make sense to enforce moves by the final score difference, even though this would go against the meaning of the outputs, which are roughly probabilities?</li>
</ul>

<p>Also, I developed the game engine in node.js thinking to use Synaptic as framework, but I am not sure it can work with convolutional networks (I doubt there's a way to fix the weights associated to local perceptive fields). Any advice on other libraries that are compatible with node? </p>
"
719,"<p>I am working on <a href=""https://github.com/nfmcclure/tensorflow_cookbook/blob/master/09_Recurrent_Neural_Networks/02_Implementing_RNN_for_Spam_Prediction/02_implementing_rnn.py"" rel=""noreferrer"">this code</a> for spam detection using recurrent neural networks. </p>

<p>Question 1. I am wondering whether this field (using RNNs for email spam detection) worths more researches or it is a closed research field. </p>

<p>Question 2. What is the oldest published paper in this field? </p>

<p>Quesiton 3. What are the pros and cons of using RNNs for email spam detection over other classification methods?</p>
"
720,"<p>O'Reilly recently published an article about the machine learning paradox. (<a href=""https://www.oreilly.com/ideas/the-machine-learning-paradox"" rel=""nofollow noreferrer"">link</a>)</p>

<p>What it says goes basically like this: no machine learning algorithm can be perfect. If it was, it means it is overfitting and so it is not really perfect because it will not perform ok in real world scenarios.</p>

<p>I searched and I couldn't find any other references to this paradox. The closest I got is the <a href=""https://en.m.wikipedia.org/wiki/Accuracy_paradox"" rel=""nofollow noreferrer"">Accuracy Paradox</a>, which says that the usefulness of a model is not really well reflected in its accuracy.</p>

<p>This doesn't sound quite ok to me. For example, a linear model could be perfectly learned, ""overfitted"" and predicted in the real world. So I suspect it is really about finding the right set of data points from which the results can be inferred. This is, we are trying to approximate from uncertain data, but with the right data we can stop approximating and start calculating.</p>

<p><strong>Is my line of thinking correct? Or is there really no perfect machine learning?</strong></p>

<p>UPDATE: In the light of currently received answers, I think my last paragraph (my line of thinking) can be rephrased as: If we have a model simple enough, why can't we overfit the model, knowing that it will behave correctly in non-trained data? This assumes the training data completely represents the real-world data, which would imply a single model that we can train on.</p>

<p>Keep in mind that what we conceive as ""simple"" or ""feasible"" is arbitrary and only depends on computation power and available data -- aspects with are external to ML models themselves.</p>
"
721,"<p>So I've been working with neural networks and artificial intelligence for a while and what I'm trying to do right now is, from a genotype I have (a sum of sensors, neurons and actuators) draw how the neural network is (with recurrent/recursive connections being showed nicely, etc.)</p>

<p>What I have done now in javascript is this:
<a href=""https://i.stack.imgur.com/GWl9G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GWl9G.png"" alt=""I know, its ugly""></a></p>

<p>I have achieved this using SigmaJs a Javascript node drawing library, but I think it's still ugly, and what I'm looking for is a node drawing library that can achieve recursive connections in a nice way (right now I'm drawing them with a red color as you can see on the image).</p>

<p>I have examined a lot of GitHub repositories and websites that can be helpful but aren't worth since they aren't that nice.</p>

<p>Has anyone got an idea of what can I use, in javascript?  If not, in any other language, how can I achieve what I want?</p>

<p>Regards,
Miguel</p>
"
722,"<p>Is it possible to train a neural network to learn something via video footage?</p>

<p>In other words, if i have a video teaching me how to draw an animal from scratch, can i then use this video to teach the computer to draw the animal in the same way?</p>

<p><strong>EDIT:</strong></p>

<p>Video footage is essentially a sequence of images, any image processing capabilities available to us through machine learning are possible when applied to videos using a sequencial network (lstm, RNN etc.)</p>

<p>So i guess the difficult part becomes mapping the activity to an action like moving a pen or something</p>
"
723,"<p>I'm struggling to understand the GAN Loss Function as provided in <a href=""https://danieltakeshi.github.io/2017/03/05/understanding-generative-adversarial-networks/"" rel=""noreferrer"">Understanding Generative Adversarial Networks</a>, a page written by Berkeley PhD candidate Daniel Seita.</p>

<p>In the standard cross-entropy loss we have an output that has been run through a sigmoid function and a resulting binary classification.</p>

<p>Sieta stats, ""Thus, For [each] data point x<sub>1</sub> and its label, we get the following loss function ..."" </p>

<blockquote>
  <p>H((x<sub>1</sub>, y<sub>1</sub>), D) = −y<sub>1</sub> log D(x<sub>1</sub>) − (1 − y<sub>1</sub>) log(1 − D(x<sub>1</sub>))</p>
</blockquote>

<p>This is just the log of the expectation which makes sense, but how can, in the GAN loss function, we process the data from the true distribution and the data from the generative model in the same iteration?</p>
"
724,"<p>While conducting research, I recently stumbled upon the deep learning and natural language processing concepts. In <a href=""https://ai.stackexchange.com/questions/1970/how-would-an-ai-learn-language"">this</a> question they say that the ‘grammar induction’ is a ‘supervised learning’ mode. So I was wondering:</p>

<p>Let’s say that there’s a way-more-than-human intelligent alien probe orbiting our planet. It can receive, decode and analyze all the broadcasting signals leaving the Earth. For all we know at this moment, how could it learn the basics of a language with nothing else but our broadcasting signals, so without the help from a ‘supervisor’? How would an artificial intelligence human engineer (theoretically?) face the problem?</p>

<p>I’m interested on the more “technical” side of the issue. </p>
"
725,"<p>First of all, I'm a beginner studying AI and this is not an opinion oriented question or one to compare programming languages. I'm not saying that is the best language (actually I know almost nothing about Python). But the fact is that most of the famous AI frameworks have primary support for Python. They can even be multilanguage supported, for example, TensorFlow that support Python, C++ or CNTK from Microsoft that support C# and C++, but the most used is Python (I mean more documentation, examples, bigger community, support etc). Even if you choose C# (developed by Microsoft and my primary programming language) you must have the Python environment set up.</p>

<p>I read in other forums that Python is preferred for AI because the code is simplified and cleaner, good for fast prototyping.</p>

<p>I was watching a movie with AI thematics (Ex_Machina). In some scene, the main character hacks the interface of the house automation. Guess which language was on the scene? Python.</p>

<p>So what is the big deal, the relationship between Python and AI?</p>
"
726,"<p>I am a machine learning newbie. I am trying to understand backpropagation algorithm. I have a training dataset of 60 instances/records. So what is the correct order of the process:</p>

<ul>
<li><p>Forward pass of the first instance. calculate the error.</p></li>
<li><p>Weight update using back propagation.</p></li>
<li><p>Forward pass of the second instance. calculate the error.</p></li>
<li><p>Weight update using back propagation. and so on... <strong><em>(or)</em></strong></p></li>
<li><p>Forward pass of all instances one by one. (noting the error as vector)</p></li>
<li><p>Weight update using back propagation.</p></li>
</ul>

<p>This video <a href=""https://www.youtube.com/watch?v=OwdjRYUPngE"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=OwdjRYUPngE</a> is similar to the second process. Is it correct?</p>
"
727,"<p>I know nowadays agencies are using GPUs in order to accelerate AI, but how fast should be it to be efficient, I mean I know that depends of how large and complex the assignment is but what would be a way to measure its efficiency and what kind of technology(amount of GPUS,RAM,STORAGE) and techniques need to be used in order to get enough efficency?</p>

<p>Any thoughts from experts would be appreciated</p>
"
728,"<p>Is there a way to teach reinforcement learning in applications other than games? </p>

<p>The only examples I can find on the Internet are of game agents. I understand that VNC's control the input to the games via the reinforcement network. Is it possible to set this up with say a CAD software?</p>
"
729,"<p>I want to train my neural network by evolution, that is I want to recombine the weights of the best performing neural networks in each evolution cycle or generation. my initial instinct was to represent weights as they are ,which is variable of type double, and either</p>

<p>1- swap weights between the two parent network</p>

<p>2- generate random number between the two weights </p>

<p>but what I need is to represent the weights as binary string and then carry out the crossover on the string as usual.</p>

<p>what I want to ask is how can I take my double[] of weights and convert that into string[] with byte representation of the number? and should the chromosome contain an array of string where each string represents a single weight?</p>
"
730,"<p>Let's say we have the basic scenario where two AGIs of about the same intelligence (but not same origins/code/model) have to communicate as efficiently as possible to achieve a common goal. Now we could have 2 starting points for that:</p>

<p>1) Either all they have is a common communication bus (e.g. sound, light, radio, etc.) and instruments (e.g. transceivers) to support it, and they have to figure out the rest.</p>

<p>2) Or they are some kind of advanced chatbots, but since the human language is lacking a lot to be used as a highly efficient protocol, they will have to communicate with what they have, to build a proper one.</p>

<p>Would it be possible to somehow induce them to communicate, and try to figure out what each other ""say""? How could this be done?</p>

<p>And a more abstract question is how could this protocol ""look"" like?</p>
"
731,"<p>I'm new in ML and AI, actually learning right now. But I'm thinking about starting taking part in some projects in ML. Is <a href=""https://www.kaggle.com/competitions"" rel=""nofollow noreferrer"">kaggle</a> is a good place to find projects in ML to work on?</p>
"
732,"<p>I'm looking to build a sequence-to-sequence model that takes in a 2048-long vector of 1s and 0s as my input and translating it to my known output of (a variable length) 1-20 long characters (ex. GBNMIRN, ILCEQZG, or FPSRABBRF).</p>

<p>My goal is to create a model that can take in a new 2048-long vector of 1s and 0s and predict what the output sequence will look like.</p>

<p>I've looked at some github repositories like: <a href=""https://github.com/llSourcell/seq2seq_model_live/blob/master/2-seq2seq-advanced.ipynb"" rel=""nofollow noreferrer"">https://github.com/llSourcell/seq2seq_model_live/blob/master/2-seq2seq-advanced.ipynb</a> <a href=""https://github.com/hans/ipython-notebooks/blob/master/tf/TF%20tutorial.ipynb"" rel=""nofollow noreferrer"">https://github.com/hans/ipython-notebooks/blob/master/tf/TF%20tutorial.ipynb</a></p>

<p>but I'm not sure how to implement it with my problem. Are there any projects that have done something similar to this/how could I implement this with the seq2seq models currently out there?</p>
"
733,"<p>I am developing AI in the form of NEAT, and it has passed certain tasks like the XOR problem outlined in the <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">NEAT Research Paper</a>. In the XOR Problem, the fitness of a network was determined by an existing function (XOR in this case). It also passed another tests. One I developed was to determine the sine at a certain point X in radians. It also worked, but yet again, its fitness was determined by an existing function (sin (x)). </p>

<p>I've recently been working on training it to play Tic Tac Toe. I decided that to determine its fitness, it would play against a ""dumb"" AI, placing O's in random locations on the grid, and gaining fitness based on whether or not it placed X's in a valid location (losing fitness if it placed an X on top of another X or an O), and gaining a lot of fitness if it won against the ""dumb"" AI. This would work, but when a network got really lucky and the ""dumb"" AI placed O's in impractical locations, the network would win and gain a lot of fitness, making it very difficult for another network to beat that fitness. Therefore, the learning process did not work and I was not able to generate a Tic Tac Toe network that actually worked well.</p>

<p>I do not want the GA to learn based off an ""intelligent"" tic tac toe AI because the whole point of me training this GA is so that I do not have to make the AI in the first place. I want it to be able to learn rules on its own without me having to hard code an AI to be very good at it.</p>

<p>So, I got to thinking, and I thought it would be interesting if the fitness of a network could be determined based off how well it played against OTHER NETWORKS in its generation. This does seem similar to how humans learn to play games, as I learned to play chess by playing against other people hundreds of times, learning from my mistakes, and my friends also increased in their ability to play chess as well. If GA's were to do that, that would mean I don't have to program AI to play the game (in fact, I wouldn't have to program a ""dumb"" AI as well, I would only have to hard code the rules of the game, obviously). </p>

<p>My questions are:</p>

<ol>
<li><p>Has there been any research or results from GA's determining their fitness based off competing against each other? I did some searching but I have no idea what to look for in the first place (searching 'NEAT fight against each other' did not work well :-(  )</p></li>
<li><p>Does this method of training a GA seem practical? It seems practical to me, but are there any potential drawbacks to this? Are GA's meant to only calculate predetermined functions that exist, or do they have the potential to learn and do some decision making?</p></li>
<li><p>If I were to do this, how would fitness be determined? Say, for the tic tac toe example, should fitness be determined based on whether or not a network places its X's or O's in viable locations, and add fitness if it wins and subtracts fitness if it loses? What about tying the game?</p></li>
<li><p>Should networks of the same species compete against each other? If they did, then it would seem impractical to have species in the first place, as networks in the same species competing against each other would not allow a successful species to rise to the top, as it would be fighting against each other.</p></li>
<li><p>Kind of out of topic, but with my original idea for the tic tac toe GA, would there be a better way to determine fitness? Would creating an intelligent AI be the best way to train a GA?</p></li>
</ol>

<p>Thanks for your time, as this is somewhat lengthy, and for your feedback!</p>
"
734,"<p>By means of parts of speech tagging, words of a given sentence can be assumed to be <code>noun</code>/<code>verb</code> etc, but if the sentence is for instance:</p>

<pre><code>""My favourite book is harry potter and the prizoner of azkaban""
</code></pre>

<p>note that the inputs I receive would be from a chat interface so having a fixed format for the data can't be expected. Is there a way to identify <code>""harry potter and the prizoner of azkaban""</code> as a proper noun from such messages?</p>

<p>Currently this query tags as:</p>

<pre><code>My|PRP$ 
favourite|JJ 
book|NN 
is|VBZ 
harry|JJ 
potter|NN 
and|CC 
the|DT 
prizoner|NN 
of|IN 
azkaban|NN
</code></pre>

<p>I would like to know if this can be handled some way, or if there is another algorithm that can handle this?</p>
"
735,"<p>Which one would you recommend for a first approach to deep learning? I'm a neuroscience student trying for the first time computational approaches, if that matters.</p>
"
736,"<p>I have read some articles, some tutorials but I am still didn't implemented any AI system. So , My question may seem inappropriate for the giants in this field. But I have build certain program , downloaded to microcontroller and it will perform its task. But How to do all this with machine learning. Can I implement AI engine using C like language and make it working in any GPP uC? Please feel free to modify, edit and upgrade this questions if you get my actual problem idea.</p>
"
737,"<p>Lisp was originally created as a practical mathematical notation for computer programs, influenced by the notation of Alonzo Church's lambda calculus. It quickly became the favored programming language for artificial intelligence (AI) research, according to Wikipedia. </p>

<p>If Lisp is still used in AI, then is it worthy of learning it, particularly in the context of machine learning and deep learning?</p>
"
738,"<p>I primarily want to know, if you have been given a quantity/feature and it's characteristics then how will you classify that feature? What's the intuitive criteria? From vision POV it becomes a bit easier but in general how will you understand them and put then in a right category or is it even a right question to ask in every domain of AI?  </p>
"
739,"<p>To tune the parameters of Particle swarm optimization (PSO), there are two methods offline and online. In offline manner, the meta-optimization is used to tune the parameters of PSO by using another overlying optimizer. In the online manner, there are two techniques, Self-Adaptation, ""consisting of adding some or all of the optimizers behavioural parameters to the search-space, thus making them subject to optimization along with the problem at hand"". Another technique is Meta-Adaptation ,"" in which an overlaying optimizer is trying to tune the parameters of another optimizer in an online manner during the optimization of a problem.""</p>

<p>""The concept of meta-optimization. A black-box optimizer is used in an offline manner as an overlaying meta-optimizer for finding good behavioural parameters of another optimization method, which in turn is used to optimize one or more actual problems.""</p>

<p>In standard PSO the particles are initialized by using uniform random numbers and these particles are updated using update equations. the best solution is selected based on the best value of objective function.</p>

<p>In my work. I have two data sets, training and theoretical dataset and I need to initialize the particles by using training data instead of random numbers.</p>

<p>In this case, how can I tune the parameters of PSO using training and theoretical data set.</p>

<p>Also, I have a problem which is, I got the best cost in the initial step of PSO and in the initial step there are no parameters or update equations.</p>

<p>Is it possible to tune the parameters using Machine Learning method? How can I do this?</p>
"
740,"<p>I need to report accuracies of my neural model in a conference paper as compared to various baselines. What are the accepted standards for reporting accuracies in a fair manner?</p>

<p><strong>Neural Model:</strong>
To be specific, I'm using 60% as train set, 20% as validation set and 20% as test set to report the accuracy of my neural model. </p>

<ol>
<li>Should I take an average or highest of 3 runs accuracy where in each
run I randomly sample 60% as training data from the total 80% train + validation set.</li>
<li>My neural model is computationally intensive and therefore it is not feasible to perform a k-fold cross validation. Will my accuracy results be accepted by the academic community without a k-fold cross validation? Since my data set is large, I assume using 20% of it used solely for testing should be a fair indicator of accuracy.</li>
</ol>

<p><strong>Bag-of-words (BOW):</strong></p>

<ol start=""3"">
<li><p>How do I report the accuracy for this model so as to perform a fair comparison?</p></li>
<li><p>Should I train BOW on only 60% of data (same as which my neural
model is being trained on) or should I train BOW on 80% of data
(train + validation for my neural model)? Which is the accepted way?
I then test BOW on the same remaining 20% of data (as in neural
model) in either of the above case.</p></li>
<li>The other approach is to perform k-fold cross validation but the test set will not be the same 20% as the test set on which my neural model is being evaluated. Is this approach recommended though?</li>
</ol>

<p>Any other information on how to report accuracy results in a research paper comparing neural models (train, val, test) with linear models-BOW,SVM (train,test) is welcome. Please help.</p>
"
741,"<p>Why does not a researcher like Geoffrey Hinton with his valuable works in machine learning (especially neural networks) get Turing award?</p>
"
742,"<p>How many decades are we far from achieving a virtual world just like our real world (or little graded down version) with AI. If in far future if we are able to achieve that (able to provide all the rules and manipulate all  huge amounts of data) will a time leap be possible. Assuming time in virtual world moves much faster, as the computation rate is very high.So few hours of computation in real world will be years in the virtual world. So if we run the virtual world for few days and attain data from its final stage, will we get some future technology in our hand? </p>

<p>This concept of virtual world was put by me, assuming such powerful AI wont be integrated directly to our world without isolation tests.</p>

<p>I was thinking about P and NP  problems, fermi paradox, path which will be taken by future AI and time leap and somehow reached to this question.</p>

<ul>
<li>Fermi paradox (Assuming this world is also a virtual world created by much smarter species- which explains the isolation to a bit.) 
Assuming that's the case wont nesting of virtual world put pressure on the  root system(most outermost system) running virtual world.</li>
</ul>

<p>I know its weird, but was curious.
So the final question is .. the whole above stuff is a concept.. how much of reality of AI and its future can be compared with this concept.</p>
"
743,"<p>I'm currently pursuing Computer science engineering.So I would like to know where to start and what mathematics is needed to jump in.</p>
"
744,"<p>There is a lot of information on the internet about <code>image classification</code> and <code>object identification</code>. I want to know how I can extract (or create) geometric information from an image.</p>

<p>For example, if the image classifier can identify that there is a sphere in the image, how can i generate 3d vertices's and edges from that knowledge?</p>

<p><a href=""https://i.stack.imgur.com/UixLL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UixLL.png"" alt=""enter image description here""></a></p>

<p>I want to find all the edges and points on these shapes</p>
"
745,"<p>Alright, I want to write a mobile app that lets you take a photo of your equation, detects the equation, transforms it from pixels to text and then solves if it's possible.</p>

<p>Right now, I am doing the part where I receive a photo (2D array of pixels) and what to output a resized part of this photo (the part has a rectangular shape) that just contains the equation and some pixels around it.</p>

<p>Basically, I need a model that takes a 2D array and returns a resized version of it.</p>

<p>Since I am new to AI, I don't know what techniques have been successful in optical character recognition.</p>

<p>Any suggestions?</p>
"
746,"<p>I talked with a graduate computer science who said one challenge of making artificial human-like is making random decisions, and that computers can't be random, that they always need a ""seed."" But, if a computer's outcomes are determined by the chaotic movements of electrons, it doesn't seem like it should be difficult to program inherent uncertainty into a computer. So, what exactly is stopping people from harnessing this basic component of reality to allow artificial intelligence to make randomized decisions? I mean, all you'd need is different neural pathways that rely on the superposition of electrons, and that's it. </p>
"
747,"<p>I am working on creating a Lie Detector AI based on Facial Expressions and Body language. I want to know the best python libraries available for this task.</p>
"
748,"<p>I have an application of neural networks (standard MLP architecture) where I want to forecast a tanh output (ranging from -1 to +1) with about 1500 input features in ~700 samples.
Each sample represents a snapshot of database tables at a given time of day - f.ex. at 11:50.</p>

<p>Since I have so few samples, the network is VERY sensitive to overfitting. Although it seems overkill to use neural networks, I find them to perform better than my initial experiments with other approaches, because I can very precisely tune their objective function and regularization, along with a nice possibility for multinomial regression.</p>

<p><strong>A severe problem in the task at hand</strong> is that the data in the tables may be adjusted over time. So the database might say that a given feature was last changed at 2016-01-01 11:45, but in reality, a slight change was made a 2016-01-15 07:38. In practice, this means that the training data is unrealistically precise, and that I may instead want to consider it a ballpark estimate.
For example, a value of 153.18 may instead be considered a value of ~140-165.</p>

<p><strong>To alleviate the problem, I have theorized a ""jitter"" neuron</strong> that - during training time, at each iteration - ""jitters"" the input by subtracting or adding a random value from the feature. The random value to add or subtract is a ratio of the standard deviation of the feature, and the ratio is itself random.</p>

<p>The layer is placed right after the input neurons and has as many neurons as there are input features. In my example, the layer is therefore 1500 neurons wide.</p>

<p><strong>For example</strong>, consider an input feature that - across all training samples - has a standard deviation of +- 100.</p>

<p>At training time, a data sample is loaded with the feature value of 1122 (for the sake of example, lets ignore input standardization), and the jitter neuron will incur a randomized change in its value.</p>

<p>We define a ""jitter"" ratio of 0.1, meaning that the ratio to ""jitter"" the feature with is drawn from a standard distribution with a standard deviation of 0.1 and a mean of 0. In the example, a random roll of the dice lands us in +1.5 standard deviations, outputting a jitter ratio of 0.15.</p>

<p>Given that the training set has a standard deviation of 100, we add to the input feature (that has a value of 1122) a final jitter of +15, resulting in an input feature value of 1137.</p>

<p>The same operation goes for all input features. Pragmatically, I do this in keras by generating a jitter-ratio-matrix with mean=0 and std=0.1. The std-value can be an arbitrary amount, but we must not jitter the input data too much.</p>

<p><strong>The intuitive justification</strong> is that it is representative of my real world scenario. Without going into too many details, the input features are typically things such as weather, whose forecasts are naturally unstable, and any changes to the database values backwards in time is likely to be a ""typical"", small adjustment.</p>

<p><strong>On a more theoretical level</strong>, the justification is that it prevents overfitting to specific input features in some samples, as the next iteration across the same data sample will output significantly different output values after the forward pass through the network, as a slight change in the input feature may have a profound impact on the output after the feature has undergone compounding, non-linear activations.</p>

<p>Additionally, I perceive the jitter-neuron to be a form of data-augmentation, like it is done for especialle image classification; instead of the architect generating n augmented samples according to some heuristic (for example, in images, it is normal to crop and rotate the same image in several different ways to enlarge the training set), the jitter neuron generates a theorhetically infinite amount of augmentations at runtime.</p>

<p>I keep imagining the parameter hyperspace for the neuron weights in the first layer; instead of having many potential pits (i.e. areas of overfitting), the jitter smooths out these pits, as the jittered input data now generates a different loss value.</p>

<p><strong>Early experiments</strong> do not provide very good results, but not bad either. They just, sort of... Stay the same... However, my dataset is rather small, and it is <strong><em>notoriously difficult to fit</em></strong> with any model.</p>

<p><strong>Therefore, I ask of you</strong> what you think of the concept of jitter neurons:</p>

<ul>
<li>Does it seem sensible?</li>
<li>Any theoretical reason that it should/should not work?</li>
<li>Is it, for some statistical reason or other, inherently inferior to dropout? (notice that I also add dropout, albeit smaller rates)</li>
<li>Any proposals for making it better?</li>
<li>Comments, etc.</li>
</ul>
"
749,"<p>It is assumed in computer science that the human mind can be replicated with a Turing machine, therefore Artificial General Intelligence (AGI) is possible.  To assume otherwise is to believe in something mystical, and mystical beliefs are false.</p>

<p>I do not know of any other argument that AGI is possible, and the foregoing argument is extremely weak.</p>

<p>Is there a rigorous proof that AGI is possible, at least in theory?  How do we know that everything the human mind can do can be encoded as a program?</p>
"
750,"<p>I'm new in this subject matter. I'm a programmer so I understand sometimes how hard it's to make an AI that can play games in an intellligent manner.</p>

<p>And some AI's, such as some chess players, are extremely well coded and have defeated humans in several matches. But I think that they won simply because computers can make calculations way faster than humans can <strong>not because</strong> they learned from their opponents.</p>

<p>And if you put the <strong>same AI vs same AI</strong> who'll win? Will the game continue indefinitely or will the game eventually finish because the AI's play randomly?</p>

<p>So I start wondering if machine-learning and self-learning are really possible, and if AI's simply make some decisions randomly? Will they really become smart? </p>

<p><strong><em>I hope I've been clearly to all of you :)</em></strong></p>

<p><strong><em>Update</em></strong> - I think both are connected. Let me give an example of real world.</p>

<p><strong><em>Little babies</em></strong> need to be teached and they learn a lot by them parents. Like machines do :-) They are teached by their ""parents"".(<strong>MACHINE-LEARNING PROCESS</strong>)</p>

<p>But when babies <strong>start growing</strong> they start learning things by themselves, they learn what is bad what hurts and who is friendly completely alone. So machines should be like that they at some point start learning by themselves.  If babies didn't start learning by themselves, imagine them asking to everyone ""Is this good?"" (<strong>SELF-LEARNING</strong>)</p>

<p>Is my thinking right?</p>

<p>But you know how hard is the process of start learning by our own and the complexity of our brain, and the specific time in our lifes that <strong>RANDOMLY</strong> we start thinking by our own. </p>

<p>So my question is...if to us humans is <strong>difficult to explain</strong> that process and how it works, how can we teach it to machines?</p>

<p>I hope I've been clearer than the previous one ;)</p>
"
751,"<p>I was checking services like <a href=""https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/"" rel=""nofollow noreferrer"">Microsoft Azure's Cognitive Services Computer Vision API</a> and <a href=""https://cloud.google.com/vision/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=2016-q1-cloud-latam-ML-skws-freetrial&amp;dclid=COD5h5Pj79QCFclThgodr2kFNg"" rel=""nofollow noreferrer"">Google's Vision API</a> and they are amazing. I was wondering if these services, or any other cloud service for that matter, can recognize an image's content and classify it on a set of fixed categories defined by me, not by the Cognitive Service provider.</p>

<p>For example, I have different products and I will take several pictures of each product. I want to then use the cloud service and upload all the pictures of each product, so that I can then take a picture of one product and the Computer Vision algorithm will tell me which product I am seeing.</p>

<p>Is it possible? Is there a third party solution for this problem? If so, how many pictures do I need to train each product's recognition?</p>

<p>I hope I was clear. Thanks in advance for any light on the topic!</p>
"
752,"<p>I have a complex neural net that will take forever on my laptop to train and i dont have a computer with a GPU, is there a way to run a python script on another computer without having to install an IDE on that computer? (for instance, if i went to an internet cafe)</p>
"
753,"<p>I was reading about <a href=""https://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29"" rel=""noreferrer"">John McCarthy</a> and his orthodox vision of Artificial Intelligence. To me, it seems like he was not very much in favour of resources (like time and money) being used to make AIs play games like Chess. Instead, he wanted more to focus on passing the Turing Test and AIs imitating human behavior.</p>

<p>I  have also read many articles about major companies like IBM, Google, etc. spending millions of dollars in making AIs to play games like Chess, Go, etc.</p>

<p><strong>To what extent is this justified?</strong></p>
"
754,"<p>I am trying to run Deep Q-learning algorithm on a game which i made in python using pygame library. The algorithm accepts the game screen (4 frames) as input to neural network which used as the function approximator.</p>

<p>The game looks like this ...</p>

<p><a href=""https://i.stack.imgur.com/Zv7e9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Zv7e9.png"" alt=""enter image description here""></a></p>

<p>Player can move both the paddles and randomly a white ball is generated from the center of screen. If the paddle touches the white ball reward of +1 is awarded if it misses the reward of -1 is awarded and the same reward is passed in the Q-learning algorithm to learn. Only 3 actions are possible move to left, move to right and stay</p>

<p>Here is the code for my Deep Q learning algorithm...</p>

<pre><code>from __future__ import division, print_function
from keras.models import Sequential
from keras.layers.core import *
from keras.layers.convolutional import Conv2D
from keras.optimizers import Adam
from scipy.misc import imresize
import collections
import numpy as np

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import wrapped

# stack the four frames together
def preprocess_images(image):

    if image.shape[0] &lt; 4:
        xt_list = []
        for i in range(image.shape[0]):
            x_t = imresize(image[i],(100,100))
            x_t = x_t.astype('float')
            x_t /= 255.0
            xt_list.append(x_t)
        num = 4 - len(xt_list)
        length = len(xt_list)
        for x in range(num):
            x_t = xt_list[length-1]
            xt_list.append(x_t)
        s_t = np.stack((xt_list[0],xt_list[1],xt_list[2],xt_list[3]),axis=2)

    else:
        xt_list = []
        for i in range(image.shape[0]):
            x_t = imresize(image[i],(100,100))
            x_t = x_t.astype('float')
            x_t /= 255.0
            xt_list.append(x_t)
        s_t = np.stack((xt_list[0],xt_list[1],xt_list[2],xt_list[3]),axis=2)
    s_t = np.expand_dims(s_t,axis=0)
    return s_t

# generate data to train neural network
def gen_next_batch(experience,model,num_actions,gamma,batch_size):

    batch_indices = np.random.randint(low=0,high=len(experience),size=batch_size)
    batch = [experience[i] for i in batch_indices]
    X = np.zeros((batch_size,100,100,4))
    Y = np.zeros((batch_size,num_actions))
    for i in range(len(batch)):
        s_t, a_t, r_t, s_t1, game_over = batch[i]
        X[i] = s_t
        Y[i] = model.predict(s_t)[0]
        Q_sa = np.max(model.predict(s_t1)[0])
        if game_over:
            Y[i,a_t] = r_t
        else:
            Y[i,a_t] = r_t + gamma*Q_sa
        return X,Y

# Neural Network model implemented using keras
model = Sequential()
model.add(Conv2D(32,kernel_size=8,strides=4,kernel_initializer='normal',padding=""same"",input_shape=(100,100,4)))
model.add(Activation('relu'))
model.add(Conv2D(64,kernel_size=4,strides=2,kernel_initializer='normal',padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(32,kernel_size=3,strides=1,kernel_initializer='normal',padding='same'))
model.add(Activation('relu'))
model.add(Flatten())
model.add(Dense(512,kernel_initializer='normal'))
model.add(Activation('relu'))
model.add(Dense(3,kernel_initializer='normal'))
model.add(Activation('linear'))
opt = Adam(lr=1e-06)
model.compile(loss='mse',optimizer=opt)


NUM_ACTIONS = 3
GAMMA = 0.99
INITIAL_EPSILON = 0.1
FINAL_EPSILON = 0.0001
NUM_EPOCHS = 10000
MEMORY_SIZE = 50000
BATCH_SIZE = 64
EPSILON = INITIAL_EPSILON

experience = collections.deque(maxlen=MEMORY_SIZE)
game = wrapped.Paddle()


for x in range(1000):
    game.reset()
    game_over = False

    a_0 = 2
    x_t, r_0, game_over = game.step(a_0)
    s_t = preprocess_images(x_t)

    while  not game_over:
        s_t1 = s_t
        a_t = np.random.randint(low=0,high=NUM_ACTIONS,size=1)[0]

        x_t, r_t, game_over = game.step(a_t)
        s_t = preprocess_images(x_t)
        experience.append((s_t1,a_t,r_t,s_t1,game_over))
    print('Random Actions')
    print(len(experience))

Reward_list = []
for i in range(NUM_EPOCHS):
    game.reset()
    loss = 0.0
    R = 0

    a_0 = 2
    x_t, r_0, game_over = game.step(a_0)
    s_t = preprocess_images(x_t)

    while not game_over:

        s_t1 = s_t

        if np.random.rand() &lt;= EPSILON:
            a_t = np.random.randint(low=0,high=NUM_ACTIONS,size=1)[0]
        else:
            q = model.predict(s_t)[0]
            a_t = np.argmax(q)

        x_t, r_t, game_over = game.step(a_t)
        s_t = preprocess_images(x_t)

        experience.append((s_t1,a_t,r_t,s_t,game_over)) # stores experiences

        X,Y = gen_next_batch(experience,model,NUM_ACTIONS,GAMMA,BATCH_SIZE)
        loss += model.train_on_batch(X,Y)

        R += r_t

    if EPSILON &gt; FINAL_EPSILON:
        EPSILON -= (INITIAL_EPSILON - FINAL_EPSILON)/NUM_EPOCHS

    print('Episode : %d | Epsilon: %f | Reward : %f | Loss : %f'%(i+1,EPSILON,R,loss))
    Reward_list.append(R)

    if (i+1) % 1000 == 0:
        plt.plot(Reward_list)
        plt.xlabel('Episodes')
        plt.ylabel('Reward')
        plt.savefig('/output/reward.png')
        model.save(""/output/RL_MODEL.h5"",overwrite=True)
</code></pre>

<p>I Trained the Neural Network for 10000 Epochs and the Total Reward per episode looks like this, which clearly indicates the algorithm is not learning...(Max of +1 and Min of -1 reward is possible in a episode) 
<a href=""https://i.stack.imgur.com/zXymK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zXymK.png"" alt=""enter image description here""></a></p>

<p>Can any one suggest me what i am doing wrong. I am having very hard time in implementing the reinforcement learning algorithms. I have tried to implement same algorithm on an another game but had the same issue. Is it related to my epsilon-greedy policy, or i am not training enough or something else. Please Help me...</p>
"
755,"<p>So i understand that as a network learns about an output with regards to an input, weights are updated according to how wrong the guess was for that node. so over time, the weights move in the ""direction"" towards the correct value.</p>

<p>Is it possible to use a seperate neural network, that takes as input the weights of the first network WHILE it trains to trys and approximate that ""direction"" and in effect, pushing the weights in that direction faster?</p>
"
756,"<p>I am trying to build a ML Agent to find the closest matching image from a given set. The user will draw something and the agent should list the closest matching images. </p>

<p>Very similar to these examples</p>

<ol>
<li><a href=""https://sketchx.eecs.qmul.ac.uk/"" rel=""nofollow noreferrer"">https://sketchx.eecs.qmul.ac.uk/</a></li>
<li>Emoji search in Android keyboard</li>
</ol>

<p>One unique problem I've is, each image will represent a category. Imagine we have product images and user will draw something and we have to find the products close to the drawing. So category in my case will be product Id. </p>

<p><a href=""https://i.stack.imgur.com/hHwig.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hHwig.jpg"" alt=""enter image description here""></a></p>

<p>I would like to evaluate the approach before trying out. </p>

<p>There are lot of examples to classify images, however if I use the item identifier instead of category it should work. But am trying to find the best approach for this problem. </p>
"
757,"<p>I am a newbie to the field so please be gentle.</p>

<p>I am trying to perform a binary classification of tweets using Machine Learning. </p>

<p>The 'normal' way of doing this seems to be putting a hand-classified tweet's words into a big vector, then use that vector as input to an algorithm, which then predicts new tweets based on this data.</p>

<p>My question is: is there a standard method, or algorithm, that can include other input, such as the location of a tweet, in this process? </p>

<p>I could just add tweet location at the end of the vector I suppose, but that would give it a very small weighting. </p>

<p>Any pointers are much appreciated.</p>
"
758,"<p>You need a library to recognize the machine numbers.</p>

<p>There is a choice between two libraries Keras and OpenCV</p>

<p>Which is better to choose? Or is there an even better solution?</p>

<p>Programming language Python3</p>
"
759,"<p>I'm not quite sure how I should go about creating a multi-label image KNN classifier using python as a lot of the literature I have read does not explicitly explain this methodology. Specifically, I am not sure how I would be able to potentially yield multiple labels per image using the KNN classifier architecture. Any insight would be greatly appreciated! (new to coding by the way)</p>
"
760,"<p>I am currently in the pre-process of starting an image classification and extraction project which needs to output multiple softmax and absolute values from a single image like such:</p>

<pre><code>{
 time: ""20:20"", 
 teams: [
   {
     red: { goals: 2},
     blue: { goals: 1},
   },
   {
     scored_by : [{
      john: 80%, 
      kyle: 51%, 
      darren: 20%
     }
   ]}
 ]
}
</code></pre>

<p>I can create multiple models which are responsible for different task such as reading the time from the image as well as the score and eventually combine both. I would however like to make sure I maximize on efficiency to make sure the process is as fast as possible.</p>

<p>Any pointers in the right direction would be greatly appreciated.</p>

<p>With kind regards,
Dennis</p>
"
761,"<p>I have created 22 different Convolutional neural networks that all test for the presence of unique objects in an image (each one of the classifiers is unique). </p>

<p>Each sample in the test set has the output of a 22-long vector that looks something like this [0, 1, 1, 0, 0, 1, ..., 1], the binary nature of the vector representing the presence/absence of specific objects. </p>

<p>I have implemented this already in keras and reach around 97% accuracy avg for the 22 models. Is there any specific ensemble methods that can allow me to combine all 22 classifiers?</p>
"
762,"<p>I have not seen this explicitly stated anywhere so I was curious. Say I have network trained to meet my segmentation needs using 250x250 images. After this training is complete and I wish to submit images in production for segmentation, do those production submitted images need to be 250x250 as well or can they be any reasonable size?</p>

<p>If they must be resized to 250x250 for segmentation, is it possible to scale up the segmentation regions to apply to a larger image? If so what is the name of that technique so I can research it a bit more.</p>
"
763,"<p>I can't seem to understand how an AI learns. Without having a programmer tell it what to do, how would a program create or generate some solution to a problem and then use information gained in future problems? I understand how chess AI works. But what's really confusing is how an AI would improve or learn.</p>
"
764,"<p>Is anyone able to recommend some resources (preferably books) on the topic of neural networks that goes beyond that of introductory reading?</p>

<p>I'm still relatively new to the subject, however I have successfully created my own Neural Network so I wouldn't consider myself a beginner, so I'm looking for something more intermediate.</p>
"
765,"<p>Both AI and Computer Science are Sciences, as I understood from Wikipedia, Computer Science is everything that has any relation to computers. And AI is commonly defined as</p>

<blockquote>
  <p>Study of machines that take the prerogative of humans (creating musical pieces e.t.c</p>
</blockquote>

<p>But recently, when I was reading, I read this sentence : ""In Computer Science, AI is [...]""</p>

<p>So my question is really : Is there a part of AI studies that do not refer to Computer Science?</p>
"
766,"<p><strong>My datasets are not actual images, so using methods with ImageDataGenerator or pre-trained networks might not apply in this case.</strong></p>

<p>Data Structure: Each ""image"" is a 2048-long vector that has float values between 0 and 1.</p>

<p><a href=""https://i.stack.imgur.com/S3UfL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S3UfL.png"" alt=""an &quot;image&quot; plotted""></a></p>

<p>Each ""image"" was associated with a label (multi-label classifcation) and the goal is to perform classification via Keras 2D CNN's.</p>

<p>What are common techniques for finding which parts of the ""images"" contribute most to classification via convolutional neural nets?</p>

<p>I already implemented the CNNs in keras and have already successfully trained on my images.</p>

<p>*No my data is not time series; however, my model works with either the keras Conv1D and Conv2D layers. </p>
"
767,"<p>Analogies are quite powerful in communication. They allow to explain complex concepts to people with no domain knowledge, just by mapping to a known domain. Hofstadter <a href=""https://cogsci.indiana.edu/"" rel=""noreferrer"">says they matter</a>, whereas Dijkstra says they are dangerous. Anyway analogies can be seen as a powerful way to transfer concepts in human communication (dare I say <a href=""https://en.wikipedia.org/wiki/Transfer_learning"" rel=""noreferrer"">transfer learning</a>?).</p>

<p>I am aware of legacy work, such as <a href=""https://en.wikipedia.org/wiki/Case-based_reasoning"" rel=""noreferrer"">Case-Based Reasoning</a>, but no more recent work about the analogy mechanism in AI.</p>

<p>Is there consensus whether or not analogy is necessary to AGIs, and how critical would they be?</p>

<p>Note / update:
- @mindcrime points out the problem with the ""critical"" term. It says in the title, for now, and for consistency. A clearer title could be: ""Is analogy necessary to AGI ?"". Necessity is here like in Logics, as in ""necessary and sufficient"".
- The scope is now explicit, on AGIs.</p>

<hr>

<p>Please consider backing your answers with concrete work or publications.</p>
"
768,"<p>I am trying to build a model which would take an email message (in English, extracted subject, and body of the email) and identify if it has a question, request or a proposal. Basically, I would like to see the mails that I've not replied but needs a reply. The model can be used as a ""filter"" in an email client.</p>

<p>What is the best way to go about it?</p>

<p>Related Work:</p>

<p><a href=""https://github.com/ParakweetLabs/EmailIntentDataSet/wiki"" rel=""nofollow noreferrer"">Parakweet Lab's Email Intent Data Set</a></p>

<p><a href=""https://www.cs.cmu.edu/~tom/EMNLP2004_final.pdf"" rel=""nofollow noreferrer"">Learning to Classify Email into ""Speech Acts""</a></p>
"
769,"<p>We have always known that gradient descent is a function of two or more variables. But how can we geometrically represent gradient descent if it is a function of only one variable?</p>
"
770,"<p>I read a lot about this, I understand how it work, but I would like the most simple example you can provide me, because I have no clue how I would make it in code. No matter the language( I would appreciate if it's derived from c), because I'm not here to copy-paste, and understand the essence.</p>
"
771,"<p>I am working to make my first trained model for image recognition, using the programming language R. </p>

<p>First I am attempting to make a <strong>function that takes a PNG-image as input, resizes it to 128x128 pixels, and converts it into a row-vector of numbers representing the colours in each pixel.</strong> I then want to add this to a file that contains my training-set and labels (the image bank). </p>

<p>I am having some trouble with this, being a beginner in R, as the numerical values I get out from the following code does not make sense to me - often I get a large amount of 1's, and the pictures appear to be completely white if I try to export and look at the image. </p>

<p>Is there any obvious mistake here? I assume it must be where I convert to numerical values. Alternatively, is there an alternative approach to this task that would be better? </p>

<pre><code>load_png_to_image_bank &lt;- function(wd, input_file, label) { 

  # Remembers old working directory 
  oldwd &lt;- wd

  # Sets input wd to file location
  setwd(wd)

  # Loads imager package
  library(imager)

  # Imports file and resizes to 128x128 pixels
  image &lt;- load.image(input_file)
  image &lt;- resize(image,128,128)

  # Finds numerical values for each pixel
  rasterized_vector &lt;- as.data.frame(as.vector(image))
  rasterized_vector &lt;- as.data.frame(t(rasterized_vector))

  # Adds label defined by input variable
  rasterized_vector &lt;- as.data.frame((cbind(1, rasterized_vector)))
  names(rasterized_vector)[1] &lt;- label

  # Sets wd to predetermined place for image bank file
  setwd("" ... "")

  # Writes vector to file
  write.table(rasterized_vector, 
              output_file, 
              row.names=F, 
              na=""NA"",
              append = T, 
              quote= FALSE, 
              sep = "";"", 
              col.names = F)

  # Resets working directory to initial state
  setwd(oldwd)

  }
</code></pre>
"
772,"<pre><code>def Interpretation_be(sentence):

     be = ( 'be', 'been', 'is', 'was', 'are', 'were' )
     place = ('there', 'here')
     gender_identity = ('he', 'she')
     indicator =('it', 'that', 'this', 'these', 'those')
     exist =('exist')
     equal=('equal')
     condition=('condition', 'status', 'quality', 'state', 'situation' 'circumstance') 
     number =('1', '2', '3', '4', '5')
     consequence=('consequence', 'result')
     to = ('to') 


     if sentence.split()[1] in be:
          if sentence.split()[0] in place:
               print(sentence.replace(sentence.split()[1],'exists'))
          if sentence.split()[0] in gender_identity:
               print(sentence.replace(sentence.split()[1],'equal to'))
          if sentence.split()[0] in indicator:
               print(sentence.replace(sentence.split()[1],'equal to'))

     if sentence.split()[1] in exist:
           if sentence.split()[0] in place:
               print(sentence.replace(sentence.split()[1],'is'))

     if sentence.split()[1] in equal:
          if sentence.split()[0] in condition:
                  print(sentence.replace(sentence.split()[1],'is'))
          if sentence.split()[0] in number:
                  print(sentence.replace(sentence.split()[1],'is'))
          if sentence.split()[0] in consequence:
                  print(sentence.replace(sentence.split()[1],'is'))

     if sentence.split()[2] in to:
           print(sentence.replace(sentence.split()[2],'toward'))
           ##need to discern between preoposition to and infinitive to##

##result_check##
Interpretation_be('i have been to Spain')
Interpretation_be('there was a cat')
Interpretation_be('there is a woman')
Interpretation_be('he is a boy')
Interpretation_be('it is a cat')
Interpretation_be('there exist a hat')
Interpretation_be('situation equal to bad')
Interpretation_be('1 equal one')
Interpretation_be('consequence eqaul bad')
</code></pre>

<p>I am trying to make up English rephraser as a most basic start of natural language processing.</p>

<p>While building up like above code, I had wanted to get some already-classified text data which criteria of classification is word-class, such as Noun, Verb, Preposition etc.</p>

<p>Any good text data that I can obtain? </p>

<p>Please let me know. </p>
"
773,"<p>How would one go about solving the <a href=""https://en.wikipedia.org/wiki/15_puzzle"" rel=""nofollow noreferrer"">15 squares puzzle</a> using a <a href=""https://en.wikipedia.org/wiki/Genetic_algorithm"" rel=""nofollow noreferrer"">Genetic Algorithms</a> approach?  In particular, I'd like to understand how you would 
represent the ""chromosome"" in the evolving system.  That is, what's the relationship between the (artificial) ""genes"" and some sort of <a href=""https://pdfs.semanticscholar.org/6f25/ac7608edfa5efe7d13e05f38abb171490676.pdf"" rel=""nofollow noreferrer"">phenotypical expression</a> w/r/t the problem.  It seems like genes would somehow represent moves or sequences of moves but I'm not entirely clear how this would work.</p>
"
774,"<p>I'm doing some testings on NLP and I was thinking to write a code that works like this.</p>

<ul>
<li><p>Subject -> User input -> Output</p></li>
<li><p>Dog ownership -> I own a dog -> Yes</p></li>
<li><p>Dog ownership -> My dog is called Joe -> Yes</p></li>
<li><p>Dog ownership -> I don't have a dog -> No</p></li>
</ul>

<p>Which branch or algorithm would be the best approach for this problem?</p>
"
775,"<p>I'm trying to come up with the right algorithm for a system in which the user enter symptoms and we starts triggering questions related to that symptoms and his answers will result a disease which is related to the answer which is given by the user</p>

<p>Let's assume that the user entered the following input:</p>

<p>Symptom - Deafness</p>

<p>Q1. How long have you had a problem with deafness</p>

<p><em>A)From few days B)From few weeks to months C)More than month D)Since Birth</em></p>

<p>Q2. What was the onset of the deafness</p>

<p><em>A) Sudden B) Gradual</em></p>

<p>Now we have a knowledge base like if a user select option 1 from question 1 and option 2 form question 2 then we will give him some disease. But i need an algorithm which will give % of success in backend so that i can throw the results of disease for example if a user select option 2 from question1 and option 1 from question 2, then when we compare from our knowledge base there will be one set over there which has  option 1 from question 1 and option 2 from question 2 then its a ""SOME"" disease.. now if we compare from our knowledgebase and we found even 50% of the choices is resulting this disease we will throw that disease name.</p>

<p>NOW i am confused what algo should be use for this approach for ai.</p>
"
776,"<p>By this I mean a single axiom for placement.  <em>(I'm working on a solution that involves positional valuation and vectors, and appears to be solid, but my assumption would be this has been done previously, like most mathematical techniques one ""discovers"";)</em></p>
"
777,"<p>I'm going to give a talk, and I'm preparing the material. The purpose of the conversation is to convince companies in my region that it is possible to apply artificial intelligence in solving everyday business problems.</p>

<p>I would like some examples to be able to present, and so I came here to ask</p>

<blockquote>
  <p>Have you used artificial intelligence to solve a problem at work? What kind of problem?</p>
</blockquote>
"
778,"<p>I want to produce a bot in Python that automatically generates short football summaries from Whoscored data. For my first stage I generate the articles with different sentence templates and lots of rules where the data is used.</p>

<p>Now I want to move to the next stage and start looking into NLP and more advanced NLG. I already scraped numerous articles to create a corpus. How should I move on and do next? Any advice would be much appreciated as I'm new in this.</p>

<p>Thanks!</p>
"
779,"<p>I need some help in developing a Markov Model for a crossroads there is no one way road and i am assuming at this time that traffic is only allowed to go straight no turns are allowed. There are 4 roads and 4 signals(agents).</p>
"
780,"<p><a href=""http://www.vocativ.com/culture/science/most-powerful-poker-computer-cepheus/"" rel=""nofollow noreferrer"">Cepheus</a> is an artificial intelligence designed to play Texas Hold'em. By playing against itself and learning where it could have done better, it became very good at the game. <a href=""http://slatestarcodex.com/2015/01/17/links-12014-link-for-you-know-not-whence-you-came-nor-why/"" rel=""nofollow noreferrer"">Slate Star Codex</a> comments:</p>

<blockquote>
  <p>I was originally confused why they published this result instead of heading to online casinos and becoming rich enough to buy small countries, but it seems that it’s a very simplified version of the game with only two players. More interesting, the strategy was reinforcement learning – the computer started with minimal domain knowledge, then played poker against itself a zillion times until it learned everything it needed to know. </p>
</blockquote>

<p>Apparently Cepheus currently just plays against one person. Seeing as it managed to develop amazing strategy for this ""very simplified"" environment, what's stopping it from working on real/full poker games?</p>
"
781,"<p>I'm developing a chatbot, and to get the answer I'm using the Naive Bayes classifier by sorting the questions and answers. For those who want to see the whole project code and more definitions follow the link of <a href=""https://github.com/ZehLuckmann/Roger"" rel=""nofollow noreferrer"">GitHub</a></p>

<p>To develop I am using the TextBlob library for python, the problem is that when training my classifier it is always returning the same message, regardless of the input I use. The message is:</p>

<blockquote>
  <p>""Tudo bom?""</p>
</blockquote>

<p>I still can not identify the problem, I do not know if the problem is in the way my data is willing to perform the training or if it is the way I am training the classifier.</p>

<p>My class that performs the sorting process is this:</p>

<pre><code>#encoding: utf-8
#!/usr/bin/env python
from textblob.classifiers import NaiveBayesClassifier
from textblob import TextBlob
import logging

class Talk(object):
    """"""The Talk class is responsible for returning the response
    Based on the information exported. Using the classification
    According to Bayes' theorem
    """"""
    def __init__(self):
        """"""
        Class builder

        Cl -&gt; Stores the classifier
        Accuracy -&gt; Stores the accuracy of the algorithm
        """"""
        self.__cl = None
        self.__accuracy = 0


    def train(self, train_set):
        """"""
        Train with the list of information consisting of phrases and their
        Respective classifications:
        """"""

        logging.debug('Inicia treinamento da previsão de intenção')
        self.__cl = NaiveBayesClassifier(train_set)
        logging.debug('Treinamento da previsão de intenção finalizado')

    def test(self, test_set):
        """"""
        Performs tests with the list of information formed
        Of sentences and their respective classification to obtain the accuracy:
        """"""

        logging.debug('Inicia teste da previsão de intenção')
        self.__accuracy = self.__cl.accuracy(test_set)
        logging.debug('Teste da previsão de intenção finalizado')
        logging.info('Precisão da previsão: {}'.format(self.__accuracy))

    def response(self, phrase):
        """"""
        Returns the phrase response according to the created classifier
        """"""
        logging.debug('Analisa a frase ""{}""'.format(phrase))
        blob = TextBlob(phrase,classifier=self.__cl)
        result = blob.classify()
        logging.debug('Resposta: ""{}""'.format(result))
        return result
</code></pre>

<p>Follow the link in my file with training information and test data</p>

<ul>
<li><a href=""https://github.com/ZehLuckmann/Roger/blob/master/src/roger/database/dados_treino_ze.csv"" rel=""nofollow noreferrer"">Training</a></li>
<li><a href=""https://github.com/ZehLuckmann/Roger/blob/master/src/roger/database/dados_teste_ze.csv"" rel=""nofollow noreferrer"">Test</a></li>
</ul>
"
782,"<p>In <code>Artificial Intelligence: a Modern Approach</code>, when it talks about strategies to improve efficiency of resolution inference(section 9.5.6), it says selecting the <em>set of support</em> and resolving one of elements in it first are helpful. But I cannot understand the way it select the <em>set of support</em> and why.</p>

<p>The original excerpt as follows:</p>

<blockquote>
  <p><strong>Set of support</strong>: Preferences that try certain resolutions first are helpful, but in general it is more effective to try to eliminate some potential resolutions altogether. For example, we can insist that every resolution step involve at least one element of a special set of clauses—the <em>set of support</em>. The resolvent is then added into the set of support. If the set of support is small relative to the whole knowledge base, the search space will be reduced dramatically.</p>
  
  <p>We have to be careful with this approach because a bad choice for the set of support will make the algorithm incomplete. However, if we choose the set of support S so that the remainder of the sentences are <strong>jointly satisfiable</strong>, then set-of-support resolution is complete. For example, one can use the negated query as the <em>set of support</em>, on the assumption that the
  original knowledge base is consistent. (After all, if it is not consistent, then the fact that the query follows from it is vacuous.) The set-of-support strategy has the additional advantage of generating goal-directed proof trees that are often easy for humans to understand.</p>
</blockquote>

<p>What does it mean that the remainder of the sentences are <strong>jointly satisfiable</strong>? Why could one use the negated query as the <em>set of support</em>?</p>

<p>Thanks in advance, I hope someone could shed some light on it:)</p>

<p>Ps. I'm a English leaner. I may not present this problem very well, and I'm very sorry for that. But I‘m very serious about it and I've try my best to make it as clear as I can. So if you're about to down vote it, please comment below and give me some advice to improve it, I'll be very grateful for that. Thanks again!</p>
"
783,"<p>In section 10.3.2 in <em>Artificial Intelligence: a Modern Approach</em> there is a piece of pseudocode that describes the graph plan algorithm. The graph plan algorithm constructs a planning graph for a problem and extract a solution from it if there is one.</p>

<p>Here is the pseudocode:</p>

<pre><code>function GRAPHPLAN(problem) returns solution or failure
    graph ← INITIAL-PLANNING-GRAPH(problem) 
    goals ← CONJUNCTS(problem.GOAL)
    nogoods ← an empty hash table
    for tl=0 to ∞ do
        if goals all non-mutex in St of graph then
            solution ← EXTRACT-SOLUTION(graph, goals, NUMLEVELS(graph), nogoods) 
            if solution != failure 
                then return solution
        if graph and nogoods have both leveled off 
                then return failure                 
        graph ← EXPAND-GRAPH(graph, problem)
</code></pre>

<p>Also a PPT for <a href=""http://www.cs.mtu.edu/~nilufer/classes/cs5811/2014-fall/lecture-slides/cs5811-ch10b-graphplan.pdf"" rel=""nofollow noreferrer"">Graphplan</a></p>

<p>What confuses me for a long time is when the <code>nogoods</code> levels off? The book describe no-goods as follows:</p>

<blockquote>
  <p>In the case where EXTRACT-SOLUTION fails to find a solution for a set of goals at a given level, we record the (level,goals) pair as a no-good.</p>
</blockquote>

<p>But in that case, every time the first <code>EXTRACT-SOLUTION</code> after <code>EXPAN-GRAPH</code> fails, there will always be at least a new no-good produced: whose <code>level</code> becomes the new level of the planning graph and the <code>goals</code> is always the same--which is the goal of the problem. So when does it level off?</p>

<p>There is another sentence about no-goods in the book:</p>

<blockquote>
  <p>If the function EXTRACT-SOLUTION fails to find a solution, then there must have been at least one set of goals that were not achievable and were marked as a no-good. </p>
</blockquote>

<p>From above excerpt, I learn that <code>goals</code> in no-goods is a set of goals that are not achievable in the <code>level</code>. But, IMHO, that could vary when we chose different actions in <code>EXTRACT-SOLUTION</code>. The example in the book provides a good instance:</p>

<blockquote>
  <p>Consider an air cargo domain with one plane and n pieces of cargo at airport A, all of which have airport B as their destination. In this version of the problem, only one piece of cargo can fit in the plane at a time. The graph will level off at level 4, reflecting the fact that for any single piece of cargo, we can load it, fly it, and unload it at the destination in three steps. But that does not mean that a solution can be extracted from the graph at level 4; in fact a solution will require 4n − 1 steps: for each piece of cargo we load, fly, and unload, and for all but the last piece we need to fly back to airport A to get the next piece.</p>
</blockquote>

<p>In the example above, the set of goals not achievable at level 4 varies based on what actions has been taken.</p>

<p>Could someone help clear up this problem? I know there must be some misunderstanding, welcome to point it out, please. Thanks in advance:)</p>

<p>Ps. I'm a English leaner, I may not present this problem very well, so I'm very sorry for any reason that you may want to down vote it. But I‘m very serious about it and I've try my best to make it as clear as I can. So if you're about to down vote it, please comment below and give me some advice to improve it, I'll be very grateful for that. Thanks again!</p>
"
784,"<p>If I create a program which takes an input, gives an output and then requires a response to let it know whether the answer it gave was any good does it count as AI? </p>

<p>If not, what is the process of AI? Does it not always need specific parameters? For example, I ask it ""Who is the president of the USA?"", and I have programmed it to look for news articles in SEOs and remove the ""Who"" part, is that AI?</p>
"
785,"<p>After witnessing the rise of deep learning as automatic feature/pattern recognition over classic machine learning techniques, I had an insight that the more you automate at each level, the better the results, and I therefore turned my focus to <a href=""https://en.wikipedia.org/wiki/Neuroevolution"" rel=""nofollow noreferrer"">neuroevolution</a>.</p>

<p>I have been reading neuroevolution publications with the same desire to automate at every level.</p>

<p>Do genetic algorithms evolve? Do they get better at searching through the solution space for each generation over time? Is this legitimately ""<a href=""https://en.wikipedia.org/wiki/Evolution"" rel=""nofollow noreferrer"">evolution</a>""?</p>
"
786,"<p>Given this data set:</p>

<pre><code>User 1: {'Artist': 1, 'Public Figure': 9, 'Film Director': 1, 'Education': 1, 'Musician/Band': 4, 'Musician': 1, 'Community': 1, 'Author': 4, 'Politician': 1, 'TV Show': 2, 'Entrepreneur': 1, 'Journalist': 4, 'Product/Service': 1, 'Defense Company': 1, 'Computer Company': 2, 'Nonprofit Organization': 3, 'Computers &amp; Internet Website': 4, 'Media/News Company': 3, 'Podcast': 1, 'News &amp; Media Website': 2, 'Charity Organization': 1, 'Government Organization': 1, 'Magazine': 1}
User 2: {'Nonprofit Organization': 1, 'Movie': 2, 'Musician/Band': 22, 'Public Figure': 2, 'Entrepreneur': 2, 'TV Show': 2, 'Medical Company': 1, 'American Restaurant': 1, 'Sports &amp; Recreation': 1, 'Media/News Company': 2, 'Sports Team': 1, 'Artist': 3, 'Musician': 1, 'College &amp; University': 1, 'Athlete': 1, 'Music': 1, 'App Page': 1, 'Comedian': 1, 'Interest': 2, 'Product/Service': 1, 'Book Series': 1
</code></pre>

<p>I believe I can draw conclusions that the strength of the connection is high or low based on the fact that they like or dislike the same categories of topics. What machine learning technique would be best to apply to these connection types, if I had, say, 500 users with categories and weights for each?</p>

<p>I would like to automatically apply varying weights, if most people have Musician/Band or like Movies, that should be less interesting than when people like ""Defense Company"" like in User 1.</p>
"
787,"<p>Is there any way apply reinforcement learning algorithms in computer vision problems?</p>
"
788,"<p>I'm learning Neural Networks, and everything works as planned but, like humans do, adjusting themselves to learn more efficiently, I'm trying to understand conceptually how one might implement an auto adjusting learning rate for a Neural Network.</p>

<p>I have tried to make it based on error, something like how bigger is error learning rate is getting bigger as well. <sub><em>[Could use some clarification here--not entirely sure what you're saying.  If can clarify, I'm happy to clean up the English. -DukeZhou]</em></sub></p>

<p>*If you want give me an example give it on a C based language or math because I don't have experience with Python or Pascal. </p>
"
789,"<p>I have been following the <a href=""http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml"" rel=""noreferrer"">ML course by Tom Mitchel</a>. 
The inherent assumption while using Decision Tree Learning Algo is: <strong>The algo. preferably chooses a Decision Tree which is the smallest.</strong> </p>

<p><strong>Why is this so</strong> when we can have bigger extensions of the tree which could in principle perform better than the shorter tree?</p>
"
790,"<p>I'm first year student in machine learning and I really recently started to immersing myself. </p>

<p>I need to <strong>calculate number of</strong>:</p>

<ul>
<li>matrix additions</li>
<li>matrix multiplications</li>
<li>matrix divisions </li>
</ul>

<p>which are processed in the well known convolutional neural network - <a href=""https://en.wikipedia.org/wiki/AlexNet"" rel=""nofollow noreferrer"">AlexNet</a>.</p>

<p>I found some <a href=""http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf"" rel=""nofollow noreferrer"">materials</a> about it, but I'm really confused where to start.</p>

<p>So, the overall structure might looks like:</p>

<p><a href=""https://i.stack.imgur.com/FdSg4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FdSg4.png"" alt=""AlexNet structure overview""></a></p>

<p>But, how can I calculate operations for each type distinctly?</p>
"
791,"<p>I'm now reading a book titled <a href=""http://shop.oreilly.com/product/0636920052289.do"" rel=""noreferrer"">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a> and in the Chapter 10 of the book, the author writes the following:</p>

<blockquote>
  <p>The architecture of biological neural networks (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as shown in Figure 10-2.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/fsge8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fsge8.png"" alt=""enter image description here""></a></p>

<p>However there seems to be no link to any research there. And the author didn't say it assertively given that he used <em>""it <strong>seems</strong> that neurons are often organized in consecutive layers""</em>.</p>

<p>Is this true and how strongly is it believed? What research is this from?</p>
"
792,"<p>I want to learn about Artificial Intelligence. As a beginner, I have no Idea what so ever about Ai . I want to have a clear idea about what AI and Its subfields are and How it works so that I can determine what I really want to learn about Ai .</p>

<p>So Can anyone provide me with some resources about ai?</p>
"
793,"<p>I am developing an LSTM for sequence tagging. 
During the development, I do various changes in the system, for example, add new features, change the number of nodes in the hidden layers, etc. 
After each change, I check the accuracy using cross-validation on a development corpus. </p>

<p>Currently, in each check, I use 100 iterations to train the system, which takes a lot of time. So I thought that maybe, during development, I can use only e.g. 20 iterations. Then, each check will be faster. After I find the best configuration, I can switch back to 100 iterations to get better accuracy.</p>

<p>My question is: is this consideration correct? I.e, if feature-set A is better than feature-set B with 20 training iterations, is it likely that A will be better than B also with 100 training iterations?</p>

<p>Alternatively, is there a better way to speed up the development process?</p>
"
794,"<p>I have a set of data representing many sellers and the items they sell. There is an overlap between the various sellers' items. These items come from various distributors and typically an item comes from a unique distributor. </p>

<p>I am trying to come up with an algorithm that will group items by the distributors that sold them to the sellers. Any ideas come to mind?</p>
"
795,"<p>Has anyone had a chance to tinker with multiple major AI platforms such as TensorFlow, Cognitive Talk, Quill etc... </p>

<ul>
<li>What are the strengths and weaknesses of different AI platforms? </li>
</ul>

<p>Comprehensive articles that tackle this topic would be helpful.</p>
"
796,"<p>I'm currently looking in to the possibility of using machine learning to detect fraudulent transactions on our website based on the events that happen for each user.</p>

<p>I'd like to be able to stream events in to it, such as sign up, order placed, inviting another user, etc along with the the time and some how come up with a probability of how likely it is that the person is acting in a fraudulent manner </p>

<p>I'm totally new to machine learning and everything I read goes straight over my head basically :/</p>

<p>Can someone please explain the type of neural network I'd need, how I'd decide how to set it up and how I would go about training it?</p>
"
797,"<p>I am looking for AI podcasts that are purely academic oriented that I can use for learning purposes. Thanks for any resource pointers.</p>

<p>The AI podcasts I am aware of are (not sure how many of these can be considered academic):</p>

<ul>
<li><a href=""https://blogs.nvidia.com/ai-podcast/"" rel=""nofollow noreferrer"">The AI Podcast</a></li>
<li><a href=""https://itunes.apple.com/us/podcast/linear-digressions/id941219323"" rel=""nofollow noreferrer"">Linear Digressions</a></li>
<li><a href=""https://itunes.apple.com/us/podcast/oreilly-bots-podcast-oreilly/id1145426486"" rel=""nofollow noreferrer"">O'Reilly Bots Podcast</a></li>
<li><a href=""https://soundcloud.com/a16z/artificial-intelligence"" rel=""nofollow noreferrer"">A16z Podcast</a></li>
</ul>
"
798,"<p>There are two general ways in which AI can interact with humans:</p>

<ol>
<li><p>Implants of AI devices into human bodies or brains &mdash; Such implantation has already begun with health monitors and could grow to include cognitive access to general purpose digital computing.  Once such is accepted, it is possible that humans will emerge with behavior largely defined by its implants therefore qualify as a hybrid AI systems.  </p></li>
<li><p>Embedding of AI systems into business or government organizations &mdash; It is possibliites that the behavior of organizations using AI systems for business decisioning will eventually be defined in behavior largely by those systems and would therefore qualify as a hybrid AI systems.</p></li>
</ol>

<p>How probable are these possibilities, considering human history and current cultural trends in technology acceptance?</p>
"
799,"<p>From the lecture in machine learning I know, that a linear activation function can only produce a linear function, but I don't know if it can produce a connected linear function like the one in the image? This function consists of multiple concatenated lines.</p>

<p><a href=""https://i.stack.imgur.com/y51Kh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y51Kh.png"" alt=""enter image description here""></a></p>
"
800,"<p>I'm very new to the field of AI and HMMs. My question is can HMMs be used to model <strong>any</strong> time series data? Or does the data have to be that of a Markov process?</p>

<p>In HTK documentation, I see that the first few lines state that it can model any time series (""HTK is a toolkit for building Hidden Markov Models (HMMs). HMMs can be used to model any time series and the core of HTK is similarly general-purpose."")</p>
"
801,"<p>I recently started learning about reinforcement learning and currently I am trying to implement the <a href=""http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html"" rel=""nofollow noreferrer"">SARSA algorithm</a>, however I do not know how to deal with $Q(s', a')$, when $s'$ is the terminal state. First, there is no action to choose in this state, and second, this $Q$-factor will never be updated either because the episode ends when $s'$ is reached. Should I initialize $Q(s', a')$ to something other than a random number? Or should I just ignore the $Q$-factors and simply feed the reward $r$ into the update?</p>
"
802,"<p>I'm aiming to create a neural network that can learn to predict the next state of a board using the rules of Conway's Game of Life. This is technically three questions, but I felt that they needed to be together to get the full picture and I don't want to spam the Artificial Intelligence SE with new questions.</p>

<p>My network will look at each cell individually (to reduce computing power needed and to increase learning speed) and its surrounding cells. 9 input nodes for the network will go into one hidden layer. The output layer will be one node for the state of that cell in the next state of the game.</p>

<p>Nodes have two states (alive and dead) and connections between nodes can either transfer that value, or invert it.</p>

<p>For the learning part, I was going to make use of mutation and natural selection. The starting network will have the input and output layers with no hidden layer and no connections. My idea was then to introduce mutation by randomly generating a number of new networks by adding nodes and randomly connecting them to inputs and to the output. The number of nodes in the middle layer will be limited to 512, since there are 512 possible inputs, however I may reduce this if it is too slow. <strong>Should I also have it randomly delete nodes and connections, in case they also make improvements?</strong></p>

<p>Each network will be tested on the same board state, and their accuracy will be calculated by comparing their output to a correct output generated by a computer program. The most accurate network will then be used for the next generation.</p>

<p>My issue is that I don't know how to program the nodes. <strong>Should the nodes in the hidden layer perform a logical AND on all of their inputs or an OR?</strong></p>

<p>I know that the network won't learn the rules within the first few turns, but <strong>how do I know if it will ever get above 90% accuracy, or even just 50%?</strong></p>
"
803,"<p>I am writing my own recurrent neural network in Java to understand the inner workings better. While working through the math, I found that in timesteps later than 2 the gradient of weight w of neuron n depends on the gradients of all neurons at all timesteps before. A handwritten example is given, I tried to write as clearly as possible.</p>

<p>Could anyone verify this so I can finish my network? Am I missing a piece or is my premise wrong, as in the output of a neuron is s(Wx + Vh + b), where h is the last step of <em>only</em> neuron n?</p>

<p><a href=""https://i.stack.imgur.com/QFMfz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QFMfz.jpg"" alt=""The feedforward formulas for the first three timesteps""></a>
<a href=""https://i.stack.imgur.com/jW9mq.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jW9mq.jpg"" alt=""My derivations of the gradients of the specific weight 1 of neuron 1, just as an example""></a></p>
"
804,"<p>A few days ago I asked the question, if a NN with linear activation function can produce a function concatenated of linear functions, what actually is impossible (<a href=""https://ai.stackexchange.com/questions/3753/can-a-nn-with-linear-activation-functions-produce-a-connection-of-linear-functio"">Can a NN with linear activation functions produce a connection of linear functions?</a>). </p>

<p>Now I have here some classification examples, but I really cannot perfectly decide, which one is based on which approach:</p>

<p><a href=""https://i.stack.imgur.com/NqLDL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NqLDL.png"" alt=""enter image description here""></a></p>

<p>1 -> C The perceptron does not look for the maximum separation margin.</p>

<p>2 -> E Neural network with linear activation function</p>

<p>3 -> A Linear SVM, because of the maximum separation margin.</p>

<p>4 -> B Because of the hyperbolic shape of the hyperplane.</p>

<p>5 -> D? Logisitc regression? I tought it can only linear separate?</p>

<p>6 -> F I guess the NN with tanh activation function, because of the no very smooth shape, which comes from the too small hidden layer size.</p>

<p>I actually don't get how the logistic regression classifier should be able to produce a hyperplane like in 5? What did I classify wrong here?</p>
"
805,"<p>According to <a href=""http://www.independent.co.uk/life-style/gadgets-and-tech/news/facebook-artificial-intelligence-ai-chatbot-new-language-research-openai-google-a7869706.html"" rel=""nofollow noreferrer"">this article</a>, two Facebook ai's had the following ""creepy"" negotiation over a transaction:</p>

<blockquote>
  <p>Bob: i can i i everything else . . . . . . . . . . . . . .</p>
  
  <p>Alice: balls have zero to me to me to me to me to me to me to me to me to</p>
  
  <p>Bob: you i everything else . . . . . . . . . . . . . .</p>
  
  <p>Alice: balls have a ball to me to me to me to me to me to me to me</p>
  
  <p>Bob: i i can i i i everything else . . . . . . . . . . . . . .</p>
  
  <p>Alice: balls have a ball to me to me to me to me to me to me to me</p>
  
  <p>Bob: i . . . . . . . . . . . . . . . . . . .</p>
  
  <p>Alice: balls have zero to me to me to me to me to me to me to me to me to</p>
  
  <p>Bob: you i i i i i everything else . . . . . . . . . . . . . .</p>
  
  <p>Alice: balls have 0 to me to me to me to me to me to me to me to me to</p>
  
  <p>Bob: you i i i everything else . . . . . . . . . . . . . .</p>
  
  <p>Alice: balls have zero to me to me to me to me to me to me to me to me to</p>
</blockquote>

<p>If we first look at Bob's, he's asking for what he wants by proposing that he have all items but one and not revealing which is the one he doesn't want.  By design or by chance, this is actually a strong negotiating technique because he reveals nothing other than the fact he is willing to come to an agreement.</p>

<p>Alice appears to either ask for no balls, or to say they have no value to her and then obsess about things coming to her, perhaps iterating on the other items.  She would appear to be the better communicator because she at least gets to the point of saying ""have a ball"".  But she refuses to give anything away beyond that.</p>

<p>But Bob seems to stand firm saying he wants ""everything else"" but not giving away what he is willing to go without.</p>

<p>Perhaps these two are not such bad negotiators after all?</p>
"
806,"<p>I'm attempting to program my own system to run a neural network. To reduce the number of nodes needed, it was suggested to make it treat rotations of the input equally.</p>

<p>My network aims to learn and predict Conway's Game of Life by looking at every square and its surrounding squares in a grid, and giving the output for that square. Its input is a string of 9 bits:</p>

<p><a href=""https://i.stack.imgur.com/p9UwK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/p9UwK.png"" alt=""Glider""></a></p>

<p>The above is represented as 010 001 111.</p>

<p>There are three other rotations of this shape however, and all of them produce the same output:</p>

<p><a href=""https://i.stack.imgur.com/eyDkl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eyDkl.png"" alt=""Glider rotations""></a></p>

<p>My network topology is 9 input nodes and 1 output node for the next state of the centre square in the input. How can I construct the hidden layer(s) so that they take each of these rotations as the same, cutting the number of possible inputs down to a quarter of the original?</p>

<p><strong>Edit:</strong></p>

<p>There is also a flip of each rotation which produces an identical result. Incorporating these will cut my inputs by 1/8th. With the glider, my aim is for all of these inputs to be treated exactly the same. Will this have to be done with pre-processing, or can I incorporate it into the network?</p>
"
807,"<p>This concerns a set of finite, non-trivial, combinatorial games <a href=""http://www.fundamentalcombinatronics.com/rules-of-m/"" rel=""nofollow noreferrer"">[M]</a> in the form of an app. <a href=""http://www.mclassgames.com/powerofm/"" rel=""nofollow noreferrer"">A sample game can be found here</a>. </p>

<p>Because this is a mass market product, we can't take up too much space, and the AI needs to be able to run locally since connectivity cannot be assumed. The current size of the Android kernel is &lt; 7MB.</p>

<p>The goal is not sheer AI strength, but respectable AI strength, sufficient to beat the above-average human player. <em>(The current strongest, weak automata, using a few heuristics, is already capable of beating the average human player.)</em> </p>

<p>Because the games are finite, the gametrees eventually become tractable, allowing for perfect endgames.  Resource stealing strategies and trap-avoidance can also be effected with shallow look-ahead at all phases, and the patterns are much easier for automata to recognize than for humans.</p>

<p><strong>In this context, reinforcement would be mostly utilized to ""tune the automata to the style of the human player,"" and produce different automata on different devices, which could subsequently play against each other as proxies for their human partners.</strong></p>

<p>The game data can be stored efficiently, initially requiring only 2 bytes per position (a value 0-9 and a coordinate 1-81, although the number of coordinates will grow in basic game extensions, and require 2 bytes for larger-order gameboards such as <a href=""http://www.samurai-sudoku.com/sudoku3.gif"" rel=""nofollow noreferrer"">""Samurai"" Sudoku</a>.) So the first turn on a given game requires only 2 bytes, the second turn bytes, etc., up to between 50 and 70 turns on an 81 cell gameboard.  Additionally, because it's a square grid, we can reduce for symmetry.  But even with an average number of turns at 50, that's only about 40,000 games for 200MB.  </p>

<ul>
<li>Weighting Openings</li>
</ul>

<p>My feeling is that reinforcement would be useful in weighting openings.  If the game data is a string, these strings can be compared, and the smaller the sample size (the fewer the turns included,) the more connections there will be. In this case, the sequence doesn't matter, only the set of value/coordinates for a given turn. Abstraction can be utilized in that certain individual value/coordinates are interchangeable. <strong>My thought is the automata can weight openings based on how often they lead to desirable outcomes in the form of a win.</strong>  </p>

<ul>
<li>Weighting Heuristics</li>
</ul>

<p>Since we're having good, initial results with heuristics, and these are the most efficient method of decision-making, <strong>I'm thinking about weighting evaluation functions so that certain heuristics take precedence under different conditions.</strong>  (For instance, when to expand vs. when to consolidate. When to make a choice with immediate benefit over a choice with long-term benefit. Introduction of meta-strategies that modify foundation strategies.)</p>

<ul>
<li>Database pruning</li>
</ul>

<p>Because the allocated volume will be capped, <strong>it's probably going to be necessary to ""prune"" the database when info is no longer relevant. (For instance, when a new strategy emerges that renders previous strategies obsolete.)</strong> We also probably need a method to help the automata to recognize such situations, so it doesn't persist in potentially obsolete strategies for more than two games without starting to try alternatives.  </p>

<hr>

<p><strong>Q: Can reinforcement learning be meaningfully applied toward these goals under these restrictions?</strong>  </p>

<p><strong>Q: Are my inclinations for approaching this useful or problematic?</strong></p>

<p><strong>Q: Are there methods I'm not considering that could be applicable under these restrictions?</strong></p>
"
808,"<p>In general, what possibilities are there for reinventing job descriptions that could be replaced by an automated AI solution? My initial ideas include:</p>

<ul>
<li>Monitoring the AI and flagging its incorrect actions.</li>
<li>Possibly taking over the control in very challenging scenarios.</li>
<li>Creating/gathering more training/testing data to improve the accuracy of the AI.</li>
</ul>
"
809,"<p>In Ch-14.4 @ <strong>Pattern Recognition and Machine Learning</strong> by <strong>Bishop</strong> it is mentioned that <em>tree-based models are more widely used in Medical Diagnosis</em>. </p>

<p>Apart from giving better performance, is there a human-centric reason  for this trade off <strong>as medical diagnosis is mainly performed by human?</strong></p>
"
810,"<p>I'm trying to optimize a combination of 8 cards with 64 card characters. No repeats and order doesn't matter.</p>

<p>n!/(n!(n-r)!) = 4,426,165,368 combinations</p>

<p>I have everything set up, including the data scrapper. But I don't know how many games my machine needs to learn from to start seeing patterns. For example, in 14,000 games analyzed, only 834 decks had a certain character.</p>

<p>Analyzing the next 7 cards from 834 decks is 621,216,192</p>

<p>So I guess I need more data before reliable patterns emerge... but how much data? Thank you and god bless</p>
"
811,"<p><strong>Does Google, or any other service, have a syntax and grammar api?</strong></p>

<p>I am looking for a service which will allow an essay to be checked for correct spelling, gramma and any other relevant features.</p>
"
812,"<p>I always thought rule-based was synonymous with logic-based AI. Logic has axioms and rules of inference, whereas rule-based ai has a knowledge base (essentially axioms) and if-then rules to create new knowledge (essentially inference rules).</p>

<p>But in their famous article ""What is a Knowledge Representation?"", Davis, Shrobe and Szolovits seem to imply that they are not:</p>

<blockquote>
  <p>Logic, rules, frames, etc., each embody a viewpoint on the kinds of things that are important in the world. Logic, for instance, involves a (fairly minimal) commitment to viewing the world in terms of individual entities and relations between them. Rule-based systems view the world in terms of attribute-object-value triples and the rules of plausible inference that connect them, while frames have us thinking in terms of prototypical objects.</p>
</blockquote>

<p>Is this only saying that rule-based are propositional whereas logic-based is usually meant to mean predicate logic? Or is there more to it than this?</p>
"
813,"<p>I've been looking recently into what uses AI - specifically machine learning - may have in automating engineering design.  For a long time there have been algorithms that solve constraint satisfaction problems, and to me it makes sense to consider engineering problems as a superset of constraint satisfaction problems.  In spite of this, I haven't been able to find any cases of engineering design being automated other than a couple of cases of genetic algorithms being used to optimise structural members.</p>

<p>So my question is, why can't I find any examples?  The first thing that springs to mind is that I just haven't been looking hard enough - if this is the case, could anyone point me in the right direction?  The other obvious answer is that it isn't a widely researched area - if so, why not?  Is it just due to lack of interest or are there technical hurdles (abstraction, complex logic &amp; reasoning, etc.) that make this a much more difficult problem than computer vision, games, and so on?</p>
"
814,"<p>Frameworks like <a href=""http://pytorch.org"" rel=""noreferrer"">PyTorch</a> and TensorFlow through <a href=""https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html"" rel=""noreferrer"">TensorFlow Fold</a> support Dynamic Computational Graphs and are receiving attention from data scientists.</p>

<p>However, there seems to be a lack of resource to aid in understanding Dynamic Computational Graphs.</p>

<p>The advantage of Dynamic Computational Graphs appears to include the ability to adapt to a varying quantities in input data.  It seems like there may be automatic selection of the number of layers, the number of neurons in each layer, the activation function, and other NN parameters, depending on each input set instance during the training.  Is this an accurate characterization?</p>

<p>What are the advantages of dynamic models over static models?  Is that why DCGs are receiving much attention?  In summary, what are DCGs and what are the pros and cons their use?</p>
"
815,"<p>I have an industrial problem which I'm trying to cast as a Traveling Salesman problem (TSP) in 3D euclidian space. There are physical limitations which implies that some subpaths may or may not be valid based on simple rules. </p>

<p>What algorithm is best to deal with the TSP given that there are rules/model/constraints?</p>

<p>It could be done with Genetic algorithms for example, but the only way i see how to incorporate those rules is by including them somehow within the fitness function. But i feel there should be more suitable approaches.</p>

<p>Would reinforcement Q-learning or other algorithms be more appropriate for a rule-based euclidian TSP?</p>
"
816,"<p>Suppose that you have 80 neurons in a layer, where one neuron is bias. Then  you add a dropout layer after the activation function of this layer.</p>

<p>In this case, does it have a chance to drop out the bias neuron, or does the dropout only affect the other 79 weight neurons?</p>
"
817,"<p>3 SVD Based Methods</p>

<p>For this class of methods to find word embeddings (otherwise known
as word vectors), we first loop over a massive data set and accumulate word co-occurrence counts in some form of a matrix X and then
perform Singular Value Decomposition on X to get a USV^T decomposition. We then use the rows of U as the word embeddings for all
words in our dictionary. Let us discuss a few choices of X.</p>

<p>Above is the excerpt from the standford univ cs224n lecture 1 notes.</p>

<p>Above USV refer to what? There's no prior explanation about it so I ask here.</p>
"
818,"<p>Are there any real-world examples of ""bad"" AI behaviour? I'm not looking for hypothetical arguments of malicious AI (AI in a box, paperclip maximizer), but for actual instances in history where some AI directly did something bad due to its direct action.</p>

<p>Interpretations of the meaning of ""AI"", and ""bad"" are left open due to obvious reasons. Be free with your interpretation, but use some common sense please.</p>

<p>Ex: Microsoft Tay became a racist not too long after being hooked up to the internet, thanks to internet trolls ""teaching"" her bad things.</p>

<p>I can't think of any other instances. So the following examples are just hypothetical scenarios to demonstrate what I mean.</p>

<p>Ex: A self-driving car drove off-track after being presented with an adversarial example, crashing into people.</p>

<p>Ex: Surgery bot goes haywire and does something unintended.</p>
"
819,"<p>So, <a href=""https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/"" rel=""nofollow noreferrer"">Deepmind is pushing for a human level Starcraft bot</a> and <a href=""https://openai.com/the-international/"" rel=""nofollow noreferrer"">Open AI just created a human level 1vs1 Dota bot</a>. </p>

<p>Unfortunately, I've no clue what that signifies because I've never played Starcraft nor Dota nor do I have more than a fleeting acquaintance with similar games. </p>

<p>My question is what the difference between Starcraft and Dota is from a AI perspective and what scientific significance the respective super human bots would have. </p>
"
820,"<p>I am currently searching for a supervised learning algorithm that can be used to predict the output given a large enough training set.</p>

<p>Here's a simple example:</p>

<p>Training set: A: 1 B: 330 C: 1358.238902 Result: 234.244378</p>

<p>Test data: A: 893 B: 34 C: 293 Result: ?</p>

<p>My intention is to predict ""?"" using the input values and result given in the training set.</p>

<p>What algorithm would be effective for this problem given the wide range of my input/output values? Would this require some sort of regression algorithm?</p>
"
821,"<p>Is this related to <a href=""https://en.wikipedia.org/wiki/Supervised_learning"" rel=""nofollow noreferrer"">supervised</a> and <a href=""https://en.wikipedia.org/wiki/Unsupervised_learning"" rel=""nofollow noreferrer"">unsupervised</a> machine learning?  Is it related to AI assisted human learning, and what is the distinction?</p>

<p>Also, why is assisted machine learning seen as an opportunity and unassisted machine learning seen as a threat?</p>
"
822,"<p>I've been looking at both of these python courses on Udemy:</p>

<p><a href=""https://www.udemy.com/artificial-intelligence-az/"" rel=""nofollow noreferrer"">https://www.udemy.com/artificial-intelligence-az/</a></p>

<p><a href=""https://www.udemy.com/deeplearning/"" rel=""nofollow noreferrer"">https://www.udemy.com/deeplearning/</a></p>

<p>Would it be possible to integrate these topics together? Would i need to learn other courses to integrate them?</p>
"
823,"<p>I can recall that a professor once said that decision trees are not good for incremental learning, as they have to be rebuilt from the ground up if new training examples arrive.</p>

<ul>
<li>Is this basically true? Quick googling just brought me to a lot of papers trying to fit decision trees into incremental learning</li>
<li>What other algorithms fall under this category?</li>
<li>Are Neural Nets good for incremental learning? What other algorithms would be good?</li>
</ul>
"
824,"<p>I've gone through several descriptions of CNNs online and they all leave out a crucial part as if it were trivial.</p>

<p>A ""volume"" of neurons consists of several parallel layers (""feature maps""), each the result of convolving with a different kernel.</p>

<p>Between volumes there is usually a step where layers are pooled and subsampled.</p>

<p>The next volume has a different number of parallel layers.</p>

<p>How do the feature maps from one volume connect to the feature maps of the next volume? Is it one-to-many? Many-to-many? Do N kernels apply to each of M feature maps in the first volume, yielding N*M feature maps in the second volume? Are these N kernels the same for each feature map in the first volume, or do different kernels apply to each one? </p>

<p>Or is the number of maps in the second volume not necessarily a multiple of the number in the first volume? If so, do maps in the first volume get cross-synthesized somehow? Or maybe different numbers of maps in the second volume follow from each one in the first?</p>

<p>Or is it some other of umpteen trillion possibilities?</p>
"
825,"<p>I have a question regarding <strong>Answer Set Programming</strong> on how to make an existing <strong><em>fact</em></strong> invalid, when there is already (also) a <strong><em>default statement</em></strong> present in the Knowledge Base.</p>

<p>For example, there are two persons <code>seby</code> and <code>andy</code>, one of them is able to drive at once. The scenario can be that <code>seby</code> can drive as seen in Line 3 but  let's say, after his license is cancelled he cannot drive anymore, hence we now have Lines 4 to 7 and meanwhile <code>andy</code> learnt driving, as seen in Line 7. Line 6 shows only one person can drive at a time, besides showing <code>seby</code> and <code>andy</code> are not the same. </p>

<blockquote>
<pre><code>1 person(seby).
2 person(andy).
3 drives(seby).
4 drives(seby) :- person(seby), not ab(d(drives(seby))), not -drives(seby).
5 ab(d(drives(seby))).
6 -drives(P) :- drives(P0), person(P), P0 != P.
7 drives(andy).
</code></pre>
</blockquote>

<p>In the above program, Lines 3 and 4 contradict with each other, and the Clingo solver (which I use) obviously outputs <code>UNSATISFIABLE</code>. </p>

<p>Having said all these, deleting Line 3 and getting the problem solved is not what I'm epecting. The intention behind asking this question is to know whether it is possible now to make Line 3 somehow invalid to let Line 4 do its duty.</p>

<p>However, Line 4 can also be written as:</p>

<blockquote>
  <p>4 drives(P) :- person(P), not ab(d(drives(P))), not -drives(P).</p>
</blockquote>

<p>Thanks a lot in advance.</p>
"
826,"<p>I asked this question on the physics part of this website and was advised to post it here my question is very simple it seems the current goal of man is to create an artificial intelligence despite the associated fears to man that such a creation could cause.</p>

<p>As I can figure man's approach is two fold and its goal is being attempted by computer programming or by electronic physical neural nets. </p>

<p><strong>My simple question</strong>  is as I can see it, intelligence is born of sentience by man and the occupants living world. So which came first in the physical world we occupy, sentience or programming, genetic or an abstraction bound by physical rules ?</p>

<p>Or as a direct result of the structure of the brains throughout the living world,
 which we may be possible to replicate. If sentience was first doesn't it mean that we will have to alter the definition of programming in order to achieve this goal by that method?</p>
"
827,"<p>I am just curious what AI would be harder to create from a strictly engineering point of view. AI which would win 1vs 1 game with the best player in starcraft or AI which would control a team the whole team in dota2 and win against the best team?  </p>
"
828,"<p>I'm a little bit stuck:</p>

<p>I implemented an AI with GOAP (Goal oriented Action Planning, <a href=""http://alumni.media.mit.edu/~jorkin/gdc2006_orkin_jeff_fear.pdf"" rel=""nofollow noreferrer"">http://alumni.media.mit.edu/~jorkin/gdc2006_orkin_jeff_fear.pdf</a>) for a simulation game. That works fine.</p>

<p>Now I want that the agents can cooperate (e.g. doing actions together). <em>What is in this case the best AI-Design that the GoapActions keep loose couplet?</em></p>

<p>Should they plan together? (what is in this case the ""worldstate""?)Or Should they share their plans? </p>

<p><strong>Example</strong><br>
Agent1:
Worldstate Agent 1: isLonely= true<br>
Goal Agent1: isLonely = false</p>

<p>Plan Agent1: AskAgent2ToTalk -> TalkToAgent2</p>

<p>Agent2
Worldstate Agent 2: hasWood = false<br>
Goal hasWood = true</p>

<p>Plan Agent2: GetAxe -> ChopWood -> BringWoodToSupply</p>

<p><em>How I get this constellation?</em></p>

<p>Agent1 Plan: TalkToAgent2<br>
Agent2 Plan: TalkToAgent1 -> GetAxe -> ChopWood -> BringWoodToSupply</p>

<p>Or if they are talking and one of the agents is interrupted (e.g. by an attacking enemy) the other agent must know that his TalktoAgent2 Action has ended.</p>
"
829,"<blockquote>
  <p>Artificial Intelligence is any device that perceives its environment
  and takes actions that maximize its chance of success at some goal.</p>
</blockquote>

<p>I got this definition from <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow noreferrer"">Wikipedia</a> that cited ""Russell and Norvig (2003), Artificial Intelligence: A Modern Approach"".</p>

<p>A <a href=""https://en.wikipedia.org/wiki/Transistor"" rel=""nofollow noreferrer"">transistor</a> is a device that amplifies or switches electronic signals when it received an input signal.</p>

<p>Could one say the transistor is the AI?</p>

<p>It is certainly a basic building block of every AI out there, but is it an AI itself, albeit the most basic one?</p>

<p>I'm looking at it from a technological and economic point of view, leaving philosophy aside. From an economic perspective it seems to be an AI because transistor does useful work that it took an intelligent human to perform less than a century ago. And it does it completely on its own.</p>
"
830,"<p>Is it possible to feed a neural network, the output from a random number generator and expect it learn the hashing/generator function. So that it can predict what will be the next generated number? Does something like this already exist? If research is already done on this or something related to (predict pseudo random numbers) can anyone point me to the right resources. Any additional comments or advice would also be helpful.</p>

<p>Currently I am looking at this library and its related links.
<a href=""https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent"" rel=""noreferrer"">https://github.com/Vict0rSch/deep_learning/tree/master/keras/recurrent</a></p>
"
831,"<p>To my understanding, <strong>Logistic Regression</strong> is an extension of <strong>Naive Bayes</strong>. 
Suppose,</p>

<p>X = {x<sub>1</sub>, x<sub>2</sub>........x<sub>N</sub>},</p>

<p>Y = {0, 1}, </p>

<p>each x<sub>i</sub> is i.i.d and </p>

<p>the P(x<sub>i</sub>|Y=y<sub>k</sub>) ~ <em>N</em> (&mu;,&sigma;<sup>2</sup>) is a Gaussian Distribution.</p>

<p>So in order to create <strong>Linear Decision Surface</strong>, we take the assumption of each pdf P(x<sub>i</sub>|y<sub>k</sub>) having variance(&sigma;) independent of the value of Y i.e. &sigma;<sub><em>(i,k)</em></sub> = &sigma;<sub>i</sub><br>
(i &rarr;x<sub>i</sub>, k &rarr; y<sub>k</sub>).</p>

<p>Finally we end up learning the coefficients (&omega;<sub>0</sub>, &omega;<sub>i</sub>) that represent the Linear Decision Surface in following equation:</p>

<p>P(Y=0|X)/P(Y=1|X) = &omega;<sub>0</sub> + &Sigma;<sub>i</sub>(&omega;<sub>i</sub>.x<sub>i</sub>)   (Linear Decision Surface)
Even though the derivation of Linear Regression coefficients (&omega;<sub>0</sub>, &omega;<sub>i</sub>) involves the assumption of Conditional Independent x<sub>i</sub> given Y, </p>

<ol>
<li>Why is it said that learning these coefficients from training data are <strong>somewhat more free</strong> from conditional indep. assumption as compared to learning the regular Bayesian Distribution coefficients (&mu;, &sigma;)?</li>
</ol>

<p>I came across this while following <a href=""https://youtu.be/z_xPu9KrgCY?t=2008"" rel=""nofollow noreferrer"">this course here</a>.</p>

<p>Any clarification/suggestion would be very helpful.
Thanks</p>
"
832,"<p>I studied the articles on <a href=""http://neuralnetworksanddeeplearning.com/"" rel=""nofollow noreferrer"">Neural Networks and Deep Learning</a> from Michael Nielsen and developed a simple neural network based on his examples. I understand how backpropagation works and I already taught my neural network to not only play TicTacToe but also improve his own play by learning from his own successes using backpropagation.</p>

<p>Going forward with my experiments, I am facing the problem, that I won't always be able to show the network good moves to use for learning (maybe because I simply don't know what is correct in a certain situation), but I might be required to show it bad moves to avoid (because some of the bad moves are obvious). Teaching the network what to do using backpropagation is easy, but I haven't found a way to teach it what to avoid using similar techniques.</p>

<p>Is it possible to teach simple neural networks using negative examples like this or do I need other techniques? My gut feeling says, that it might be possible to ""invert"" gradient descent into gradient ascent to solve this problem. Or is it more complicated than this?</p>
"
833,"<p>Can someone explain the differance between <code>tf.contrib.DNNClassifier</code> (learn) and <code>tf.estimator.DNNClassifier</code>?</p>

<p><code>tf.contrib.DNNClassifier</code> (learn) works but gives warnings:</p>

<blockquote>
  <p>WARNING:tensorflow:From C:\Anaconda3\lib.. scalar_summary ...is
  deprecated and will be removed after 2016-11-30. Please switch to
  tf.summary.scalar.</p>
</blockquote>

<p>But I can load the layers names and values with <code>get_variable_names</code> and <code>get_variable_value</code>.</p>

<p><code>tf.estimator</code> works fine but how do I get the layers name and values?
See code below with a switch (if True/False) for the two versions</p>

<p>Instalation in Windows 10<br>
3.6.1 |Anaconda 4.4.0 (64-bit)| (default, May 11 2017, 13:25:24) [MSC v.1900 64 bit (AMD64)]
Tensorflow is installed in the root with:<br>
pip install --upgrade tensorflow --ignore-installed (This is the only combination that works for me)<br>
pip list gives
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.4)</p>

<pre><code>#Example of DNNClassifier for Iris plant dataset.
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
from sklearn import datasets
from sklearn import metrics
from sklearn import model_selection
import os
import tensorflow as tf
from tensorflow.contrib import learn
os.environ['TF_CPP_MIN_LOG_LEVEL']='3'  # get rid oc tf_jenkins WARNING

def main():
    # Load dataset.
    iris = datasets.load_iris()
    x_train, x_test, y_train, y_test = model_selection.train_test_split(
        iris.data, iris.target, test_size=0.2, random_state=42)

    # Define the training inputs and train
    get_train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={'x':x_train}, y=y_train, num_epochs=None, shuffle=True)
    #Predict 
    get_test_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={'x':x_test}, y=y_test, num_epochs=1, shuffle=True)

    # Build 3 layer DNN with 10, 20, 10 units respectively.    
    feature_columns = [tf.feature_column.numeric_column('x', shape=np.array(x_train).shape[1:])]    
    #Select code 
    if True: # tf.estimator...
        classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
            hidden_units=[10, 20, 10], n_classes=3)
        #Train
        classifier.train(input_fn=get_train_input_fn, steps=200)

        scores = classifier.evaluate(input_fn=get_test_input_fn)
        print('Accuracy (tf.estimator): {0:f}'.format(scores['accuracy']))
        # get_variable_names, get_variable_value. How ?????

    else: # learn (tf.contrib)
        classifier = learn.DNNClassifier(feature_columns=feature_columns,
            hidden_units=[10, 20, 10], n_classes=3)
        #Train    
        classifier.fit(input_fn=get_train_input_fn, steps=200) 

        scores = classifier.evaluate(input_fn=get_test_input_fn,steps=1)
        print(""\nTest Accuracy (learn): {0:f}\n"".format(scores[""accuracy""]))

        # Get data
        names=classifier.get_variable_names()
        data={}
        for name in names:
            data[name]=classifier.get_variable_value(name)

if __name__ == ""__main__"":
    main()
</code></pre>
"
834,"<p>I wonder on the following concept: a given neural network gets two audio input (preferably music) and gives a real number between 0 and 1 which describes ""similarity"" between the second and the first track.</p>

<p>As far as my understanding of neural networks go, the problem fits the concept of NNs, as pattern recognition in music can help determine similarities and discrepancies in audio, see voice recognition.</p>

<p>However, due to the nature of long and complex inputs, and the vague nature of learning datasets (how similar, for instance, Diana Ross: It's your move, and the vaporwave legend Floral Shoppe exactly are? 0.9? 0.6? other?), such a network would be extremely slow and convoluted.</p>

<p>Is it possible today to build and train such a model? If yes, how would it look like?</p>
"
835,"<p>I am getting confused reading online about Gradient Descent, Convex and Non Convex Loss functions.</p>

<p>Multiple resources I referred to mention that MSE is great because it's convex. But I don't get how, especially in the context of Neural Networks.</p>

<p>Lets say we have the following:</p>

<ul>
<li><span class=""math-container"">$X$</span>: Training Dataset</li>
<li><span class=""math-container"">$Y$</span>: Targets</li>
<li><span class=""math-container"">$\Theta$</span>: Set of Parameters of the Model (NN Model with Non Linearities)</li>
</ul>

<p>Then:</p>

<p><span class=""math-container"">$$\operatorname{MSE}(\Theta) = (\text{Feedforward}_{\Theta}(X) - Y)^2$$</span></p>

<p>Now I don't seem to agree that this MSE Loss function is always convex, it depends strongly on <span class=""math-container"">$\text{Feedforward}_{\Theta}$</span>, right?</p>
"
836,"<p>The textbook Deep Learning by Goodfellow, Bengio, and Courville can be viewed in individual html chapters <a href=""http://www.deeplearningbook.org/"" rel=""nofollow noreferrer"">here</a>. </p>

<p>I'm currently reading Chapter 15 on Representation Learning and saw this for algorithm 15.1 on page 530:</p>

<p><a href=""https://i.stack.imgur.com/n9JEx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n9JEx.png"" alt=""Page 530""></a></p>

<p>There no condition after <code>end for</code> and similarly no end condition for <code>end if</code>. Is that intentional? If so, what is the correct way to interpret it? I haven't encountered algorithm syntax like this in the past. </p>
"
837,"<p>Let's take our standard <a href=""https://wiki.lesswrong.com/wiki/Paperclip_maximizer"" rel=""nofollow noreferrer"">paperclip maximizer General AI</a> and attempt to obtain precisely one million paper clips, over course of a year, without destroying the universe in the process.</p>

<p>Most maximization directives make the process run-away. As cheaply as possible will crash world economy. As good clips as possible will turn the universe into super-synthesizer that assembles atom-perfect paperclips. Adding a deadline on these maximization processes will probably result in terrorizing the staff into readjusting the deadline, or invention of time travel (after consuming the solar system to invent it.) Minimizing resource usage would likely result in closure of all industry world-wide. You know, the standard horror scenarios.</p>

<p>What about the directive of minimizing AI's influence on the world while completing the task? Would it be safe, or can you spot a runaway scenario where it could result in dire effects?</p>
"
838,"<p>I don't play nearly enough Chess to be able to answer.  For context, AlphaGo is stronger than the current strongest human player, but AlphaGo's game play has been cast as ""inhuman"" in the sense that it doesn't resemble human play. <em>(In Go, this can involve aesthetic qualities.)</em></p>

<p>Really I'm wondering about ""narrow"" application of the Imitation Game/Turing Test, where one might design an automata to play more like a human, so that human players would be unable to determine if their opponent was a human or an automata.</p>
"
839,"<p>I have followed the pseudocode in the <a href=""https://arxiv.org/abs/1212.5701"" rel=""nofollow noreferrer"">ADADELTA paper</a> (top right on page&nbsp;3), and wrote the following Python code for solving the optimization problem L(x)&nbsp;=&nbsp;x^2:</p>

<pre><code>&gt;&gt;&gt; import math
&gt;&gt;&gt; 
&gt;&gt;&gt; Eg = Ex = 0
&gt;&gt;&gt; p = 0.95
&gt;&gt;&gt; e = 1e-6
&gt;&gt;&gt; x = 1
&gt;&gt;&gt; history = [x]
&gt;&gt;&gt;
&gt;&gt;&gt; for t in range(100):
...   g = 2*x
...   Eg = p*Eg + (1+p)*g*g
...   Dx = -math.sqrt(Ex+e)/math.sqrt(Eg+e)*g
...   Ex = p*Ex + (1-p)*Dx*Dx
...   x = x + Dx
...   history.append(x)
...
&gt;&gt;&gt; print(history)
[1, 0.9992838851718654, 0.998764712958258, 0.9983330059505671, 0.9979531670003327, 0.9976084468473033, 0.997289409971406, 0.996990129861279
7, 0.9967066004793412, 0.9964359664599116, 0.9961761091943561, 0.9959254064337589, 0.9956825840862665, 0.9954466203957414, 0.99521668152795
22, 0.994992076841149, 0.9947722269598268, 0.9945566404428509, 0.9943448963795858, 0.9941366311727671, 0.9939315283404335, 0.99372931053536
84, 0.9935297332202913, 0.9933325795977276, 0.9931376565033874, 0.9929447910484566, 0.9927538278504536, 0.9925646267313296, 0.9923770607899
557, 0.9921910147771753, 0.9920063837173206, 0.991823071731977, 0.9916409910308515, 0.9914600610415929, 0.9912802076558476, 0.9911013625730
931, 0.9909234627271605, 0.990746449783029, 0.9905702696936243, 0.9903948723080783, 0.9902202110243085, 0.9900462424799209, 0.9898729262763
75, 0.9897002247321224, 0.9895281026610697, 0.9893565271732506, 0.9891854674950317, 0.989014894806555, 0.9888447820944316, 0.98867510401796
52, 0.9885058367874141, 0.9883369580529869, 0.9881684468034365, 0.9880002832732533, 0.9878324488575824, 0.9876649260340926, 0.9874976982911
152, 0.9873307500614499, 0.9871640666613011, 0.9869976342338704, 0.9868314396971792, 0.9866654706957432, 0.9864997155557601, 0.986334163243
507, 0.9861688033266723, 0.9860036259383794, 0.9858386217436783, 0.9856737819083067, 0.9855090980695381, 0.9853445623089551, 0.985180167126
9962, 0.9850159054191441, 0.9848517704536291, 0.9846877558505386, 0.9845238555622267, 0.9843600638549337, 0.984196375291527, 0.984032784715
2862, 0.983869287234659, 0.9837058782089235, 0.9835425532346933, 0.9833793081332108, 0.983216138938377, 0.9830530418854679, 0.9828900134004
96, 0.9827270500901729, 0.9825641487324376, 0.9824013062675131, 0.9822385197894608, 0.9820757865381995, 0.9819131038919638, 0.9817504693601
732, 0.9815878805766881, 0.9814253352934307, 0.9812628313743479, 0.9811003667896978, 0.9809379396106398, 0.9807755480041119, 0.980613190227
9784, 0.9804508646264328, 0.9802885696256415]
</code></pre>

<p>Here, <code>Eg</code>, <code>Ex</code>, <code>e</code>, <code>p</code>, <code>g</code> and <code>Dx</code> are <span class=""math-container"">$E[g^2]$</span>, <span class=""math-container"">$E[\Delta x^2]$</span>, <span class=""math-container"">$\rho$</span>, <span class=""math-container"">$\epsilon$</span>, <span class=""math-container"">$g$</span> (or <span class=""math-container"">$\nabla L(x)$</span>) and <span class=""math-container"">$\Delta x$</span>, respectivelly, and <code>history</code> is the record of all values that <code>x</code> has obtained.</p>

<p>For the hyperparameters <span class=""math-container"">$\rho$</span> and <span class=""math-container"">$\epsilon$</span>, I use the same values that they use in the paper, and I initialize <span class=""math-container"">$x$</span> to 1.</p>

<p>As can be seen when printing <code>history</code>, the convergence is extremely slow for such a simple optimization problem, and after 100 iterations the method has barely got 2&nbsp;% closer to the optimum (x&nbsp;=&nbsp;0). It feels like I must have misunderstood some crucial part of the paper.</p>

<p>For example, the paper claims that the update step &Delta;x will have the same unit as x, if x has some hypothetical unit. While this is probably a desireable property, it is as far as I'm concerned not true, since the premise that <span class=""math-container"">$RMS[\Delta x]$</span> has the same unit as <span class=""math-container"">$x$</span> is incorrect to begin with, since <span class=""math-container"">$RMS[\Delta x]_0 = \sqrt{E[\Delta x]_0 + \epsilon} = \sqrt{0 + \epsilon}$</span> which is a unitless constant, so all <span class=""math-container"">$\Delta x$</span> become unitless rather than having the same unit as <span class=""math-container"">$x$</span>. (Correct me if I'm wrong.)</p>

<p>Have I made some error when implementing the algorithm, or why is the convergence so slow? Is it supposed to be this slow?</p>

<h1>Edit</h1>

<p>After changing <code>Eg = p*Eg + (1+p)*g*g</code> to <code>Eg = p*Eg + (1-p)*g*g</code> (i.e. correcting the error spotted by Dennis), convergence is now significantly better. It still doesn't get very close to 0 after the first 100 iterations; x only goes down to 0.597. However, after 400 iterations or so, the convergence really starts to kick in and in the following 100 iterations, x goes down from its current value at 0.0156 to 2.07e-8, and then to 3.24e-57 after yet another 100 iterations!</p>

<p>I plotted <span class=""math-container"">$x$</span>, <span class=""math-container"">$E[\Delta x^2]$</span> and <span class=""math-container"">$E[g^2]$</span> in logarithmic scale against the number of iterations, and this is what I found:</p>

<p><a href=""https://i.stack.imgur.com/E1FAE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1FAE.png"" alt=""enter image description here""></a></p>

<p>I'm not really sure what this means in terms of performance ""in the wild,"" since this only is a toy problem, free from stochasticity in the optimization and with only one parameter.</p>

<p>About the unit analysis, I guess <span class=""math-container"">$\epsilon$</span> shouldn't be unitless as I wrote that it was, as otherwise, it cannot be added to <span class=""math-container"">$E[g^2]$</span> (which has the same unit as <span class=""math-container"">$g^2$</span>). Also, I guess that in practice, <span class=""math-container"">$\epsilon$</span> must be treated as two different constants that may have different units when added to <span class=""math-container"">$E[\Delta x^2]$</span> and when added to <span class=""math-container"">$E[g^2]$</span>; otherwise, <span class=""math-container"">$x$</span> will end up having the same unit as <span class=""math-container"">$g$</span>.</p>

<h2>Verdict</h2>

<p>A funny thing is that even though <span class=""math-container"">$\epsilon$</span> should only be a tiny number that shouldn't affect the process significantly unless you divide by something really small, the process turns out to be highly sensitive to its value. This is because <span class=""math-container"">$\epsilon$</span> essentially is the only thing that really makes <span class=""math-container"">$E\left[ \Delta x^2 \right]$</span> grow. Higher <span class=""math-container"">$\epsilon$</span>-values will make <span class=""math-container"">$E\left[ \Delta x^2 \right]$</span> grow faster and will make the optimization process go significantly faster. In my opinion, this seems to indicate a major design flaw in the algorithm.</p>
"
840,"<p>According to Russell and Norvig, a knowledge-based agent will only add a sentence to its knowledge base if it <em>follows logically</em> from what it previously knows, or directly observes. To <em>follow logically</em> essentially means that if the premises are true, then the conclusions are guaranteed to be true. So the agent will only add the sentence if it 100% sure the sentence is true.</p>

<p>Is this hyperskepticism in logic justified? Couldn't the agent be more efficient if it added a sentence if it were 99% sure it were true? It could potentially add a lot more true sentences, and only occasionally add false ones. There would need to be a mechanism for unlearning sentences, but as long as the vast majority of sentences added are true, why couldn't that be done?</p>

<p>I essentially asked this question <a href=""https://en.wikipedia.org/wiki/Wikipedia:Reference_desk/Miscellaneous#.22Proof.22_of_the_principle_of_skepticism.3F"" rel=""nofollow noreferrer"">here</a>, and someone suggested that I post it here.</p>
"
841,"<p>As I know, the current state of the art methods for training deep learning networks are variants of gradient descent / stochastic gradient descent.<br>
What are the best known gradient-free training methods (mostly in visual tasks context)?</p>
"
842,"<p>I'd like a general explanation of that in AIs that were to mimic judges, prosecutors or lawyers, on very general terms they would act on this way for each case:</p>

<p>A judge AI would give a verdict, having the following input:</p>

<p>All sources of law that have some relationship with the act being judged, knowing the relative importance of each of that sources.</p>

<p>All the facts presented to it, being theorically able to distinguish if some fact were false if there are enough evidence for that.</p>

<p>Prosecutors AI would act into searching the facts that would most help for accusing, matching them with known sources of law so they can make the best accusation.</p>

<p>Lawyers AI would act exactly as prosecutors but for defense of the one being accused.</p>

<p>Obviously AIs are way too far nowadays from something like this, but I find strange that it doesn't even look that this has been done as a proof of concept, the most near thing I find by searching on the Internet is <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence_and_law"" rel=""nofollow noreferrer"">this</a>. I'm not sure of the reach of everything that it's explained there but it looks more of assisting judges, prosecutors and lawyers rather than making what they do. I find this a bit strange, although I'm very far from being an expert on AIs, I think that for very simple cases an AI could even do those things.</p>

<p>In the answer about possible development, I'd like to know about which would be the facts that would make developing an AI like the one I'm mentioning to be possible or not.</p>

<p>PD: I don't find any suitable tag, I ask an admin to put a better one if needed.</p>

<p>Requested edit: My question is about the general state of the art of this field (of AIs being able to somehow mimic a judge, a prosecutor or a lawyer), and how pausible is a development in this way and how it would develop (if this is with current knowledge impossible to even try until a far future, or if we are at least near for stablishing AIs understanding law, which would be the logical way for a type of AIs like them evolving (understing laws, judging, being able to see videos..). I know it's broad so I'm happy with a general explanation.</p>
"
843,"<p>I read some light material earlier about the possibility of building AI agent trees, which leaf agents optimizing for primitive tasks, while higher level agents optimizing for orchestrating direct descendant agents. </p>

<p>According to my understanding, each agent would have different objective functions, and possible moves. </p>

<p>I wonder if someone could give some insight on how to implement such complex agent, hopefully with an example. </p>
"
844,"<p>What we are doing in the image processing training.  We are storing some form of data which is going to act as the knowledge or experience of the system. </p>

<ul>
<li>In which form can the system store it's training data?</li>
</ul>

<p>For example, with the hand written recognition, we can represent the digits as combinations of curves and straight lines. For every round of training the recognition system stores data.  Is the data typically stored in a flat file (such as txt) or a database?</p>

<p>I have seen in <a href=""https://en.wikipedia.org/wiki/Tesseract_(software)"" rel=""nofollow noreferrer"">Tesseract OCR</a> that there is a text file that stores the x0,y0,x1,y1. They are the pixel points that represents the square on the training image that has the picture.</p>

<p>I need a efficient form of knowledge for Machine Learning, and would appreciate advice, context, or an explanation of the merits or downsides of different approaches. </p>

<blockquote>
  <p>I need a form of knowledge that stored in a system. human brain evaluate '7' as 'horizontal line and vertical left crossed line start from right of the horizontal line'. like that machine must have some conceptual data to represent their knowledge.</p>
</blockquote>
"
845,"<p>I'm quite new to image processing and AI. But I have the expertise to create a network that can be used in object detection and recognition. Most of the time I've used ANN or Naive Bayes.</p>

<p>Now, I want to develop a method of action recognition, something like identifying whether one is jogging, running or walking by applying ANN. However, I really don't have idea how the sequence of frames can be classified. </p>

<p>In static image, segmentation and feature extraction is easy. But in regard to a moving image, I'm unsure of the approach.  </p>

<p>Thanks in advance!</p>
"
846,"<p>I was trying to code a single layer perceptron to understand binary AND:</p>

<p>1 1 1<br>
0 1 0<br>
1 0 0<br>
0 0 0  </p>

<p>I made up this code </p>

<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;math.h&gt;
int main()
{
int input1, input2;
float weight1 = 0.3, weight2 = 0.4;
int output;
int training1, training2, expectedoutput;
int i;
int j=1;

//TRAINING
for(i=0; i&lt;10000;i++)
{   


    if(j=1)
    {
        training1 = 0;
        training2 = 1;
        expectedoutput = 0;
    }
    if(j=2)
    {
        training1 = 1;
        training2 = 0;
        expectedoutput = 0;
    }
    if(j=3)
    {
        training1 = 0;
        training2 = 0;
        expectedoutput = 0;
    }
    if(j=4)
    {
        training1 = 1;
        training2 = 1;
        expectedoutput = 1;
        j=1;
    }
    output = weight1*training1 + weight2*training2 + 2;

    if(output != expectedoutput )
    {
        weight1 = weight1 + 0.156 * training1 * (expectedoutput - output);
        weight2 = weight2 + 0.156 * training2 * (expectedoutput - output);
    }
    j++;
}

printf(""training done\n"");
printf(""weight1 = %f"" ""weight2 = %f\n"",weight1,weight2);

//TESTING THE PERCEPTRON
for(i=0; i&lt;5 ; i++)
{
scanf (""%d%d"", &amp;input1, &amp;input2 );
output = weight1*input1 + weight2*input2;
printf(""\n%d\n"", output);
}
return 1;    
}
</code></pre>

<p>its supposed to input the 4 cases repeatedly and with a learning rate of 0.156 (which i set randomly) and i used the threshold as a weight of 2. </p>

<p>However after the training the perceptron still doesnt give the expected output. Is my understanding of perceptron rule wrong? Please help thank you!</p>
"
847,"<p>I want to write an algorithm which indicates to a robot the first point in time when it is reasonably safe to cross a road.  Assume that the robot's goal is to travel to a location that requires a road crossing and that the robot is ready to cross.</p>

<p>Simple algorithms and decision making will probably not suffice.  What features and capabilities must the algorithm have to provide crossing safety?  What existing AI methods might be useful to consider for this endeavor?</p>

<p>For the first iteration, we can assume the traffic pattern is normal in that no vehicles are driving over the curb and there are no high speed chases or other safety related abnormalities.</p>
"
848,"<p>Within the AI development field, the main focus seems to be on the pattern recognition and ""learning"" aspect of the thing... but I think the bigger questions is motivation.</p>

<p>Learning seems to be very easy to explain, as when the sensory input is being interpreted there is a feedback loop that adjust the environmental variables accordingly.</p>

<p>A human child has this inborn curiosity... (and hunger/thirst)... its often referred to as Maslow's pyramid, but what could possibly motivate a machine to form some sort of actions?</p>

<p>Should a machine have some sort of DNA-like structure that would describe it's hierarchy of needs, what should/could a machine NEED?</p>

<p>EDIT: Yes, its obvious that a statistical algorithms can't have motivation, the question is more of... ""how to fake the motivation and what it should be?""</p>

<p>EDIT2: Having trouble to phrase the question correctly, but.. I am not talking about motivation to learn, but about some sort of inner ""drive/push"" to do something.</p>

<p>Example: Child is curious about an item, and explores it, but where does the interest towards the item originate from? - It detected a new item in the environment it hasn't seen? I guess the algorithms drive should originate from encountering new phenomena... but in that case, what would force it to synthesise new ideas by combining the old ones?</p>
"
849,"<p>Hypothetically, the <strong>symbol</strong> (Triangle) is sticked to an item and i need to <strong>find and recognize that symbol and try to calculate the orientation of the item</strong> it is sticked into. In degrees. How would you guys suggest i approach this problem? Do i still need NN for this? Thank you :) I just need to hear other people's thoughts.</p>

<p><a href=""https://i.stack.imgur.com/vDcze.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vDcze.png"" alt=""sample""></a></p>
"
850,"<p>Nick Bostrom talks in his book <a href=""https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies"" rel=""nofollow noreferrer"">Superintelligence</a> about the many dangers of AI. He considers it necessary that strong security mechanisms are put in place to ensure that a machine, once it gains general intelligence far beyond human capabilities, does not destroy humanity (most likely by accident). He describes this as a very delicate process that most likely will go wrong.</p>

<p>Considering that new technologies often neglect the necessary precautions and  that this is highly relevant to national security, I wonder if there are already government agencies overseeing big technology companies like Deepmind. We are currently far away from an intelligence explosion or a technological singularity, but I would assume that governments want to have a foot in the door as soon as they realize and understand the dangers.</p>

<p>So my question is, what government agencies currently investigate and maybe even control AI development? The answer can be general or for a specific country if there is a big difference between countries.</p>
"
851,"<p>In this <a href=""https://people.cs.umass.edu/~domke/courses/sml2011/08autodiff_nnets.pdf"" rel=""nofollow noreferrer"">note</a> Justin Domke says that</p>

<blockquote>
  <p>In practice, neural networks seem to usually find a reasonable solution when the number of layers is not too large, but find poor solutions when using more than, say, 2 hidden layers.</p>
</blockquote>

<p>But in <a href=""https://www.quora.com/Artificial-Neural-Networks/Artificial-Neural-Networks-How-can-I-estimate-the-number-of-neurons-and-layers/answer/Yoshua-Bengio?share=7b58dc3b&amp;srid=hqJd"" rel=""nofollow noreferrer"">Bengio's remark</a>, he says </p>

<blockquote>
  <p>Very simple. Just keep adding layers until the test error does not improve anymore.</p>
</blockquote>

<p>There seems to be a conflict. Can anyone explain why they suggest differently? Or am I missing something?</p>
"
852,"<p>As I understand it from this <a href=""http://videolectures.net/deeplearning2015_vincent_machine_learning/"" rel=""noreferrer"">video lecture</a>, there are three types of deep learning:</p>

<ul>
<li>Supervised</li>
<li>Unsupervised</li>
<li>Reinforcement</li>
</ul>

<p>All these can serve to train a NN either <strong>only prior</strong> to its deployment or <strong>during</strong> its operating as well. </p>

<p>For the latter, I read ""continuous"" learning <a href=""http://deepmind.com/blog/enabling-continual-learning-in-neural-networks/"" rel=""noreferrer"">here</a>  and <a href=""http://medium.com/@Synced/deep-learning-in-real-time-inference-acceleration-and-continuous-training-17dac9438b0b"" rel=""noreferrer"">here</a>. And ""dynamic"" learning <a href=""http://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59511"" rel=""noreferrer"">here</a> and <a href=""http://www.datasciencecentral.com/profiles/blogs/static-dynamical-machine-learning-what-is-the-difference"" rel=""noreferrer"">here</a>.</p>

<p>My questions are these:
Which term is to be used for a learning system that keeps on learning? And if it's ""continuous"", is there an opposing term (such as ""static"" for ""dynamic"") for those systems that stop learning before being deployed? </p>
"
853,"<p>Intelligence ... changes based on the environment and situation</p>

<p>Human are now inventing machines exhibiting some features of their own Intelligence.  There appears to be a possibility that, in the future, machines will be able to invent machines.</p>

<p><strong>Question 1:</strong> Is there any possibility that, at that time, such machines may become beyond the control of humans?.</p>

<p>Species having superior intelligence may lead to control of the world, whereas human beings are now on top.  Humans are currently the most intelligent species, but computers demonstrate more accuracy than humans.</p>

<p><strong>Question 2:</strong> Can anyone specify an existing tool that can do this?</p>
"
854,"<p>Currently big tech companies like Microsoft, Google, and Amazon (to name a few) offer cognitive services on their cloud platforms.</p>

<p>With these services it is possible to identify faces, objects, texts, sounds, etc.
Do you know how these services work internally? </p>

<p>The only info I could find was based on the API level. I assume the services use some neural network, which is trained by amounts of data. </p>

<p>In my experience the Google services are more accurate then the Azure services. Perhaps the Google services are better and longer trained?</p>
"
855,"<p>In working with a social services agency that provides a continuum of programs across the behavioral health and child welfare spectrum the need to adequately manage individual worker and total program caseloads has become increasingly difficult due to burgeoning demand.</p>

<p>Each program has already developed a profile for ranking the complexity of a client based a select number of factors (i.e. diagnosis, previous encounters, results of assessment instruments, etc.). </p>

<p>What we are trying to develop now is an approach to act as a load balancer for new cases to ensure that individual workers' caseloads are balanced with the complexity of cases. We can do this manually to some degree but if there was a way to automate and take the human bias out of it, that is the goal. </p>

<p>Are there are any algorithms or adaptable approaches that have been used to do such a task (even in other sectors)? </p>
"
856,"<p>I know the basics of Artificial Neural Networks. For instance; make dot product with the weights and every neuron from previous layer. Adjust the weight by error. And done, That is how I see neural networks, but I saw in lot of videos in the graphical representation that every neuron has it's synapse with certain neurons. And when it change the input for example they create new synapses with other certain neurons. So whats what I'm asking for is:</p>

<p>How I can make my neurons assign the synapses with the neuron that fits it?</p>
"
857,"<p>I am considering some possibilities to improve a Variable Gain nonlinear control system.</p>

<p>One of the drawbacks of the current technique is that the change of the gains is discrete and the switching effect is highly undesirable...</p>

<p>The gain matrices are 4x16 and are calculated for some values of a bounded variable and then selected trough a lookup table.</p>

<p>So, what my idea was to try and do this with an Neural Network, that would receive as input the control variable and give the gain matrix as output, allowing my control to be continuous or almost continuous.</p>

<p>Is this idea possible/feasible?</p>

<p>My knowledge on neural nets is very poor, so I hope you can enlighten me somehow.</p>
"
858,"<p>I wrote a solution to the one shot Prisoner's dilemma:<br>
&nbsp;<br>
&nbsp;</p>

<h1>Introduction</h1>

<p>My solution applies to a prisoner dilemma involving two people (I have neither sufficient knowledge of the prisoner's dilemma itself, nor sufficient mathematical aptitude/competence to generalise my solution to prisoner's dilemma where number of agents (n) > 2).</p>

<p>Let the two agents involved be A and B, assume A and B are maximally selfish (they care solely about maximising their payoff). If A and B satisfy the following 3 requirements, then whenever A and B are in a prisoner's dilemma together, they would choose to cooperate.
1. A and B are perfectly rational.
2. A and B are sufficiently intelligent, such that they can both simulate each other (the simulations doesn't have to be perfect; it only needs to sufficiently resemble the real agent that it can be used to predict the choice of the real agent).
3. A and B are aware of the above 2 points.</p>

<p>Solution</p>

<p><a href=""https://i.stack.imgur.com/DTFJc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DTFJc.png"" alt=""enter image description here""></a></p>

<p>A and B have the same preference.
(A,B) = (D,C) > (C,C) > (D,D) > (C,D).<br>
(B,A) = (D,C) > (C,C) > (D,D) > (C,D).    </p>

<p>My solution relies on A and B predicting each other's behaviour. They use a simulation of the other which guarantees high fidelity predictions.</p>

<p>If A adopts A defect invariant strategy (always defect) i.e committing to defection, then B will simulate this, and being rational B will defect.
Vice versa.  </p>

<p>If A adopts a cooperate invariant strategy, then B will simulate this and being rational, B will defect.<br>
Vice versa.</p>

<p>To commit to a choice means to decide to adopt that choice irrespective of all other information. Committing means not taking into account any other information in deciding your choice.   </p>

<p>Assuming A commits to a choice. Let the choice A commits to be k.        </p>

<p>Then B's strategy would be:</p>

<p><a href=""https://i.stack.imgur.com/nZyc2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nZyc2.png"" alt=""Diagram to Illustrate""></a></p>

<h2>Diagram to Illustrate</h2>

<p>h(x) is a function that takes in B's prediction of A's choice as input and outputs the rational response to that choice. As B prefers (D,C) over (C,C) and (D,D) over (C,D), then B would choose to defect whatever choice it predicts for A. Thus, adopting an invariant strategy will cause B to adopt the defect invariant strategy (insofar as B accurately predicts A). Being rational, and preferring (D,C) and (C,C) over (D,D) A would not adopt an invariant strategy.<br>
Vice Versa.</p>

<p>If both of them were to decide to simultaneously adopt invariant strategies, they would adopt the defect invariant strategy. However, as they both prefer (D,C) and (C,C) (in that order) over (D,D) they would both strive to get a better outcome than (D,D) unless no better outcome is possible.</p>

<p>This means that A and B's choices depends on what they predict the other would do.</p>

<p>If A predicts B will defect, A can cooperate or defect.<br>
If A predicts B will cooperate, A can cooperate or defect.<br>
Vice versa.         </p>

<p>As A is not committing, A's strategy is either predict(B) (picks what A predicts B will pick) or !predict(B) (picks the opposite of what A predicts B will pick. I.e cooperate if A predicts B will defect, and defect if A predicts B will cooperate).</p>

<p>Vice versa.</p>

<p>To not commit is to decide to base your strategy on your prediction of the choice the opponent adopts. You can either choose the same choice as what you predicted, or choose the opposite of what you predicted (any other choice is adopting an invariant strategy).</p>

<p>If A adopts !predict(B).  A gains an outcome ranked 1 or ranked 4 in its preferences.</p>

<p>If A adopts predict(B)  A gains an outcome ranked 2 or 3 in its preferences.
Vice versa.</p>

<p>We can have:</p>

<ol>
<li>predict(B) and predict(A)    </li>
<li>predict(B) and !predict(A)    </li>
<li>!predict(B) and predict(A)   </li>
<li>!predict(B) and !predict(A).   </li>
</ol>

<p>Now A's decision is dependent on predict(B).<br>
But B's decision (and thus predict(B)) is dependent on predict(A) (and thus A's decision).
A = f(predict(B) = g(B = f(predict(A) = g(A)) ) ).</p>

<p>f(x) is a function that deterministically returns either x or !x (according to the strategy adopted by the agent implementing it).  </p>

<p>g(x) is a function the stochastically return x or !x it returns x with a probability of p. Given that the agents are simulating each other, we can safely assume that p is high (sufficiently close to 1).</p>

<p>Vice versa.</p>

<p>The above assignment is circular and self-referential. If A and/or B tried to simulate it, it leads to a non terminating recursion of the simulations.</p>

<p>A's strategy would be:
<a href=""https://i.stack.imgur.com/kIhJ5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kIhJ5.png"" alt=""enter image description here""></a></p>

<h2>Diagram to Illustrate</h2>

<p>As such, A and B cannot both decide to base their decision on the decision of the other.</p>

<p>Yet, neither A nor B can decide to commit to an option.</p>

<p>What they can do, is to predispose themselves to an option. To predispose themselves is to decide on their choice independent of the other agent. Assuming I received no further information, what would I do? A predisposition is not a final choice, and should not be mistaking for committing to a choice of action. An agent who predisposes themselves can change their choice based on how they predict the other agent would react to that predisposition. </p>

<p>Assuming A predisposes themselves. Let the predisposition made be q. The assignment becomes:    </p>

<p>A = f(predict(B) = g(B = f(predict(A) = g(q)) ) ).</p>

<p><a href=""https://i.stack.imgur.com/lPi6W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lPi6W.png"" alt=""enter image description here""></a></p>

<h2>Diagram to Illustrate</h2>

<p>Only one of them needs to predispose themselves.</p>

<p>Assuming A predisposes themselves.</p>

<p>If A predisposes themselves to defection, then the only two outcomes are ranked 1 and 3 in their preferences (for A) and 3 and 4 in their preferences (for B).</p>

<p>Upon simulating this, B being rational would choose to defect (resulting in outcome 3). (D,D) is a <em>Nash equilibrium</em>, and as such once they reach there, being rational neither A nor B would change their strategy.
(Note that this outlaws the !predict(A) strategy.</p>

<p>If A predisposes themselves to cooperation, then the two possible outcomes are ranked 2 and 4 in their preferences (for A) and 1 and 2 in their preferences (for B).</p>

<p>Upon simulating this, if B chooses to defect, then B is adopting a defect invariant strategy (which has been outlawed), and A will update and choose to defect, resulting in outcome 3. As B is rational, B will choose the outcome that leads to outcome 2, and B will decide to cooperate.</p>

<p>If B chooses to defect, and A simulates B choosing to defect if A predisposes themselves to cooperation, then A will update and defect, resulting in (D,D). If B chooses to cooperate if A predisposes themselves to cooperation, if A updates and chooses to defect, then B would update and choose to defect resulting in D,D. Thus, once they reach (C,C) they are at a reflective equilibrium (in the sense that if one defects, then the other would also defect, thus no one of them can increase their payoff by changing strategy).</p>

<p>(Thus, B will adopt a predict(A) strategy).</p>

<p>Vice Versa.</p>

<p>Because A is rational, and predisposing to cooperation dominates predisposing to defection (the outcomes outlawed are assumed not to manifest), if A predisposes themself, then A will predispose themself to cooperation.
Vice Versa.</p>

<p>Thus if one agent predisposes theirself, it will be to cooperation, and the resulting outcome would be (C, C) which is ranked second in their preferences.<br>
What if A and B both predispose themselves?
We can have:<br>
1. C &amp; C<br>
2. C &amp; D<br>
3. D &amp; C<br>
4. D &amp; D.   </p>

<p>If C &amp; C occurs, the duo will naturally cooperate resulting in (C, C). Remember that we showed above that the strategy adopted is predict(B) (defecting from (C, C) results in D, D).</p>

<p>If C &amp; D occurs, then A being rational will update on B's predisposition to defection and choose defect resulting in (D, D).</p>

<p>If D &amp; C occurs, then B being rational will update on A's predisposition to defection and choose defect resulting in (D, D).</p>

<p>If D &amp; D occurs, the duo will naturally defect resulting in (D, D).</p>

<p>Thus seeing as only predisposition to cooperation yields the best result, at least one of the duo will predispose to cooperation (and the other will either predispose themselves to cooperation or not predispose at all) the resulting outcome is (C, C).</p>

<p>If the two agents can predict each other with sufficient fidelity (explicit simulation is not necessary, only high fidelity predictions are) and are rational, and know of those two facts, then when they engage in the prisoner's dilemma, the outcome is (C, C).</p>

<p>Therefore, a cooperate-cooperate equilibrum can be achieved in a single instance prisoner's dilemma involving two rational agents given that they can predict each other with sufficient fidelity, and know of their rationality and intelligence.</p>

<p><strong><em>Q.E.D</em></strong><br>
Thus if two super intelligences faced off against each other in the prisoner's dilemma, they would reach a cooperate-cooperate equilibrium. This solution also applies to two rational robots who have mutual access to the other's source code.<br>
&nbsp;<br>
&nbsp;         </p>

<h1>Prisoner's Dilemma With Human Players</h1>

<p>In the above section I outlined a strategy to resolve the prisoner's dilemma for two superintelligent AIs or rational bots with mutual access to the other's source code. The strategy is also applicable to humans who know each other well enough to simulate how the other would act in a given scenario. In this section I try to devise a strategy applicable to human players.</p>

<p>Consider two perfectly rational human agents A and B. A and B are maximally selfish and care only about maximising their payoff.</p>

<p>Let:<br>
(D,C) = W<br>
(C,C) = X<br>
(D,D) = Y<br>
(C,D) = Z<br>
The preference is W > X > Y > Z.   </p>

<p><a href=""https://i.stack.imgur.com/MaEfs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MaEfs.png"" alt=""enter image description here""></a></p>

<p>A and B have the same preference.</p>

<p>The 3 conditions necessary for the resolution of the prisoner's dilemma in the case of human players are:<br>
1. A and B are perfectly rational.<br>
2. They each know the other's preference.<br>
3. They are aware of the above two facts.         </p>

<p>The resolution of the problem in the case of superintelligent AIs relied on their ability to simulate (generate high fidelity predictions of) each other. If the above 3 conditions are met, then A and B can both predict the other with high fidelity.          </p>

<p>Consider the problem from A's point of view. B is as rational as A, and A knows B's preference. Thus, to simulate B, A merely needs to simulate themselves with B's preferences. Since A and B are perfectly rational, whatever conclusion A with B's preferences (A*) reaches is the same conclusion B reaches. Thus A* is a high fidelity prediction of B.<br>
Vice versa.</p>

<p>A engages in a prisoner's dilemma with A*. However, as A* as the same preferences as A, A is basically engaging in a prisoner's dilemma with A.<br>
Vice Versa.<br>
&nbsp;<br>
An invariant strategy is outlawed by the same logic as in the AI section.          &nbsp;<br>
A = f(predict(A*) = g(A* = f(predict(A) = g(A)))<br>
Vice Versa.          </p>

<p>The above assignment is self referential, and if it was run as a simulation, there would be an infinite recursion.     </p>

<p><a href=""https://i.stack.imgur.com/G3k2s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G3k2s.png"" alt=""enter image description here""></a></p>

<h2>Diagram to Illustrate</h2>

<p>Thus, either A or A* needs to predispose themselves.</p>

<p>If the predisposition A makes is q, then the assignment becomes:       </p>

<p>A = f(predict(A*) = g(A* = f(predict(A) = g(q)))    </p>

<p><a href=""https://i.stack.imgur.com/4Aa1A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Aa1A.png"" alt=""enter image description here""></a></p>

<h2>Diagram to Illustrate</h2>

<p>However, as A* is A, then whatever predisposition A makes is the same predisposition A* makes. Both A and A* would predispose themselves. It is necessary for at least one of them to predispose themselves, and the strategy that has the highest probability of ensuring that at least one of them predisposes themselves is each of them individually deciding to predispose themselves. Thus, we enter a situation in which both of them predispose themselves. As A* = A, A's predisposition would be the same as A*'s predisposition.
Vice Versa.</p>

<p>We have either:
1. (C,C)
2. (D,D)</p>

<p>If A predisposes themselves to defection, then we have (D,D). (D,D) is a Nash equilibrium as A and/or A* can only perform worse by unilaterally changing strategy at (D,D). As A and A* are rational, they would predispose themselves to cooperation.</p>

<p>If A and A* predispose themselves to cooperation, If A and/or A* tried to maximise their payoff by defecting, then the other (as they predict each other) would also defect to maximise their payoff. Defecting at (C,C) leads to (D,D). Thus, neither A nor A* would decide to defect at (C,C). (C,C) forms a reflective equilibrium.
Vice Versa.</p>

<p>As B's reasoning process closely reflects A*'s (both being perfect rationalists, and having the same preferences), the two agents would naturally converge at (C,C).<br>
&nbsp;<br>
<strong><em>Q.E.D</em></strong></p>

<p>I think I'll call this process of basing your decision in multi-agent decision problems (that involve at least two agents who are perfectly rational, know each other's preferences and are aware of those two facts from the perspective of one of those agents satisfying the above 3 criteria) by modelling the other agents (who satisfy those 3 criteria) as simulations of yourself with their preferences recursive decision theory (RDT). I think convergence on RDT is natural for any two sufficiently (they may not need to be perfect, if they are equally rational, and rational enough that they try to predict how the other agent(s) would act) rational agents who know each other's preferences and are aware of the above two facts.</p>

<p>If a single one of those criteria is missing, then RDT is not applicable.</p>

<p>If for example, the two agents are not equally rational, then the more rational agent would choose to defect as it strongly dominates cooperation.</p>

<p>Or they did not know the other's preferences, then they would be unable to predict the other's actions by inserting themselves in the place of the other.</p>

<p>Or if they were not aware of the two facts, then they'd both reach the choice to defect, and we would be once again stuck at a (D,D) equilibrium. I'll formalise RDT after learning more about decision theory and game theory (so probably sometime this year (p &lt; 0.2), or next year (p: 0.2 &lt;= p &lt;= 0.8) if my priorities don't change). I'm curious what RDT means for social choice theory though.      </p>
"
859,"<p>It is possible that the signal handling of a neuron is outside the engineering comprehension of the most astute of human brains, even after the relationships of inputs to outputs are statistically characterized and the mapping of genetic information to neuron structure and composition is complete.  Deep comprehension of the detailed function of neurons within their role in adaptive system behavior seems to be lacking, and no rational proof of the feasibility of comprehending neurons has yet been published.</p>

<p>A visual representation of neuron functionality might look like the visual representation of a genome, a blur of information, when viewed in its entirety.  The economy of the world is much more complex system, astronomically more daunting.  Even the map of the metabolic processes of earthen life (shown below) defies comprehensive display in a single view.</p>

<p>Comprehending these complex biological systems can only be done piecemeal.  Studying these systems in their entirety may take a single mind years (or even millennia), and the distortions of memory over time may prohibit placing the whole system within the finite capacity of the cerebral cortex. </p>

<p>Yet computer science has no doubt advanced.  Many features of the mammalian brain have already been simulated, and those simulations exceed the capacities and speeds of mammals.  Examples include mail sorting, flight planning, and game playing.  With such progress, the simulation of the less obviously mechanical functions of the human mind appeared to many to be within the grasp of scientific advancement.  Other AI theorists have expressed their doubts.</p>

<p>The question of whether a system of a few billion elements can be simulated without first simulating a generalization of its elements arose in the twentieth century.  The spiking neuron model is an example of research inspired by that question.</p>

<p><strong>Is the most elemental unit of biological computation, the neuron<sup>1</sup>, comprehensible by a brain of comprised of neurons?</strong></p>

<p>If the answer is no, which is quite possible, then new approaches to AI may be required to simulate the capabilities of the mind that are less obviously computational in nature &ndash; capabilities like intuition, inspiration, compassion, jazz composition, or revolutionary thought.</p>

<p><a href=""https://i.stack.imgur.com/sH4lI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sH4lI.png"" alt=""Known Metabolic Pathways of Earthen Life""></a></p>

<p>[1] <a href=""https://www.ncbi.nlm.nih.gov/pubmedhealth/PMHT0024269/"" rel=""nofollow noreferrer"">https://www.ncbi.nlm.nih.gov/pubmedhealth/PMHT0024269/</a></p>
"
860,"<p>Due to my ignorance in this space I am not sure if a similar approach has already been proposed for what I outline below (I am not sure what terminology to even search for):</p>

<p>I have been doing some brainstorming about architectures for AI systems, and was considering the feasibility of an approach that borrows heavily from real-time game loop simulations. </p>

<p>That is to say, an approach that runs the AI simulation itself in real-time at a fixed rate regardless of external events, as opposed to feeding inputs in (near) real time into an existing network (i.e. clocked by events) and reading the outputs. This seems to me to be one way to allow for a temporal awareness and even some form of primitive ""consciousness"" within the AI system. </p>

<p>In pseudocode, here is what I've been thinking for a real-time AI loop:</p>

<pre><code>network = LoadNetwork();
registeredInputs = network.GetAllRegisteredInputs();
registeredOutputs = network.GetAllRegisteredOutputs();
timeStepTick = 0;
while(alive)
{
    //Iterate over inputs and process - Could use event-driven approach here, but would potentially require locking on the network if concurrency is applied
    foreach(registeredInput in registeredInputs)
    {
        //If the input has new samples since last iteration, apply it to mapped neurons
        if(registeredInput.HasNewSamples())
        {
            //Get new samples of input (these samples live in time domain)
            samples = registeredInput.GetNewSamples();
            foreach(sample in samples)
            {
                network.ProcessInputSample(timeStepTick, registeredInput, sample);
            }
        }
    }

    //Iterate over outputs and process 
    foreach(registeredOutput in registeredOutputs)
    {
        //Map relevant neuron outputs to the registered output ""frame"" buffer
        registeredOutput.Update();

        //Whatever registered this output is responsible for reading the buffer at whatever rate is required 
    }

    //Whichever simulation time step algorithm - arbitrarily clock this network at 1khz
    Step(timeStepTick, 1);
}
</code></pre>

<p>Using this sort of approach, I speculated about the possibility of registering certain internal inputs in addition to external inputs:</p>

<ul>
<li>""Brain wave"" signals: Could feed a series of generated sinusoidal waveforms into the network as a kind of baseline stimulation.</li>
<li>Time of day signal: Simulation of things like circadian rythym. Could simply be a waveform with frequency of 1 day.</li>
<li>Time Step signal: Feed current time step value (monotonic, starts at 0) as a sense of age. </li>
<li>Feedback signals: Certain output neurons selected to be mapped back in as inputs. These would create a continuous loop of signaling between neurons which would continue even in the absence of any external input signals.</li>
</ul>

<p>The inputs and outputs would obviously need to be sampled at a rate that falls within the Nyquist rate of the network (would be 512hz for the above psuedocode). For video data, this would be pretty easy as most video content is clocked within &lt;100hz (tradeoff being large frame data). Audio data might need to be resampled as it usually runs around 48khz, but that isnt to say the simulation couldnt be clocked at something like 100khz to address both needs. The audio samples would be ingested at their native sample rate, and video frames would only be handled if a new frame arrives.</p>

<p>The continuous load of such a system on the underlying hardware should be very easy to calculate based on number of mapped inputs, outputs, and sample rate. It might even be feasible to let the network run as fast as hardware allows (above some minimum constraint for input/output sampling rate), which could make it ""better"" in some arbitrary sense.</p>

<p>I have no idea what sort of neuron algorithm would be most appropriate here (I suspect existing algorithms will not work well), but I do think that it would have to be very efficient from a computational perspective considering the potential tick rate of the simulation. All of the magic definitely lives in the <code>ProcessInputSample()</code> method in the code above.</p>

<p>Obviously, the current applications for such a network are very dubious due to the way it operates. This in my mind simply feels like a very complex DSP filter that just takes input waveforms (either external or internal) convolves them with various amounts of temporal phasing, and pipes to a series of output buffers that the real world interacts with. That being said, this also seems like the sort of ""flexibility"" that is required for an AI to learn and operate in a way that does not require some arbitrary number of iterations and a specific training model. It has always seemed to me that LTSM-style neurons were a bandaid for the temporal memory issue. Why push that computational concern to every single neuron when you can make it an inherent property of the AI by placing its entire simulation in the time domain?</p>
"
861,"<p><strong>Scenario:</strong> I am trying to create a dataset with images of choice for different animal classes. I am going to train those images for classification using CNN.</p>

<p><strong>Problem:</strong> Lets assume I somehow don't have the privilege to collect too many images and was only able to collect few of them for each class. Here's the list: -</p>

<p>Baboon : 800</p>

<p>Fox : 1000</p>

<p>Hyena : 5000</p>

<p>Giraffe : 43</p>

<p>Zebra : 88</p>

<p>6: Hippopotamus: 233</p>

<p>7: Yak: 578</p>

<p>8: Polar Bear : 456</p>

<p>9: Lion : 3442</p>

<p>10: Indian Tiger : 40,000</p>

<p>Questions are: -</p>

<p>Is this a good dataset to train the CNN model.? I am worried about the quantity each class have.</p>

<p>Will it be helpful if I augmenting the data ? I think am going to.</p>

<p>In future the above mentioned dataset is going to increase. So there is a chance that I will train the model again. Should I create a model that fits the data of the present size or should I create a bigger one inorder to adjust future data ?</p>

<p>Thank you for your time.</p>

<p>I can get data from Internet. But this question is about the approaches to take when we are bounded by less data like the one in National Data Science Bowl (classifying Planktons).</p>
"
862,"<p>Information security has become a thriving field during the last years. It is a broad domain ranging from <strong>planing</strong> and <strong>building</strong> over <strong>testing</strong> to <strong>operating</strong> different <strong>applications</strong>, <strong>systems</strong> and <strong>networks</strong> in a secure fashion. From small embedded systems to large scale enterprise applications, security is always an important challenge.</p>

<p>With some exceptions, I have hardly seen neural networks used in this domain to tackle those challenges. The most common exception is breaking CAPTCHAs with CNNs during penetration testing, analysis of network traffic and malware/spam detection.</p>

<p>What are other challenges in information security that could be solved better with neural networks than with classical algorithms? Please include an explanation why those challenges would benefit from neural networks.</p>
"
863,"<p>Suppose there are 10K images of sizes 2400 x 2400 are required to use in CNN.Acc to my view conventional computers the people use will be of use. Now the question is how to handle such large image sizes where there is no privileges of downsampling.</p>

<p>Here's the system requirements:- </p>

<p>Ubuntu 16.04 64-bit 
RAM 16 GB
GPU 8 GB
HDD 500 GB</p>

<p>1) Are there any techniques to handle such large images which are to be trained ?<br/>
2) What batch size is reasonable to use ? <br/>
3) Is there any precautions to take or any increase and decrease in hardware resources that I can do ?</p>
"
864,"<p>We are careening into the future which may hold unpredictable dangers in relation to AI. I've haven't yet heard of Chappie or Robocop style police robots, but militarized drone tech is replacing many conventional weapons platforms.  I love the idea that I may one day be able to transfer my consciousness to a computer, and improve my capabilities and potential.  But few humans are purely ""good"", what constitutes morality can differ greatly among individual humans. </p>

<ul>
<li>How do we move forward toward the <a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow noreferrer"">singularity</a> in a way that protects humans, as opposed to possibly lead to our extinction?</li>
</ul>
"
865,"<p>I'm am quite new to deep learning but I think I found just the right real-world situation to start using it. The problem is that I have only used such algorithms to predict outcomes.  For my new project, I need information to feed a machine with to <em>optimize</em> outcomes. Could someone explain briefly how I should proceed? I'm stuck.</p>

<p><strong>Here's the situation:</strong></p>

<p>I have a machine that takes planks of wood with different grades of wood available throughout its length and has to cut it into blocks provided in a cut list. This machine will always choose the highest <em>score</em> it can get from a given plank. The <em>score</em> is obtained by multiplying each block's <em>area</em> by its <em>multiplicator</em>. The algorithm I want to build has to give that machine a <em>multiplicator</em> for each block listed in a cut list. All of the physical output from this machine will be stocked on shelves by a robot until needed. The cutting machine is allowed to downgrade parts of a plank if it helps it reach a higher score.</p>

<p>The value has to act as an incentive for the machine to give me the block I need the most without downgrading too much wood.</p>

<p><strong><em>OPTIMIZATION GOALS</em></strong></p>

<ul>
<li>Make sure each block is in stock by the time it is needed, but not too early without reason</li>
<li>Downgrade as little area of wood as possible (some species are very expensive)</li>
</ul>

<p><strong><em>INPUT NODES</em></strong></p>

<ul>
<li>Amount of time before this block is needed</li>
<li>Grade of wood for this block</li>
<li>Amount of this block needed</li>
<li>Block's area (Maybe?)</li>
</ul>

<p><strong><em>FEEDBACK PROVIDED TO THE ALGORITHM</em></strong></p>

<ul>
<li>Amount of time in advance that the block was ready (must be as low as possible)</li>
<li>Area of wood downgraded * number of grades skipped</li>
</ul>

<p><strong><em>EXPECTED RETURN DATA</em></strong></p>

<ul>
<li>A <em>multiplicator</em> that will give that block an optimal its priority relative to others</li>
</ul>

<p><strong><em>INFORMATION I DON'T HAVE BUT COULD GATHER</em></strong></p>

<ul>
<li>Mean ratio of each grade for each species of wood</li>
</ul>

<p>What I've figured out so far is that I may need my feedback to be smashed in only one value in order to make it the output node. The problem is that I can't understand how to make this algorithm to determine a <em>multiplicator</em>. Am I wrong in trying to solve this through deep learning?</p>
"
866,"<p>I have an idea for a transformative decision rule (TDR), but I want to know if there's already a name for it.     </p>

<blockquote>
  <p>Any ordinal scale should be converted to a scale such that all  the outcomes are assigned consecutive values.          </p>
</blockquote>

<p>Let <em>v</em> be the function that assigns values to outcomes (defined as ordered pairs of acts and states).     </p>

<p>Let <em>x</em> and <em>y</em> denote any particular such ordered pairs.<br>
Let <em>P</em> be the preference.<br>
Let <em>X+</em> be the set of all outcomes that are more preferred than <em>x</em> (similarly also for <em>y</em>) <em>X+</em> in <em>P</em>.
&nbsp;     </p>

<blockquote>
  <p>If <em>x</em> in <em>Y+</em>, and <em>x</em> is the closest outcome to <em>y</em> in <em>Y+</em>, then <em>v(x) = v(y) + 1</em> for all <em>x,y</em> in P.       </p>
</blockquote>

<p>I could have explained it more formally mathematically, but Latex doesn't seem to render here. If there's no name for it, I'm thinking of ""The Law of Consecutive Ordinals (LCO)"". As for why I support the rule, it's because it's a rule that needs to be applied to an ordinal scale before I apply an effective decision rule (EDR) that I thought of for ordinal scales. I want to know if there's a name for that principle.</p>
"
867,"<p>I've been working on a project (Android Game), in which the player has to confront with some obstacles/enemies whom he has to destroy. So, is there a way in which we can monitor how the user of the game plays and accordingly to generate (and timely updating) that trained model, which can be used later in the game to make the user think of a different way to defeat an enemy unlike going in the same streamline flow which he has followed till now.</p>

<p>I've implemented Q-Learning and Genetic algorithms on the PC, to make the Game AI for some games like 'tetris' to make the computer play on it's own. But, haven't done it in Android till now.</p>

<p>Already searched: I've also referred to some websites in which they suggested using Neural Networks to encounter the same.</p>

<p>But, I'm unable to get an accurate procedure in which this can be done. Please suggest me a way in which I can monitor the user's input on how he plays.</p>

<p>Thank you.</p>
"
868,"<p>My question is more about ""is it possible?"" and ""is this the right approach?"" So let me explain what my idea is:</p>

<p>I think about a system which gets XML documents in various structures but with essentially the same data structure in it. For the example, lets assume each document contains data about one or more persons. So the AI would recognize a name. Somewhere else in the document there is the post address of our fictional person. The AI should now ""see"" the address and conclude, it belongs to our person. Anywhere else, there is a phone number in the document. Again, our AI should see the connection between our person and this phone number.</p>

<p>This wouldn't be a job for an AI if there wasn't a catch. If the task was merely to find and map strings like addresses and phone numbers, we could simply use a regex to match our ""target strings"". The catch in this scenario would be this: the XML document might contain other data, which does not belong to our person but is a valid phone number for example and thus will match an regex.</p>

<p>So the big question is: Would it be possible for an AI to learn this and if yes, with which framework would someone create such an AI?</p>

<hr>

<p>Sample XML document:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8"" ?&gt;
&lt;document&gt;
    &lt;data&gt;
        &lt;foo&gt;
            &lt;bar&gt;
                &lt;person&gt;
                    &lt;name&gt;John Doe&lt;/name&gt;
                &lt;/person&gt;
            &lt;/bar&gt;
            &lt;address&gt;
                &lt;street&gt;Main street 1&lt;/street&gt;
                &lt;city&gt;1111 Twilight town&lt;/city&gt;
                &lt;country&gt;sample country&lt;/country&gt;
            &lt;/address&gt;
            &lt;phone&gt;+123 123 123&lt;/phone&gt;
        &lt;/foo&gt;
        &lt;foo&gt;
            &lt;bar&gt;
                &lt;person&gt;
                    &lt;name&gt;Jane Doe&lt;/name&gt;
                &lt;/person&gt;
            &lt;/bar&gt;
            &lt;address&gt;
                &lt;street&gt;Broadway 42&lt;/street&gt;
                &lt;city&gt;4521 Traverse town&lt;/city&gt;
                &lt;country&gt;sample country&lt;/country&gt;
            &lt;/address&gt;
            &lt;phone&gt;+123 412123&lt;/phone&gt;
        &lt;/foo&gt;
    &lt;/data&gt;
    &lt;creator&gt;
        &lt;!-- Note: While this looks like a valid person, --&gt;
        &lt;!-- this data should not be matched by the AI --&gt;
        &lt;name&gt;Sam Smith&lt;/name&gt;
        &lt;office&gt;
            &lt;street&gt;Seaside road 5&lt;/street&gt;
            &lt;city&gt;4521 Traverse town&lt;/city&gt;
            &lt;country&gt;sample country&lt;/country&gt;
        &lt;/office&gt;
        &lt;phone&gt;+123 555 555&lt;/phone&gt;
    &lt;/creator&gt;
&lt;/document&gt;
</code></pre>
"
869,"<p>I would like to get into AI development in python but i don't know how. I really hope that i could code Artificial Intelligent by my own someday.
anybody got any suggestions?</p>
"
870,"<p>I'm currently in the process of learning about using CNNs in image recognition. Many of the different resources I read that were explaining the motivation referred to the fact that these networks are (to some degree) translationally invariant. My understanding is as follows:</p>

<ul>
<li>Fully connected networks are ill-suited to image recognition because of the high dimensionality of the data and especially because they do not preserve the spatial relationships between pixels. I care about the state of the pixels surrounding pixel X as well as X itself.</li>
<li>CNNs remedy this because they look at the image in 2 dimensions so that surrounding pixels are being considered as well (the kernel does this). However, while I care about what is around X, I don't care where X is. So, I apply the kernel the same everywhere and I make a bunch of layers so that I can get a bunch of kernels.</li>
<li>I also use pooling. This reduces the data dimensions but also adds some of that translation invariance.</li>
</ul>

<p>So, my question is: What feature of the CNN is causing the invariance? I saw some explanations saying it was a result of the maps basically activating when a certain feature shows up, regardless of where. Other said that the pooling meant that if a horizontal line, for instance, showed up in one place vs. a pixel over, the pooling would activate the same for both. It seems like the first reason would be totally sufficient and the second not really adequate but I could also see it being both.</p>

<p>I also read a paper (<a href=""https://arxiv.org/pdf/1606.09549.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1606.09549.pdf</a>) about fully-convolutional networks. In section 2.1, the authors explain that they intentionally chose to make it fully-convolutional because that allowed them to ""compute the similarity at all translated sub-windows on a dense grid in a single evaluation.""</p>

<p>That sentence makes me think that it is more the first explanation. Anyways, I hope to gain a better intuitive understanding of how the different parts of the CNN come together to work particularly well on images. Thanks!</p>
"
871,"<p>For example,
There is a list of function names</p>

<p>If we choose ""index"" and ""sort"" and ""probability""
Index(sort(probability(data)))
This form an simple algorithm</p>

<p>Is there any python library for knowledge representation and planning for algorithm creation?</p>
"
872,"<p>There are two potential approaches when performing cross-over operation in genetic algorithm: perform cross-over on</p>

<ol>
<li>Elites in the pool, probably the ones that are also going to be directly transfered to the next generation</li>
<li>all the population present in the pool.</li>
</ol>

<p>Is there any certain belief that cross-over on only elites of the population, converges the solutions faster? I guess in order to escape from local minima, cross-over on all the population is needed; on the other hand I also say why performing cross over on weak population?</p>

<p>Any idea?</p>
"
873,"<p>Is there a way to train AI to find aspecific line or symbol in a image and crop it?</p>

<p>OpenCV scripts finds a face and crops it: how can I add my annotations? </p>

<p>Lest say I have a image like this:</p>

<p><code>
 +------+
 |  *  *|
 |      |
 |  *  *|
 |      |
 +------+
</code></p>

<p>I want it to find the * and crop it.</p>
"
874,"<p>I'm working on implementation of the backpropagation algorithm for a simple neural network which predicts a probability of survival (1 or 0) and I can't get it above 80% no matter how much I try to set the right hyperparameters. I suspect that's because my backpropagation is implemented incorrectly since I tried 2 different types of code and both give me same results. Is my backpropagation implemented correctly? Also how can I improve my model to give a better prediction?</p>

<pre><code>class NeuralNetwork(object):

def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
    # Set number of nodes in input, hidden and output layers.
    self.input_nodes = input_nodes
    self.hidden_nodes = hidden_nodes
    self.output_nodes = output_nodes
    self.lr = learning_rate

    # Initialize weights
    self.input_hidden_weights = np.random.randn(hidden_nodes, input_nodes) # 10x7
    self.hidden_output_weights = np.random.randn(output_nodes, hidden_nodes) # 1x10


    # Sigmoid activation funciton
    self.sigmoid = lambda x: 1/(1+np.exp(-x))
    self.diff_sigm = lambda x: x*(1-x)

def train(self, input_list, label_list):

    # Create an array of inputs and labels
    inputs = np.array(input_list, ndmin=2).T # 7x1
    labels = np.array(label_list, ndmin=2) # 1x1

    # Forward propagation
    hidden_layer = self.sigmoid(np.dot(self.input_hidden_weights, inputs))        
    output_layer = self.sigmoid(np.dot(self.hidden_output_weights, hidden_layer))

    final_output = output_layer


    # Error function
    output_errors = labels-final_output

    # Backpropagation  
    output_delta = output_errors * self.diff_sigm(output_layer)
    hidden_delta = np.dot(self.hidden_output_weights.T, output_delta) * self.diff_sigm(hidden_layer)

    # Update the weights
    self.hidden_output_weights += np.dot(output_delta, hidden_layer.T) * self.lr
    self.input_hidden_weights += np.dot(hidden_delta, inputs.T) * self.lr


    """"""
    # Backpropagation
    hidden_errors = np.dot(self.hidden_output_weights.T, output_errors)        
    hidden_grad = hidden_layer * (1.0 - hidden_layer)

    # Update the weights
    self.hidden_output_weights += self.lr * np.dot(output_errors.T, output_layer.T) # update hidden-to-output weights with gradient descent step    
    self.input_hidden_weights += self.lr * np.dot(hidden_errors * hidden_grad, inputs.T)  # update input-to-hidden weights with gradient descent step
    """"""
</code></pre>
"
875,"<p>I understand that a neural network basically distorts(non-linear transformation) and changes the perspective(linear transformations) of input space to draw a plane to classify data. How does the network deduce if an input is one side of a plane and therefore output the decision? Thanks in advance.</p>
"
876,"<p>Recently I was working on a problem to do some cost analysis of my expenditure for some particular resource.
I usually make some manual decisions from the analysis and plan accordingly.</p>

<p>I have a big data set in excel format and with hundreds of columns, defining the use of the resource in various time frames and types(other various detailed use).
I also have information about my previous 4 years of data and actual resource usage and cost incurred accordingly.</p>

<p>I was hoping to train a NN to predict my cost beforehand and plan even before I can manually do the cost analysis.</p>

<p>But the biggest problem I'm facing is the need to identify the features for such analysis. I was hoping there is some way to identify the features from the data set.</p>

<p><em>PS - I have idea about PCA and some other feature set reduction techniques, what I'm looking at is the way to identify them in the first place.</em></p>
"
877,"<p>I'm relatively new to neural networks and was wondering what an implementation of <a href=""https://www.researchgate.net/profile/Bing_Zhao23/publication/271546169_Study_on_NNPID-based_adaptive_control_for_electro-optical_gyro_stabilized_platform/links/56e745f408ae438aab881bab.pdf"" rel=""noreferrer"">this paper</a> would look like. More specifically, how are the correct values of Kp, Ki, and Kd determined at run time so it can be back propagated?</p>
"
878,"<p>When designing a machine-learning system, there are various parameters that have to be determined. I am interested in the following general question: is it possible to construct a dataset on which the system will have good performance with some specific set of parameters, but not with other paramteres?</p>

<p>To be more concrete, let's focus on neural networks. Suppose we have a simple neural network: a multilayer perceptron with a single hidden layer. The size of the input is fixed, the activation function is fixed (e.g. tanh), and the output is binary. The only parameter that has to be determined is the size of the hidden layer. </p>

<p>My question is: given a number $n$, is it possible to construct a dataset $D_n$ such that:</p>

<ul>
<li>The MLP with $n$ hidden nodes has good performance on $D_n$ (e.g. in 10-fold cross validation);</li>
<li>The MLP with $n-1$  hidden nodes has bad performance on $D_n$</li>
</ul>

<p>?</p>

<p><sub>Note: I <a href=""https://cstheory.stackexchange.com/q/38955/9453"">asked in CS theory</a> but got no reply.</sub></p>
"
879,"<p>To understand the inner workings of neural networks, a fair amount of mathematical concepts is required. Backpropagation alone is a challenging technique if you are not fluent in calculating local gradients. And that's just the start of the journey.</p>

<p>But the more I study neural networks, the more I get the impression that all those difficult mathematical concepts are only required if you are doing actual research in neural networks or want to know what's happening under the hood. If you ""just"" want to implement an AI utilizing a neural networks, there are several <strong>high level programming frameworks and libraries</strong> readily available including <strong>model zoos for state of the art neural networks</strong> (e.g. VGG, GoogLeNet and ResNet), that can be used.</p>

<p>So my question is, <strong>does a developer require a deep understanding of all the details nowadays</strong>, or have we reached a level, where <strong>frameworks take care of those details</strong> for us?</p>
"
880,"<p><strong>CAPTCHAs</strong>, which are often seen in web applications, are working under the assumption, that they <strong>pose a challenge which a human can solve easily while a machine will most likely fail</strong>. Prominent examples are identifying distorted letters or categorizing certain objects in images.</p>

<p><strong>Neural networks are threatening this approach</strong>, as they are capable of solving problems that are easy for humans and difficult for classic algorithms. Especially with the incredible results modern CNN architectures have achieved in image recognition during the last years, the established forms of CAPTCHAs won't be able to distinguish a human and a machine using a neural network anymore.</p>

<p><strong>Is this the end of CAPTCHAs as we know them or are there evolved versions available</strong> or at least in the making <strong>that still pose a challenge to modern neural networks</strong>?</p>

<p>Clarification: I am talking about challenges that are feasible for use in web applications and do not have an unjustifiable impact on usability.</p>
"
881,"<p>I am fairly new to Neural Networks and have a requirement where the output classes are not defined from the start.</p>

<p>More and more classes will be introduced later on based on incoming data.</p>

<p>This means that every time I introduce a new class I would need to retrain...</p>

<p>I have just worked on ANN so don't know much depth of other techniques.</p>

<p>Is there a way where I can deferentially train the model without touching the old model much..??</p>
"
882,"<p>I am working on a project in which a drone is up to learn to fly. I am using <a href=""https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies"" rel=""nofollow noreferrer"">NEAT</a>. </p>

<p>For the first experiment I want it to learn how to hover inside a 3x3x3 meters box. My input is 6 sensors for each direction.  Output is same as in a drone so thrust (normalized to 0-1), aileron, rudder and elevator. </p>

<p>Initially just used a time as fitness, and after many generations it hovers inside the box.  It only really learns to use the thrust in function of up and down sensors, but it never learns to react to input from other sensors because they are not directly connected to fitness. </p>

<p>I would like to get some ideas about a good test for my problem. Should the drone be put to fly a track with obstacles? Should I have some more input data ? Should I define fitness to better reflect good reactions for input sensor data?</p>

<p>Thanks in advance</p>

<p>NOTE: the drone uses relatively accurate physics, I am able to do tasks using a controller. </p>
"
883,"<p>The sequence in which new technologies are developed can be crucial for it's success and it's safe use. For example, before developing a nuclear reactor you want to have technologies like reliable cooling systems and secure ultimate disposal zones available and tested. Otherwise we might have a bad time.</p>

<p>I see AI with general superhuman intelligence as important predecessor for dangerous technologies like nanobots or generators using nuclear fusion, because superhuman intelligence can be helpful in foreseeing dangers that might lead to catastrophe using these technologies in an insecure way.</p>

<p>But what about important technologies we want to have <strong>before</strong> inventing an AI with general superhuman intelligence? Solving the control-problem is obvious, but are there other technologies we should have in advance?</p>
"
884,"<p>How can i add local database for voice recognition for visual studio c#. I want to get the commands from the database and also the function for send-keys I want to get it from the database how will I do that?</p>
"
885,"<p>I was wondering if anyone can suggest a good framework for reasoning with <strong>incomplete information</strong>.</p>

<p>I have found <a href=""http://larkc.org/"" rel=""nofollow noreferrer"">Large Knowledge Collider</a> but it appears dead for some time. Do you possibly have any other suggestions for a maintained project worth checking?</p>

<p>Since many comments are gravitating towards a different direction let me add one approach that I found a potentially good answer to my question - <a href=""https://www.researchgate.net/publication/220802281_Rough_Set_Based_Decision_Tree_Model_for_Classification"" rel=""nofollow noreferrer"">Rough Set Based Decision Trees</a>.</p>

<p>I would hope there is more than only this approach... could you please help me identify them?</p>
"
886,"<p>I was just wondering if some one could provide a nice tutorial on how to use the Recent tensor-flow object detection API to train custom network say like VGG-16? (Just USE the VGG-16, VGG-19, Inception-v3 etc as a fixed feature extractor in the Faster RCNN implementation). </p>

<p>From their Documentation I know it can be done. As, am an amateur in AI, I am unsure how to move forward.</p>

<p>P.S: This could really help some keras fans (like me :P) who have A trained CNN in keras. They could just make a .pb file out of it and insert it as a fixed feature detector/extractor in the Tensorflow Object detection API. Voila!!! Now Keras is integrated with the Tensorflow's object detection API</p>
"
887,"<p>Where can I find training datasets like the ones provided for linguistic training, but to train a program to program itself. </p>

<p>I want to input this training dataset to a programming script and it should use it to program itself.</p>

<p>What do I need to consider in this kind of artificial intelligence?</p>
"
888,"<p>There should be some <a href=""https://en.wikipedia.org/wiki/Performance_indicator"" rel=""nofollow noreferrer"">Key Performance Indicators</a> designed for measuring AI performance. For example, the number of entities examples you have to feed it in order to obtain single task on a testing entity with repeatable 97% accuracy.</p>

<p>Is there any of such measure constructed?</p>

<p>Motivation: you cannot learn AI to be good at playing a game released 1 month ago because there is not enough data on how to play. It doesn't matter how clever you are, present brute force paradigm in AI just doesn't fit in such circumstances.</p>
"
889,"<p><a href=""https://en.wikipedia.org/wiki/Stochastic_hill_climbing"" rel=""noreferrer"">Stochastic Hill Climbing</a> generally performs worse than Steepest <a href=""https://en.wikipedia.org/wiki/Hill_climbing"" rel=""noreferrer"">Hill Climbing</a>, but what are the cases in which the former performs better?</p>
"
890,"<p>I have replicated the NN that can play Breakout, but I don't know how use the pre-trainded checkpoints from DQN-tensorflow devsisters (github):</p>

<p><a href=""https://github.com/devsisters/DQN-tensorflow/issues/39"" rel=""nofollow noreferrer"">https://github.com/devsisters/DQN-tensorflow/issues/39</a></p>

<p>Anyone can help?</p>
"
891,"<p>We have several hundred employees processing word documents, mainly applying some defined rules on how the final document should look like. Some of those rules are: title formating, figure positions, text style among others.</p>

<p>I would like to optimise this process and replace the human by AI. The human will move from processing to quality control (QC) so we will have a much higher quality.</p>

<ul>
<li>My question is: How to approach this problem and what would you recommend as initial start point?</li>
</ul>

<p><em>Background: I am new to AI, although I've been following this topic closely for about a year, and want to move forward with implementation.  I have experience with VBA, PHP, C, C++, and some other languages.</em></p>
"
892,"<p>Please I am interested in building a news recommendation app that will be powered by artificial intelligence. I have being making a whole lot of research on this and will be glad if somebody can tell what is needed to build such app in Android.  </p>

<p>It is something similar to this <a href=""https://www.recent.io/"" rel=""nofollow noreferrer"">app</a></p>
"
893,"<p>Imagine that we have a black box that have 100 binary inputs and 30 binary outputs.</p>

<p>We can generate values for inputs and get relevant set of outputs.</p>

<p>How does one teach a neural network to predict the binary inputs (or list of input values with probabilitys) using the outputs?</p>

<p>Need practice.</p>
"
894,"<p>I am working on software which deblurs the motion blur created by camera movement. 
 I've surveyed some research papers and determined this process requires deep learning and CNN. Now I'm looking for some books that would be useful in getting a more complete picture of the process.   </p>
"
895,"<p>I'm developing a Game AI which tries to master racing simulations. I already trained a CNN (alexnet) on ingame footage of me playing the game and the pressed keys as the target. As the CNN is only making predictions on a frame-to-frame basis, and I resized the image input to 160x120 due to GPU memory limitations, it cannot read the speedometer, thus seems not to have a feeling for its current velocity. I thought of different ways to fix this issue:  </p>

<ol>
<li><p>Crop the captured image down to the size of the speedometer, which displays the current speed in mph, and feed the low resolution game image, as well as the relatively high-res image (70x30) of the current speed into the neural network, which makes predictions based on the two images.  </p></li>
<li><p>As I don't know whether alexnet can serve as an OCR as well, my second thought was to use an existing one (like tesseract-oct/pytesser) on the cropped image and feed its output to the fully connected layer.  </p></li>
<li><p>I already tried to implement an optical-flow algorithm, but sadly, non of the python ones seems to output good real-time results. I wonder whether I can input the current frame as well as the last one, and let alexnet figure out the movement.</p></li>
</ol>

<p>As the processing has to happen in real time, and the only performance reviews of pytesser I found reported a processing time of ~100ms (never tested that). My Question is, what method would work best. </p>

<p>Thanks!</p>

<p>Edit: Optical flow would have the advantage of the AI knowing in which direction other cars are moving as well.</p>
"
896,"<p>Openai's gym website redirects to the <a href=""https://github.com/openai/gym"" rel=""noreferrer"">GitHub repository</a>.  Why did the openai's gym website close?</p>
"
897,"<p>In my application, I have inputs and outputs that could be represented as graphs. I have a number of acceptable pairs of input and output graphs. I want to use these to train a model.</p>

<p>I am looking for pointers where simple examples of learning methods with graphs as input are discussed. Please note that the graph size is not fixed.</p>

<p>A sample input is</p>

<pre><code>Graph:
  Node A: Component X with parameter size = 12
  Node B: Component Y with parameter size = 30
  Node C: Component Y with parameter size = 30
  A connects to B
  A connects to C
</code></pre>

<p>Sample output:</p>

<pre><code>Node A: x=0, y=0
Node B: x=-21, y=0
Node C: x=21, y=0
</code></pre>

<p>In this case, we expect the model to understand that input graph is symmetric and a particular way of arranging them is preferred. We want to train the model over a large set of such input-output pairs and then use it to generate output on new inputs.</p>
"
898,"<p>Suppose a thinking AI agent exists in the future with far more computational power than the human brain. </p>

<p>Also assume that they are completely free from any human interference. <em>(The agents do not interact with humans.)</em>  Since they are not inherently biased to survive as in the case of humans and they do not have any moral values, what are the possibilities that can arise when it get into existential crisis?</p>

<p>Is there any literature that discuss the above issue? </p>

<p>Alternately, is this question flawed in some fundamental way?</p>
"
899,"<p>By optimal I mean that:</p>

<ul>
<li>If max has a winning strategy then minimax will return the strategy for max with the fewest number of moves to win.</li>
<li>If min has a winning strategy then minimax will return the strategy for max with the most number of moves to lose.</li>
<li>If neither has a winning strategy then minimax will return the strategy for max with the most number of moves to draw.</li>
</ul>

<p>The idea is that you want to win in the fewest number of moves possible but if you can't win then you want to drag out the game for as long as possible so that the opponent has more chances of making mistakes.</p>

<p>So, how do you make minimax return the best strategy for max?</p>
"
900,"<p>I understand that AI researchers are trying to create AI designs that allow for desired behavior without undesirable side-effects. A classic example of an attempt is Isaac Asimov's ""Three Laws of Robotics"". This idea seems to have been debunked due to its vague phrasing. Why have AI researchers not accepted an idea like the following (just making the laws more specific):</p>

<blockquote>
  <p>What if the UN voted for the country with the fairest laws and all
  utility functions have 2/3 of their points made up of not breaking any
  of those laws. If the ai has a question, it could look at court
  precedent just like a judge would or ask humans (with two-thirds of
  its points on the line, it should be pretty cautious). </p>
  
  <p>People have been looking for loopholes in law for thousands of years
  and there may not be any catastrophic ones left. (It certainly
  wouldn't be violent)</p>
</blockquote>

<p>I must be missing something if this is still an open problem.</p>
"
901,"<p>I'm developing a Game AI which tries to master racing simulation. I already trained a CNN (alexnet) on ingame footage of me playing the game and the pressed keys as the target. I had two main issues with this setup:  </p>

<ol>
<li><p>Extracting the current speed from the speedometer, in order to feed it to the AI. That question was already solved <a href=""https://ai.stackexchange.com/questions/4013/game-ai-fast-python-ocr-or-cropped-image-input"">here</a>.  </p></li>
<li><p>During testing, I noticed that the AI cannot make small adjustments on straight roads and cuts corners a lot.</p></li>
</ol>

<p>I'm pretty sure that the second issue is caused by the game handling pressed keys binary (e.g. 'a' pressed -> 100% left turn). The AI just can't make precise movements. In order to solve that issue, I want to emulate a joystick, which controls the game precisely. Using <a href=""https://www.pygame.org/"" rel=""nofollow noreferrer"">pygame</a> I already managed to capture my controller inputs as training data.<br>
The controller has two axes, one for turning, the other one for throttle/breaking. Both axis can have any value between -1 and 1:  </p>

<pre><code>Axis 0: Value -1 -&gt; 100% left
        Value +1 -&gt; 100% right

Axis 1: Value -1 -&gt; full break
        Value +1 -&gt; full throttle
</code></pre>

<p>My goal is to train alexnet to output analogue raw axis values, given the current speed and captured frame, and feed its predictions into the joystick emulator. I found someone on GitHub who tried <a href=""https://github.com/Sentdex/pygta5/issues/8#issuecomment-301934355"" rel=""nofollow noreferrer"">something similar</a> and could't achieve good results even on a modified alexnet. Because of this, I was wondering whether it is even possible to modify the CNN to output analogue values instead of using it as an image classifier.<br>
My question is, whether it is worth putting the effort into editing alexnet, instead of using a whole different model. I found some models online like the <a href=""https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/"" rel=""nofollow noreferrer"">NVIDIA End-to-End Self Driving model</a>, which sadly only controls the steering angle, and seems to be made for low speed casual driving.</p>

<p>Thanks!</p>
"
902,"<p>I am currently writing an engine to play a card game, as there is no engine yet for this particular game.</p>

<p>I am hoping to be able to introduce a neural net to the game afterwards, and have it learn to play the game.</p>

<p>I'm writing the engine in such a way that is helpful for an AI player. There are choice points, and at those points, a list of valid options is presented. Random selection would be able to play the game (albeit not well).</p>

<p>I have learned a lot about neural networks (mostly NEAT and HyperNEAT) and even built my own implementation. I am still unsure how best build an AI that can take into account all the variables in one of these types of games. Is there a common approach? I know that Keldon wrote a good AI for RftG which has a decent amount of complexity, I am not sure how he managed to build such an AI.</p>

<p>Any advice? Is it feasible? Are there any good examples of this? How were the inputs mapped?</p>

<p>EDIT: I have looked online and learned how neural networks work and usually how they pertain to image recognition or steering a simple agent. I'm not sure if or how I would apply it to making selections with cards which have a complex synergy. Any direction towards what I should be looking into would be greatly appreciated.</p>

<p>About the game: The game is similar to Magic: The Gathering. There is a commander which has health and abilities. Players have an energy pool which they use to put minions and spells on the board. Minions have health, attack values, costs, etc. Cards also have abilities, these are not easily enumerated. Cards are played from the hand, new cards are drawn from a deck. These are all aspects it would be helpful for the neural network to consider. </p>
"
903,"<p>I've just watched the <a href=""https://www.youtube.com/watch?v=MSwoNAODrgk"" rel=""nofollow noreferrer"">9th episode of HTM school</a> about the ""boosting"" and ""inhibition"" ideas. However, I couldn't find the neuroscience counterpart of these terms and concepts. Since <a href=""https://en.wikipedia.org/wiki/Hierarchical_temporal_memory"" rel=""nofollow noreferrer"">HTM</a> is a biologically-constrained theory, the ""boosting"" and ""inhibition"" concepts must have a neuroscience counterpart. What are they?</p>

<p>The video also discusses <a href=""https://en.wikipedia.org/wiki/Homeostasis"" rel=""nofollow noreferrer"">homeostasis</a> and <a href=""http://www.scholarpedia.org/article/Homeostatic_Regulation_of_Neuronal_Excitability"" rel=""nofollow noreferrer"">homeostatic regulation of neuronal excitability</a>. Do these concepts have something to do with ""boosting"" and ""inhibition""?</p>
"
904,"<p>I was trying to categorical variable engineering following <a href=""https://arxiv.org/abs/1604.06737"" rel=""nofollow noreferrer"">this</a> paper. The code is the following:</p>

<pre><code>import random
import pandas
import numpy as np
import tensorflow as tf

from tensorflow.contrib import layers
from tensorflow.contrib import learn
from __future__ import print_function

from sklearn.preprocessing import LabelEncoder
</code></pre>

<p>My dataset looks like the following. It's has 2 independent variable ('X1' &amp; 'X2')and 1 dependent variable ('lable'). 'X2' is the categorical variable. I want to create an embedding vector for this variable and run the simple linear regression to predict 'label'using Tensorflow. I could use any other method. But since linear regression is easiest to understand, I'm trying that.</p>

<pre><code>df = pd.DataFrame({'X1': np.array([""A"",""A"",""B"",""C"",""B"",""C"",""B"",""C"",""C"",""B"",
                         ""A"",""B"",""A"",""C"",""A"",""A"",""C""]),'X2': np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1]),
                       'label': np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])})

# For variable 'X1', I'm creating levels.

encoder = LabelEncoder()
encoder.fit(df.X1.values)
X = encoder.transform(df.X1.values)

# Recreating dependent variable list.

y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])

# Setting Hyper-parameters

training_epochs = 5
learning_rate = 1e-3
cardinality = len(np.unique(X))
embedding_size = 2
input_X_size = 1
n_hidden = 10

# Setting up variables:

embeddings = tf.Variable(tf.random_uniform([cardinality, embedding_size], -1.0, 1.0))


h = tf.Variable(tf.truncated_normal((embedding_size + len(df.X1), n_hidden), stddev=0.1))


W_out = tf.get_variable(name='out_w', shape=[n_hidden],
                            initializer=tf.contrib.layers.xavier_initializer())

# Embedding:

embedded_chars = tf.nn.embedding_lookup(embeddings, x)
embedded_chars = tf.reshape(embedded_chars, [-1])
embedded_chars= embedded_chars + np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])

# Multiplying with Hidden Layers:

layer_1 = tf.matmul(embedded_chars,h)
layer_1 = tf.nn.relu(layer_1)
out_layer = tf.matmul(layer_1, W_out)

# Define loss and optimizer

cost = tf.reduce_sum(tf.pow(out_layer-y, 2))/(2*n_samples)
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
#    Run the graph

init = tf.global_variables_initializer()

# Launch the graph

with tf.Session() as sess:
    sess.run(init)

    for epoch in range(training_epochs):
        avg_cost = 0.

        _, c = sess.run([optimizer, cost],
                        feed_dict={x: X, y: Y})
print(""Ran without Error"")
</code></pre>

<p>While running the code, I'm getting the following error.</p>

<blockquote>
  <p>ValueError: Shape must be rank 2 but is rank 1 for 'MatMul_1' (op:
  'MatMul') with input shapes: [17], [19,10].</p>
</blockquote>

<p>I'm not able to add the continuous variable with embedding variable.</p>

<p>Can anyone please guide me how to do it?</p>

<p>Thank you!</p>
"
905,"<p>Is there any well defined method to define or represent evil in <strong>abstract</strong> logic, binary or AI form?</p>

<p>Video games method of representing evil is relative to the player context (thus subjective, and not pure abstract evil in an objective sense).</p>

<p>What I am asking is there any data defined as well-known evil?</p>

<p>Example:</p>

<pre><code>var x=666;

if (isEvil(x)) {
    //do something.
}
</code></pre>

<p>Remark:
Evil Number descried in <a href=""http://mathworld.wolfram.com/EvilNumber.html"" rel=""nofollow noreferrer"">http://mathworld.wolfram.com/EvilNumber.html</a> doesn't qualify as well-known evil data. </p>

<h1>Following Up:</h1>

<p>One of the main objectives of the question is to <strong>understand scientifically the limits of evil in AI</strong></p>

<p>According to my understanding of: <a href=""https://en.wikipedia.org/wiki/Evil"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Evil</a> I think it's mandatory to explore ""evil"" in religion context in order to come up with valid model for evil. <em>But I don't want go into (religion) debates or any divergence at this stage</em>. hence below points Sums up my understanding:</p>

<ol>
<li>The only well-known Evil source is the devil (our creator declared the devil as the first common enemy for ALL humans).</li>
<li>Whispering is devil method of attack, If human followed the whisper it will lead to evil. and gradually human Evil grow...  </li>
<li>There are other points but I don't see its related to AI in any means.</li>
</ol>

<p>Based on the above, I asked myself: <strong>since AI is human creation, where the evil in AI will come from??!</strong> my answer is: directly from us and indirectly by following the devil. So all crimes committed by Evil AI bounded to AI architect/designer/unethical hacker.</p>

<p>The next stage in getting closer to model evil, is to define and classify the evil acts:</p>

<h2>Definitions:</h2>

<ol>
<li>Define evil in AI context (draft ver. 0.1): committing crimes against nature, civilizations or humans. And reprogramming, modifying or attacking tech devices/machines to perform malicious agenda.   </li>
<li>Crime is broad and relative to the party: example: breaking one government regulations based on the orders of other government. <em>I mean as long each group of humans makes its own laws and regulations unified justice can't be applied on Evil AI.</em></li>
</ol>

<p>If my assumption of bounding evil to crime is valid then <strong>evil classification inherits crime classification</strong> which seems well-defined:
 <a href=""https://en.wikipedia.org/wiki/Crime#Classification_and_categorisation"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Crime#Classification_and_categorisation</a>  </p>

<p>Next step is to pick an easy to model crime class, prepare training data, ... Do you agree with the follow up? Do you agree that Boolean logic can't determine evil without AI?</p>
"
906,"<p>I'm currently developing a blackjack program that I posted on code review SE about a month ago and, after making a few changes based on the users suggestions and my friends' suggestions, I want to create an AI that essentially uses the mathematics of blackjack to make decisions. Almost like an AI that can card count, but different because card counting isn't very effective with blackjack.</p>

<p>I've figured out some of the math myself based on what I've read across multiple sites and books dedicated to the game, but I'm just not sure how I can get started with it. At this point, all suggestions will be helpful.</p>

<p>With that in mind, does anyone have any ideas or suggestions for how I might go about making a base plan for implementing it? It doesn't have to be language specific, but if it will help with an answer, the language I plan to use to do this would be Python. </p>
"
907,"<p>I am a student of software engineering. We studied AI in last semester but it was all theory. Now I am interested in practically using AI. I have searched AI tutorials everywhere but i can't find single one. I have studied all theory, okay not all but much theory to understand the concepts of AI. If anyone can guide me to the right direction or any book or tutorial where i can learn a little bit of using AI in practical, i would be very thankful..</p>
"
908,"<p>Below is an excerpt in an instructor's manual on ML that is explaining deep neural networks, using cat recognition (what else!) from images as example. On how DL performs this feat, the excerpt said that,</p>

<blockquote>
  <p>Assume that the first layer returns the number of pixels that are
  brown/black/blue/red, and the second layer finds the most common
  color, and the third layer returns “cat” if previous layer had
  supplied “brown”. [..] Mathematically, this model would be, for the
  first layer, [ sum(r = 255, g=255, b=255), ..., ..., sum(r=255, g=0,
  b=0)] -- this is just a set of appropriately positioned relu functions
  (okay, for r=234, we’d need two relu functions, so two layers, but you
  get the idea). The second layer would be a softmax layer. The third
  layer is simply an identity!</p>
</blockquote>

<p>Now I worked with deep nets, but I am not sure how I can structure a DL to do this. ReLu is simply a max(0,x), so how would I filter out pixel vals for example 128,128,128 and sum them up? Wouldn't the convolution layer play a role here too? What would the layout of a simple deep net be that does what is described above?</p>

<p>Thanks,</p>
"
909,"<p>How much power will it bring to its creator? </p>
"
910,"<p>To start, I'm not a programmer/computer scientist/et al... - I work in Finance and have, through my job, self-thought myself VBA for excel and outlook and would consider myself as being in the upper half of the population of people who write VBA creating browser apps for scraping (and interpreting site's DOMs in doing so), creating Arrays of multi-dimensional arrays (arrays of 3D arrays - haven't gone for a 4D array as I haven't needed to) and generally focused on making as efficient macros. I've (very) basic understanding of JAVA also (I even created an basic android app!).</p>

<p>By pure accident, learning the above has given me a massive interest in all thing related to IT developments and the massive development which I truly believe will be the development in Human History (for better or worse has yet to be determined!) is AI.</p>

<p>I want to know all that I can about AI so was wondering if you all could assist an absolute noob on the subject. I know nothing of Python (which seems the language of choice for AI) but anything that would help would be great.</p>

<p>I'm not completely naive to the fact the brightest minds in the IT world are are working on this and concepts are going to be difficult for someone like me but I'm willing to try!</p>

<p>Any and all help/suggestions are welcome!</p>

<p>Thanks in advance.</p>
"
911,"<p>I have to analyse sequences of actions that look more or less like <a href=""https://gist.github.com/Morpheu5/eeea1c8b7b76b13f6010af2296cff336"" rel=""noreferrer"">this</a> JSON blob. The question I'm trying to answer is whether there are recurring (sub)patterns that different users adopt when asked to perform a certain specific task -- in this case, the task is to build a mathematical formula using <a href=""https://www.isaacphysics.org/equality"" rel=""noreferrer"">this</a> editor. In particular I'd like to know if there are multiple significantly different ways in which people build the same expression.</p>

<p>I thought of creating a Markov model, but that would only give me the most likely sequence of actions of length <em>N</em>. An obvious alternative would be to build trees and count how many times a certain path occurs in the dataset. However, the nature of the expression-building process means that the sequences can be polluted by many confounding, non-significant actions (such as streaks of UNDO-REDO, deleting symbols, and the likes).</p>

<p>I might go the ""longest common subsequence"" route, but I'm not sure that would tell me if there are ""significantly different"" ways of building the same expression (in quotes because, for now, I don't have a rigorous definition of ""significantly different"", but, for example, one way would be to drag and drop-in-place all the symbols in the correct order, and another way would be to drag all the symbols onto the canvas, and then place them in the correct spots).</p>

<p>I thought this might be a nice challenge for some AI algorithm, but I'm quite a noob at that, so I'm open to suggestions.</p>
"
912,"<p>I studied the article <a href=""https://www.intelnervana.com/demystifying-deep-reinforcement-learning/"" rel=""nofollow noreferrer"">""Demystifying Deep Reinforcement Learning""</a> extensively during the last days, while trying to implement the proposed algorithms myself.</p>

<p>My goal is to have an agent learn by playing a simple board game against itself using the methods of deep reinforcement learning. The algorithm described in pseudo code in the chapter called ""Deep Q-learning Algorithm"" is straight forward, but I cannot wrap my head around the fact that the replay memory only gets initialized once and never gets cleared.</p>

<p>Besides the obvious issue of growing too large for the available memory, there seems to be a more fundamental flaw. In the beginning the games will appear almost random, because the Q-function just has been initialized randomly. This means that bad moves will hardly be punished by the opponent. It makes sense to learn from those random encounters in the beginning, as we do not have any better data to learn from and this actually describes the observed behavior of the environment very well.</p>

<p>But when the agent improves, the old memorized plays will no longer be valuable, as certain moves will no longer be played by the opponent and therefore the old games will no longer reflect the current behavior of the environment.</p>

<p>That's at least my interpretation. Now I am wondering if this is simply missing from the pseudo code in the article or if my way of thinking is wrong in this regard. My question is, if we need to flush the replay memory regularly in the given setup and how often it should be done?</p>
"
913,"<p><em>I've posted this question on data science community, but haven't received an answer yet. So I also posted it here in the hope that more people could see it.</em></p>

<p>I'm learning about the <a href=""http://Restricted%20Boltzmann%20machine"" rel=""nofollow noreferrer"">restricted Boltzmann machine</a> (RBM), and I just came up with two naïve understandings of this model. But it seems these two understandings are so different, can someone tell me how can I combine these two understandings?</p>

<ol>
<li>My first understanding goes like this:</li>
</ol>

<p><strong>The  hidden units are just structural support and we don't care about what those hidden vectors really are.</strong> We introduce those hidden units just for RBM to gain more expressive power for probability distribution :</p>

<p>let's say I have only one image, and I transform this image to a binary vector and feed this vector into a RBM with random variables (all the weights and biases are chosen randomly). Then, by turning on the machine, the first hidden vector would be constructed. But this hidden vector does not tell us anything, it would be only used to reconstruct a visible vector.
<strong>(my understanding for this reconstructed visible vector is this vector is a vector encoded in the defined RBM in the first place, we are not really construct something new, but we just happen to sample this vector from the defined RBM).</strong>
And we just run this loop of construction and reconstruction for infinitely many times. Finally, what we will get is just the probability distribution encoded in this RBM with random variables.</p>

<ol start=""2"">
<li>My second understanding goes like this:</li>
</ol>

<p><strong>RBM can be used to perform dimensionality reduction, and those hidden vectors are some abstract representations of the raw inputs</strong>:</p>

<p>Given a RBM, each hidden units of the RBM would be a classifier, and what it does is to check the input vector lies in which side of the hyperplane defined by this hidden units (by its weights and bias). So, if we input an image into this RBM, the RBM will project this input vector onto many hyperplanes defined by all the hidden units, and thus for an input vector, the corresponding hidden vector is very important. It is some abstract representation. And we can further feed this representation into other learning models.</p>

<p>So, these are my understandings, and the thing that troubles me the most is what the role that hidden vectors actually plays?</p>

<p>If you can answer this question by explaining how RBM is used for MNIST that would be extremely helpful for me.</p>

<p>Thanks!</p>
"
914,"<p>Question is regarding Deep Reinforcement Learning using Policy Gradients.</p>

<p>Cutting edge policy gradients algorithms are TRPO (Trusted Region Policy Optimization) and PPO (Proximal Policy Optimization).</p>

<p>When using single continuous action then normally you would use some random distribution (for example Gaussian) for the loss function. The rough version is:</p>

<p>$L(\theta) = log(P(a_1)) * A$</p>

<p>Where $A$ is the advantage of rewards $P(a_1)$ is characterized by $\mu$ and $\sigma^2$ that comes out of neural network like in Pendulum environment here: <a href=""https://github.com/leomzhong/DeepReinforcementLearningCourse/blob/69e573cd88faec7e9cf900da8eeef08c57dec0f0/hw4/main.py"" rel=""noreferrer"">https://github.com/leomzhong/DeepReinforcementLearningCourse/blob/69e573cd88faec7e9cf900da8eeef08c57dec0f0/hw4/main.py</a></p>

<p>Problem is that I cannot find any paper on 2+ continuous actions using policy gradients (not Actor-critic methods that use a different approach by transferring gradient from Q-Function). </p>

<p>Do you know how to do this using TRPO for 2 continuous actions in LunarLander environment? 
<a href=""https://gym.openai.com/envs/LunarLanderContinuous-v2/"" rel=""noreferrer"">https://gym.openai.com/envs/LunarLanderContinuous-v2/</a></p>

<p>Is following approach correct for policy gradient loss function?</p>

<p>$L(\theta) = (log(P(a_1))+log(P(a_2)))*A$</p>
"
915,"<p>In Artificial intelligent, concerning the topic ""finding solution by searching"", we discussed partial searching and complete searching algorithms. But I can't understand the difference between partial searching and complete searching.</p>

<p>I also want to know if 'local searching' and 'complete searching' are the same or different. Please include an explanation. Thanks in advance.</p>

<blockquote>
  <p>I refer to the book <a href=""https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach"" rel=""nofollow noreferrer"">""Artificial intelligent modern approach""</a> by Peter
  Norvig, Stuart J. Russel, chapter 4 ""LOCAL SEARCH
  ALGORITHMS AND OPTIMIZATION PROBLEMS"".</p>
</blockquote>
"
916,"<p>After reading this <a href=""https://pure.york.ac.uk/portal/files/13014166/CowlingPowleyWhitehouse2012.pdf"" rel=""nofollow noreferrer"">paper</a> about Monte Carlo methods for imperfect information games with elements of uncertainty, I couldn't understand the application of determinization step in <a href=""https://gist.github.com/kjlubick/8ea239ede6a026a61f4d"" rel=""nofollow noreferrer"">author's implementation</a> of the algorithm for Knockout game.</p>

<p>Determinization is defined as transformation from instance of imperfect information game to instance of perfect one. It means that all players should see the cards of each other after the determinization step. </p>

<p>Why can't the players see the cards of each other in the code above?</p>
"
917,"<p>I am building a system that should take text without punctuation and automatically add punctuation.</p>

<p>I found some papers about <a href=""https://scholar.google.co.in/scholar?q=automatic%20punctuation&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C5&amp;oq=automatic"" rel=""nofollow noreferrer"">automatic punctuation</a>, but they are mostly about spoken language understanding, they use cues such as prosody to detect potential places for punctuation. In my case the input is written text. Are there papers about automatic punctuation of written text?</p>

<p>My current idea is to treat each punctuation mark as a class (. , ; : ? ! ?!) and add a class for ""no punctuation"". Then, use e.g. an LSTM and classify each word to one of the classes. 
An alternative approach is to first use a binary classifier to detect words after which there should be a punctuation mark, then use a multiclass classifier only on the punctuated words, to choose the right mark. Which of these approaches, if any, is good?</p>
"
918,"<p>I am developing a heuristic solution for <a href=""https://en.wikipedia.org/wiki/Blocks_world"" rel=""nofollow noreferrer"">blocks world</a> problem.</p>

<p>I tried using number of blocks out of place as my h(n). It seems little ineffective.</p>

<p>Can someone please point out a suitable heuristic for the problem and explain with few examples how it will work.</p>

<pre><code>Blocksworld Problem Example:

Initial(starting State):
Stack 0: D,B
Stack 1: A,E
Stack 2: C
Stack 3: F


Goal State:
Stack 0: A,B,C,D,E,F
</code></pre>
"
919,"<p>Not sure if this is the proper venue for this type of question. Are there any visual elements, patterns, colors, graphs, maps, or even programming fonts that are used and even recurring in artificial intelligence and machine learning processes? As a designer, it is not entirely clear what these ""look"" like realistically, as most generally associate AI with the kind of colors and textures generally seen in sci-fi films.</p>
"
920,"<p>I just got into AI few months ago and I noticed most of the images in train-sets are usually low quality(almost pixelated). </p>

<p><strong>My questions is</strong>, Does the quality of training images affect the accuracy of the NN? I tried google but I couldn't find an answer.</p>
"
921,"<p>I did a little bit of research on the most famous organizations and institutions that promote friendly AI in an open and collaborative way, like <a href=""https://openai.com/"" rel=""nofollow noreferrer"">OpenAI</a>, <a href=""https://intelligence.org/"" rel=""nofollow noreferrer"">MIRI</a> and others. From what I have gathered, most of them have strong ties to the US with some influences from Central Europe and South East Asia. But I didn't find a single instance where Russia participates in any of those groups.</p>

<p>So my question is, does Russia contribute to open research concerning friendly AI? Are there any open AI projects with strong ties to Russia?</p>
"
922,"<p>I building a deep learning model to detect what drug user use. I have many symptoms and duration of each drug. I create X and y data but, for example, LSD have an effect duration of 180 - 720 minutes. I really need make 540 arrays? I really want a help.</p>

<p>My LSD array:</p>

<pre><code>[28, 180],
[28, 720],
[29, 180],
[29, 720],
[30, 180],
[30, 720],
[31, 180],
[31, 720],
[32, 180],
[32, 720],
[33, 180],
[33, 720],
[34, 180],
[34, 720],
[35, 180],
[35, 720],
[36, 180],
[36, 720],
[37, 180],
[37, 720],
[1, 180],
[1, 720],
[38, 180],
[38, 720],
[12, 180],
[12, 720],
[9, 180],
[9, 720],
[24, 180],
[24, 720],
[17, 180],
[17, 720],
[7, 180],
[7, 720],
[4, 180],
[4, 720],
</code></pre>

<p>In first position I have differents symptoms and in second position duration. I just duplicated each symptoms and set min duration and max duration. But this return to me a perfection model. I know, I need add all minutes to each symptoms, but how I make this using python?</p>

<p><strong>List of symptoms</strong></p>

<pre><code>0 - relaxamento
1 - euforia
2 - diminuicao da memoria a curto prazo
3 - boca seca
4 - habilidades motoras debilitadas
5 - olhos vermelhos
6 - humor
7 - aumento frequencia cardiaca
8 - aumento apetite
9 - concentracao debilitada
10 - sensacao de poder
11 - ausencia de medo
12 - ansiedade
13 - agressividade
14 - excitacao
15 - perda do apetite
16 - tremores
17 - dilatacao da pupila
18 - dentes anestesiados
19 - insonia
20 - movimentos descontrolados
21 - espasmos maxilar
22 - dor de cabeça
23 - visao turva
24 - nauseas
25 - desidratacao
26 - periodos de depressao
27 - perda total da memoria
28 - ilusões
29 - alucinações
30 - grande sensibilidade sensorial
31 - experiências místicas
32 - flashbacks
33 - paranoia
34 - perda da noção temporal e espacial
35 - confusão
36 - perda do controle emocional
37 - sentimento de bem-estar
38 - pânico
39 - sonolencia
40 - batimentos cardiacos diminuem
41 - insuficiencia respiratoria
42 - desanimo
43 - desinteresse pela vida familiar/profissional
44 - sensacao de estar no paraiso
45 - mal-estar
46 - Incapacidade de sentir prazer
47 - Incapacidade de sentir dor
</code></pre>

<p>** Durations effects (in minutes) **</p>

<pre><code>Cannabis. 120 - 240
Cocain. 30 - 40
Ecstasy. 240 - 480
LSD. 180 - 720
Heroin. 45 - 60
</code></pre>

<p>My full code:</p>

<pre><code>X = [
    #cannabis
    [0, 120],
    [0, 240],
    [1, 120],
    [1, 240],
    [2, 120],
    [2, 240],
    [3, 120],
    [3, 240],
    [4, 120],
    [4, 240],
    [5, 120],
    [5, 240],
    [6, 120],
    [6, 240],
    [7, 120],
    [7, 240],
    [8, 120],
    [8, 240],
    [9, 120],
    [9, 240],
    #cocain
    [1, 30],
    [1, 40],
    [10, 30],
    [10, 40],
    [11, 30],
    [11, 40],
    [12, 30],
    [12, 40],
    [13, 30],
    [13, 40],
    [14, 30],
    [14, 40],
    [15, 30],
    [15, 40],
    [7, 30],
    [7, 40],
    [16, 30],
    [16, 40],
    [17, 30],
    [17, 40],
    [18, 30],
    [18, 40],
    #ecstasy
    [19, 240],
    [19, 480],
    [20, 240],
    [20, 480],
    [21, 240],
    [21, 480],
    [22, 240],
    [22, 480],
    [23, 240],
    [23, 480],
    [24, 240],
    [24, 480],
    [25, 240],
    [25, 480],
    [26, 240],
    [26, 480],
    [27, 240],
    [27, 480],
    [15, 240],
    [15, 480],
    #LSD
    [28, 180],
    [28, 720],
    [29, 180],
    [29, 720],
    [30, 180],
    [30, 720],
    [31, 180],
    [31, 720],
    [32, 180],
    [32, 720],
    [33, 180],
    [33, 720],
    [34, 180],
    [34, 720],
    [35, 180],
    [35, 720],
    [36, 180],
    [36, 720],
    [37, 180],
    [37, 720],
    [1, 180],
    [1, 720],
    [38, 180],
    [38, 720],
    [12, 180],
    [12, 720],
    [9, 180],
    [9, 720],
    [24, 180],
    [24, 720],
    [17, 180],
    [17, 720],
    [7, 180],
    [7, 720],
    [4, 180],
    [4, 720],
    # Heroin
    [39, 45],
    [39, 60],
    [29, 45],
    [29, 60],
    [40, 45],
    [40, 60],
    [41, 45],
    [41, 60],
    [42, 45],
    [42, 60],
    [43, 45],
    [43, 60],
    [44, 45],
    [44, 60],
    [12, 45],
    [12, 60],
    [45, 45],
    [45, 60],
    [46, 45],
    [46, 60],
    [1, 45],
    [1, 60],
    [13, 45],
    [13, 60],
    [24, 45],
    [24, 60],
]
""""""
    # DROGAS

    0 - Cannabis
    1 - Cocain
    2 - Ecstasy
    3 - LSD
    4 - Heroin
""""""
y = [ 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5)

from sklearn import tree
my_classifier = tree.DecisionTreeClassifier()

my_classifier.fit(X_train, y_train)

predictions = my_classifier.predict(X_test)

print(predictions)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, predictions))
</code></pre>

<p>Sorry for my bad english :(
Thanks</p>
"
923,"<p>Are there any algorithms or software libraries that can be used to detect the similarity of concepts in text, regardless of articulation, grammar, synonyms, etc.?</p>

<p>For example, these phrases:</p>

<blockquote>
  <p>Outside, it is warm.</p>
  
  <p>Outside, it is hot.</p>
  
  <p>Outside, it is not cold.</p>
  
  <p>It is not cold outside.</p>
</blockquote>

<p>Should be similar to this phrase:</p>

<blockquote>
  <p>It is warm outside.</p>
</blockquote>

<p>Ideally, the algorithm or software would be capable of generating a score from 0 to 1, based on the concept similarity. The goal is to use this algorithm or software to map a large number of statements to a single, similar original statement. It is for this mapping of a given statement to the original statement that the aforementioned similarity score would be generated.</p>

<p>Does such an algorithm of software already exist?</p>
"
924,"<p>I have some images of the empty parking as shown below. </p>

<p><a href=""https://i.stack.imgur.com/sDQLM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sDQLM.png"" alt=""Empty parking lot""></a></p>

<p>I 'd like to use deep learning to extract the parking spots. But in the beginning,am confused whether there are several ways to do the following; </p>

<ol>
<li>Treat each parking spot as an object and use object detection to find
the parking space.</li>
<li>Treat the parking spot as a few key points, instead of finding the
spot, try to find the coordinates of key points on the parking line.</li>
</ol>

<p>Any ideas on how would I work on these two methods  </p>
"
925,"<p>I asked this question on /r/learnmachinelearning, but there was no answer so I'm reposting it here.</p>

<p>I was reading this article on detecting rectangles in an image, <a href=""https://medium.com/towards-data-science/object-detection-with-neural-networks-a4e2c46b4491"" rel=""nofollow noreferrer"">here</a>. My doubt is in the part where the model works fine with detecting a single object, but struggles with two rectangles detection.</p>

<p>The author reasons this as follows:</p>

<pre><code>We train our network on the leftmost image in the plot above. Let’s say 
that the expected bounding box of the left rectangle is at position 1 in the 
target vector (x1, y1, w1, h1), and the expected bounding box of the 
right rectangle is at position 2 in the vector (x2, y2, w2, h2). Apparently, 
our optimizer will change the parameters of the network so that the first 
predictor moves to the left, and the second predictor moves to the right. 
Imagine now that a bit later we come across a similar image, but this 
time the positions in the target vector are swapped (i.e. left rectangle at 
position 2, right rectangle at position 1). Now, our optimizer will pull 
predictor 1 to the right and predictor 2 to the left — exactly the opposite 
of the previous update step! In effect, the predicted bounding boxes stay 
in the center.
</code></pre>

<p>I don't understand how this reasoning is correct, apart from the fact that when they try to flip the rectangles to mitigate this error, accuracy actually improves (so there's experimental observation, but not much theoretical reasoning).</p>

<p>The reason I think so is because in case of single rectangle also the network has to learn for all differently placed objects just as in the two-rectangle-case, so there too, it should predict boxes in the somewhat the center only. I have to concede I am only a noob in this, so I would love to find out where I am wrong in my reasoning, because experimentally I am wrong (i.e. accuracy does improve when rectangles are flipped).</p>

<p>Thoughts? Also if this is not the correct sub/forum for these type of questions, please feel free to guide me towards those that better suit the content.</p>
"
926,"<p>I've been reading about AI/Deep Learning, etc. to understand robots (i.e. Elon Musk warning).</p>

<p>But I must be missing something...
Can the entire field be summed-up in this one sentence?
""Neural Networks use the sigmoid function and gradient descent to fine-tune weights.""</p>

<p>For me, a real-world comparison is:
1) Take a wood box and randomly drill different sized holes in the bottom of it.
2) Fill the box with coins and shake it.
3) Mark which holes have coins dropping out.
4) Start over with a new wood box and drill new holes.
5) Repeat until perfect sorting of coins is achieved.
<em>Note: you can even have ""layers"" of boxes and paint it all black</em></p>

<p>In the end, you have a finely tuned ""black box"" to ""sort"" data. (referring to NN's now)
But, what you do NOT have is AI - ""artificial intelligence"".
The whole world should call it ""recognition software"" instead.</p>

<p>Everyone knows that intelligence is to ""think outside the box"".
(examples: Observation, Reflection, Innovation)</p>

<p>So my question is:
Have mathematicians discovered any methods/algos that actually mimic intelligence?
(What keywords should I Google?)</p>
"
927,"<p>I am currently working on my MSc thesis, where I try to automatically convert description of an algorithm in natural language to source code in Python (the algorithms are quite simple, like revert an array of 100 elements) using deep learning. The main problem is that I need a lot of data to do that. Does anyone know any available datasets consisting of pairs {short natural language description, source code}?</p>

<p>I know the Heartstone cards dataset (really useful and close to my needs, but still not enough), the Django dataset (Django code commented line by line - it doesn't really contain the description of whole algorithm, it rather translates english to Django code line by line). I tried to contact with few sites like for eg. Sphere Online Judge but to no avail.</p>

<p>Every help would be appreciated.</p>
"
928,"<p>I have been playing around with the Pong-v0 game. After few days, the AI was able to beat it. I stopped the script for a moment and ran it again. For my surprise it seems it started all over again. </p>

<p>Where and how does OpenAI store the experience?<br>
How can I continue the learning procedure in another computer?</p>
"
929,"<p>I used <a href=""http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/"" rel=""nofollow noreferrer"">this</a> project for example(framework - caffe, arhitecture of net - mod of AlexNet, 400 images are used for training). I have this result:
<a href=""https://i.stack.imgur.com/HxymZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HxymZ.png"" alt=""enter image description here""></a></p>

<p>or this:
<a href=""https://i.stack.imgur.com/arWIT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/arWIT.png"" alt=""enter image description here""></a></p>

<p>Solver:</p>

<pre><code>net: ""./CDNet/Models/train.prototxt""
test_iter: 500
test_interval: 500
base_lr: 0.001
lr_policy: ""step""
gamma: 0.1
stepsize: 100000
display: 50
max_iter: 450000
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: ""./CDNet/Models/Training/cdnet""
solver_mode: GPU
</code></pre>

<p>Model of net:
<a href=""https://i.stack.imgur.com/OdUp9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OdUp9.png"" alt=""enter image description here""></a></p>

<p>Can anybody explain such behavior of accuracy and loss of my Net? What I am doing wrong?</p>

<p>Author of tutorial has got this result: 
<a href=""https://i.stack.imgur.com/0ea9J.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0ea9J.png"" alt=""enter image description here""></a></p>
"
930,"<p><a href=""https://i.stack.imgur.com/HBEY2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HBEY2.png"" alt=""Information Gain or Entropy Formula:""></a></p>

<p>And i know how the calculation are performed with help of above formula but i am not understanding why we are using this formula. Can anyone explain the above formula???</p>
"
931,"<p>As a researcher, I am getting interested in deep learning (as everyone else:)), and I decided to start with the variational eutoencoders, since I am more interested in unsupervised than supervised learning. I have already read tutorials on the general idea of the mechanism of VAE, and what people usually suggest is to use TensorFlow, Keros, etc. to implement them. However, I do not just want to <strong>use</strong> machine learning, but also want to <strong>develop novel algorithms</strong> on top of the existing ones, so I want to deeply understand the mathematical details. And in my experience, in order to understand them, reading is not enough, you need to implement. Any suggestions on where/how to start? I have a general machine learning and programming background, but no experience on deep learning. I even don't know what exactly back-propagation is used for, if this would help.</p>
"
932,"<p>In example, if there is a simple feed-forward neural network with 3 input neurons, 3 hidden neurons, and one output neuron; is it possible to predict a the value of an input neuron given the values and weights for the other two inputs and the output.</p>
"
933,"<p>This question is related to the usage of NN in critical systems (those where a failure can cause life threatening situations - autopilots for example) and the need for formal guarantees on their behavior. <a href=""http://arxiv.org/abs/1702.01135"" rel=""nofollow noreferrer"">Here</a> is, for example, a paper that verifies NN used in the controls of an unmanned air-vehicle. </p>

<p>There are numerous tools and techniques for the formal verification (not just testing and simulation, but actual mathematical proof) of, say, floating-point calculation (properties like: will not overflow, will not accumulate an error greater than x, etc).</p>

<p>Now take a DNN where the weights are real numbers. That is the DNN as a concept. But there is the implementation of the NN, where the weights must be encoded as floating-points (<a href=""http://arxiv.org/abs/1603.01025"" rel=""nofollow noreferrer"">some</a> even as low precision as 3-bits integers, apparently). And then, (faster) computation is done on these encodings. Someone might argue that you lose precision, but others might answer that it may be a good thing  since one must prevent a NN for over-fitting anyway...</p>

<p>Questions: </p>

<ul>
<li>Is there a way to quantify the effect of this encoding on the robustness/precision/stability-of-classification, in a comparable way to what we have in (non learning) software verification?</li>
<li>Are there NN architectures (like Sum-Product Networks(?)) that are more amendable to offer such guarantees?</li>
</ul>
"
934,"<p>DNN can be used to recognize pictures. Great. For that usage, it's better if they are somewhat flexible so as to recognize as cats even cats that are not on the pictures on which they trained (i.e. avoid overfitting). Agreed.</p>

<p>But when <a href=""http://ieeexplore.ieee.org/abstract/document/7778091/"" rel=""noreferrer"">one uses</a> NN as a replacement for numerical tables in an Air Collision Avoidance System (ACAS), it is primarily to reduce the ""required storage space by a factor of 1000"". For this usage, what we want from the NN is to say ""take a slight left turn"" or ""turn right hard"" if another ship comes slightly close on the right or rapidly close on the left, respectively. </p>

<p>For this usage, where the answer is much simpler than recognizing a cat, isn't overfitting a good thing? What would overfitting ""look like"" in this case and why would it be bad ?</p>

<p>This question somewhat relates <a href=""http://ai.stackexchange.com/questions/3473/is-there-such-a-thing-like-the-machine-learning-paradox"">to this one</a>, where a general idea seems to be ""Machine Learning is used for intractable things, you don't need ML for tractable things"". And while it is quite correct that ACAS can be implemented without NN, I wouldn't call NN ""useless"" for ACAS, because a factor 1000 reduction in required space will always come in handy.  </p>
"
935,"<p>For implementing a neural network algorithm that can play air hockey, I had two ideas for input, and I'm trying to figure out which design would be most viable.</p>

<p>The output must be two analog values that dictate the best position on half of the table for the robot to be at a specific point in time, evaluated 60 times per second.</p>

<p>Having consulted with a professor who has experience with implementing parallel algorithms, it was recommended to me to use a convolutional neural network with a single hidden layer, and directly process the image data as the input layer, after processing it to visualize a direct view of the table and somehow emphasize the puck and mallets with pre-processing. I have already started work on this and have successfully implemented object detection for the puck and mallets using OpenCV to get the center coordinates for all 3 entities.</p>

<p>However, having been able to successfully and accurately pre-process these data at 60 times per second, my thought was to feed those (after normalizing the values using the error function) directly on the input layer and possibly implement a deep learning algorithm that employs more than one hidden layer. The problem is that I don't have any experience implementing neural networks, so I'm not even sure what type of layer would be best for this, or how I should seed the weights.</p>

<p>Another reason I want to consider my idea is that given the relatively few inputs and outputs, probably won't need a GPU to execute forward-propagation 60 times per second, whereas with the convolutional network my professor recommended, I <strong>know</strong> I'll need to implement using CUDA somehow.</p>

<p>Which of the two input methods would be most recommended for this, and if I were to try and implement my idea, what types of layers should I consider? Also any recommendations for existing frameworks to use for either approach would be highly appreciated.</p>
"
936,"<p><a href=""https://i.stack.imgur.com/YEfpr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YEfpr.png"" alt=""First Image""></a></p>

<p>Notice, at the very right, the term multiplied with <span class=""math-container"">$\lambda$</span> is <span class=""math-container"">$d_i$</span>.</p>

<p><a href=""https://i.stack.imgur.com/Jjrmq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jjrmq.png"" alt=""Sec image""></a></p>

<p>Now, however, the term is <span class=""math-container"">$d_m$</span>, which was <span class=""math-container"">$d_i$</span> in the first equation.</p>

<p>In my opinion, first equation seems reasonable. The <span class=""math-container"">$\lambda$</span> in second one actually seems to work like discount factor.</p>

<p>In another chess engine, Giraffe, the author cited same paper, noted the first equation, but then proceeded on to implement second one. </p>

<p>I have thought quite hard, and am afraid i'm missing something very obvious.  </p>

<p><a href=""https://arxiv.org/pdf/cs/9901002.pdf"" rel=""nofollow noreferrer"">KnightCap Paper</a></p>

<p><a href=""https://arxiv.org/pdf/1509.01549.pdf"" rel=""nofollow noreferrer"">Giraffe Paper</a></p>
"
937,"<p>Given a virtual game map (picture) and a racing car at the map's starting point, I'm trying to build an algorithm that would help me generate a route that would get the car from the beginning to the endpoint.</p>

<p><strong>route definition</strong>: a very complex .xml file that includes all the data the car needs in order to navigate the road successfully.</p>

<p>What I have:</p>

<ul>
<li>I have thousands of different maps and I could get any number of maps I'd like.</li>
<li>For every map, I have a complex external algorithm using picture analysis that builds a route for me - although this route is not so accurate.</li>
<li>For some percentage of the maps, I also have a specific ""good working car solution"" - which is basically a manually built route file.</li>
</ul>

<p>What I'm trying to achieve:
Basically, I'm trying to improve the complex route generator algorithm which I don't have access to.
I want to use the percentage of manual routes I have and compare them to the routes that are generated for the same maps. From the comparison and the differences between the manual and the automatically created route files, I want to build a second-step smart algorithm that ""learns"" what should be changed in the automatically generated routes and have that algorithm run as a second step and make it as close as possible to the handmade route.</p>

<p>My final goal is to be able to build an accurate route for maps I don't have the manual route for. I want to be able to produce an accurate route for every map - using both the external algorithm and the one I will build on top of it.</p>

<p>This entire subject is very new to me and I'd like to get your advice as to how should I approach this complex problem, and what could make this task easier for me.</p>
"
938,"<p>In their work <em><a href=""https://arxiv.org/abs/1603.00748"" rel=""nofollow noreferrer"">Continuous Deep Q-Learning with Model-based Acceleration</a></em>, the author demonstrate great results of applying Imagination Rollouts for model-based acceleration of learning. They test their algorithm on manipulation tasks in a Mujoco simulation. 
I wonder whether similar accelaration would be possible in classic environments like <code>Pendulum-v0</code>.</p>
"
939,"<p>There's a real possibility that self driving cars become more than just a high tech novelty and they start changing the market.</p>

<p>As seen in <em>Logan</em>, self driven commercial freight might be among the first. This makes sense to me. The automotive industry might be harder to overtake, but truckers aren't highly qualified. Anybody can get a CDL.</p>

<p>Also in the movie, we see that the trucks aren't perfect. One causes a large accident, and sets spooked horses running frantically across the highway.</p>

<p>Sam Harris has questions about the fundamental flaw of a technology that is expected to take lives into its own hands autonomously existing when it has to make a choice between one life or another.</p>

<p>If a car is hurdling towards a mass of people on the side of the road through some mistake caused by road conditions, should it veer towards a nearby ditch where it may endanger its passengers?</p>

<p>That's just one example of this. I've spoken with my friend about this a bit, and we've talked about Asimov's laws of robotics, which I think are a good starting point. As far as I know, there's four laws. Feel free to add if I've missed any:</p>

<ol>
<li>A robot cannot harm humanity, or through inaction allow humanity to come to harm.</li>
<li>A robot cannot harm a human, or through inaction allow a human to come to harm, except where this would conflict with the zeroth law.</li>
<li>A robot must obey any orders given to it by a human, except where this would conflict with the zeroth or first laws.</li>
<li>A robot must do everything it can to protect itself, except where this would conflict with the zeroth, first, or second laws.</li>
</ol>

<p>Under this simple system, we already have our automated cars choosing to save a group of lives over one life. Discussion about the worth of elderly lives versus children's lives I think is unimportant.</p>

<p>No company should be building machines that prioritize police over civilians or civilians over criminals. Nobody over 60 is getting in the Nissan they know cares more about everyone that's not in the car than it does it's passenger. The job of the machine is to make unbiased choices between saving one life or two. And it should always pick two.</p>

<p>But one-for-one choices are going to likely be determined by factors like what action is the easiest to perform at X miles per hour. Of course it makes the most sense that the vehicle itself is at the bottom of its own list of priorities. It's a tool that can be easily replaced.</p>

<p>But there's one thing I've not mentioned yet, and that's the processing speed of the on board computer. If self driving cars prove incapable of making split second decisions because they take their sweet time analyzing, we won't want to ride them.</p>

<p>Rather interested in philosophy and ethics, the programming part is where my knowledge is lacking. I can assume some basic facts. You'd have to make space in the vehicle for a computer, some kind of image recognition software would need to tell what's a human. Perhaps pets and whatnot could also be included.</p>

<p>And maybe it wouldn't be unreasonable to think since the passengers don't need to look out the windshield, that it could be replaced with a widescreen TV, or some other ancillary features.</p>

<p>Could we bridge the gap in knowledge here, and somebody give me an idea of whether this kind of thing is even possible?</p>
"
940,"<p>I've only just started looking into Machine Learning, and so far most of the examples I've seen involve starting with a training set of observations, each representing a value for a number of 'features', and using that to train a model - then evaluating the trained model against a second test set of data to allow refinements to the model. The basic operation the model is asked to do is to predict the value of one feature given others.</p>

<p>When talking about art (by which I would include music, poetry, etc), we can no longer assert that things are correct - but perhaps we could isolate a number of subjectively-judged features, such as whether a work is 'mysterious' or 'joyful' to a particular human judge, and then train the model to our judge's taste. However, The problem here would seem to be that having a human being add judgement features to each of our data points sounds like a very slow task, and it might often be impractical to get to the number of 'assessed works' that would allow the model to be trained.</p>

<p>How could the process of training a model to produce art of a certain 'taste' be done in a practicably short space of time?</p>
"
941,"<p>Let's say I train an RNN on the sequence of characters in <em>War and Peace</em>. After doing so is there a way I can take an arbitrary set of letters, e.g. HNTASK, and have it (ideally) tell me that the most probable sequence of that set is THANKS? </p>

<p>Also the algorithm must use ONLY the characters in that set; i.e. I don't want it to guess that <em>U</em> comes after <em>H</em>, even if <em>U</em> appeared in the training examples. After each character it can only pick from remaining ones. I would assume one would somehow disable the illegal output neurons to do this.</p>

<p>(I know there are far simpler algorithms for transforming jumbled-up letters to English words. This is just a toy example for an important problem.)</p>
"
942,"<p>My question assumes that a private researcher doesn't have access to anything stronger than a modern PC with a high end GPU to implement his projects. He can also use cloud computing but with limited funds as well.</p>

<p>Is it still feasible to do research with those restrictions? <a href=""http://www.businessinsider.de/heres-how-much-computing-power-google-deepmind-needed-to-beat-lee-sedol-2016-3"" rel=""nofollow noreferrer"">AlphaGo used 1,202 CPUs and 176 GPUs</a> to beat Lee Sedol. Is this enormous power only required to achieve the final optimizations or have we already reached a state where high end research can only be done with larger funding?</p>

<p>Please include in your answer examples of recent research results and the required infrastructure that was used to create them.</p>
"
943,"<p>I want to know what is the difference between a normal chip or processor and a processor designed for AI?</p>
"
944,"<p>It has been shown that it is possible to use unsupervised learning techniques to produce good feature detectors in CNNs. I can't understand what drives specialization of those feature detectors. In publications <a href=""https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"" rel=""noreferrer"">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a> (page 6) they show a very reasonable set of edge and blob detectors with little or no overlap. It goes against intuition - in absence of incentive to specialize, one would expect at least some duplication of learned kernels.</p>
"
945,"<p>I am a first-semester grad student in Robotics and have taken a course on machine learning for robotics. I am completely new to machine learning.</p>

<p>I am to select and execute a problem statement on my own as a part of the course. I have selected the following problem statement -</p>

<p>""Reinforcement learning for mapless navigation of mobile robots"" based on this <a href=""https://arxiv.org/pdf/1703.00420.pdf"" rel=""nofollow noreferrer"">research</a>. The goal is to develop a mapless motion planner which enables a robot to navigate by avoiding obstacles.</p>

<p>Since I am completely new to Machine learning and AI, I am not able to gauge whether the problem statement is challenging enough for a beginner to execute in a timeframe of 8 weeks? (given that I can dedicate around 6 hours a week)</p>

<p>I am planning to code in MATLAB since I am highly comfortable with MATLAB</p>

<p>Also, please leave any suggestions/ modifications to refine my approach and have a better understanding in this area. </p>

<p>Thank you!</p>
"
946,"<p>I'm currently in the research stage of building a web app in ASP.NET where the user can input a URL to an Amazon product, then the app would determine how likely its reviews are to be genuine. I need help figuring out what algorithm to use in determining if a certain review is likely to be deceptive. I want my app to behave similarly to Fakespot or ReviewMeta. I realise a tool like this won't be 100% accurate and that's fine.</p>

<p>So far I read parts of <a href=""https://www.amazon.co.uk/Language-Processing-Prentice-Artificial-Intelligence/dp/0131873210"" rel=""noreferrer"">this book</a> which seems to be recommended a lot to NLP newbies but haven't found anything that applies to such a specific problem. I also read <a href=""http://www.aclweb.org/anthology/P14-1147"" rel=""noreferrer"">this article</a> but it's based on hotel, restaurant and doctor reviews. I'm trying to find a more general method that can be applied to any product. Any help would be greatly appreciated!</p>
"
947,"<p><a href=""https://ai.stackexchange.com/questions/10/what-is-fuzzy-logic"">This question</a> covers in detail, what fuzzy logic is and how it relates to other math fields, such as boolean algebra and sets theory.</p>

<p><a href=""https://ai.stackexchange.com/questions/118/how-can-fuzzy-logic-be-used-in-creating-ai"">This question</a> is also very related, but the answers are focused more on general intuition and <em>potential</em> applicability. The only working system based on fuzzy logic, mentioned there, is <a href=""https://en.wikipedia.org/wiki/Mycin"" rel=""noreferrer"">MYCIN</a>, which goes back to the early 70s. This quote from wiki summarizes my impression of it:</p>

<blockquote>
  <p>MYCIN was never actually used in practice.</p>
</blockquote>

<p>From my experience in AI, the best tool to deal with uncertainty is <a href=""https://en.wikipedia.org/wiki/Bayesian_probability"" rel=""noreferrer"">Bayesian probability</a> and inference. It allows to apply not only a wide range of probabilistic tools, such as expectation, MLE, cross-entropy, etc, but also calculus and algebra.</p>

<p>Can you call fuzzy logic a ""pure theoretical"" concept, which only played its role in the early development of AI? Are there real practical applications of fuzzy logic? What problem would you recommend to solve and to <em>code</em> using fuzzy logic?</p>
"
948,"<p>I'm currently developping an application which allows psychologists to manage their schedule and budget. As a proof of concept, I would like to create an intelligent appointment service. There can be 3 cases:</p>

<ol>
<li>I know the client, I need to guess the day and time for his next appointment</li>
<li>I know the day, I need to guess which client and at what time</li>
<li>I know nothing, I need to guess which client, which day and what time</li>
</ol>

<p>I'm currently in the process of learning deep learning algorithms just to get a bit of theory, but it's a little bit overwhelming.</p>

<p>There are features I know I can extract from the appointments:</p>

<ul>
<li>Day preference in the week (always on monday, say)</li>
<li>Reccurence (every two weeks or such)</li>
<li>Nb of days since last appointment</li>
<li>Whether the client was present or not to his last appointment</li>
<li>etc..</li>
</ul>

<p>I know there are things like ""features extraction"" that you can train a neural network to find the features itself, but all examples refers to image recognition or speech analysis.</p>

<p>I want the algorithm to train on the existing and future appointments (stored in a MongoDB). I would also like that the algorithm trains live, that is if it proposes an appointment to the user and the user takes it, it should train positively. On the other hand, if the user navigates or change any parameter, the algorithm should adjust its weights accordingly.</p>

<p>I also know I should start by extracting data from the DB that will be transformed in a vector or matrix, then the algorithm is supposed to train on that data. </p>

<p>Is this correct? How can I start and what kind of architecture do I need?</p>
"
949,"<p>I have to model an AI that should be able to understand clues and find the answer from a specified word database. I came across several papers that solves the problem with training neural networks or processing the clues with training several machine learning architectures on clues that exist on several databases. However, they all seem overkill for the course currently I'm taking.</p>

<p>So, what would be the most simple approach to solve that problem?</p>
"
950,"<p>Suppose I have CFG L that pumps out words in that language. I want a machine learning system that can detect if a word w came from L or not. It has access to a stream from L that is constantly producing words in L at random. In this case, the system can only be trained with positive examples. The system also has access to a verifier for L. The system (if it chooses to) can generate strings and verify that the string is in L or not. However, the system could just cheat and just use the verifier, so during the test phase, it gets disabled (during the learning phase it use the verifier as much as it wants). </p>

<p>Moreover, L can be augmented and changed, and the system should adapt to that change.</p>

<p>For instance, let's consider this regex (a regex is a CFG): <code>(aa)*</code></p>

<p>This language only produces a string of <code>aa</code>'s of even length. Suppose I modified the regex to <code>(aa|bb)*</code> This system should adapt to recognize this new language.</p>

<p>What kind of methods/approaches should I consider in my design?</p>
"
951,"<p>Let's say we already have an intelligent system (as a computer program). We give it some simple data from outside world, and system tries to predict it. The question would be: what kind of patterns this system should be able to recognize?</p>

<p>For example, system gets binary data stream(zeroes and ones).<br>
1) Should it recognize patterns like: 1 0 11 0 111 0 ....?<br>
2) Should this system be able to recognize prime numbers in ascending order written with ""1"" with ""0"" between them?</p>

<p>Is there any research about it?</p>
"
952,"<p>Being by no means an expert in the field, I do however know of solvers such as Z3 that can generate a correct program provided you can express your constraints as a set of logical rules, such as <a href=""http://barghouthi.github.io/articles/17/synthesis-primer.html#test-driven-synthesis"" rel=""nofollow noreferrer"">here</a>.</p>

<p>However, in the example given above, logical rules are designed ""by hand"" from a set of examples, so the ""compilation step"" from the examples to the set of rules must be done in the user's head.</p>

<p>Does a system automating this ""compilation step"" exist (i.e automated rule set deduction from a set of examples), that would allow the user to see and make use of the set of rules?</p>
"
953,"<p>Image recognition can be used to classify images. But I wanted to find few parameters like height of person, his legs, his hand etc. Will CNN helpful for this type of output ?</p>
"
954,"<p>My network works on 32x32 normalized (translationally) but noisy images. Its task it to determine whether image has simple symmetry (horizontal/vertical). It needs to be reasonably robust to rotation (up to 20 degrees). </p>

<p>I approached this task with a simple perceptron-like net with 2 hidden layers. It performs reasonably well (on limited amount of data that I have) but I can't shake off the feeling that this design is absolutely the worst for what i need.</p>

<ol>
<li>It is hard to judge generalization capacity of the net (overfits
really easily) </li>
<li>It performs alot worse than a simple deterministic
program that i wrote for the same task</li>
</ol>

<p>Symmetry is such a simple concept but my neural network (being feed forward type) can't represent it efficiently. What I have in mind is a kind of RNN that decides what axis to fold the image along and then judging on how well folded parts of image match. Are there papers on something like that?</p>
"
955,"<p>While working with darkflow, I encountered something that I can't understand.</p>

<p>I understand that maxpooling with size=2,stride=2 would decrease the output size to half of its size.</p>

<p>However, if the max-pooling is size=2,stride=1 then it would simply decrease the width and height of the output by 1 only.</p>

<p>However, the darkflow model doesn't seem to decrease the output by 1.</p>

<p>Here is the model structure when I load the example model <code>tiny-yolo-voc.cfg</code>.</p>

<pre><code>Source | Train? | Layer description                | Output size
-------+--------+----------------------------------+---------------
       |        | input                            | (?, 416, 416, 3)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 416, 416, 16)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 208, 208, 16)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 208, 208, 32)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 104, 104, 32)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 104, 104, 64)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 52, 52, 64)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 52, 52, 128)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 26, 26, 128)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 26, 26, 256)
 Load  |  Yep!  | maxp 2x2p0_2                     | (?, 13, 13, 256)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 512)
 **Load  |  Yep!  | maxp 2x2p0_1                     | (?, 13, 13, 512)**
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)
 Init  |  Yep!  | conv 3x3p1_1  +bnorm  leaky      | (?, 13, 13, 1024)
 Init  |  Yep!  | conv 1x1p0_1    linear           | (?, 13, 13, 125)
-------+--------+----------------------------------+---------------
</code></pre>

<p>The bold text part is causing the confusion. My expectation what (?,12,12,512) but it is not. It retains the same size (13,13)</p>

<p>The corresponding model info from the <code>.cfg</code> file is:</p>

<pre><code>[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=1
</code></pre>

<p>Why is the output height/width not decreasing by 1?</p>
"
956,"<p>After obtaining the result of algorithm fuzzy c means on the iris flower data set. Results are to be checked for their correctness. So, how to derive it in matlab. I mean code for that. please suggest.</p>
"
957,"<p>I was reading a little bit about the Deepstack poker program:</p>

<p><a href=""https://www.ualberta.ca/science/science-news/2017/march/artificial-intelligence-deepstack-outplays-poker-professionals"" rel=""nofollow noreferrer"">DeepStack the first computer program to outplay human professionals at heads-up no-limit Texas hold'em poker</a></p>

<p><a href=""https://arxiv.org/pdf/1701.01724.pdf"" rel=""nofollow noreferrer"">DeepStack: Expert-Level Artificial Intelligence in
Heads-Up No-Limit Poker</a></p>

<p>The first article mentions processor used was an NVIDIA GeForce GTX 1080 graphics card, but I'm trying to get an understanding of how much memory this type of AI requires.</p>

<p>Related: How much has recent work in mathematical analysis of poker contributed to Deepstack and other strong poker AI? </p>
"
958,"<p>I have been working for ages on a Neuroevolution AI program, where cars learn how to race around a track.  Presently, I have a rudimentary fitness function that awards points for every degree traveled in the CW direction about the <strong>center of the window</strong> (removes points in CCW dir) and removes a certain amount of points for every collision that occurs. Cars also lose points for every moment they stay still and/or are colliding with something. My end goal is to create cars that can complete a track full of obstacles, faster than a human controlled car.</p>

<p>Q: Is there a  better fitness function that would result in more efficient cars that: <strong>1. make better use of their sensors</strong>, <strong>2. are efficient in getting around the track (don't weave like a drunk driver)</strong> and <strong>3. are faster in general (cars are too cautious...this is a race dammit!)</strong>
<em>(Half of the population seems to just spin around.)</em>  </p>

<p><em>Edit</em>: I have fully implemented my Neuroevolution program. <a href=""https://youtu.be/hnrQV0nKJxU"" rel=""nofollow noreferrer"">My implementation</a> However, you'll find that it isn't perfect.  My question is <strong>how should I alter my fitness function to generate better driving cars</strong>.  Currently, the cars will only turn left or right if the outer sensors are activated.  However, if the sensors directly in front are activated, the neural network is configured in such a way that ""ignores"" this signal. So the cars crash head-on into obstacles but avoid obstacles in their periphery.  I think this is due to the fact that the fitness function (which gives points for the displacement in 3 second time period) is too generous and removes the incentive for cars to avoid obstacles.  I've tried altering the punishment for collisions and the reward for driving in the right direction but it still isn't performing the way I would like it to.
<br></p>

<p><a href=""https://i.stack.imgur.com/22rM5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/22rM5.png"" alt=""Population of Cars in Racing Ring""></a>
<a href=""https://i.stack.imgur.com/ER7uW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ER7uW.png"" alt=""Revive mode (revisiting past generations of cars""></a></p>
"
959,"<p>I recently started working on very simple machine learning codes in Python and I came across a big problem: teaching the system to improve on its guesses.</p>

<p>So this is what the code is about: 
I will have a list of organisms with their features stated in numerical values. I want to write a code that identifies that whether the organism is a cat or a fish or neither based on their characteristics. (For example an organism with a high fur value and 4 legs is more probable to be a cat.)</p>

<p>My idea for the neural network is to have five input nodes(for the five characteristics) and 2 output nodes (one for how cat it is and one for how fish it is). The input nodes are multiplied by a weight value and then all summed together to produce one of the output nodes. This repeats itself for the other output. How much the system got wrong is just the difference between the value of the output nodes and how cat/fish the being actually is. </p>

<p>But how can I use this information to correct the weights of the input nodes? Since the weights are randomly generated they can be in the ""wrong"" directions to begin with. For example if the subject is a cat then we should be expecting high fur and leg values. But what if the weight for the leg value is negative while the weight for the fur value is positive? Adding or multiplying the weight by the error won't bring us any closer to accurately determining the being. Is my neural network flawed to begin with? Or is there a rule of thumb in choosing back propagation algorithms?</p>

<p>Thanks.
<a href=""https://i.stack.imgur.com/0LDQ0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0LDQ0.jpg"" alt=""the neural network""></a></p>
"
960,"<p>i've been trying lately to search the internet to find a result for this but no useful results, unfortunately, we're making an uber-like application during our discussion we needed a way to validate a rider's rating after he completes his ride with a taxi, but the main question is, does uber use an algorithm to do so? and how?</p>
"
961,"<p>I'm working on a system which reads large data from Excel and tries to interpret something from it. If it doesn't understand something, the user will correct it and slowly system will learn and try to do everything itself. Is there any AI SaaS or anything that can help in this?</p>

<p>The use case is, I'm reading bank statements from different banks and they all have different types of narration for similar transactions. I want to put them all into an accounting software without any human intervention.</p>
"
962,"<p>Traditionally, ""<a href=""https://en.wikipedia.org/wiki/Strong_AI"" rel=""nofollow noreferrer"">strong AI</a>"" refers to <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">Artificial General Intelligence</a>, the human mind understood as an algorithm (<a href=""https://en.wikipedia.org/wiki/Chinese_room#Strong_AI"" rel=""nofollow noreferrer"">Searle, Chinese Room</a>) and <a href=""https://en.wikipedia.org/wiki/Artificial_consciousness"" rel=""nofollow noreferrer"">Artificial Consciousness</a>. </p>

<p>But recent advances in Artificial Intelligence, most notably breakthroughs involving Machine Learning and Neural Networks, have led to strong AI being used in the context of an algorithmic intelligence that can outperform the top humans in specific tasks.  (This is sometimes qualified as ""strong narrow AI"", but the qualifier seems less used time goes on.) </p>

<ul>
<li>Because the traditional usage of strong AI are based on theoretical concepts, where the current usage is based on practical, real-world results, does the new usage of ""strong AI"" supercede the traditional usage?  </li>
</ul>

<p>If this is the case, at what point does the Wiki need to be updated?  Is a major paper focused on re-coining the term necessary to formally reference the new usage of the term, or are numerous, informal uses sufficient?</p>

<hr>

<p>Related: ""Strong"" has a precise, analytic, formal usage in the context of <a href=""https://en.wikipedia.org/wiki/Solved_game"" rel=""nofollow noreferrer"">solved games</a>.  This game theoretic usage is similar in spirit to new usage of strong AI in that it connotes results that can be validated. </p>
"
963,"<p>Is there a technical term for an image classifier that classifies on a single class but is classifying on an amount like how full a glass of water is rather than different classes?</p>
"
964,"<p>I am having issues getting started with a multi class problem with multiple features and hoping someone could please point me in the right direction. </p>

<p>I have data that is structured like this for training:</p>

<pre><code>Item State Code1 Code2 Code3 Route
---  ---   ---   ---   ---   ---
item1 MI   A1    33    blue  Route1
item2 TX   A3    35    yellow Route2
item3 NM   A4    36    green  Route3
item4 NM   A4    37    green  Route3
</code></pre>

<p>Essentially I am trying to figure out where to even start. The goal is to know where to route the Items based on the features State, Code1,2, and 3. The route is dependent on a mix of the codes and state, and I want to build a model that says when I have code X, Y, Z and Color XX, then it is probably route 1 (some routes of course in the training data might have X, Y as codes and a different Z) </p>

<p>I am assuming I will need to one-hot encode the features like the State and codes? But from there does anyone know of which type of model I should go for? I would assume a Neural Net of some kind, I've explored CNN's and Random Forrest. </p>
"
965,"<p>I am looking for something similar to IBM Watson but open source.  </p>
"
966,"<p>I'm trying to develop a project that recognises handwriting and converts into text. What are the algorithms and tools to be used?</p>
"
967,"<ol>
<li><p>How to train darkflow for my custom object really really fast during debugging in quad core PC and without GPU? (Can I train with about 10 images and test with only those images, just to check if all convolutions are working as expected. And with 20 epoch?). There is only once class and all license plates are similar in pattern with varying in angle and its digits in plate.</p></li>
<li><p>I am using tiny yolo config and weight. So, what all parameters I should tune in yolo based .cfg file to do it? I feel if TV like object can be detected with same training weight and config for tiny yolo then license plate too.</p></li>
</ol>

<p><strong>Overview:</strong>
I am training for object, license plate, with darkflow. I tried with about 100 custom datasets I had created. This is only for initial POC, actual implementation will have more number of images. And upon testing with test images with trained graph, object is not highlighted rather highlighting squares are shown at random location within image and random in number, starting with 2-3 square boxes to lot many number. But non of those were highlighting to the actual license plate object. It took me 20 hour of training time to verify it. I used training images for testing as well, and also a plane black screen images to test what's going on. But highlighting squares are still random in number and location even on blank screen image.</p>
"
968,"<p>I  recently became interested in how creativity is generated in NN. My understanding of NNs is that the output always known, given that it they are trained with target values, but how does one train a network to be creative, I mean in such cases would the task be to create something novel, but the actual target would not be known.  How does creativity express itself in a rule based system, such as NN?</p>

<p>One would need to redefine the cost-function, but how does one imply creativity in a rule based system?</p>
"
969,"<p>I'm working on a content based recommendation engine for ebooks. I create document vectors with 300 features for every ebook using a word2vec model trained on google news and determine recommendations based on the closest vectors. So far I have run tests on a dataset consisting of 200 books from the gutenberg project in four different categories.</p>

<p>I find that a small group of books appears to be recommended a lot more then others. The most recommended book is recommended for over half the dataset. This book is Theaetus, in which the <a href=""https://gist.github.com/HartgerV/50ccdb999393eb1fd2b50725e14cd0fa"" rel=""nofollow noreferrer"">most commmon tokens</a> are fairly specific to the book (tokens like socrates, theaetus). I can find no intuitive reason why this book would match so well with over half my dataset. </p>

<p>Is this common behavior when using document vectors to determine similarity? Are there any methods that would reduce this effect?</p>
"
970,"<p>I've gotten curious about this topic and am wondering what the stack exchange community has to say about it. Also, does anyone know of any professors/researchers who have published papers pertaining to this?</p>
"
971,"<p>How are the layers in a encoder connected across the network for normal encoders and Auto-encoders in deep neural networks</p>
"
972,"<p>I am trying to replicate the <code>FCN</code> concept described <a href=""https://www.youtube.com/watch?v=nDPWywWRIRo&amp;t=16m30s_"" rel=""nofollow noreferrer"">here</a> for semantic segmentation. It seems people have successfully trained such models by removing fully connected layers from the popular pre-trained models such as <code>VGG</code> and adding upsampling and unpooling layers.</p>

<p>I understand that <code>transpose convolution</code> and <code>unpooling</code> in upsampling layers provide counterparts for <code>convolution</code> and <code>max/avg pooling</code> in earlier downsampling layers resp., but what are the counterparts for non-linearities such as <code>ReLU</code>? What about <code>dropout</code>? There seems to be no discussion of this in the video.</p>

<p>Thanks</p>
"
973,"<p>I was going through ""<a href=""https://www.tensorflow.org/tutorials/wide_and_deep"" rel=""nofollow noreferrer"">Wide &amp; Deep Learning</a>"" tensorflow tutorial &amp; it's quite simply explained the process. But I missed few of the things. If someone can please explain them to me, it will be of great help:</p>

<p>1) Why <code>occupation</code> and <code>native_country</code> we are using <code>tf.feature_column.categorical_column_with_hash_bucket</code> and again using <code>tf.feature_column.embedding_column</code>?</p>

<p>2) Why in <code>tf.feature_column.embedding_column</code>, we are taking <code>dimension=8</code>, even though they have more unique values?</p>

<p>3) Why we are using <code>crossed_columns</code> variables? In the document there's an explaination given. But I'm not fully understanding it.</p>

<p>The questions might sound silly, but I'm not sure of the answers to these questions.</p>

<p>Thank you!</p>
"
974,"<p>How can l leverage Artificial Intelligence and Virtual Reality to create Intelligent Automatic Story Generation?</p>

<p>My idea is to come up with a system that is empowering the user’s imagination to create and experience stories that engage and immerse them. So they can have the control over the story creation/narrative?</p>

<p>A system that could enable the followings use case:</p>

<p>1- We sit down in our entertainment room. Select a genre from a menu. Pick the character types for the cast. Decide on a generic plot or give the system the option of picking a random choice. Sit back and enjoy.</p>

<p>2- Let's say, I want to watch a sci-fi movie set in Paris with Marlon Brando and Jim Carrey. Then the system instantly generate such movie.</p>

<p>It has come to my attention that SoarTech developed a pattern-of-life capability for the U.S. Army Simulation: <a href=""https://youtu.be/CitevFwa3TE"" rel=""nofollow noreferrer"">Agent-based Patterns of Life for Virtual Environments</a></p>

<p>That the university of Twente in Netherlands has been working on a project called the “Virtual Storyteller”.</p>

<p>Here is a research paper entitled <a href=""http://wwwhome.cs.utwente.nl/~theune/PUBS/Narrator-final-clin17.pdf"" rel=""nofollow noreferrer"">The automatic generation of narratives</a>.</p>

<p>And more recently that IBM has developed a speech recognition sandbox</p>

<p>I know that the answer probably requires tech, AI, speech, VR and AR, and Watson integration specialists but any help is welcome.</p>

<p>Watson probably won't be a necessary component of the solution but as long it us an efficient voice recognition software with speech to animation features.</p>
"
975,"<p>For example...</p>

<p>1) If a dog is crossing the road, I'd expect the car to try to avoid it. But what if this leads to .00001% more risk for the driver? What is the 'risk cut-off'?</p>

<p>2) What if a cockroach is crossing the road? Will the car have a list of animals okay to run over?</p>

<p>3) What if a kid is crossing the street and avoiding it would kill the driver?</p>

<p>These questions seem to not really have an answer, yet self driving cars are almost ready. What are they doing about all of this?</p>
"
976,"<p>I want to use deep learning to estimate the value of a function based on some data. However, the loss function would be neither convex nor concave. Can I know if it is a big deal in deep learning? Is training a deep network, when loss function is convex, the same as optimizing a convex problem or not? I would be thankful if any paper has addressed this issue. </p>
"
977,"<p>If you walk your dog, he'll be happy. If you don't walk him, he'll chew up your furniture. If he's happy or if he chews your furniture, he'll mess up your apartment. If he messes up your apartment, you'll be annoyed. 
Does your dog mess up your apartment?
Are you annoyed at him?</p>

<ol>
<li>Write the knowledge base given above in propositional logic.</li>
<li>Convert the knowledge base into conjunctive normal form.</li>
<li>Use resolution to prove by contradiction that your dog made a mess. Connect the two clauses to a new clause that the can be collapsed into (see the example figure).</li>
</ol>

<p>Example figure: <a href=""https://i.stack.imgur.com/l2LGL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l2LGL.png"" alt=""Example figure""></a></p>
"
978,"<p>I'm looking to design a neural network that can predict which runner wins in a sports game, where the amount of runners varies between 2-10. In each case, specific data about the individual runners would be fed into the neural network.</p>

<p>What design would be most advantageous for such a neural network? </p>

<p>Essentially this is a ranking problem where the amount of inputs and outputs are variable.</p>
"
979,"<p>Is there any existing software which analyzes idea flow in the text (or between different texts, e.g scientific articles) and for example visualizes it with some graph?</p>
"
980,"<p>I have a large dataset of vehicles with the ground truth of their lengths (Over 100k samples). Is it possible to train a deep network to measure/estimate vehicle length ? I haven't seen any papers related to estimating object size using deep neural network.</p>
"
981,"<p>I'm working on a image classification problem using neural-network. In the training data set, 90% of the samples fall into 10% of all categories, while 10% of the sample fall into the other 90% categories. So example is not evenly distributed among all categories. If we assume this distribution reflects the real world distribution, do I need to filter my dataset before training so that each category has similar number of samples?</p>

<p>Thanks a lot!</p>
"
982,"<p>Looking for an explanation of the linear regression estimation method in deep learning.</p>
"
983,"<p>I selected the below data set for forecast and predict using artificial neural network as my final year project. <a href=""https://archive.ics.uci.edu/ml/datasets/Bank+Marketing"" rel=""nofollow noreferrer"">https://archive.ics.uci.edu/ml/datasets/Bank+Marketing</a>. I normalized the data set and try to train the network using matlab anntool. But I didn't get a good result. The data didn't plot along the curve. I want to train the network and predict for new input values. What is the matter with my network train. Please help me.<a href=""https://i.stack.imgur.com/qv0Pb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qv0Pb.png"" alt=""enter image description here""></a></p>
"
984,"<p>I’ve been experimenting on several datasets and found something very strange while implementing ML.<br>
I’ll Explain after the code…</p>

<pre><code>import numpy as np
from sklearn import datasets
iris = datasets.load_iris()

# 4 features in np array - 150 rows

case = 1        # change cases to see variation

if case == 1:   # first feature deleted
    iris.data = np.delete(iris.data,0, 1)

if case == 2:   # first 2 features deleted
    iris.data = np.delete(iris.data,0, 1)
    iris.data = np.delete(iris.data,0, 1)

if case == 3:   # first 3 features deleted (1 feature left)
    iris.data = np.delete(iris.data,0, 1)
    iris.data = np.delete(iris.data,0, 1)
    iris.data = np.delete(iris.data,0, 1)

if case == 4:   # only second feature deleted from np array
    iris.data = np.delete(iris.data,1, 1)

if case == 5:   # only third feature deleted from np array
    iris.data = np.delete(iris.data,2, 1)

if case == 6:   # only last feature deleted from np array
    iris.data = np.delete(iris.data,3, 1)

# print iris.data
# exit()

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
pred = gnb.fit(iris.data, iris.target).predict(iris.data)
# pred = gnb.fit(iris.data, iris.target).predict(test_data)

from sklearn.metrics import accuracy_score
print accuracy_score(iris.target, pred)
</code></pre>

<p>I’m using the basic fisher iris dataset from sklearn, it has 150 rows and 4 columns (features).<br>
Using training data as test data.</p>

<p>So i tried to remove a few features and see if accuracy changed.
And I thought it would.<br>
But till case <strong>1</strong>, <strong>2</strong>, and <strong>3</strong>, I removed 1, 2 and 3 features respectively and there was no change in the accuracy. It stayed 96%.</p>

<p>Then on running cases <strong>4</strong>,<strong>5</strong> and <strong>6</strong>, The accuracy changed. Why?</p>

<p>On comparing case <strong>2</strong> and <strong>4</strong>,<br>
Both have second feature removed from the dataset, so clearly, removing second feature is responsible for change in accuracy (as seen in case <strong>4</strong>)<br>
So why does it not change in case <strong>2</strong>?<br>
Just because it had first feature removed too? to balance out the second? (If that were true, case <strong>1</strong> would have given different accuracy)<br>
Why does accuracy not change in first 3 cases, but it does in the last 3 cases?</p>

<p>Is ML dependent on the order in which features are feeded to the algorithm?</p>

<p>What am I missing here?</p>

<p>Would be great if someone could clear this doubt.</p>

<p>Thanks!</p>
"
985,"<p>In the paper <a href=""https://arxiv.org/pdf/1706.01905.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.01905.pdf</a> they describe the noise that they add to the parameter vector as: <a href=""https://i.imgur.com/geFValY.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/geFValY.png"" alt=""""></a>
is ""I"" simply the identity matrix or am i missing something?</p>
"
986,"<p>Without using any of Matlab's neural network tools, I'm writing a program to simulate an OR gate with a perceptron. I have seen many tutorials, but I still can't understand why we need weights to train a perceptron for such a simple purpose.  </p>

<p>One way is to program the perceptron with the conditions <code>(0,0)=0. (1,0)=1. (0,1)=1. (1,1)=1</code>. So the two inputs to the perceptron would be either zeroes or ones. I don't see the purpose of weights here. Assuming weights are 1, for the second training example, the output would be <code>1*1 + 0*1 = 1</code>. For the last example, it would be <code>1*1 + 1*1 = 2</code>. So an activation function which says <code>if output &gt;= 1, output = 1 else output =0; end</code> should suffice. This would successfully simulate an OR gate. So why do I need to ""train"" any weights?    </p>
"
987,"<p>In the hill climbing algorithm, the greater value, compared to the current value, is selected, but I cannot understand why it takes the larger value instead of the smaller one. Why is that?</p>

<p>I greatly appreciate the inclusion of figures in your answers.</p>
"
988,"<p>I'm starting to get into the AI field and wanted as a side project to try and make an algorithm to play old games. The problem is i need to pass everything that is happening on the screen to the program and there is no real api to do that.
Does anyone know of a way to get the input any other way than using image recognition?
Thank you in advance.</p>
"
989,"<p>in my research on videogames path finding I'm using ant colony optimization. not only to find shortest path, to add some unpredictability and adaptiveness  to bots path finding. it works the way as players move in the map, they add some fromone to map. so it adds up probbality that bots choose path like players. I have sent the paper but judges said you need a benchmark and distinguish to previous works. so can you tell me how can I benchmark this work?</p>

<p>thank you for helping </p>
"
990,"<p>I am new to this field and would like to know that for what kind of data types other than images, recommendation system can be created using machine learning. Suppose for contents like audio or video, is it necessary to use data set of actual audio files or video files or just text information about the file is enough for this system ? </p>
"
991,"<p>I've been tinkering with an artificial life simulator, critterding for many years and lately ive been branching the project and adding fitness functions.</p>

<p>Im still quiet new to genetic algorithms and currently critters in the simulation have very complex and different sized dna files which i want to be able to cross, currently they can only reproduce asexually and evolve purely through mutation.</p>

<p>For example, thier genome consists of:
Box part dimensions
Joint locations and angle constraints which connect those boxes
And a list of neurons, thier properties and synaptic connections</p>

<p>The number of boxes and thier joints can change by mutation as well as the number of neurons and synaptic connections and so im having a hard time trying to think up a crossover approach which can handle these variables. </p>

<p>So my question is, can anyone point me in the right direction for research material or give me insight into handling such dna structures for crossing? </p>
"
992,"<p>This might sound silly to someone who has plenty of experience with neural networks but I bothers me...</p>

<p>I mean randomising initial weights might give you better results that would be somewhat closer to what trained network should look like, but it might as well be exact opposite of what it should be, while 0.5 or some other average for the range of reasonable weight value would sound like a good default setting...</p>

<p><strong>Why do initial weights for neurons are being randomised rather than 0.5 for all of them?</strong> </p>
"
993,"<p>To classify images we are using the tensorflow incection v3 NN.</p>

<p>Is there a similar approach to classify sounds? As for example to be able to recognize the person who is talking or classify a general sound? </p>
"
994,"<p>I understand that in <a href=""http://old.sztaki.hu/~szcsaba/papers/RLAlgsInMDPs-lecture.pdf"" rel=""nofollow noreferrer"">Reinforcement Learning algorithms</a> such as <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer"">q-learning</a>, to prevent selecting the actions with greatest q-values too fast and allow for exploration, we use eligibility traces. </p>

<p><strong>Here are some questions;</strong> </p>

<blockquote>
  <p>Does epsilon-greedy solve the same problem?</p>
  
  <p>Do these two approaches aim to attain same objective?</p>
  
  <p>What are the advantages of each over the other?</p>
</blockquote>
"
995,"<p>AIs that rely on MCTS - like AlphaGo - create their decision tree as the game progresses. Do they start from scratch each game and build a new tree or do they keep the tree and grow it from game to game further?</p>

<p>Besides the possible limitations of storage space for the search tree, I don't see any obvious drawback in keeping and growing the tree, which seems to me to be the preferred option. Are there other reasons to start from scratch each game?</p>
"
996,"<p>I know a little about these subjects. I found them similar to each other. Anybody can explain the differences between them?</p>
"
997,"<p>I need to write a recurrent autoassociative network in python . I have read <em>Explorations in Parallel Distributed Processing: A Handbook o f Models,Programs, and Exercises</em> but I can't understand how are the activation updating cycles written. any information about this would be helpful. specially if i could find an algorithm for this .</p>
"
998,"<p>Does fitness proportionate selection select multiple individuals? So i read on Wikipedia and on multiple stack exchange threads about fitness proportionate selection or rather roulette selection but what i dont understand is how do i not select multiple of the same individuals? Should i pop them from my array and recalculate the probabilities or after selection remove duplicates? Or is there some sort of purpose for having multiple of the same selected? </p>
"
999,"<p>I want to create a project where my Raspberry Pi Micro-controller can listen to my parrot talking and then talk back to him. I would need my rpi to listen to some of his phrases at least 100 times so that the pi could recognize that certain phrase and then say it back to him and also to know when to use it to talk back to him. I would also like to use it to teach him new phrases and new sounds. I would like to know if this is even possible and what hardware and software I would use to go about this? </p>

<p>I have already asked this in the Raspberry Pi stack exchange but have recieved no answers. </p>
"
1000,"<p>I want to implement <a href=""https://arxiv.org/abs/1607.04381"" rel=""nofollow noreferrer"">DSD: Dense-Sparse-Dense training for deep neural networks</a> by Han et al. In short, the paper suggest the following training scheme to improve the network accuracy:</p>

<p><a href=""https://i.stack.imgur.com/QkuUk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QkuUk.png"" alt=""enter image description here""></a></p>

<ol>
<li>Train as usual till convergence, </li>
<li>prune the network and train with sparsity constraint, </li>
<li>remove the sparsity constraint and let the pruned connections to recover.  </li>
</ol>

<p>My question is about step 2: train with sparsity constraint. The paper mentions training with a binary mask specifying the pruned weights to keep ""untouched"" so the sparsity constraint is satisfied, however that means implementing a dedicated layer that takes the binary mask as an additional blob and handles it accordingly.  </p>

<p>I wonder a simpler approach will give the same result: what if after the pruning step I keep the location of the pruned weights and then use dense training, but after each iteration to zero the originally pruned weights?</p>

<p>The forward path is the same, taking zero weights for the pruned weights anyway.  But would it negatively affect the backward path - since the constraint isn't there, or is it equivalent to the formal training scheme?</p>

<p>Thanks.</p>
"
1001,"<p>I have a liberal arts background so I need help understanding <a href=""https://arxiv.org/pdf/1708.09839.pdf"" rel=""nofollow noreferrer"">this paper</a>, particularly pages 26 to 30. The authors test a four-camera system for localization, mapping, and obstacle detection for self-driving cars. The paper seems to say the multi-camera system can map the environment to within an average of 7 cm (2.8 inches) of accuracy (with the largest error being 16 cm or 6.3) and detect obstacles to within 10 cm (3.9 inches) of accuracy. Am I getting this right?</p>

<p>Given that automotive lidar can detect objects to within 1.5 cm (0.6 inches) of accuracy, and given that for driving purposes the difference between 1.5 cm and 7 cm, 10 cm, or 16 cm seems quite small, can a multi-camera system be used instead of lidar in a self-driving car application? How do driving speeds affect things? What crucial elements of the problem space might I be overlooking or misunderstanding?   </p>
"
1002,"<p>I want to train a feedforward neural network to play a video game called Puyo Puyo 2, using reinforcement learning. More specifically, I'm trying Q-learning but I'm open to better alternatives.</p>

<p>In this video game, you start with an empty 12x6 board which you fill with falling pairs of colored blobs called puyos. When 4 or more puyos of the same color are connected horizontally and/or vertically, they disappear and the puyos above them fall down, potentially creating new groups of puyos that disappear and so on, creating a chain (here is a chain example : <a href=""https://puyonexus.com/chainsim/image/5SoqU.gif"" rel=""nofollow noreferrer"">https://puyonexus.com/chainsim/image/5SoqU.gif</a>). 
My goal is to teach the neural network to build the longest chains possible given a random sequence of puyo pairs. 
The game's information is incomplete though, as you only see the current and the next two pairs at a given time.</p>

<p>What is the best choice for the output layer: a layer with 22 neurons, one for each possible way to place a pair at a given time, and a softmax activation function, or a layer with a single output node, with a linear activation function? Or something else? Also what do you think is a good number of hidden layers and neurons per layer and what activation function should I use for the hidden layers?</p>

<p>What should my target Q-value be when I evaluate a move, so that I can train the network on each game situation? Right now I'm thinking about using the maximum chain possible after the move is done, divided by half of the number of moves played so far. This is because if you play optimally and you are lucky enough, you can increase your maximum chain by 1 every 4 puyos you place, that is every two moves. Thus my Q-value would stay between 0 and 1. 
What is the best update formula I can use? Is the one <a href=""https://en.wikipedia.org/wiki/Q-learning#Algorithm"" rel=""nofollow noreferrer"">on wikipedia</a> correct?</p>

<p>If I use a softmax output layer, how do I update the target Q-value vector? The update formula gives me an update but then the vector doesn't represent probabilities anymore. Can I add the update then take the softmax of the vector again just to make it probabilities again? </p>
"
1003,"<p>I read through the publication <a href=""https://deepmind.com/research/publications/mastering-game-go-without-human-knowledge/"" rel=""nofollow noreferrer"">Mastering the game of Go without Human Knowledge</a>. It doesn't seem to use GANs, just a new form of search and reinforcement learning.</p>
"
1004,"<p>It appears to me that the thoughts in our heads or at least mine are a result of the language they know. I do not have any thought that is not a consequence of the information that is contained within the language I know. I have thoughts that occur as the words we use in language and are particular to my voice  The structure of that language forms constraints as to the depth of thought that my mind can possibly come up with as far as I can percieve. Does this mean that an intelligence is limited to thoughts it's able to associate words with since the minds language is a result of an recursive association of words associated to those thoughts which depend upon a leap of faith between the observer and an undeveloped mind. Or is there simply a limit of conception bound by a language that is limited by its dictation and consequently a limit upon the perception of that so called intelligence. And thus a limit on any artificial creation or simulation of it bound by limits of dictation within that language and it's knowledge of it, and doesn't this mean that thoughts structure develops knowledge of language by association of the physical reciept of information external to it. So how do you approach an attempt at it's recreation through a formal language that allready has its own structure.</p>
"
1005,"<p>So, i need to use an MLP to predict a 12x12 matrix composed of floating points. 
The matrices are as this one that follows:
<a href=""https://i.stack.imgur.com/j8pfK.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/j8pfK.jpg"" alt=""enter image description here""></a>
Most matrices have this ""pattern"".</p>

<p>As input, i have 7 floating points, such as these:</p>

<p>""2.0"", ""0.23"", ""239.10"",""3.5"", ""12,0"", ""10.6"", ""0.62"".</p>

<p>To simplify the output matrix, i've converted it in a array with the 144 elements of the matrix.
So far i'm using MLPRegressor from Scikit.The problem is, there's absolutely no pattern in the predicted results.The predicted results include negative numbers, numbers really big and no ""pattern"" for the indexes whatsoever.
Is there a way to adjust these things in the model or the problem is on my dataset?
Thank you very much!</p>

<p><strong>Update:</strong></p>

<p>I wasn't very clear about the problem, so i'll try to explain it better.</p>

<p>About the problem: Predict water distribution in irrigation systems. I already have a program to simulate this, and works with rather good precision. The challenge now was to make an AI model to simulate this, instead of the old methods.</p>

<p>About the inputs: Inputs are one float representing the pressure of the sprinkler used to distribute water, two floats representing the speed and angle of the wind at the time data was collected. The 4 remaining are numbers representing some configuration of the sprinker (these numbers change very little between instances, but they do have impact in the result, which is why i decided to mantain them in the dataset).</p>

<p>About the output: Output is an array of 144 elements, representing the 12*12 matrix. Each element of the matrix contain a number representing the amount of water that was collected in that point. These collectors where evenly spread around the sprinkler. So the position of the matrix matter a lot, since in most cases the first and last lines and columns will have 0 or a close to 0 (but positive) number - this may vary depending especially on the speed and angle of the wind, but also on the sprinkler.</p>

<p>About the dataset: I have available 75 instances. They are all stored in a CSV file, where the 7 inputs and the 144 outputs are, each in one line.</p>
"
1006,"<p>I've been researching the following topic. Or rather, I would like to but I can't find anything because I'm not sure what to look for.</p>

<p>I am interested weather there are some concepts or models that explain how humans (or cognitive systems in general) trade off <em>remembering</em> what to do in a particular situation versus <em>thinking</em> about the board state and devising a new solution. Consider the following example:</p>

<p>A person plays chess. On their turn they must decide what to do. They could act according to one of the following extreme strategies:</p>

<ol>
<li>Remembering all possible board configurations and how the game ended (we'll call that <em>Memory</em>)</li>
<li>Use the known set of rules to <em>simulate</em> the game in their mind and choose the optimal path (<em>Optimization</em>)</li>
</ol>

<p>I'd say the best choice is a trade off between these two. But how? When to choose one of the strategies?</p>

<p>Is there a research field that is working on these kind of questions? What would I look for?</p>
"
1007,"<p>I have a huge text where a paragraph is written about a project execution. I need to parse that data and make sense out of it. For example the text states, ""Est. 200k USD has been committed on the event of sales summit on 8th Sep""</p>

<p>By this statement, i need to make sense of it as approx 200k USD is the revenue, There was a sales summit held on 8th Sep, The amount is already committed.</p>

<p>Likewise many such statements will be there. I need to parse and make a conclusion out of it. Is there anything anyone suggest?</p>

<p>What i have tried so far: Am trying to find a text analytics tool which will group the text into few classifications. Next am trying to write my own logic to tag each paragraph to certain master data. Also trying to use a tool by name orange which is currently helping visualize the data, but not make sense of it.</p>

<p>I think i need to be doing something with Machine learning algorithms. Not sure though.</p>
"
1008,"<p>I'm running A3C (Asynchronous Actor-Critic Agents) to learn a game where an agent needs to catch 3 rewards. The input of my network, among other things, is the relative position of the 3 rewards against the agent. </p>

<p>However, as the agent catches these rewards, my network can't just decrease its input size. What are ways of handling this?</p>

<p>My current solutions have been:</p>

<ul>
<li><p>set remaining features to zero - have tried, bad results</p></li>
<li><p>just show the closest reward - loss of information, might not be optimal</p></li>
<li><p>repeat existing reward positions to remaining features - we are giving more emphasis to some rewards (the ones that are repeated) over others</p></li>
</ul>
"
1009,"<p>I was wondering if anyone can think of any examples of integration of audio-visual inputs, or other modalities, failing or resulting in undesired outcomes in artificial cognitive systems?</p>

<p>I'm interested in any issues, or artifacts, that have arisen when different information about the same event have been integrated in such a system.</p>

<p>Published, or discussed in print online, or even a TED talk would be great sources if you know of any. Anecdotal evidence of such scenarios that you may have experienced are also of interest.</p>

<p>Thank you all for your time.</p>
"
1010,"<p>Could a multi-camera SLAM system for self-driving cars that is accurate to <a href=""https://arxiv.org/pdf/1708.09839.pdf"" rel=""nofollow noreferrer"">under 10 cm (3.9 in)</a> at parking lot speeds (i.e. very low driving speeds) retain this level of accuracy at high driving speeds (e.g. highway speeds or city driving speeds) if the cameras were <strong>a)</strong> 60 fps and <strong>b)</strong> had a global shutter?</p>
"
1011,"<p>I talk about the robot from: <a href=""http://www.hansonrobotics.com/robot/sophia/"" rel=""nofollow noreferrer"">Hanson Robotics</a>, which was <a href=""https://techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/"" rel=""nofollow noreferrer"">granted the right to citizenship from Saudi Arabia</a>.</p>

<p>I have found the following articles:</p>

<h1>Your new friend is a humanoid robot</h1>

<p>source: <a href=""http://www.theaustralian.com.au/life/say-hello-to-your-new-friend-sophia-the-humanoid-robot/news-story/070299a8d11b7d636848f1b8dd753530"" rel=""nofollow noreferrer"">theaustralian.com.au</a></p>

<blockquote>
  <p>Like Amazon Echo, Google Assistant and Siri, <strong>Sophia can ask and answer questions about discrete pieces of information</strong>, such as what types of movies and songs she likes, the weather and whether robots should exterminate humans.</p>
  
  <p>But her general knowledge is behind these players and she doesn’t do maths. <strong>Her answers are mostly scripted</strong> and, it seems, from my observation, her answer are derived from algorithmically crunching the language you use.</p>
  
  <p>Sometimes answers are close to the topic of the question, but off beam. Sometimes she just changes the subject and asks you a question instead.</p>
  
  <p>She has no artificial notion of self. <strong>She can’t say where she was yesterday, whether she remembers you from before</strong>, and doesn’t seem to amass data of past interactions with you that can form the basis of an ongoing association.</p>
  
  <p>Questions such as: <em>“What have you seen in Australia?”</em>, <em>“Where were you yesterday?”</em>, <em>“Who did you meet last week?”</em> and <em>“Do you like Australia?”</em> are beyond her.</p>
</blockquote>

<hr>

<h1>Why Sophia the robot is not what it seems</h1>

<p>source: <a href=""http://www.smh.com.au/comment/why-sophia-the-robot-is-not-what-it-seems-20171030-gzbi3p.html"" rel=""nofollow noreferrer"">smh.com.au</a></p>

<blockquote>
  <p>You can often fool this sort of software by introducing noise. That could be literal noise – machines aren't great at filtering out background noise, as anyone with a hearing aid will tell you – or it could be noise in the sense of irrelevant information or limited context. You could ask <strong>""what do you think of humans?""</strong> and then follow up with <strong>""can you tell more about it?""</strong> The second question requires the robot to define ""it"", remember what it said last time, and come up with something new.</p>
  
  <p>In the case of the ABC interview, <strong>the questions were sent to Sophia's team ahead of time so they were possibly pre-scripted</strong>. Just like an interview with a human celebrity!</p>
</blockquote>

<hr>

<h1>Pretending to give a robot citizenship helps no one</h1>

<p>source: <a href=""https://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia"" rel=""nofollow noreferrer"">theverge.com</a></p>

<blockquote>
  <p>Sophia is essentially a cleverly built puppet designed to exploit our cultural expectations of what a robot looks and sounds like. It can hold a stilted conversation, yes, but its one-liners seem to be prewritten responses to key words. (As Piers Morgan commented during an interview with Sophia, <strong>“Obviously these are programmed answers.”</strong>)</p>
</blockquote>

<hr>

<p>Updates:</p>

<ul>
<li><a href=""https://qz.com/1121547/how-smart-is-the-first-robot-citizen/"" rel=""nofollow noreferrer"">Inside the mechanical brain of the world’s first robot citizen</a></li>
<li><a href=""https://buildabuddha.quora.com/Sophia-the-Uncanny-Robot-AI-Is-Not-What-You-Think"" rel=""nofollow noreferrer"">Sophia, the Uncanny Robot AI, Is Not What You Think.</a></li>
<li><a href=""https://www.quora.com/How-much-of-Robot-Sophia%E2%80%99s-speech-is-likely-scripted-with-your-understanding-of-the-progress-in-NLP"" rel=""nofollow noreferrer"">How much of Robot Sophia’s speech is likely scripted with your understanding of the progress in NLP?</a></li>
</ul>
"
1012,"<p>I have read (<a href=""https://arxiv.org/pdf/1410.5401.pdf%20(http://Neural%20Turning%20Machines)"" rel=""nofollow noreferrer"">here</a> and <a href=""http://research.cs.queensu.ca/~akl/cisc879/papers/SELECTED_PAPERS_FROM_VARIOUS_SOURCES/05070215382317071.pdf"" rel=""nofollow noreferrer"">here</a>
) about the computational power of neural networks and a doubt came up.</p>

<p>There is a way to reduce an ANN to another ANN (not taking into count the training algorithm) ? e.g. Reduce a Recurrent Neural Network to a Multilayer Perceptron, meaning that if I have a trained RNN, I can get a MP that maps the same inputs given to the RNN to the same outputs produced by the RNN.</p>

<p>And if exists an answer to the above question, we can show the equivalence between neural networks, e.g., all problems solved by an Multilayer Perceptron can be solved by a Recurrent Neural Network but the opposite is not true, i.e., MP is subset of RNN (I do not know if this is true, is just an example). So, if we obtain this relationship between all neural networks, we can get a neural network X that is more powerful than others, so, we can throw away all other neural networks because X can solve any problem that other NN can. Is this reasoning correct ?</p>

<p>Thanks.</p>
"
1013,"<p>I'm very interested in writing a Spiking Neural Network engine (SNN) from scratch, but I can't find the basic information I need to get started.</p>

<p>For example, I've seen pictures of the individual signals that combine to form a neuron pulse in several research papers, with no information on the equations in use.  It's not the focus of the papers, and the authors assume the readers have that knowledge already.  Some papers reference software that provides this foundation (NEST, pyNN, etc.), but the documentation for the software is similarly light on details.</p>

<p>There is a ton of information out there on the more common network types, but SNN have not yet made it into the mainstream.</p>

<p>So where do I get this basic information?  Has someone pulled together any recipes/examples/tutorials for an SNN, as has been done with all the other network types?  </p>
"
1014,"<p>I am trying to train a CNN regression model using the ADAM optimizer, dropout and weight decay.</p>

<p>My test accuracy is better than training accuracy.
But as I know, usually train accuracy is better than test accuracy.</p>

<p>So I wonder how this is happening. </p>

<p><a href=""https://i.stack.imgur.com/MgVBt.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MgVBt.png"" alt=""my grph""></a></p>
"
1015,"<p>I'm trying to find a planning approach to solve a problem that attempts to model learning of new material. We assume that we only have one resource such as Wikipedia, which contains a list of articles represented as a <strong>vector</strong> of knowledge it contains and an <strong>effort</strong> to read that article.</p>

<p><strong>Knowledge vector and effort</strong></p>

<p>Before we start, we set a size for the vector, depending on the number of different types of knowledge. For example, we can define the items in the vector to be <code>(algebra, geometry, dark ages)</code>, and then 'measure' all the articles from this point of view. So, a math article will probably be <code>(5,7,0)</code>, since it will talk a lot about algebra and geometry but not about the dark ages. It will also have an <strong>effort</strong> to read it, which is simply an integer.</p>

<p><strong>Problem</strong></p>

<p>Given all the articles (represented as knowledge vectors with an effort), we want to find the optimal set of articles that help us to reach a <strong>knowledge goal</strong> (also represented as a vector). </p>

<p>So, a knowledge goal can be <code>(4,4,0)</code>, and it's enough to read an article <code>(2,1,0)</code> and <code>(2,3,0)</code>, since, when added, it adds up to the knowledge goal. We want to do this with <strong>minimal effort</strong>.</p>

<p><strong>Question</strong></p>

<p>I've tried to some heuristics to find an approximation, but I was wondering if there is any state of the art strategic planning method that can be used instead?</p>
"
1016,"<p>I have set myself the challenge of detecting the locations of players/bots in videos of a well known first person shooter game (this is for a youtube series I'm planning on doing). I'm not sure which AI approach I should apply to this problem - I'm a complete novice at this!</p>

<p>My first thought was that the face/head seems to have the most detail so I could train a convolution neural network on images of sprite heads and general background - however this seem not to work too well, I've certainly not exhausted different network architectures/typologies but it wasn't learning all that well.</p>

<p>My second approach was to use a HAAR cascade. This seemed to be an obvious choice since it's fast and good at detecting objects (rather than multi-classifying). However my cascade stops after 5 or 6 stages (using OpenCV) as it seems to have reach a great accuracy, but it doesn't detect when I feed it the training images, let alone other images.</p>

<p>I also looked into pedestrian detection and got a stock version of that working. However this seemed to struggle when/if the sprites are crouching or in unusual positions (and it isn't great on standing sprites tbh).</p>

<p>So, is there a branch of machine learning/AI that is more applicable to this problem? If not which should I continue to work on?</p>
"
1017,"<p><a href=""https://deepmind.com/blog/alphago-zero-learning-scratch/"" rel=""noreferrer"">Alpha Go Zero</a> contains several improvements compared to its predecessors. Architectural details of Alpha Go Zero can be seen in this <a href=""https://applied-data.science/static/main/res/alpha_go_zero_cheat_sheet.jpg"" rel=""noreferrer"">cheat sheet</a>.</p>

<p>One of those improvements is using a single neural network that calculates move probabilities and the state value at the same time, while the older versions used two separate neural networks. It has been shown that the merged neural network is more efficient according to the paper:</p>

<blockquote>
  <p>It uses one neural network rather than two. Earlier versions of AlphaGo used a “policy network” to select the next move to play and a ”value network” to predict the winner of the game from each position. These are combined in AlphaGo Zero, allowing it to be trained and evaluated more efficiently.</p>
</blockquote>

<p>This seems counter intuitive to me, because from a software design perspective this violates the principle <a href=""https://en.wikipedia.org/wiki/Separation_of_concerns"" rel=""noreferrer"">separation of concerns</a>. That's why I am wondering, why this merge has been proven beneficial.</p>

<p>Can this technique - merging different tasks in a single neural network to improve efficiency - be applied to other neural networks in general or does this require certain conditions to work?</p>
"
1018,"<p>I want to design a simple model that predicts the movement of coordinates with RNNs.</p>

<p>In a typical three-dimensional LSTM model, one feature is encoded as one hot encoding, and the <code>x</code> value is input as a three-dimensional matrix.
(e.g., In <code>seq2seq</code>, <code>abc =&gt; [[[1,0,0],[0,1,0],[0,0,1]]]</code>)</p>

<p>However, since I have to predict the <code>x</code> and <code>y</code> coordinate values, one hot encoding will inevitably produce a four-dimensional matrix.</p>

<p>What should I do with back-propagation? Or should I design a 3-D matrix using scalar values without one hot encoding?</p>
"
1019,"<p>I'm trying to answer the following question of chapter 6 exercises of <em><a href=""https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach"" rel=""nofollow noreferrer"">Artificial Intelligence: A Modern Approach</a></em> by Peter Norvig and Stuart Russell about dealing with constraints. </p>

<p>This is in the context of <a href=""https://en.wikipedia.org/wiki/Constraint_satisfaction_problem"" rel=""nofollow noreferrer"">Constraint Satisfaction Problem</a> and how you can re-formulate some problems with the constraints expressed as a bunch of binary constraints to use the generalized solver CSP algorithm.</p>

<p>But I'm stuck with that exercise, I can't sketch a demonstration.</p>

<pre><code> 6.6 Show how a single ternary constraint such as ""A + B = C"" can be turned into three
 binary constraints by using an auxiliary variable. You may assume finite domains. (Hint:
 Consider a new variable that takes on values that are pairs of others values, and
 consider constraints such as ""X is the first element of the pair Y."") Next, show how
 constraints can be eliminated by altering the domain of variables. This completes the 
 demonstration that any CSP can be transformed into a CSP with only binary constraints.
</code></pre>

<p>Thanks in advance!</p>
"
1020,"<p>I want to develop a virtual assistant like Google Assistant, Alexa, etc., for mobile phones.</p>

<p>What I should be researching apart from M/C Learning and Python? </p>
"
1021,"<p>I'm having the following problem: `</p>

<p>I'm training a multi-output CNN and using the relative values of the outputs in my loss function. The net is learning well, but as the absolute values of the outputs are not regularized in anyway in the loss function, the values of the outputs keep rising. This causes a situation where the result I need (values of the outputs relative to each other) are quite right, but the huge absolute values produce underflow resulting NaN values at some point of the training. Is there some way to constrain these absolute values (perhaps elsewhere than the loss function?)</p>

<p>I'm using a custom loss function implementing a recovery angular error metric, so the loss function is somewhat complex. A weighed mean of the net outputs is fed to this function, so using both the net outputs and the weighed mean would lead to some quite problematic gradient derivation. Would it be maybe possible to constrain the value range of the net outputs in the net structure itself?</p>
"
1022,"<p>So, my question is a bit theoretical. I have been trying to implement a perceptron based classifier with outputs 1 and 0 depending on  the category. I have used 2 methods: The <code>example by Example learning method</code> and <code>Batch learning method</code>. I also have defined another method which will measure accuracy according to the formulae <code>number_of_samples_classified_correctly/total_number_of_samples</code>(I'm not sure this should be the correct definition for accuracy and you are welcome to suggest a better measure). Now there are a few confusions i'm facing. Firstly, the accuracy of example by example learning is different from batch learning by 2%. Also the <strong>best</strong> accuracy achieved in both cases is depending on the slopes. So where exactly is the mistake?(Batch learning algorithm=<code>error*input_vector</code>( where error can be 1,-1 or 0 ) summed over all input vectors and then added to weights).</p>

<p><a href=""https://i.stack.imgur.com/Jla9w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jla9w.png"" alt=""For initial slope[1,-1] giving an accuracy of 88% example by example learning""></a></p>

<p><a href=""https://i.stack.imgur.com/DejqO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DejqO.png"" alt=""For initial slope[1,-1] giving an accuracy of 88% batch learning""></a></p>

<p><a href=""https://i.stack.imgur.com/gaKBn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gaKBn.png"" alt=""For initial slope[1,1] giving an accuracy of 84% example by example learning""></a></p>

<p><a href=""https://i.stack.imgur.com/cDryg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cDryg.png"" alt=""For initial slope[1,1] giving an accuracy of 88% batch learning""></a></p>

<ul>
<li>For initial slope[1,-1] giving an accuracy of 88% example by example learning</li>
<li>For initial slope[1,-1] giving an accuracy of 88% batch learning</li>
<li>For initial slope[1,1] giving an accuracy of 84% example by example learning</li>
<li>For initial slope[1,1] giving an accuracy of 86% batch learning</li>
</ul>
"
1023,"<p>A question for developers of projects for pattern recognition. How best to organize the architecture of such a service?</p>

<p>At what stage do you conduct logic? (for example, for the recognition of a photo of a male blue jacket, a cascade of queries is performed: ""recognizing men"" -> ""recognizing the jacket"" -> ""recognizing the color of the jacket."")</p>

<p>Does it make sense to implement all search options within a single neural network or is it better to create a set of individual neuronets that are confined to fairly simple tasks?</p>
"
1024,"<p>As you might know, <a href=""https://arxiv.org/abs/1710.09829v1"" rel=""noreferrer"">Capsule Networks</a> have been recently introduced by <a href=""https://en.wikipedia.org/wiki/Geoffrey_Hinton"" rel=""noreferrer"">Hinton</a>. There also have been several heads up within his <a href=""https://www.youtube.com/watch?v=TFIMqt0yT2I"" rel=""noreferrer"">talks</a>. </p>

<p>As expected, the paper elaborates on the idea way theoretically! However, as a fan of <a href=""https://en.wikipedia.org/wiki/Occam%27s_razor"" rel=""noreferrer"">Occam's razor</a>, I was wondering if anybody can simplify the idea behind the Capsule Networks or CapseNets. </p>

<p>Thanks</p>
"
1025,"<p>I have a simple gauge displaying analog values ranging from 0 to 4.
<a href=""https://www.fragdenstein.de/wordpress/wp-content/uploads/2014/10/heizung_wasser_auffuellen_beitragsbild.jpg"" rel=""noreferrer"">Here is an image of the gauge</a>. Unfortunately there is no way to get a analog or digital signal for the value.</p>

<p>How do I read the value of the gauge? </p>

<p>My idea is to make an image every 5 minutes and get the value by analyzing it. I am thinking of manually generating reference images with the black needle in different positions and then compare it to the real image. </p>

<p>Since all the processing should be done on a raspberry pi without internet connectivity, a good approach would be a preconfigured docker image for image comparison which helps doing the image comparison locally, maybe supported by a python or php script.</p>

<p>How to proceed?</p>
"
1026,"<p>Just a though passed through my head whatever method is adopted to achieve it. With programming or neural nets where do you think the location of that intelligence will be by these methods in the physical world. In me my intelligence or lack of it is behind my eyes and is carried around by my body but remains located there.</p>

<p>Know I am a stalwart of a dualistic nature, mind and material. But if iam a wrong and intelligence can be created purely from the material by programming, complexity or who knows, what still remains is, I may be alone, as I said that consious intelligence has a location so where will it be within a computer, it's ram it's processor or will the programming itself create a bi location with no precise boundary or absolute position of this ghost in the machine.</p>
"
1027,"<p>I'm working on a multi-player game that involves a board/map where there are walls, cover or open ground. The players then take turns moving units around and undertaking discrete actions, like move, fire etc.</p>

<p>I've been exploring adding an AI for single-player games, or to enhance multi-player games. I've looked at:</p>

<ul>
<li>Behaviour Trees</li>
<li>Utility-based decisions</li>
<li>GOAP</li>
</ul>

<p>The problem I see with all of these is that in order for the AI to use specific multi-unit tactics, like a pincer attack, or leap-frogging units; that behaviour needs to be explicitly added separate to the above.</p>

<p>So, how can a Behaviour Tree tell an AI how to identify it should use a Flanking manoeuvre, i.e. move a unit around to hit from the side? How does the AI identify that it's a good plan, that is action across multiple ""moves""? How can the AI identify where to move the individual units in order to pull this off (how does it identify the flank)?</p>
"
1028,"<p>I bought an Intel Movidius Neural Compute stick a few weeks ago. Even though I can use it with the examples, I want to actually use it for something! The documentation is messy, and hard to work through.</p>

<p>I've already looked through some of the websites that have been mentioned in other posts.</p>

<p>I specifically want to make an inference/reward-based AI that tries to guess a bunch of numbers over a period of time. The AI has to guess the best 10 guesses, and then it gets shown the set. The closer answers are rewarded, and the farther answers are penalised.</p>

<p>In the end, I want it to try and guess the most likely set of numbers in a random number generator, kind of like how a quantum computer outputs answers.</p>

<p>This could be used for all sorts of things! Even if it's already built, I want to program it from scratch so I can learn how this sort of thing works (teach a man to fish type of thing).</p>

<p>So, considering my goals above, how should I start? Also, could this answer questions with close to the same accuracy as a quantum computer, if it's fed enough data?</p>
"
1029,"<p>Is there a general adversarial network that can take multiple low quality images of a subject to create a higher quality image of the subject? SRGANS just take a single low res image and make it high res but I need something that can take multiple low quality (not necessarily low res) images to create a higher quality image. In low quality I mean you could have one image with a reflection or partially blurry or has a small obstruction that another image might compensate for. Would be good is if you could combine the lower quality images to create on higher quality image. Is there any GAN that could do this?</p>
"
1030,"<p>I am fairly new to deep learning in general and I am currently facing a problem I want to solve using neural networks and I am unsure if it is a <em>classification</em> or <em>regression</em> problem. I am aware that classification problems are about classifying whether an input belongs to class A or class B (or class C ...) and regression problems are about mapping the input to some sort of continuous output (just like the house pricing problem).</p>

<p>I basically want to measure the body temperature of a person using a simple video camera. To me, this seems like more of a <em>regression</em> type of issue rather than <em>classification</em>, because of the actual continuous result values I want the neural network to produce from the input video frames, e.g. 39°Celsius. But a question that came to my mind was: What if I use every integer value in the range from 35°C to 42°C as a possible output class? This would make it a classification problem, am I right? What would be the correct approach here and why? Classification or regression?</p>

<p>Thank you!</p>
"
1031,"<p>I was reading Hinton's new paper, ""Dynamic Routing Between Capsules"" and didn't understand the term ""activity vector"" in the abstract.</p>

<blockquote>
  <p>A capsule is a group of neurons whose activity vector represents the
  instantiation parameters of a specific type of entity such as an
  object or object part. We use the length of the activity vector to
  represent the probability that the entity exists and its orientation
  to represent the instantiation paramters. Active capsules at one level
  make predictions, via transformation matrices, for the instantiation
  parameters of higher-level capsules. When multiple predictions agree,
  a higher level capsule becomes active. We show that a
  discrimininatively trained, multi-layer capsule system achieves
  state-of-the-art performance on MNIST and is considerably better than
  a convolutional net at recognizing highly overlapping digits. To
  achieve these results we use an iterative routing-by-agreement
  mechanism: A lower-level capsule prefers to send its output to higher
  level capsules whose activity vectors have a big scalar product with
  the prediction coming from the lower-level capsule.</p>
</blockquote>

<p><a href=""https://arxiv.org/pdf/1710.09829.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1710.09829.pdf</a></p>

<p>I thought a vector is like an array of data that you are running through the network.</p>

<p>I started working through Andrew Ng's deep learning course but it's all new and terms go over my head.</p>
"
1032,"<p>I'm looking to build a conversational AI. The goal is to create an AI that is your friend (as opposed to personal assistants that are meant for carrying out commands like ""send a text, play that song..."" etc). </p>

<p>What are the underlying techniques involved ? Is there any good open source project I can build on top of. What kind of training data do I need ? I looked at Jasper - doesnt look like a good fit based on a shallow evaluation.</p>

<p>I'm imagining that the user text needs to be parsed out (like for eg using Google's syntaxnet). May be it queries some knowledge graph and then uses NLG. Or it uses some kind of seq-seq modelling like LSTM/RNN to generate responses. </p>
"
1033,"<p>Given an ease with which a human can read a text scrambled in a special way, like the following passage popular a while ago</p>

<blockquote>
  <p>Aoccdrnig to a rscheearch at Cmabrigde Uinervtisy, it deosn't mttaer
  in waht oredr the ltteers in a wrod are, the olny iprmoatnt tihng is
  taht the frist and lsat ltteers be at the rghit pclae. The rset can be
  a toatl mses and you can sitll raed it wouthit porbelm. Tihs is
  bcuseae the huamn mnid deos not raed ervey lteter by istlef, but the
  wrod as a wlohe.</p>
</blockquote>

<p>could you train a computer to restore the original? Are there any studies/known instruments that can achieve this?</p>

<p>I am talking about mildly scrambled/damaged text, like the above, not a totally random permutations or random omission of letters.</p>
"
1034,"<p>I've mostly seen (e.g. in <a href=""http://karpathy.github.io/2015/05/21/rnn-effectiveness/"" rel=""nofollow noreferrer"">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>) that when training RNN's on text for something like language modeling, the text is usually featurized character-by-character using a 1-hot encoding.</p>

<p>For example, the text ""hello"" would be represented like</p>

<pre><code>{h: 1, e: 0, l: 0, o: 0}
{h: 0, e: 1, l: 0, o: 0}
{h: 0, e: 0, l: 1, o: 0}
{h: 0, e: 0, l: 1, o: 0}
{h: 0, e: 0, l: 0, o: 1}
</code></pre>

<p>I was wondering if one could just as well use the ASCII encoding of the text and feed the bits in one by one. So the input ""hello"" would be input like</p>

<pre><code>0110100001100101011011000110110001101111 
</code></pre>

<p>Would the RNN have a disproportionately harder time having to figure out how the arbitrary and complex 8-bit ASCII encoding should be used? Or would the ASCII encoding lead to about the same performance as the nicer 1-hot encoding?</p>
"
1035,"<p>I'm new in this argument, my question is:</p>

<p>Can convolution be applied in other contexts different from image recognition?
Is there a good source to learn from?</p>
"
1036,"<p>Please, can someone give advice what journals are good for <strong>first publication</strong> in the field of Deep Reinforcement Learning? </p>

<p>I am in process of writing about research results of DQN related algorithms.</p>

<p>I have 3 requirements - it should be indexed in one of these databases, otherwise, I cannot receive grant money for research:</p>

<ul>
<li><a href=""https://www.scopus.com/"" rel=""nofollow noreferrer"">https://www.scopus.com/</a> </li>
<li><a href=""http://webofknowledge.com"" rel=""nofollow noreferrer"">http://webofknowledge.com</a></li>
</ul>

<p>And it should not be very expensive to publish. It should be under 1000EUR to publish, for example, Open Access license for Elsevier ""Artificial Intelligence"" journal costs around 2400EUR to publish.</p>

<p>And it should not have very long review/publishing period. For example, Elsevier ""Information Fusion"" journal currently gathers articles for July 2018, that is 8 month period till publishing. Is it normal? </p>

<p>Can you please recommend some journals that qualify &amp; you have had good experience publishing research?</p>
"
1037,"<p>What's the difference between model-free and model-based reinforcement learning? </p>

<p>It seems to me that any model-free learner, learning through trial and error, could be reframed as model-based. In that case, when would model-free learners be appropriate?</p>
"
1038,"<p>can you good people direct me to sources to start learning the aspects of AI? </p>
"
1039,"<p>What would be the result of an AI subject to a data stream or information stream, fed to it from events in an ""external world""  if it could not find any pattern within that stream by the methods it has in determining patterns</p>

<p>Would the response be because of the limitation of its methods in determining patterns or would you be able to conclude that the information from those events are undetermined and therefore could be stated as random.</p>

<p>Or would the AI be unable to ever determine using its methods whether the information fed to it was from a random undetermined world because it needed an eternity of information as to determine whether the data was random or not.</p>

<p>So what I guess I'm asking is the pursuit of creating AI by mankind on a similar level of limitation by the accumulation of ""Knowledge"" over time as is the other pursuits of mankind within science etc, and is only going to be achieved simultaneously with the acquisition of ""knowledge"" from these other pursuits. If determination of a simulated model comes from the physics of a determined world or not will there ever be a model because of the eternity of determining whether the information from the ""external world"" is determined or not.</p>
"
1040,"<p>Say I need to make an action such as ""Walk"" perform for 2 seconds using a behavior tree which has access to a timer.</p>

<p>The timer can do the following:</p>

<p>start, time (get value of timer), and reset.</p>

<p>If the behavior tree should return Success every time ""Walk"" is completed, unless the timer reset or ""Walk"" is still executing, how could such a thing be built?</p>
"
1041,"<p>I am software/hardware engineer for many years now.
I am shamed to say, I know nothing about AI and machine learning.
I have a strong background in digital signal processing, and various languages, like C,C++,Swift, etc.</p>

<p>Is there any book/guide that start right from scratch with basic theory, and then goes with examples to real life applications, current tools, examples that you can run, etc ?</p>

<p>Not too academic, or statistic, start with basic philosophy and go to practice .</p>

<p>Thank you.</p>
"
1042,"<p>I have a set of images that I already trained a CNN to classify successfully. I wonder if it would be possible to encode the images (using XOR in combination with a key of the same length as the image) and train a new net on them. </p>

<p>Thinking logically, the features still exist in the same relation to each other, just in a different form (encoded). Considering that neural networks are incredible at pattern recognition, I assume that it would still be doable.</p>

<p>For people, who cannot imagine how a xor-encoded image would look like:
<a href=""https://i.stack.imgur.com/GsIxN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GsIxN.png"" alt=""example for encoded image using random key""></a></p>

<p>For a human, it may look like rubbish, but the information is definitely there.</p>

<p>Would love to read your opinion.</p>
"
1043,"<p>This is an XOR neural net I built in Python. When I run it with random weights and keep the biases constant, it trains them perfectly and reaches the global minima, but when i add the biases in so that it has to train them too, it doesn't work at all. I think its getting stuck in a local minima because of my training function but i don't know for sure. Attached is the code to only train the weights.</p>

<pre><code>#!/usr/bin/env python
import math
import random

class neuralNetwork:
    def __init__(self):
        self.input = [[0,0],[1,0],[1,1],[0,1]] #four different sets of two inputs
        self.expected = [1,1,0,1],[0,1,1,1],[0,1,0,1] #Optimal outputs for all three nodes
        self.weights = [random.randrange(-10,10),random.randrange(-10,10)],[random.randrange(-10,10),random.randrange(-10,10)],[random.randrange(-10,10),random.randrange(-10,10)]
        #self.weights = [-20,-20,20,20,20,20] #Optimal Weights
        self.bias = [30,-10,-30] #one bias for each node
        #self.bias = [30,-10,-30] #Optimal biases
        self.out = [0,0,0,0],[0,0,0,0],[0,0,0,0] #Actual output for every node
        self.rmse= [0,0,0] 
        self.error = [0,0,0,0],[0,0,0,0],[0,0,0,0]
        self.learnRate = 1

    def nandGate(self):
        for i in range(0,4):
            self.out[0][i] = self.input[i][0]*self.weights[0][0]+self.input[i][1]*self.weights[0][1]+self.bias[0]
            self.out[0][i] = 1 / (1 + math.exp(-self.out[0][i]))

    def orGate(self):
        for i in range(0,4):
            self.out[1][i] = self.input[i][0]*self.weights[1][0]+self.input[i][1]*self.weights[1][1]+self.bias[1]
            self.out[1][i] = 1 / (1 + math.exp(-self.out[1][i]))

    def andGate(self):
        for i in range(0,4):
            self.out[2][i] = self.out[0][i]*self.weights[2][0]+self.out[1][i]*self.weights[2][1]+self.bias[2]
            self.out[2][i] = 1 / (1 + math.exp(-self.out[2][i]))

    def calcrmse(self):
        for p in range(0,3):
            self.rmse[p] = 0
        for p in range(0,3):
            for i in range(0,4):
                self.error[p][i] = self.expected[p][i] - self.out[p][i]
                self.rmse[p] += self.error[p][i]

    def train(self):
        for q in range(0,1000000):
            self.nandGate()
            self.orGate()
            self.andGate()
            self.calcrmse()
            for i in range(0,3):
                #self.bias[i] += self.learnRate*self.rmse[i]
                for p in range(0,2):
                    self.weights[i][p] += self.learnRate*self.rmse[i]
            #print(""Iter: ""+str(q))
            #print(""WEIGHTS: ""+str(self.weights))
            #print(""BIAS: ""+str(self.bias))
            #print(""NAND OUT: ""+str(self.out[0]))
            #print(""OR OUT: ""+str(self.out[1]))
            print(""AND OUT: ""+str(self.out[2]))
            #print


x = neuralNetwork()
x.train()
</code></pre>
"
1044,"<p>I've seen some articles about text generation using LSTMs (or GRUs) for text generation.</p>

<p>Basically it seems you train them by folding them out, and putting a letter in each input. But say you trained it with text which includes the string:</p>

<p>""The dog chased the cat""</p>

<p>and also includes the string:</p>

<p>""The lion chased the ca""</p>

<p>Both should be acceptable. But although they are very similar they differ entirely after the 4th character. So they will result in two very different vectors. Further the longer you roll it out the more they will differ.
How is it then possible for an LSTM to learn that ""The [something] chased the cat"" is an acceptable phrase?</p>

<p>Equally if you train it to try and learn to pair up parentheses. I can see how you could manually design it to do this, but how exactly could it be trained to do this just by entering strings like ""(das asdasdas) axd""?</p>

<p>What I'm getting at is that I don't get how it could LEARN any sort of structure more than a Markov model.</p>

<p>Any ideas? </p>

<p>(Also, I've only ever seen one article that showed a LSTM that can pair parentheses. So has this study ever been replicated?) I get that it is possible for an LSTM to do this but I don't get how it can learn to do this!</p>

<p>What I'm getting at is you usually train things with the input phrase and then compare it to the expected phrase to get the error. But in text generation there might be millions of possible acceptable expected phrases! So how can you compute an error?</p>
"
1045,"<p>I invented a chess-like board game. I built an engine so that it can play autonomously. The engine is basically a decision tree. It's composed by:</p>

<ol>
<li>A search function that at each node finds all possible legal moves</li>
<li>An evaluation function that assigns a numerical value to the board position (positive means first players is gaining the upper hand, negative means the second player is winning instead)</li>
<li>An alphabeta pruning negamax algorithm</li>
</ol>

<p>The main problem about this engine is that the optmization of the evaluation function is really tricky. I don't know which factors to consider and which weights to put. The only way I see to improve the engine is to iterate games trying each time different combinations of factors and weights. However, it computationally seems a very tough feat (Can I backpropagate without using deeplearning?).</p>

<p>I would like to use reinforcement learning to make the engine improve by playing against itself. I have been reading about the topic, but I am still quite confused. </p>

<p>What other reward is there in a game a part the win-or-lose output (1 or 0)? If I use other rewards, like the output from the evaluation function at each turn, how can I implement it? How do I modify the evaluation function to give better rewards iteration after iteration?</p>
"
1046,"<p>Is there examples of AI systems being taught grammatical rules, or have such rules built-in to them, in order to recognize commands and the like in natural language?</p>

<p>For example, is there a program, when given the instruction (either by speech or by text) in the form of ""Open file-1"", will attempt to understand the command as consisting the parts ""open"" and ""file-1"", then respond according to how each part is classified?</p>

<p>The focus here is the fact that these kind of instructions are processed with built-in rules of grammar, rather than the rules being learned through observation of data.</p>
"
1047,"<p>I want to build a personal assistant that listens to me continuously.. </p>

<p>The flow looks like this: </p>

<ol>
<li>continuously record voice </li>
<li>stream it to google speech api.</li>
<li>get back the text in real time -> parse for intent etc..</li>
</ol>

<p>Problem is, google speech api gets expensive if you record for hours. A better way to do it is to only submit the parts where I'm actually speaking to it.. Then the cost of running this full time (17 hours a day, every day) becomes very accessible.
Now my question is:</p>

<p><strong>How can i detect that a voice is present in the microphone stream?</strong> </p>

<p>I have a lot of noise in the background, dumb <code>increase in volume detection</code> is not a very good solution. I need something more intelligent. It doesn't need to be very accurate - just good enough to not break my cloud computing budget. I'm thinking that human voice sounds distinct enough that is not such a big problem to detect when is there.</p>

<p>What do you recommend me to do, given this is a real time stream - not an audio file.</p>

<p>The audio will be generated in chromium browser (electron) - with <code>getUserMedia</code> API, and using <code>node.js</code> i plan to handle the streaming logic. </p>

<p>Note: there is a built in <code>speechRecognition api</code> in <code>electron</code> - but from my experience currently it doesn't work (not even after i give it my API key), and even if would have worked, i think it has the same cost problem. So this is why i'm trying to provide my own implementation. </p>

<p>I don't know what i'm doing, any insight is welcomed:) Thank you.</p>
"
1048,"<p><strong>Has there ever been a model (neuro-physical) proposed for the human unconscious brain</strong>?  </p>

<hr>

<p>I tried reading on <a href=""https://en.wikipedia.org/wiki/Unconscious_mind"" rel=""nofollow noreferrer"">unconscious mind</a> but couldn't find anything which answers the question.  </p>

<hr>

<p>Is there any literature related to it?</p>
"
1049,"<p>I am trying to find a good evaluation function for a game with:</p>

<ul>
<li>A 7x7 tile board</li>
<li>2 players, given equal number(>=3 currently undetermined) of <em>stones</em>
placed randomly on the tiles</li>
<li>A turn is consisted of a player moving a <em>stone</em> owned by that player, vertically or horizontally but <em>not
diagonally</em> to a very next tile of itself</li>
<li>A player loses when out of moves: a player is out of moves when every
<em>stone</em> that player owns, has its very next tiles, except not for diagonals necessarily,  occupied either by the board edge or other
<em>stones</em></li>
</ul>

<p>Right now my evaluation function's return value increases:</p>

<ul>
<li>if the total moves available to the player is increasing</li>
</ul>

<p>and/or</p>

<ul>
<li>average distance to the middle tile of the board is decreasing</li>
</ul>

<hr>

<ul>
<li><strong>Question:</strong> Is there a better strategy? or how can I improve my evaluation function?</li>
</ul>
"
1050,"<p>Well, I am new to implementing ANN's and there is something that i want to know. It maybe a bit silly though.</p>

<p>I just wanted to know that if we have a simple data set say dependent only on a single variable <code>x1</code>, and if we want to train the ANN to predict the data set values (no new values, only the data set values needs to be predicted) will sorting or arranging the data set in some way beforehand make the ANN go to the optimum value quicker or it really doesn't matter?</p>

<p>If arranging does matter than how do we arrange multidimensional data and what is the intuition behind it being faster? Just like in Binary Search for a random data set we get a performance gain of <code>O(n^2)-O(nlogn)</code> over Linear Search similarly I want to know whether such performance gains are available in ANN's in any way? </p>

<p>Note: I am aware this is already taken care by Batch Learning so i am specifically talking about Example by Example learning. Also, I want to know if some arranged form of data set works well for certain forms of training e.g. Momentum method, Delta method.</p>

<p>Thanks in Advance.</p>
"
1051,"<p>I want to capture text and letters on images (png, jpeg, etc.).  Is it Possible Which algorithm/software can I use?</p>

<p>Right now I am using <code>R</code> with the <code>tesseract</code> package but it's not solving my problem: if letters on images are  different colors and different format, sizes in those situations don't work.</p>

<p>I don't have the any training data to compare text on the image with the existing data set. So in those situations is it possible?</p>
"
1052,"<p>Starting from last year, I have been studying various subjects in order to understand some of the most important thesis of machine learning like</p>

<blockquote>
  <p>S. Hochreiter, &amp; J. Schmidhuber. (1997). <a href=""https://www.bioinf.jku.at/publications/older/2604.pdf"" rel=""nofollow noreferrer"">Long short-term memory</a>. Neural Computation, 9(8), 1735-1780. </p>
</blockquote>

<p>However, due to the fact that I don't have any mathematical backgrounds, I started to learn subjects like</p>

<ul>
<li>Calculus</li>
<li>Multivariate Calculus</li>
<li>Mathematical Anaylsis</li>
<li>Linear Algebra</li>
<li>Differential Equations</li>
<li>Real Anaylsis (Measure theory)</li>
<li>Elementary Probability and Statistics</li>
<li>Mathematical Statistics</li>
</ul>

<p>Right now, I can't say I have done studying those subjects rigorously, but I know what the subjects above want to deal with. The thing is that I don't know what I have to do at this point. There are many subjects that machine learning uses to solve many problems out there and I don't know how to utilize them correctly. </p>

<p>For example, reinforcement learning is now one of the most popular topic that hundreds of thousands of researchers are now doing their research to make a breakthrough of curse of dimensionality. But, as a future employee who's gonna work in IT companies, the task on the desk wouldn't be something I expected to do. </p>

<p>Is it important to have my own expertise to work in the fields? If so, what kinds of subjects do I have to study right now?</p>

<p>For your convenience, I want to know more about Markov process and Markov decision process. </p>
"
1053,"<p>I wish to write a bot that can use screen footage to play a game, specifically for the game 'Nidhogg'.</p>

<p>To that end I have determined that a CNN should do the feature detection and a feedforward neural network should determine the action to take.</p>

<p>For this question I wish to focus on the CNN.</p>

<p>My idea was to first train the CNN as an autoencoder to aid in unsupervised pattern recognition and to add hidden layers after every epoch, but I am unsure whether this would actually be faster or lead to higher accuracy (as opposed to immediately training all hidden layers at once).</p>

<p>The reason I am unsure is because I imagined the following scenario:</p>

<pre><code>Input &gt; Hidden1 &gt; Output
</code></pre>

<p>If the output layer needs to approximate the input layer is it not wasteful to let it have its own weights backpropagated when it actually tries to determine the 'inverse' function of the hidden layer? Is it possible to 'directly' determine an inverse function for a layer even if consists of multiple filters?</p>

<p>If so, say I've trained my first hidden layer so that the error is minimal and use it as an input layer whilst I add another hidden layer; can I repeat my strategy?</p>

<pre><code>Input &gt; Hidden1 &gt; Output
    Input(Hidden1) &gt; Hidden2 &gt; Output
</code></pre>
"
1054,"<p>I have implemented the hill climbing algorithm, with side away steps, which can increase the rate of success, because, when you don't have new generated states, you can go back to previous level and choose from there another state. </p>

<p>What should I do when new generated states have bigger distance to the goal than the parent state?</p>

<p>Do I have to go back to the previous level and choose another state, from which I will continue? Or I continue with one of the states generated which has bigger distance?</p>
"
1055,"<p>I know what meaning stride has when it is just an integer number (by which step you should apply filter to image). But about <code>(1, 1)</code> or even more dimensional stride?    </p>
"
1056,"<p>As we know in call centers there are certain SOP(standard operating procedure), for example few are below,</p>

<ol>
<li>Agent greeted customer</li>
<li>Agent verified the validity of customer before providing sensitive information</li>
<li>Agent is able to hold on conversation in decent way</li>
<li>Agent closed conversation with nice ending notes</li>
<li>Customer got all information what he wanted</li>
</ol>

<p>We are planning to transcribe the call records and then from the transcribed text, we want to verify above things(Out of many) from transcribed text using natural language processing.</p>

<p>With the current state of AI technology is possible? If yes can someone give some guidance to approach this type of problem? </p>
"
1057,"<p>I am a PhD student in computer science, and currently creating a state of the art overview in applications done in Machine Ethics (a multidisciplinary field combining philosophy and AI, that looks at creating explicit ethical programs or agents). It seems that the field mostly contains theoretical arguments and there are relatively little implementations, even though there are many people with a technical background in the field.</p>

<p>I understand that because ethics are involved, there is no ground truth and since it's part of philosophy one can get lost in arguing over which type of ethics should be implemented and how this can be done best. However, in computer science, it is usual to even try a simple implementation to show the possibilities or limitations of your approach. </p>

<p>What are the possible reasons there is so little done in explicitly implementing ethics in AI and experimenting with it? </p>
"
1058,"<p>With some knowledge of machine learning and deep learning, it seems very unlikely for AI to develop into the consciousness that we imagine. </p>

<p>To me, consciousness requires a new framework that is very different from what we have today, as current forms of learning seem to be very 'shallow'. </p>

<p>What are the current limitations to artificial consciousness and sentience? Also is there a difference between artificial consciousness and sentience? </p>
"
1059,"<p>I'm new in genetic algorithms and I'm reading the A. E. Eiben and J. E. Smith book <em>Introduction to Evolutionary Computing</em> (Springer 2003).</p>

<p>On section 3.5 Recombination, page 47, second paragraph said: <em>Recombination operators are usually applied probabilistically according to a <strong>crossover rate</strong> p sub c [...]</em>.</p>

<p>What does probabilistically means here?</p>

<p>I think it means that there are a probability that the operator is going to be applied or not.</p>
"
1060,"<p><strong>If some claim they are doing Artificial Intelligence by providing a Voice Assistant, what shall we expect from this Voice Assistant for the claim of doing ""Artificial Intelligence"" to be correct?</strong></p>

<p>Examples of potential qualifications:</p>

<ul>
<li>is replying ""Hello"" to ""Hello"" enough?</li>
<li>is repeating any word you said enough?</li>
<li>is identifying you by your voice enough?</li>
<li>shall it support corrections? ""<em>what is my favorite color?""/""red!""/""no, it's blue. What is my favorite color?""/""blue!</em>""</li>
<li>shall it support emotions? ""<em>I would offer you better service if you could say thank you sometimes</em>""</li>
<li>shall it support self-awareness? ""<em>Dear master, may I ask you a question about me?</em>""</li>
<li>etc.</li>
</ul>
"
1061,"<p>I am interested to know if it's possible to use ALICE chat bot in our C/C++ program in offline mode?</p>

<p>I mean I can pass strings trough the ALICE in my C++ program and get the answer as a string(Use it as a library/function)</p>

<p>Is it possible? </p>

<p>EDIT: I asked this question from the author of Mitsuku bot <code>Steve Worswick</code> and he answered:</p>

<blockquote>
  <p>If you have an offline interpreter, the source files for ALICE are on
  the internet.</p>
</blockquote>

<p>Also</p>

<blockquote>
  <p>Search for ALICE AIML for a full 3 times Loebner Prize winning
  chatbot’s free to use source files. This assumes you have an offline
  AIML interpreter like Program O or Program AB.</p>
</blockquote>

<p>But I couldn't understand what did he mean by <code>AIML offline interpreter</code> ? What do should this interpreter do?</p>
"
1062,"<p>I am a newbie in the field of AI/ML. I am trying to implement predictive analytics model on the data generated and collected every minute from a device with sensors.</p>

<p>I have two questions: </p>

<ol>
<li>What are various ML algorithms I can use to predict the number of
people in a room given temperature, humidity, luminosity, and motion
{ 0 | 1 }?</li>
<li>What other things can we predict using the above data from the
sensor that is deployed in a closed room?</li>
</ol>

<p><strong>Context:</strong></p>

<p>The device sends temperature, humidity, luminosity, and motion(yes/no) in real-time. I deployed this device in a closed room and started collecting data. Now I want to use this data to predict the number of people in the room using the data collected. I believe a multiple (linear/poly) regression model will help me in achieving this but, wanted to know if there are any other algorithms or any other use cases I can look into.</p>
"
1063,"<p>I am very new to AI and ML alike. I am using google's OCR to extract text from image like receipts and invoice. Can someone please guide me to which techniques are used to make sense of the text. Like I would like to extract date, name of business, address, total amount, etc. If someone can please direct me to right set of algorithm industry uses for machine learning before marking this question ""too broad"", will be great.</p>

<p>I know this question is too broad, but there are no one to one answers for these questions. I will delete this question once answered and if marked not useful to keep forum clean. </p>
"
1064,"<p>I hope the experts here will bear with my basic questions.</p>

<p>I have started on Andrew Ng's machine learning course. It seems that machine learning is making correlations with known data based on as many parameters as possible. For example, if we collect data on existing property prices with information on the land area, built-in area, type of building, age of building, etc, it is possible to predict the price of another property if we input the value of the various parameters of this property. Similarly, if we keep the images (the black and white pixels) of cats, we can tell whether a new picture is a cat if it bears some resemblance to the pixels of existing labeled cat images. </p>

<p>This approach sounds great but is it practical? How much effort and zetabytes of data do we have to keep just to reach the brain power of, say, a 3-year old, who can recognize dogs, cats, tigers, a Mustang, trucks, a hamburger restaurant, and so on?</p>

<p>My next question is why does everyone have to repeat the effort of learning the same things? </p>

<p>If Google has already learned cats, or if someone already has a program to recognize handwritten digits, can this knowledge be shared and re-used? Or is it just a matter of paying for them?</p>
"
1065,"<p>""Deep Learning"" neural networks are now successful at image-recognition tasks that I would not have expected say 10 years ago.  I wonder if the current state of the art in machine learning could generally tell the difference between the sound of a dog or cat moving around a house, and a person walking in the same area, taking as input only the sound captured by a microphone. I think I could generally tell the difference, but it is hard to explain exactly how.  But this is also true of some tasks that deep learning is now succeeding at.  So, I suspect it is possible but it's not clear how you would go about it.</p>

<p>I have found algorithms to detect human speech (wikipedia:""Voice activity detection"") but separating animal and human footsteps seems more subtle.</p>
"
1066,"<p>Normally when doing a fit to some time series data (e.g., a polynomial fit), functions will return an associated error with each fitted point. I'm now trying out scikit-learn's support vector regression (SVR) fitting instead, which doesn't have any such return. There is a handy function in scikit-learn called validation_score that can tell me the accuracy score of various fits, from which I select the best one. So maybe that's good enough? But as a scientist, I'm wondering if this will get slapped down in peer review of a publication.</p>

<p>What's the right way to propagate the errors of my time series data through the SVR fit?</p>
"
1067,"<p>I've been reading a lot about TD-Gammon recently as I'm exploring options for AI in a video-game I'm making. The video game is a turn-based positional sort of game, i.e. a ""units"", or game piece's, position will greatly impact it's usefulness in that board state.</p>

<p>To work my way towards this, I thought it prudent to implement a Neural Network for a few different games first.</p>

<p>The idea I like is encoding the board state for the Neural Network with a single output neuron which gives that board states relative strength compared to other board states. As I understand, this is how TD-Gammon worked.</p>

<p>However, when I look at other people's code and examples/tutorials, there seems to be a lot of variance in the way they represent the board-state. Even for something as simple as tic-tac-toe.</p>

<p>So; specifically for tic-tac-toe, which is a better, or what is the correct representation for the  board state? I have seen:</p>

<ol>
<li>9 input neurons, one for each square. A <code>0</code> indicating a free-space, <code>-1</code> the opponent and <code>1</code> yourself.</li>
<li>9 input neurons, but using different values such as <code>0</code> for the opponent, <code>0.5</code> for free and <code>1</code> for yourself?</li>
<li>Could you use larger values? LIke <code>0</code>, <code>1</code> and <code>2</code>?</li>
<li>27 input neurons. The first 3 being square 1, the next 3 being square 2 etc. Every neuron is <code>1</code> or <code>0</code>. The first of the set of three indicates whether this square is free or not; the second indicating whether the square is occupied by your opponent or not. In the end, only one in every 3 neurons will have a <code>1</code>, the other two will have a <code>0</code>.</li>
<li>18 input neurons. The first being <code>1</code> for the X player, the second being <code>1</code> for the O player and both being <code>0</code> for a blank</li>
</ol>

<p>Then; when branching into games where the specific pieces abilities come into play, like in chess, how would you represent this?</p>

<p>Would it be as simple as using higher input values for more valuable pieces? I.e. <code>-20</code> for an opponents Queen and <code>+20</code> for your own queen? Or would you need something more complex where you define 10+ values for each square, one for each unit-type and player combination?</p>
"
1068,"<p>I have an AI class this semester. For our exam we also cover Alpha-Beta Pruning. </p>

<p>I found an old example, where I think we can stop already earlier. 
Here is a picture of it. </p>

<p><img src=""https://i.imgur.com/R99PW1Q.jpg"" alt=""my problem""></p>

<p>I think, because X wants to maximize his win, and finds 10, he knows that he cannot get better. Therefore he cuts and puts 10.  </p>

<p>I marked my ""improvement"" with red. </p>

<p>Thanks for your help. 
Sascha </p>
"
1069,"<p>If we say that we can measure intelligence and judge it with a scale of measurement. We get a quantity back from that measurment. The scale has no extremeties and has no limits in the quantity it measures unless the thing it measures has absolutes.</p>

<p>Is it possible then to concieve that AI can be created that surpasses the humans measured by the same scale.</p>

<p>If it is, doesn't it mean that the AI that has been created has not been determined by the limitations of those humans and its creation has not been determined from the intelligence associated with humans but would have to be created from something not directly attributable to intelligence and something more fundamental, and thus an AI more intelligent will be a consequence of this.</p>

<p>Or does the scale have absolutes and we have a limit in intelligence and maybe the artificial intelligence will be only created by someone that reaches that absolute measure, or any creation of which will be limited within the extremeties of the scale and be limited and not surpass the humans intelligence also confined within it?</p>
"
1070,"<p>I'm building a neural net to predict the value of a piece of art with a wide range of inputs (size, art medium, etc.) and I would like to include the author as an input as well (it is often a huge factor in the value of a single piece of art).</p>

<p>My current concern is that the name of the author isn't an ideal numerical input for a NN (i.e. If I just code each author with an increasing integer value I will be indirectly assigning more value to authors further down the list -_-). My thoughts were to create separate inputs for all the authors in my data set and then just use one hot encoding to better represent the input to the NN.</p>

<p>This approach however runs into a problem when an author that is not included in my training data is used as an input to the NN (i.e. a new author). I can get around this with an ""other author"" input field but I am worried that this won't be accurate as I would not have trained the the NN for this input (all pieces of art with a valuation have an author).</p>

<p>I haven't fully thought this through but I thought of perhaps training 2 NN's, one for a valuation without an author and one for valuation with an author to ensure I have enough training data for an ""authorless valuation"" to still be reasonably accurate.</p>

<p>I am still trying to conceptualize the best NN architecture before I get stuck into the implementation so if anyone has any suggestions/comments I would be very grateful!</p>

<p>Thanks in advance, Vince</p>

<p>P.S. I am doing this as a small competition with a friend to test a NN vs the traditional commercial valuation techniques. Please help me get a win for Computer Science over Actuarial Science.</p>
"
1071,"<p>I'd like to explore possibilities of applying deep learning on image noise reduction problem, more on photographic camera noise. What's a good NN architecture to solve problems like this? </p>

<p>EDIT 25,Nov,2017:
I have a small dataset of clean/noisy reference (~15K 4Kres images) acquired from digital camera. The target is to denoise other images from this camera type but without a reference photo.</p>
"
1072,"<p>I recently started to follow along with Siraj Raval's Deep Learning tutorials on YouTube, but I an error came up when I tried to run my code. The code is from the second episode of his series, How To Make A Neural Network. When I ran the code I got the error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\dpopp\Documents\Machine Learning\first_neural_net.py"", line 66, in &lt;module&gt;
neural_network.train(training_set_inputs, training_set_outputs, 10000)
File ""C:\Users\dpopp\Documents\Machine Learning\first_neural_net.py"", line 44, in train
self.synaptic_weights += adjustment
ValueError: non-broadcastable output operand with shape (3,1) doesn't match the broadcast shape (3,4)
</code></pre>

<p>I checked multiple times with his code and couldn't find any differences, and even tried copying and pasting his code from the GitHub link. This is the code I have now:</p>

<pre><code>from numpy import exp, array, random, dot

class NeuralNetwork():
    def __init__(self):
        # Seed the random number generator, so it generates the same numbers
        # every time the program runs.
        random.seed(1)

        # We model a single neuron, with 3 input connections and 1 output connection.
        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1
        # and mean 0.
        self.synaptic_weights = 2 * random.random((3, 1)) - 1

    # The Sigmoid function, which describes an S shaped curve.
    # We pass the weighted sum of the inputs through this function to
    # normalise them between 0 and 1.
    def __sigmoid(self, x):
        return 1 / (1 + exp(-x))

    # The derivative of the Sigmoid function.
    # This is the gradient of the Sigmoid curve.
    # It indicates how confident we are about the existing weight.
    def __sigmoid_derivative(self, x):
        return x * (1 - x)

    # We train the neural network through a process of trial and error.
    # Adjusting the synaptic weights each time.
    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):
        for iteration in range(number_of_training_iterations):
            # Pass the training set through our neural network (a single neuron).
            output = self.think(training_set_inputs)

            # Calculate the error (The difference between the desired output
            # and the predicted output).
            error = training_set_outputs - output

            # Multiply the error by the input and again by the gradient of the Sigmoid curve.
            # This means less confident weights are adjusted more.
            # This means inputs, which are zero, do not cause changes to the weights.
            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))

            # Adjust the weights.
            self.synaptic_weights += adjustment

    # The neural network thinks.
    def think(self, inputs):
        # Pass inputs through our neural network (our single neuron).
        return self.__sigmoid(dot(inputs, self.synaptic_weights))

if __name__ == '__main__':

    # Initialize a single neuron neural network
    neural_network = NeuralNetwork()

    print(""Random starting synaptic weights:"")
    print(neural_network.synaptic_weights)

    # The training set. We have 4 examples, each consisting of 3 input values
    # and 1 output value.
    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])
    training_set_outputs = array([[0, 1, 1, 0]])

    # Train the neural network using a training set
    # Do it 10,000 times and make small adjustments each time
    neural_network.train(training_set_inputs, training_set_outputs, 10000)

    print(""New Synaptic weights after training:"")
    print(neural_network.synaptic_weights)

    # Test the neura net with a new situation
    print(""Considering new situation [1, 0, 0] -&gt; ?:"")
    print(neural_network.think(array([[1, 0, 0]])))
</code></pre>

<p>Even after copying and pasting the same code that worked in Siraj's episode, I'm still getting the same error.</p>

<p>I just started out look into artificial intelligence, and don't understand what the error means. Could someone please explain what it means and how to fix it? Thanks!</p>
"
1073,"<p>I have a Neural Network that I'm want to use to self-play Connect Four. The neural network receives the board state and is to provide an estimate of the states desirability.</p>

<p>I would then, for each move, use the highest estimate, occasionally I will use one of the other moves for exploration.</p>

<p>I intend to use TD lamba to calculate the errors for each state to back propagate through the network.</p>

<p>But I'm confused about when this should actually occur. Do I store the estimate of a state that is used and calculate the error based on the next state chosen?</p>

<p>Or do I store a history of all states and back propagate only when the game is a win/lose/draw?</p>

<p>I guess overall I'm not sure I understand when the update occurs, partially because I don't quite understand how to implement the lambda? Like if I was to apply to back prop after every move, how would I even know the value of lambda at this time-step before I know how long the game will last?</p>

<p>When self-playing, is the error the difference between that ""sides"" last move? I.e. I compare move 1 against move 3, and move 2 against move 4 etc?</p>
"
1074,"<p>I am a c# senior developer and I got a task to try and predict the potential in each new client, or maybe the worth of each customer.
I don't have experience with machine learning, but I played with accord-framework.net and got some nice results on simple task.</p>

<p>My data model for <strong>training</strong> is:</p>

<pre><code>GeoLocation, // the country of ip when registed. iso code string
Age, // number
DateRegistered, //date time
Email, //string can be broken to vendors as catergorial (gmail, yahoo, microsoft and such)
EmailValidated, //is the email really exists. bool
PhoneNumber, //string
PhoneNumberValidated, // is the phone number really exists
CampaignName, //string (may be categirial)
UserAgent, //string should I make it categorial? (has info about browser, device, verndor, operation system and such, long string)
LandedOnPage, //string first url the customer entered from 
RegisteredFromPage, //string url of the page that the user registered from
RefererUrl, //string url the client came to our site from,
NumberOfPurchases, //the amount of times the customer puschase something on our site
CustomerValueUsd, //the total amount of USD the customer spent in our site
</code></pre>

<p>The <strong>output</strong> shoud be <code>CustomerValueUsd</code></p>

<p>I have a lot of data in the history, so I can back test it.</p>

<p>My questions:</p>

<ol>
<li>Does it make sense to do this task even though I don't have an experience with machine learning? How complicated is this task considering I'm using a well known framework?</li>
<li>Assuming that I'm taking the task, which algorithm should I choose to perform this kind of task?</li>
<li>How should I build the training data? see my comments, do you think my comments are ok to start with? or maybe I can break the data directly? </li>
</ol>
"
1075,"<p>I have a problem that I have been trying to use decision trees to solve. There is a data set of pricing information for products sold by a company. The goal is to infer the pricing algorithm for each product based on the product's name, type, and characteristics. However, the data is incomplete and may contain a few inaccuracies due to manual data entry.  Products can be priced using several possible algorithms:</p>

<ol>
<li><p>Price is a fixed dollar amount (widget costs $20)</p></li>
<li><p>Price is a percentage of MSRP (widget costs X% of MSRP, note X could vary by product classes)</p></li>
<li><p>Price is based on a cost per piece (package of widgets costs $2 per piece)</p></li>
</ol>

<p>Using decision trees, I have been able to identify the products priced using algorithm #1, but have struggled to make progress on algorithm #2 and #3. Since price is a continuous variable, the leaf nodes always represent the average of the target variable. But this doesn't seem to work for #2 and #3, since these would be a function of other variables, not the dependent variable.</p>

<p>Is there a clever way to implement this using decisions trees? Any recommended alternate techniques that could identify the pricing algorithms used? Note, I am not interested in predictions, but rather explain the pricing rules/algorithm that fit the existing data.</p>

<p>I am using SAS Enterprise Miner, but may have access to other tools as well.</p>
"
1076,"<p>From my understanding and text I found in research papers online :</p>

<p><strong>1) Pixel based object recognition:</strong>
Neural networks are trained to locate individual objects based directly on pixel data.</p>

<p><strong>2) Feature based object recognition:</strong>
Contents of a window are mapped to a feature space that is provided as input to a neural classifier.</p>

<p>What do the above 2 definitions imply in a simpler language ? Especially point number 2.</p>

<p>Also, can someone point me to papers/resources/articles that would explain the above approaches in greater detail ?</p>
"
1077,"<p>Goal-driven AIs is the only kind of AI I am aware of. However, Marcus Hutter claims <a href=""http://www.hutter1.net/ai/uaibook.htm"" rel=""nofollow noreferrer"">the following</a></p>

<blockquote>
  <p>Most, if not all known facets of intelligence can be formulated as goal driven or, more generally, as maximizing some utility function. It is, therefore, sufficient to study goal driven AI.</p>
</blockquote>

<p>which doesn't necessarily imply that there are other types of AIs (apart from goal-driven), but (at least, in the way it is phrased) suggests that there are other types of AIs. If there exist other types of AIs, which are they?</p>
"
1078,"<p>While unifying two literals, can we unify a positive and negative literal. For e.g, <em>p(x) with ~P(A)</em>?</p>

<p>Is this unifiable using the substitution {x/A}. Russel &amp; Norvig textbook doesn't say anything about unifying positive and negative literals while <a href=""https://courses.cs.washington.edu/courses/cse341/03au/slides/Prolog3/sld005.htm"" rel=""nofollow noreferrer"">slides</a> from University of Toronto  says it is possible.</p>
"
1079,"<p>I have created a Gomoku(5 in a row) AI using Alpha-Beta Pruning. It makes moves on a not-so-stupid level. First, let me vaguely describe the grading function of the Alpha-Beta algorithm.</p>

<p>When it receives a board as an input, it first finds all repetitions of stones and gives it a score out of 4 possible values depending on its usefulness as an threat, which is decided by length. And it will return the summation of all the repetition scores.</p>

<p>But, the problem is that I explicitly decided the scores(4 in total), and they don't seem like the best choices. So I've decided to implement a genetic algorithm to generate these scores. Each of the genes will be one of 4 scores. So for example, the chromosome of the hard-coded scores would be: [5, 40000,10000000,50000]</p>

<p>However, because I'm using the genetic algorithm to create the scores of the grading function, I'm not sure how I should implement the genetic fitness function. So instead, I have thought of the following:</p>

<p>Instead of using a fitness function, I'll just merge the selection process together: If I have 2 chromosomes, A and B, and need to select one, I'll simulate a game using both A and B chromosomes in each AI, and select the chromosome which wins.</p>

<p>1.Is this a viable replacement to the Fitness function?</p>

<p>2.Because of the characteristics of the Alpha-Beta algorithm, I need to give the max score to the win condition, which in most cases is set to infinity. However, because I can't use Infinity, I just used an absurdly large number. Do I also need to add this score to the chromosome? Or because it's insignificant and doesn't change the values of the grading function, leave it as a constant? </p>

<p>3.When initially creating chromosomes, random generation, following standard distribution is said to be the most optimal. However, genes in my case have large deviation. Would it still be okay to generate chromosomes randomly?</p>
"
1080,"<p>Lets say I have a limited amount of training data (1000 documents each having 10000 values) and after learning with those, the program is 
 basically not allowed to fail.</p>

<p>From what I've read, even very easy tasks take a machine learning algorithm several thousand learning cycles until it is about 98% reliable.</p>

<p>Which parameters need to be tweaked so that even with many many values, the program can make reliable decisions?</p>
"
1081,"<p>We all know how robots are getting more and sophisticated and more interesting </p>

<p>What is the future of <strong>robotics</strong> in relation to AI, how and how will AI work with robotics in improving to affect our world in a positive way.</p>
"
1082,"<p>Would it not even be more helpful? It will have more time to transform the input with the extra zeros padded to the end.</p>
"
1083,"<p>I read these things on the internet like</p>

<blockquote>
  <p>My model determines the future scope..."" </p>
</blockquote>

<p>or </p>

<blockquote>
  <p>My model gives accurate readings about what the score would be...""</p>
</blockquote>

<p>What are these models? How are they designed?</p>
"
1084,"<p>Technically speaking, could we code in natural language once we pass the Turing test? Would passing the Turing test at least simplify programming languages' syntax?</p>
"
1085,"<p>From this <a href=""http://computer-go.org/pipermail/computer-go/2017-October/010307.html"" rel=""nofollow noreferrer"">link</a>, AlphaGo would take millenia to run in regular hardware.</p>

<blockquote>
  <p>They generated 29 million
  games for the final result, which means it's going to take me about 1700
  years to replicate this. </p>
</blockquote>

<p>Are these calculations correct?</p>
"
1086,"<p>A blog post called ""Text Classification using Neural Networks"" states that the derivative of the output of a sigmoid function is used to measure error rates.</p>

<p>What is the rationale for this?</p>

<p>I thought the derivative of a sigmoid function output is just the slope of the sigmoid line at a specific point. </p>

<p>Meaning it's steepest when sigmoid output is 0.5 (occuring when the sigmoid function input is 0).</p>

<p>Why does a sigmoid function input of 0 imply error (if i understand correctly)?</p>

<p><strong>Source:</strong> <a href=""https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6"" rel=""nofollow noreferrer"">https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6</a></p>

<blockquote>
  <p>We use a sigmoid function to normalize values and its derivative to
  measure the error rate. Iterating and adjusting until our error rate
  is acceptably low.</p>
</blockquote>

<pre><code>def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

def sigmoid_output_to_derivative(output):
    return output*(1-output)

def train(...)
    ...
    layer_2_error = y - layer_2
    layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)
    ...
</code></pre>

<p><strong>UPDATE</strong></p>

<p>Apologies. I don't think I was clear (I've updated the title)</p>

<p>I understand we don't need to use sigmoid as the activation funtion (we could use relu, tanh or softmax).</p>

<p>My question is about using the <code>derivative to measure the error rate</code> (full quotation from article above in yellow) -> what does the derivative of the activation function have to do with measuring/fixing the ""error rate""?</p>
"
1087,"<p>In the paper <a href=""https://arxiv.org/abs/1506.07285"" rel=""nofollow noreferrer"">Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</a> the authors described a Dynamic Memory Network in the context of question answering. Then, they also tested the network on sentiment classification and part-of-speech tagging. In those applications, what is the 'question' being inputted to the network?</p>
"
1088,"<p>I have a MySQL database that contains datetime for every check-in with an RFID card. I have millions of records in it. It shouts machine learning prediction for me.</p>

<p>So I'd like to predict popular times to see when most people use the terminal. I'd like to represent it the same way as Google does at places:</p>

<p><a href=""https://i.stack.imgur.com/J5ARp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J5ARp.jpg"" alt=""Google&#39;s popular times panel at location search""></a></p>

<p>This is the rare case when I do not ask for code, but keywords.</p>

<p>I assume I have to predict number of check-ins between time ranges. Also I am sure I have to make a dataset like this:</p>

<pre><code>                        (These are the number of check-ins in hour ranges)
month | day | day-name | 0-1 | 1-2 | ... | 6-7 etc. 
jan   | 1   | Monday   |  0  |  1  |     |  12
jan   | 2   | Tuesday  |  1  |  3  |     |  15
</code></pre>

<p>So it can predict today's popular times based on what day it is, also by day of given month (as Saturdays and Sundays will be dead, also 25th of December. A good prediction should know these from the dataset).</p>

<p>As I wrote this question I solved most of it it seems. <strong>The only thing I need is a keyword</strong> as I have little experience in this. <strong>What model fits this best?</strong></p>
"
1089,"<p>I am using the following, fairly simple code to predict an output variable which may have 3 categories:</p>

<pre><code>n_factors = 20
np.random.seed = 42

def embedding_input(name, n_in, n_out, reg):
    inp = Input(shape=(1,), dtype='int64', name=name)
    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)

user_in, u = embedding_input('user_in', n_users, n_factors, 1e-4)
artifact_in, a = embedding_input('artifact_in', n_artifacts, n_factors, 1e-4)

mt = Input(shape=(31,))
mr = Input(shape=(1,))
sub = Input(shape=(24,))

def onehot(featurename):
    onehot_encoder = OneHotEncoder(sparse=False)
    onehot_encoded = onehot_encoder.fit_transform(Modality_Durations[featurename].reshape(-1, 1))
    trn_onehot_encoded = onehot_encoded[msk]
    val_onehot_encoded = onehot_encoded[~msk]
    return trn_onehot_encoded, val_onehot_encoded

trn_onehot_encoded_mt, val_onehot_encoded_mt = onehot('modality_type')
trn_onehot_encoded_mr, val_onehot_encoded_mr = onehot('roleid')
trn_onehot_encoded_sub, val_onehot_encoded_sub = onehot('subject')
trn_onehot_encoded_quartile, val_onehot_encoded_quartile = onehot('quartile')

# Model
x = merge([u, a], mode='concat')
x = Flatten()(x)
x = merge([x, mt], mode='concat')
x = merge([x, mr], mode='concat')
x = merge([x, sub], mode='concat')
x = Dense(10, activation='relu')(x)
BatchNormalization()
x = Dense(3, activation='softmax')(x)
nn = Model([user_in, artifact_in, mt, mr, sub], x)
nn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

nn.optimizer.lr = 0.001
nn.fit([trn.member_id, trn.artifact_id, trn_onehot_encoded_mt, trn_onehot_encoded_mr, trn_onehot_encoded_sub], trn_onehot_encoded_quartile, 
       batch_size=256, 
       epochs=2, 
       validation_data=([val.member_id, val.artifact_id, val_onehot_encoded_mt, val_onehot_encoded_mr, val_onehot_encoded_sub], val_onehot_encoded_quartile)
      )
</code></pre>

<p>Here's the summary of the model:</p>

<pre><code>____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
user_in (InputLayer)             (None, 1)             0                                            
____________________________________________________________________________________________________
artifact_in (InputLayer)         (None, 1)             0                                            
____________________________________________________________________________________________________
embedding_9 (Embedding)          (None, 1, 20)         5902380     user_in[0][0]                    
____________________________________________________________________________________________________
embedding_10 (Embedding)         (None, 1, 20)         594200      artifact_in[0][0]                
____________________________________________________________________________________________________
merge_25 (Merge)                 (None, 1, 40)         0           embedding_9[0][0]                
                                                                   embedding_10[0][0]               
____________________________________________________________________________________________________
flatten_7 (Flatten)              (None, 40)            0           merge_25[0][0]                   
____________________________________________________________________________________________________
input_13 (InputLayer)            (None, 31)            0                                            
____________________________________________________________________________________________________
merge_26 (Merge)                 (None, 71)            0           flatten_7[0][0]                  
                                                                   input_13[0][0]                   
____________________________________________________________________________________________________
input_14 (InputLayer)            (None, 1)             0                                            
____________________________________________________________________________________________________
merge_27 (Merge)                 (None, 72)            0           merge_26[0][0]                   
                                                                   input_14[0][0]                   
____________________________________________________________________________________________________
input_15 (InputLayer)            (None, 24)            0                                            
____________________________________________________________________________________________________
merge_28 (Merge)                 (None, 96)            0           merge_27[0][0]                   
                                                                   input_15[0][0]                   
____________________________________________________________________________________________________
dense_13 (Dense)                 (None, 10)            970         merge_28[0][0]                   
____________________________________________________________________________________________________
dense_14 (Dense)                 (None, 3)             33          dense_13[0][0]                   
====================================================================================================
Total params: 6,497,583
Trainable params: 6,497,583
Non-trainable params: 0
_____________________________
</code></pre>

<p>But on the <code>fit</code> statement, I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-71-7de0782d7d5d&gt; in &lt;module&gt;()
      5        batch_size=256,
      6        epochs=2,
----&gt; 7        validation_data=([val.member_id, val.artifact_id, val_onehot_encoded_mt, val_onehot_encoded_mr, val_onehot_encoded_sub], val_onehot_encoded_quartile)
      8       )
      9 # nn.fit([trn.member_id, trn.artifact_id, trn_onehot_encoded_mt, trn_onehot_encoded_mr, trn_onehot_encoded_sub], trn.duration_new,

/home/prateek_dl/anaconda3/lib/python3.5/site-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1520             class_weight=class_weight,
   1521             check_batch_axis=False,
-&gt; 1522             batch_size=batch_size)
   1523         # Prepare validation data.
   1524         do_validation = False

/home/prateek_dl/anaconda3/lib/python3.5/site-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)
   1380                                     output_shapes,
   1381                                     check_batch_axis=False,
-&gt; 1382                                     exception_prefix='target')
   1383         sample_weights = _standardize_sample_weights(sample_weight,
   1384                                                      self._feed_output_names)

/home/prateek_dl/anaconda3/lib/python3.5/site-packages/keras/engine/training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    142                             ' to have shape ' + str(shapes[i]) +
    143                             ' but got array with shape ' +
--&gt; 144                             str(array.shape))
    145     return arrays
    146 

ValueError: Error when checking target: expected dense_14 to have shape (None, 1) but got array with shape (1956554, 3)
</code></pre>

<p>How do I resolve this error? Why is the final layer expecting <code>(None,1)</code> when according to the <code>summary()</code> it has to output <code>(None,3)</code>?</p>

<p>Any help would be greatly appreciated.</p>
"
1090,"<p>I'm attempting to make a bot for the Connect 4 competition on <a href=""http://riddles.io"" rel=""nofollow noreferrer"">http://riddles.io</a></p>

<p>My bot isn't horrible, like it's getting up the ladder, but it cannot compete with the winning bots.</p>

<p>I'm using a Neural Network which is fully connected with one hidden layer. Internally it uses the sigmoid function as the activator in each Neuron. I've trained it over 500,000 games with TD-Lambda back propagation. The alpha and beta values (i.e. learning rates) are set to <code>0.1</code> each, and the lambda for the eligibility trace is set to <code>0.7</code>. There are 2 outputs nodes, one to give the value of this position for Player 1, the other to give the value of the position for Player 2. Upon a win, these are back-propagated with <code>-1</code> for a loss and <code>1</code> for a win, on a draw they are both back-propagated with a <code>0</code>.</p>

<p>There is a bias input and weight for every neuron as well.</p>

<p>All of the weights are initialised to a random value between <code>+-4*sqrt(6/totalNumberWeights)</code>.</p>

<p>The board state is represented to the network as:</p>

<ol>
<li>For each space in the board, 2 values:

<ul>
<li>For the first value if player 1 occupies this space it's a <code>1</code>, otherwise it's a <code>0</code></li>
<li>For the second value if player 2 occupies this space it's a <code>1</code>, otherwise it's a <code>0</code></li>
<li>If both are <code>0</code> it would mean it's a free spot</li>
</ul></li>
<li>For each space on the board another 2 values:

<ul>
<li>For the first value, if placing a token here would result in a Connect 4 for player 1, it's a <code>1</code>, otherwise a <code>0</code></li>
<li>For the second value, if placing a token would get player 2 a Connect 4, then it's a <code>1</code> otherwise a <code>0</code></li>
<li>So if no one would win by placing a token here, it's two <code>0</code> values for these two inputs</li>
</ul></li>
<li>Two final values, the first indicating whether it's player 1's turn or not, then second indicating if it's player 2's turn or not</li>
</ol>

<p>What I see when my bot is playing is that it makes what I assume are somewhat clever moves, like it's preparing for both horizontal and diagonal moves.</p>

<p>But when the other bot will get a Connect 4 on the next move, my bot fails to place a token there. The best bot in the comp seems capable of setting itself up to get three in a row with a free space on both sides, so that it will definitely get a Connect 4. Again, my bot does not seem to be able to see this coming.</p>

<p>What I think the issue is, other than potentially my learning rates, is that I've represented the board in a bad way. Is there a better way to represent it to the Network so that it can more accurately estimate the value of a board state, and so that it doesn't fail to identify an immediate Connect 4 threat?</p>
"
1091,"<p>I would like to measure time for forward and backward times on TF-Slim (over all network and per-layer) like caffe does. However, it just logs the step/iteration time, and I have no idea of how to do that without changing the source and recompiling (takes a lot of time on my machine)</p>

<p>I also would like to log a not-rounded loss value (it rounds to 4 digits after dot)</p>
"
1092,"<p>On books and documents explaining the Naive Bayes generative algorithm I always find the following  equation:</p>

<blockquote>
  <p>argmaxy p(y|x) = argmaxy (p(x|y)*p(y)/p(x)) ≈ argmaxy (p(x|y)*p(y))</p>
</blockquote>

<p>What I don't understand is the second approximately equal, why should p(x)≈1 ?</p>

<p>Thank you very much</p>
"
1093,"<p>There has been recent uptick in interest on <strong>XAI</strong> or e<strong>X</strong>plainable <strong>A</strong>rtificial <strong>I</strong>ntelligence. Here is XAI's mission as stated on its <a href=""https://www.darpa.mil/program/explainable-artificial-intelligence"" rel=""noreferrer"">DARPA page</a>:</p>

<blockquote>
  <p>The Explainable AI (XAI) program aims to create a suite of machine learning techniques that:</p>
  
  <ul>
  <li>Produce more explainable models, while maintaining a high level of learning performance (prediction accuracy); and</li>
  <li>Enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners.</li>
  </ul>
</blockquote>

<p>Here is the <a href=""https://www.nytimes.com/2017/11/21/magazine/can-ai-be-taught-to-explain-itself.html"" rel=""noreferrer"">link</a> to a recent New York Times piece on the same - the article does a good job of explaining the need for XAI from a human interest perspective as well as providing a glancing outlook on the techniques being developed for the same. The force behind <strong>XAI</strong> movement seems to center around the (up &amp; coming?) concept of <strong><em>right to explanation</em></strong> - The requirement that Applications based on AI that significantly impact Human lives via their decisions be able to explain to stakeholders the factors/reasons leading up to said decision. </p>

<p>my question is as follows:</p>

<ul>
<li>How is it reasonable, the right to explanation, given the current standards at which we hold each other accountable?</li>
</ul>
"
1094,"<p>I am currently looking for AI uses cases for telco. What are the different AI use cases for telcos/communications service providers</p>
"
1095,"<p>Could someone recommend a starting point (article or example) for permutation invariant neural networks? </p>

<p>Definition of permutation invariant nn: Let's have neural network f, that takes as input n data points x_1 ... x_n. We say f is permutation invariant if f(x_1 ... x_n) = f(pi(x_1 ... x_n)) for any permutation pi. </p>
"
1096,"<p>As I am trying to make an AI with reinforcement learning, I have found out and implemented a lot of things such as both these topics (NNs and RL) separately. But when trying to combine them, I have ran into trouble. I have not been able to find or think of a way to properly do backpropagation with RL. So what I was trying to do was a local search for all actions and then use a neural net for the Q(s, a) function. How would one do the backpropagation in such a neural net? </p>

<ul>
<li>Up to this point, I have only done things with gradient descent. Should one use a different algorithm? Could one calculate the Q(s, a) value based on the output of the neural net with a discount factor? </li>
</ul>

<p>This is what the formula would suggest, but I could not find any confirmation.</p>
"
1097,"<p>Collecting and labeling training data for a supervised learning tasks is incredibly time-consuming and costly.</p>

<p>For instance; Let's say you wrote a script that went on Google images and got you 5000 pictures for each of 10 classes. You then use an unsupervised algorithm to cluster them. Then, you train another, supervised algorithm using the labels from the scraper as ground truth. Obviously, your network will perform more poorly than one with perfectly labelled data, but is there a way to guesstimate how much?</p>

<p>Perhaps there are 50 mislabeled images in each class. That would most likely be better than 500 mislabeled images, but I'm wondering if there is a way to predict how much (even if it is by someone's rules of thumb or something like that).</p>
"
1098,"<p>I'm training two CNNs (AlexNet e GoogLeNet) in two differents DL libraries (Caffe e Tensorflow). The networks was implemented by dev teams of each libraries (<a href=""https://github.com/BVLC/caffe/tree/master/models"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/tensorflow/models/tree/master/research/slim/nets"" rel=""nofollow noreferrer"">here</a>)</p>

<p>I reduced the original Imagenet dataset to 1024 images of 1 category -- but setted 1000 categories to classify on the networks.</p>

<p>So I trained the CNNs, varying processing unit (CPU/GPU) and batches sizes, and I observed that the losses converges fastly to near zero (in mostly times before 1 epoch be completed), like in this graph (Alexnet on Tensorflow):</p>

<p><a href=""https://i.stack.imgur.com/GXeEK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GXeEK.png"" alt=""""></a></p>

<p>In portuguese, 'Épocas' is epochs and 'Perda' is loss.</p>

<p>The weight decays and initial learning rate are the same as used on models that I downloaded, I only changed the dataset and the batch sizes.</p>

<p>Why my networks are converging this way, and not like <a href=""https://github.com/BVLC/caffe/issues/3801#issuecomment-194654755"" rel=""nofollow noreferrer"">this way</a>?</p>
"
1099,"<p>Let's suppose I have 5 images, all of which I assure you are of the same item, but from various angles and perhaps different lighting conditions. I now supply you with an additional image, and I want a score of how likely this image is to contain the item depicted in the first five pictures.</p>

<p>Let us suppose that the item isn't too complex. It won't be a pile of fabric with a pattern on it, dropped several different ways, and won't be a keychain with keys in different conformations. It will also be more complex than just a blue ball shot from different angles. How might you approach the problem of scoring this image?</p>
"
1100,"<p>I am trying to build a neural network suitable to measure similarity between pairs of images. In particular I am interested in shoes. I have a query image (e.g. a shoe that I just took a picture of) and I want to find similar shoes in a database (several thousands of images).</p>

<p>I tried using MAC feature (e.g. max pool over the entire spacial dimension on last (or some other) convolution layer of say VGG16) (here is a link to a paper <a href=""https://arxiv.org/pdf/1511.05879.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1511.05879.pdf</a>). The two MAC vectors are compared using cosine similarity. That works, but among the top matches there are always a few very strange shoes (e.g when I submit a query image with a boot I find sandals among other boots with extremely high similarity score).</p>

<p>What would be a better way of doing that? Something more robust to finding shoes similar in shape to the query image. Thanks!</p>
"
1101,"<p>Can an AI learn to play chess if you give it nothing but ""the goal is to win"" as starting criteria? If not, what is the minimum information the AI would need to be ""seeded"" with in order to learn to play chess?  What techniques could be used to create an AI that learns to play chess independently? </p>
"
1102,"<p>currently AI is advancing fast in deep learning: <a href=""http://www.telegraph.co.uk/science/2017/12/06/entire-human-chess-knowledge-learned-surpassed-deepminds-alphazero/"" rel=""nofollow noreferrer"">Entire human chess knowledge learned and surpassed by DeepMind's AlphaZero in four hours</a>.</p>

<p>As a layman, I'm taking this as a quite powerful searching algorithm, using artificial neural networks to identify the patterns of each game.</p>

<p>However, how good is AI doing in math?</p>

<p>For example, the key to the theory of the game <a href=""https://en.wikipedia.org/wiki/Nim"" rel=""nofollow noreferrer"">Nim</a> is the binary digital sum of the heap sizes, that is, the sum (in binary) neglecting all carries from one digit to another. This operation is also known as <a href=""https://en.wikipedia.org/wiki/Exclusive_or"" rel=""nofollow noreferrer"">""exclusive or"" (xor)</a> or ""vector addition over GF(2)"". </p>

<p>Is AI good enough to discover/invite operations/logics such as ""exclusive or"", or, more advanced, abstract algebra in <a href=""https://en.wikipedia.org/wiki/Finite_field"" rel=""nofollow noreferrer"">finite field</a>?</p>
"
1103,"<p>I am using Keras to train different NN. I would like to know why if I increment the epochs in 1, the result until the new epoch is not the same. I am using shuffle=False, and np.random.seed(2017), and I have check that if I repeat with the same number of epochs, the result is the same, so not random initialization is working.</p>

<p>Here I attach the picture of the resulting training with 2 epochs:</p>

<p><a href=""https://i.stack.imgur.com/DtLWl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DtLWl.png"" alt=""""></a>
And here I attach the picture of the resulting training with 3 epochs:</p>

<p><a href=""https://i.stack.imgur.com/auDvm.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/auDvm.png"" alt=""enter image description here""></a></p>

<p>Also, I would like to know why the training time is not (3/2) and how is it possible that some of them have less accuracy with one more epoch.</p>

<p>Thanks a lot!</p>
"
1104,"<p>I was thinking, what if we could combine Artificial Intelligence (Neural network for image recognition), computer hardware and a security camera for identify any breaking into our backyard at 12:00am - 8:00am? Of course my current knowledge leads me to only a simple question. So, in order to have a general idea:</p>

<ul>
<li>¿Have been this already solved using a commercial or free software?</li>
<li>¿Can this be done using TensorFlow?</li>
<li>¿Is there any free set of images with millions of them to teach any AI distinguish between a man and another moving object?</li>
<li>¿Approximate hardware requirements for doing this?</li>
</ul>

<p>If this question could be silly please mark it as off-topic. I based this idea on autonomous driving car, they can both recognize images and drive at the same time. Unless they have within a super computer I guess maybe the previous idea can be fulfill.</p>

<p>Update 1: I found this <a href=""https://ai.stackexchange.com/questions/1453/can-convnets-be-used-for-real-time-object-recognition-from-video-feed"">Can ConvNets be used for real-time object recognition from video feed?</a> but I guess it could be outdated. Right now I'm in the land of ""maybe"" (lack of knowledge).</p>
"
1105,"<p>I'm trying to get up to speed on the latest research regarding indoor localization, scene classification, navigation in changing environment, etc.</p>

<p>Any advice would be appreciated, but I'm especially interested in recent research papers from vetted sources.  </p>
"
1106,"<p>I'm trying to understand how to build the ANN on cognitrons, so I have read theory for that topic and found the scheme:<br>
<a href=""https://i.stack.imgur.com/CCPa1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CCPa1.png"" alt=""Cognitrons""></a><br>
As I got neurons are subdivided in two classes: the exciting and the inhibitory. I have written these classes:<br>
<strong>Exciting neuron:</strong>  </p>

<pre><code>typedef float signal;
typedef std::vector&lt;signal&gt; sigvec;

class inhibitory_neuron
{
public:
    inhibitory_neuron() {}
    ~inhibitory_neuron() {}
    virtual signal call(const sigvec &amp;e_inputs, const sigvec &amp;i_inputs)
    {
        unused(i_inputs);
        signal sum = 0;

        for (auto i = std::begin(e_inputs); i != std::end(e_inputs); ++i) {
            sum += *i;
        }

        return sum;
    }
};
</code></pre>

<p>Here <code>e_inputs</code> are <code>c</code>-factors (sum of <code>c</code> is <code>1</code>). But in the formula:<br>
<a href=""https://i.stack.imgur.com/5cR97.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5cR97.png"" alt=""Formula""></a><br>
I can't get it... Whaz <code>OUT_i</code> there?
And below my <strong>Exciting neuron</strong> class is:  </p>

<pre><code>class exciting_neuron : inhibitory_neuron
{
protected:
    std::unique_ptr&lt;sigvec&gt; m_e_weights;
    std::unique_ptr&lt;sigvec&gt; m_i_weights;
public:
    exciting_neuron(sigvec &amp;&amp;e_weights, sigvec &amp;&amp;i_weights)
        : m_e_weights{std::make_unique&lt;sigvec&gt;(std::move(e_weights))},
          m_i_weights{std::make_unique&lt;sigvec&gt;(std::move(i_weights))}
    {

    }
    ~exciting_neuron() {}
    signal call(const sigvec&amp; e_inputs, const sigvec&amp; i_inputs) override
    {
        auto ew = *m_e_weights;
        auto iw = *m_i_weights;
        auto ei = e_inputs;
        auto ii = i_inputs;

        if (ew.size() != iw.size() || iw.size() != ii.size())
        {
            throw std::invalid_argument(""Wrong input size."");
        }

        signal e_sum = 0, i_sum = 0;

        for (unsigned int j = 0; j &lt; ew.size(); ++j)
        {
            e_sum += ew[j] * ei[j];
        }

        for (unsigned int j = 0; j &lt; iw.size(); ++j)
        {
            i_sum += iw[j] * ii[j];
        }

        signal n = (1 + e_sum) / (1 + i_sum) - 1;
        return n &gt;= 0 ? n : 0;
    }
};
</code></pre>

<p>Here <code>e_inputs</code> are <code>a</code>-factors (exciting) and <code>i_inputs</code> are <code>b</code>-factors (inhibitory).<br>
But how should the ANN structure look? I mean I can't get where should I put an exciting neuron and an inhibitory... Has it some samples or rules of the structure, I couldn't find?</p>
"
1107,"<p>I have a ANN but when I added two or more features as inputs to it, the accuracy of my ANN has been decreased.  I have a remaining useful life (prediction of RUL) problem that I want to predict.  When I've used features like RMS and KURTOSIS or together, in spite of fact that the system should be improved, it is getting worse. </p>

<ul>
<li>Why might this be happening? What are the potential reasons for this degradation in performance?</li>
</ul>

<p>I know that when we added more nodes in layers (like hidden layers), overlearning can happen.  Would that be related to my problem re: using more than two features?</p>
"
1108,"<p>I am following Andrew Ng Course on Artificial Neural Networks.<br>
He talks about CNN's (Convolutional Neural Networks) and RNN's (Recurrent Neural Networks).<br>
What is the fundamental difference between them? Where are they applied?</p>
"
1109,"<p>I have a 10gb file of a time series 1D signal. I want to find some patterns within this signal, I know CNN's are great for this but the problem is I don't have any training data.</p>

<p>Now I could of course spend an entire week slowly making 100 versions of a certain pattern to train the CNN with. But maybe there is some other way?</p>

<p>Maybe there is a way for the neural network to work out patterns on its own and simply categorize them? Like this is pattern A, this is pattern B.</p>

<p>My ultimate goal is to look at any size data and find the occurrences of patterns within the data.</p>

<p>Does anyone have any idea how this problem could be solved? I am just starting with machine learning so I am slowly learning of what's possible in this field.</p>

<p>Thanks</p>
"
1110,"<p>Did <a href=""https://plato.stanford.edu/entries/turing/"" rel=""nofollow noreferrer"">Alan Turing</a> expect the AIs <a href=""http://www.dictionary.com/browse/double-blind"" rel=""nofollow noreferrer"">to be aware</a> they were being <a href=""https://plato.stanford.edu/entries/turing-test/"" rel=""nofollow noreferrer"">Turing tested</a> while <a href=""https://www.abelard.org/turpap/turpap.php"" rel=""nofollow noreferrer"">the game</a> was being played?</p>

<p>I think it's slightly harder <a href=""https://en.wikipedia.org/wiki/Duck_test"" rel=""nofollow noreferrer"">to look like a duck</a> while <em>pretending to be</em> a duck than when you <em>believe you are</em> a duck. It is a step further.</p>

<p>Aside from all other criticisms of original article, did Alan Turing have the time and motivation to address the point in any later writing?</p>
"
1111,"<p>Two practical questions regarding the use of autoencoders. For the size of the NN, let's say I have 50 inputs, so layer sizes go like 50-25-12-6-2-6-12-25-50</p>

<ol>
<li><p>how much data I generally need to train it and how does it depend on the size of net?</p></li>
<li><p>I'd like to weigh samples differently - e.g. there is a time component to the problem the last samples are most recent, so should weigh more. Is there an easy way to incorporate it?</p></li>
</ol>
"
1112,"<p>is it possible to give a <strong>rule of thumb estimate about the size of neural networks that are trainable on common consumer grade GPUs</strong>?
For example:</p>

<p>The <a href=""https://arxiv.org/abs/1707.02286"" rel=""noreferrer"">Emergence of Locomotion (Reinforcement)</a> paper trains a network using tanh activation of the neurons. They have a 3 layer NN with 300,200,100 units for the <em>Planar Walker</em>. But they don’t report the hardware and time ...</p>

<p>But could a rule of thumb be developed? Also just based on current empirical results, so for example:</p>

<p>X Units using sigmoid activation can run Y learning iterations per h on a 1060.</p>

<p>Or using activation function a instead of b causes a n times decrease in performance.</p>

<p>If a student/researcher/curious mind is going to buy a GPU for playing around with these networks, how do you decide what you get? A 1060 is apparently the entry level budget option, but how can you evaluate if it is not smarter to just get a crappy netbook instead of building a high power desktop and spend the saved $ on on-demand cloud infrastructure.</p>

<p>Motivation for the question: I just purchased a 1060 and (clever, to ask the question afterwards huh) wonder if I should have just kept the $ and made a Google Cloud account. And if I can run my master thesis simulation on the GPU.</p>
"
1113,"<p>In the current rush of artificial intelligence research, fueled by NN, independent of the paper I choose, the NN are always <strong>trained by themselves</strong>. Sure, there are architectures that combine CNN and RNN or LSTMs in a way that can help to solve multiple problems interacting (like labeling images with human readable text snippets), but independent of this, they always learn by themselves. </p>

<ul>
<li>Supervised networks just get a bunch of examples to learn from.</li>
<li>Reinforcement algorithms run around virtual spaces falling over hundreds of times before learning to walk properly.</li>
</ul>

<p>This might sound silly but <em>no one helps them and no one plays with them</em>. Children, researchers, puppies, dolphins, ... all intelligent beings we are aware of interact with each other. There are <a href=""https://en.wikipedia.org/wiki/Learning#Types"" rel=""nofollow noreferrer"">many forms of learning</a>, one very important one being <a href=""https://en.wikipedia.org/wiki/Social_learning_theory"" rel=""nofollow noreferrer"">social learning</a> which includes observational learning. We imitate, copy and learn from others all the time. There is also <a href=""https://www.nytimes.com/2017/04/18/books/review/knowledge-illusion-steven-sloman-philip-fernbach.html"" rel=""nofollow noreferrer"">a new idea of looking at knowledge</a> that suggests that we all aren't as independent as our culture might have taught us to think we are. That knowledge really lies in the connections between individuals instead of inside of each agent itself, just like the connections of neurons are what makes the brain work, not the neurons themselves.</p>

<p>Couldn't such interaction between learning agents be enormously valuable? If you throw 5 agents in a room (NN a Knowledge Based Agent a simple pre-programmed one etc) and let them learn from each other through observation and inspiration, what effects could one expect? </p>

<p>I am not talking about ensemble learning as this just throws several hypotheses together and takes the average over all results. I am talking about a complex way of agents interacting with each other during learning. </p>

<p>Is there any research on this idea? If so, what were the results? This is the only one I found:</p>

<ul>
<li><a href=""https://arxiv.org/abs/1605.06676"" rel=""nofollow noreferrer"">Learning to Communicate with Deep Multi-Agent Reinforcement Learning</a></li>
</ul>
"
1114,"<p>How would you solve the problem of identifying certain customer in a grocery store? Suppose our client is already signed-up on our website with an unique ID given to him. To come in to the store, firstly he must place a phone with a QR code in front of the reader, so that our server is informed on a specific client entering the shop. Then after successful authentication he heads towards the shelf with goods and pick some item. Identifying product name turned out not to be a challenge, as opposed to person. </p>

<p>From my perspective, the solution is to make a several photo of face from a different angle while person coming in. Quickly train CNN and feed it with face images when customer picks goods in order to choose the corresponding one. </p>

<p>So, what are your thoughts on this issue? What approach would you take to work it out?</p>
"
1115,"<p>At this moment, I am able to use NN to identify object such as human when given a frame from the camera. Once locate the object, then I can feed the human object image to either NN that's designed to classify male or female.</p>

<p>Let's say I get 1 frame per second from camera and perform detection, the objective is to track number of male and female walk pass the camera within the given hours.</p>

<p>My question is, the same person in multiple frames will be over counted. I couldn't wrap my head around how can I train a NN to understand that this is the same person without dive into facial recognition? I'm sure there is some tracking technique that I just don't know.</p>

<p>One little constraint, if the person left the camera frame and come back into it later, it is fine to treat it as two people.</p>

<p>Any help or direction will help!</p>

<p>Thank you all in advanced!</p>
"
1116,"<p>I'm testing various learning rates and neural network configurations. I'm testing over 10,000 games with the first 2000 having random starting moves and a general randomness throughout of about 20%, i.e. 20% of moves are random.</p>

<p>In all configurations I initiate the weights to random values.</p>

<p>What I've found is that in all comfigurations, Player 1 will win the majority of games, or Player 2 will. There's no 50/50 split.</p>

<p>Is this expected / normal?</p>
"
1117,"<p>I have deep interest in AI and want to start learning how to implement current method. I know about java and C++; are these languages sufficient?</p>

<p>I'd appreciate suggestions regarding free online courses/websites that utilize Java/C++. If I lag some knowledge which is required before starting AI then please let me know.</p>
"
1118,"<p>There seem to be so many sub-fields, so I'm interested in getting a better understanding of the approaches.</p>

<p>I'm looking for information on a single framework per answer, in order to allow for granularity without the overall answer getting too long. For instance; Deep Learning Neural Networks would be a single answer.</p>
"
1119,"<p>I heard that your ML model's quality depends directly on the quality and the quantity of data you use. </p>

<p>So I was thinking that can question answers be used as data to train an algorithm which can solve any high school science problems? Because we do have a gazillion number of high school books with millions of Question-Answers which are both high in quality as well as quantity. </p>

<p>P.S: I don't have any in-depth knowledge in any of the AI fields, so please answer accordingly!</p>
"
1120,"<p>Trying to understand the VGG architecture and I have these following questions.</p>

<ol>
<li>I understand the general understanding of increasing filter size is because we are using max pooling and so its image size gets reduced. So in order to keep information gain, we increase filter size. But the last few layers in the VGG architecture, the filter size remained same when vgg was max pooling from 14x14 to 7x7 image size, the filter size remained same at 512x512. Why wasn’t there the need to increase filter size there?</li>
<li>Also few consecutive layers, in the end, was constructed with both same filter and image size, those layers were built just to increase accuracy? (experimentation?)</li>
<li>And I couldn’t wrap around that visualization at final filters have the entire face as the feature as I understood through convolution visualizing (Matt Zieler video explanation). But max pooling causes us to see only a subset part of the image right? When filter size is 512x512 (face as the filter/feature) the image size as 7x7, so how does entire face as a filter will work on images when we are moving over small subset of the image pixels?</li>
</ol>

<p><a href=""https://i.stack.imgur.com/rVB4S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rVB4S.png"" alt=""VGG architecture summary""></a><a href=""https://i.stack.imgur.com/yfw43.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yfw43.png"" alt=""VGG architecture final layers summary""></a></p>
"
1121,"<p>I'm solving this problem similar to consumer-producer of materials (i.e. sand). </p>

<p>This is the graph of the problem:</p>

<p><a href=""https://i.stack.imgur.com/M3aRs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M3aRs.png"" alt=""enter image description here""></a></p>

<p>Where <code>Req</code> (<code>E0</code> and <code>E1</code>) are units that require material, and <code>Prod</code> (<code>C0</code> and <code>C1</code>) produce material. I'm looking for the first random solution or similar that meets the constraints i.e <code>E0</code> Require <code>10</code>, shared from <code>C0</code> or/and <code>C1</code>.</p>

<p>What would be a valid approach? 
I just think in Genetic Algorithms.</p>
"
1122,"<p>I am planning to build an app which will count the number of sqauts from videos. Assuming that the user and camera do not move, are there ways I can count the number of squats? Do such models to understand human activity and pose exist?</p>
"
1123,"<p>Currently I'm doing a project that's about creating an AI to play the game Gomoku (It's like tic tac toe, but played on a 15*15 board and requires 5 in a row to win). I have already successfully implemented a perfect tic tac toe AI using Q learning and having game states/actions stored in a table, but for a 15*15 board the possible game states become too large too implement this project.</p>

<p>My question is, should I use neural networks or genetic algorithms for this problem? And more specifically, how should I implement this?</p>
"
1124,"<p>Pieter Abbeel says that having access to the dynamics model, that is <code>P(s' | s,a)</code>, is unrealistic because it assumes we know the probability that we will reach all future states.</p>

<p>I don't understand how this is unreasonable? Could someone explain this to me like in a simple way?</p>
"
1125,"<p>I am thinking of an application where I want to teach a Neural Network to adjust some parameters on another device for a specific purpose.
I say ""adjust some parameters"" because I need the output to ""turn the knobs"" of something else, not only binary ""push buttons""</p>

<p>What kind of neural network do I need? is it a specific architecture?</p>

<p>The NNs I have used so far were sequential and outputting vectors like 
[0] or [1]
and classifiers with outputs like [1,0,0,0] [0,1,0,0] [0,0,1,0] [0,0,0,1] for example</p>

<p>But now I need outputs like [0.25, 0.99, 0.14, 0.54], </p>

<ul>
<li>0.25 for knob 1 </li>
<li>0.99 for knob 2</li>
<li>0.14 for knob 3</li>
<li>0.54 for knob 4</li>
</ul>

<p>Thank you for your answers</p>
"
1126,"<p>In the paper <a href=""https://www.nature.com/articles/nature14236"" rel=""nofollow noreferrer"">Human-level control through deep reinforcement learning</a>, the DQN architecture is presented, where the loss function is as follows</p>

<p><span class=""math-container"">$$
L_i(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \left[ \left( r + \gamma \max_{a'} Q(s', a'; \theta_i^-)  - Q(s, a; \theta) \right)^2\right]
$$</span></p>

<p>where <span class=""math-container"">$r + \gamma \max_{a'} Q(s', a'; \theta_i^-)$</span> approximates the ""target"" of <span class=""math-container"">$Q(s, a; \theta)$</span>. But it is not clear to me why. How can existing weights approximate the target (ground truth)? Isn't <span class=""math-container"">$r$</span> is a sample from a the experience replay dataset? Is <span class=""math-container"">$r$</span> a scalar value?</p>
"
1127,"<p><strong>Brief idea</strong><br>
I want to create an artificial intelligence to compete against other players in a board game.</p>

<p><strong>Game explanation</strong><br>
I have a board game similar to 'snakes and ladders'. You have to get to a final field before your opponent does. But instead of depending on luck (throwing the dices) this game uses something like 'food'. You can go as far as you'd like but it costs food to move (the more you move the more one extra field costs) and you can only get food on some special fields. And there aren't any snakes or ladders so you have to run the whole part. There are some more rules for example you can go backwards and are only allowed to go into the goal if you've got less than some amout of 'food' and there are some extra fields with other special effects.  </p>

<p><strong>For one player</strong><br>
If there was only one player as there isn't anything like 'luck' in this game I theoretically could just compute every single method to find the one and only best method. Practically I should use an algorithm that requires less computational power.  </p>

<p><strong>For two or more players</strong><br>
The challenge comes with the other player(s). I cannot visit an already taken field. And some other fields give me bonuses depending on my relative position to the other player (I'll just talk about two player games). For example only if I'm behind him that special field gives me some extra food.</p>

<p><strong>My question</strong><br>
First of all, I have to say that I'm kind of a beginner (I programmed nn, ga and rl) when it comes to artificial intelligence (don't tell me it could be to difficult for me. I know but I'm willing to learn and at least I want to try it ;) ). What would be ideally is if I had some kind of a neural network that knows the field bonuses and I would give my position, the opponents position, the food and so on (the <strong>state</strong> of the game) and it would compute a value between -100 and 100 (assuming fields from 0 to 100) of how many fields I should go (forward or backward).<br>
I read a bit about Q-learning, Deep Reinforcement Learning and Deep neural networks. Is this the right approach to solving my problem? And if yes, have you got any more concrete idea? The multiple actors and the sheer endless possibilities for moving depending on endless states make it hard for me to think of anything. Or is there a different, way better way that slipped past me?</p>

<p>Thanks for reading all this and for your help ;)</p>
"
1128,"<p>I have a two-class classification problem, where false positive error has a very big cost compared to the false negative-error. Is there a way to design a classifier for such problems (preferably with an implementation of the algorithm)?  </p>
"
1129,"<p>I have a computer science degree and have done AI courses and projects. I want to study AI in a good university, probably online because in my country there are no good universities.</p>

<p>Which  online AI masters degrees do you recommend me? Do you recommend me to try to study a ""normal"" master's degree instead of an online one?</p>
"
1130,"<p>I am interested in learning more about the capabilities of AI, one of my ideas with practical functionality is using images of the rear of a log hauling truck to measure the individual logs using AI. The diameter and length of the log determine the board foot of that log. The diameter is the variable, with most lengths being the same 8ft or 16ft length. The AI would have to measure the diameter usually in 1-inch increments. 
I assume you would have to train the AI using pictures of logs that were manually measured. The images would mostly be straight on rear shots, not much off angle. However not every tree grows perfectly round so it will need to be able to measure those to maximize board feet. </p>

<p>Using AI long term it would be interesting if it could suggest the best way to cut the log gaining the most lumber board feet from the log. </p>

<p>What would be used to develop something like this? Obviously some type of image recognition with learning.</p>
"
1131,"<p>I am trying to detect a TV channel logo inside a video file, so simply given an input <code>.mp4</code> video, detect if it has that logo present in a specific frame, say first frame, or not. </p>

<p>We have that logo in advance (although might not be the %100 same size) and the location is always fixed.</p>

<p>I already have a pattern matching-based approach. But that requires the pattern be %100 same size. I would like to use Deep Learning and Neural Network to achieve that. How can I do that? I believe CNN can have a higher efficiency?</p>

<p><img src=""https://i.stack.imgur.com/qa3Tp.png"" width=""400"">
<img src=""https://i.stack.imgur.com/LCDtS.png"" width=""400""></p>
"
1132,"<p>Even though modern chess playing programs have demonstrated themselves to be as strong (or stronger) than even the best human players for nearly 20 years now (1997 when IBM's Deep Blue defeated the world chess champion Gary Kasparov), why would a game like Chess still be considered a valuable research subject in Artificial Intelligence?  In other words, what can be gained by continuing to advance AI in areas that have already surpassed human capabilities . </p>

<p>For instance, as recently as November 2017, Google successfully challenged their deep learning technology against one of the worlds strongest <a href=""https://arxiv.org/pdf/1712.01815.pdf"" rel=""nofollow noreferrer"">chess playing programs</a>.</p>
"
1133,"<p>I am building model with medical dataset using <strong>deep learning</strong> methods.</p>

<p>Medical dataset consists of both numerical data such as age, sex
and images of xray scans(1024 x 1024) .</p>

<p>Labels consists of types of cancer .</p>

<p>I believe that ages and sex gonna affect output of network.</p>

<p>But including images will make the network biased towards images, because images will occupy most of the input layer.  </p>

<p>how can I design the input layer of network ? </p>

<p>additional information: I am not using <strong>CNN</strong> but normal deep learning network with <strong>two hidden layers</strong></p>
"
1134,"<p>I've modelled a micromanipulation domain with 22 subtasks:</p>

<ul>
<li>grasp: absinit, pregrasp, close</li>
<li>graspvertical: pregraspinit, pregrasp, close, remove</li>
<li>rotate: touchleft, touchright, closeleft, closeright</li>
<li>rotateside: touchleft</li>
<li>movebox: up, right, left</li>
<li>task: 90right, 90rightside, pickplace, pickplaceleft, graspvertical, stoprotating, done</li>
</ul>

<p>For example, if the user executes the macro “touchleft”, the robotgripper moves with (x=30,y=-30). Unfortunately the environment is noisy so in reality the touchleft-command works only sometimes. In certain condition another value like (x=28,y=-30) is necessary. So my question is of how to get a robust hierarchical architecture?</p>

<p>Short note: I've heard something about <a href=""https://pdfs.semanticscholar.org/b053/eb2a7531a3e97bc1b0a245e5ec13a64bc993.pdf"" rel=""nofollow noreferrer"">Associative Skill Memories</a> but I'm unsure if this works.</p>
"
1135,"<p>As self-driving technology is improving, there are so many companies developing  self-driving cars like Google, Uber, etc. Is it possible that we won't need any private/paid self-driving cars and the ""self-driving taxi"" becomes ubiquitous in the city? If we assume that there are such taxis everywhere, would transportation become extremely low cost or free? (The self-driving car company could benefit from broadcasting advertisements for advertising agencies.)</p>
"
1136,"<p>Capsule Networks seem to be a good solution for problems, which make up hierarchical complexity ( ( (eyes, nose, ears -> face); (fingers, nails, palm -> hand) ) -> human). NLP domain is a very clear hierarchical complexity problem, because there are words, sentences, paragraphs and chapters, whose meaning change based on the style of lower levels.</p>

<p>Is there any research papers or software tools on Capsule Networks and NLP, which I should be aware of? Is there related research papers, which have been investigating hierarchical complexity within the domain of NLP, which could be easily translated to Capsule Network?</p>
"
1137,"<p>Providing that</p>

<ul>
<li><p>more and more decisions about human life are (to be) decided by machine (like access to loans, housing, scholarship, jobs, healthcare, insurance, etc.)</p></li>
<li><p>at the same time, in many countries there are laws and codes of conduct against (negative/positive) discrimination</p></li>
</ul>

<p>Does there any industrial-accepted way to examine the AI system its legal compliance? </p>

<p>I believed that ACM/IEEE software engineer professional code of conduct can be applied here, but also like to learn more about auditing process from examiner side as well, if there's any.</p>

<p>Thank you. </p>
"
1138,"<p>I have a Macbook Air and Asus ROG 702 with a built in GeForce 1070 GPU. I'm considering getting an NVIDIA Volta for AI applications (tensorflow / pytorch) to run on either the mac or windows. Would they even be compatible? And if so, what box would I need to house the GPU? Would it be system agnostic? (I was thinking Aikito Node). </p>

<p>Thank you! All info is appreciated by this novice :D</p>
"
1139,"<p>I am trying to find literature on a network architecture that takes the following as in input:</p>

<ul>
<li>Action (like 'Up', 'Down', etc)</li>
<li>Image of current state</li>
</ul>

<p>and outputs:</p>

<ul>
<li>Image of next state</li>
</ul>

<p>I already have a lot of training data for the inputs. However, I am trying to find relevant literature/architecture for this problem. </p>
"
1140,"<p>I was watching a lecture on policy gradients vs Bellman equations. And they say that the Bellman equation indirectly creates a policy. While the policy gradient directly learns a policy?</p>

<p>Why is this?</p>
"
1141,"<p><em>TLDR : Is there an AI available that can recognize employees in a factory and tell when they entered and left pre-defined areas?</em></p>

<p>I work in a factory where we gather cycle time data from various inputs (computer interfaces, bar code readers and RFID tags). We follow both parts moving along the production line and persons working on those parts because they move from stations to stations during their day and can be more than one at the same place to help one another.</p>

<p>Our goal is to know how many people are working on each part at any given time, but the problem is that it takes time for people to check themselves in when they get to their workstation and they can also forget to do it.</p>

<p>Many employees asked me if I could find a way to track them automatically so they wouldn't need to check in everywhere they go. And then I stumbled upon <a href=""https://www.youtube.com/watch?v=O1pDOkzsFOU"" rel=""nofollow noreferrer"">Microsoft Workplace Safety Demo</a> and <a href=""http://www.scmp.com/magazines/post-magazine/long-reads/article/2123415/doctor-border-guard-policeman-artificial"" rel=""nofollow noreferrer"">Yitu System</a> (this one is scary) but they both seem to be a little overkill for my needs.</p>

<p>After learning about these, here is the ideal AI features that I'd need :</p>

<ul>
<li>Can use video feeds to detect new people in the workspace and prompt someone (via e-mail or text message) for identification.</li>
<li>Can use video feeds to recognize persons that are already known and document every time someone has entered or left a pre-defined zone (workstation) in the video feed.</li>
<li>Won't force me to have a camera for each workstation.</li>
<li>Allows one workstation to span on more than one video feed.</li>
</ul>

<p>As of now, I found nothing available that does this, but I may have missed lots of things because this is not what I am used to work with. So maybe you know something that could help me with that?</p>
"
1142,"<p>I'm creating a Decision Tree and at the very root level itself, I'm getting <strong>negative Information Gain</strong>. </p>

<p>As per my knowledge, Information Gain is always <code>&gt; 0</code>....</p>

<p>Any explanation....please....</p>

<p>Please look at the node below.....</p>

<pre><code>    [69+,42-]
     /   \        
    /     \                       
[56+,33-]  [11+,11-]

IG = H([69+, 42-]) - H([56+,33-],[11+,11-]) 

   = {(-69/111 * lg(69/111) -42/111 * lg(42/111)) 
      - 89/111 * ( -56/89*lg(56/89) -33/89*lg(33/89)) 
      - 22/111 * ( -11/22*lg(11/22) -11/22*lg(11/22))}

   = -0.004
</code></pre>

<p>It turns out that for every feature, the <code>IG &lt; 0</code>.</p>

<p>What should I do to decide the feature at the root node?</p>

<p>Thanks</p>
"
1143,"<p>I am a software engineer who has taken an interest in machine learning about a year ago. I work on a couple of projects where a large amount of data is collected, and am wondering what I can do (or practice) in order to start recognizing opportunities to implement machine learning. Until now, I've spent some time building various classifiers with tensorflow, caffe, etc. using pre-defined data sets for random things I've found online (whether it be photos, spreadshets, etc) - and I just want to be able to use it at work. What is the best way to start getting in the frame of mind where I start recognizing where I can apply it? I found this article: <a href=""https://hbr.org/2017/10/how-to-spot-a-machine-learning-opportunity-even-if-you-arent-a-data-scientist"" rel=""noreferrer"">https://hbr.org/2017/10/how-to-spot-a-machine-learning-opportunity-even-if-you-arent-a-data-scientist</a> but it didn't seem too helpful to me. If there are any other thoughts and/or resources online that you could point me to, that would be helpful. Thanks.</p>
"
1144,"<p>Model-based RL creates a model of the transition function.</p>

<p>Tabular Q-Learning does this iteratively (without directly optimizing for the transition function). So, does this make tabular Q-learning a type of model-based RL?</p>
"
1145,"<p><em>I have a <strong>video</strong> which is capture from a moving car and video showing plenty of details like pools, human, cars/buses, roads, etc, etc.</em></p>

<p>The video I am playing in unity3d and camera Showing that video and user is allow to click on video. I want to get different information from the videos like if someone clicked on video (like clicked on a bus object), then, unity UI should show that it is a bus object. <strong>It something like object detection in video within unity environment</strong>.</p>

<p>From last couple of days I searched and found different things like : </p>

<ol>
<li><a href=""https://www.assetstore.unity3d.com/en/#!/content/21088"" rel=""nofollow noreferrer""><strong>OpenCV for Unity3d</strong></a> an asset on asset store but cost is the problem.
So is there any tut or custom way available that i should write it
myself?</li>
<li><a href=""https://github.com/llSourcell/YOLO_Object_Detection"" rel=""nofollow noreferrer""><strong>Yolo</strong></a> but its in python and i found no integration guide that I
follow and integrate it in unity.</li>
<li><a href=""https://github.com/neutmute/emgucv/tree/master/Emgu.CV.Unity"" rel=""nofollow noreferrer""><strong>EmguCV Unity</strong></a> is also an option but its github don't conver any guide, I downloaded its
project there is not sample or docs. And <a href=""https://www.assetstore.unity3d.com/en/#!/content/24681"" rel=""nofollow noreferrer"">its paid version</a> cost is about 400$. </li>
</ol>

<p>Is there any third thing compatible with unity3d? or do i can use/integrate above given projects in unity?</p>
"
1146,"<p>I'm learning about NEAT from the following paper: <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf</a></p>

<p>I'm having trouble understanding how adjusted fitness penalizes large species and prevents them from dominating the population, I'll demonstrate my current understanding through an example and hopefully some one will correct my understanding.</p>

<p>let's say we have two species, A and B, species A did really well last generation and were given more children, this generation they have 4 children and thier fitnesses are [8,10,10,12] while B has 2 and thier fitnesses are [9,9] so now thier adjusted fitnesses will be A[2,2.5,2.5,3] and B[4.5,4.5].</p>

<p>now onto distributing children, the paper states :""Every species is assigned a potentially different number of offspring in proportion to the sum of adjusted fitnesses f'i of its member organisms""</p>

<p>So the sum of adjusted fitnesses is 10 for A and 9 for B thus A gets more children and keeps growing, so how does this process penalizes large species and prevent them from dominating the population?</p>
"
1147,"<p>This is the question which always being discussed, Will Artificial intelligence can be the undefeatable enemy for human being? </p>

<p>Like the Slaughter Bots( <strong>Killer Microdrones technology</strong>) can kill a human or an living thing in just some seconds, And it neither could be deceived by its target nor it could be stopped by any source, Once it's set to kill the target then it can never be stopped and it identifies its target. It is also the innovation of AI.</p>

<p>See <a href=""https://www.youtube.com/watch?v=oSyGbDDG2c4"" rel=""nofollow noreferrer"">Microdrones technology introduction</a>.</p>
"
1148,"<p>So, I have this huge amount of data, which has 7 vector features (float from 0 to 1). I am trying to build a kind of recommendation system, with a twist (it uses agents and negotiations and narratives; narratives meaning, that there will be temporal and partial order causal link dynamics, or ""short term memory"").</p>

<p>There is network effects in the data, since the more agents I have, the more potential there will be for match making; in simulation phase I will be just using randomness, instead of real data, but I believe the real dataset will also be more or less random: of course I think there will be nice clusters of correlations for the vectors, but I also want to understand the more general aspects of the problem, instead of just this specific use-case.</p>

<p>The agents will be generating a list of artifacts in co-operation and also evaluate how the generated list of artifacts match their preferences. The agents will be recommending items from their own list (and some times from a list of their related agents) and after each suggestion, there is negotiation.</p>

<p>If there would not be network effect in the data, I would use simple heuristics, which would measure the diff of each feature. However, in network effect world this does not work very well, because it will make many agents unable to co-operate.</p>

<p>I was thinking of something, where some features are preferred features by the agents in different circumstances (depending on the other agents), this will enable some elasticity to the system and IMO avoid the simple problems, of which many recommendation systems suffer (lack of context awareness; humans do not like the likes of other people, we have flexible preferences, which depend upon our mood).</p>

<p>Since this sounds like a very generic problem, it seems like I have just discovered a problem, which has been elegantly solved by someone else from the academia years ago, but I just don't know the name for this concept.</p>

<p>Instead of depending on strict evaluation criteria (only accept diff X per feature), how to choose evaluation function to make the agents more co-operative? I was thinking something, where diff has to be narrow for at least one feature, but may be big for three worst matching features; so that there would be essence of few features and diversity of several.</p>

<p>EDIT: As I have spent some more time with this problem, I have a hunch about which issues might be related:</p>

<ul>
<li>In real world networks, which depend upon energy efficient co-operation and signaling, scale-free networks seem to emerge; effective Agent-Based Model in network effect scenario should probably form co-operation networks, which follows some kind of Fitness Function.</li>
<li>Due to scale-free networks, it should be assumed, that the Agents using essential features (which are common in data) should be ranked higher and Agents using diverse features (which are rare in data) should be ranked lower; in scale-free networks (like Facebook) it has been observed, that all but 3% of nodes form edges to more popular nodes: this makes signaling efficient.</li>
<li>As a reference, in deep learning Capsule Networks have had some state-of-the-art success with this similar hierarchical ""recommendation for upper tiers"" mentality (there are much more lower layer features, than upper layer features).</li>
</ul>

<p>So, I have this idea, that when we use Agent-Based Models for solving problems with network effects, the evaluation function should evaluate Agents skill of passing the problem for more popular Agent, which could delegate it to the best Agent or attempt to solve it on it's own.</p>
"
1149,"<p>Can anybody suggest on how to get-started with Artificial Intelligence with Python specifically?  </p>

<p>What are some:
-Reliable resources I can utilize
-Basic test problems to solve
-Online tutorials and classes</p>

<p>Thanks</p>
"
1150,"<p>I can't understand the red box. what do does that <code>1</code> do before <code>{c(I)=j}</code> ?
Also how does all the algorithms work? May someone explains the to steps more clearly by one example!</p>

<p><a href=""https://i.stack.imgur.com/ylp04.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ylp04.png"" alt=""enter image description here""></a></p>
"
1151,"<p>I'm working on architecture for a game AI where, due to the nature of the game, the classical approach seems likely be sufficient to beat most humans--the endgame is tractable and traditional game-solving and is worth pursuing due to certain intrinsic properties of the game. (Strategy is a direct application of minimax, although there is a great deal of nuance in that determination.)</p>

<p>The simplified rules for the game can be found <a href=""http://www.mclassgames.com/rules-of-m/"" rel=""nofollow noreferrer"">here</a> and there is a <a href=""http://mbranegame.com/"" rel=""nofollow noreferrer"">free app</a> (no ads, no purchases, no data collected) where you can try the mechanics for the basic, non-trivial game. (Tutorial takes about 10 minutes.) </p>

<p>At some point, we'd like to integrate some form of <em>local</em> NN, But size of the database for learning/training would be dependent on how much volume the user is willing to devote to their ""mbrane"".  (These will initially be mobile devices such as phones and tablets.  New iPad pro has upt to 512GB, but most devices will be much smaller.)</p>

<p>Is this even feasible?  Today?  At some point in the future?</p>

<p>Uses: </p>

<ol>
<li>""Tuning"" AI behavior to the human player     </li>
<li>Weighting complex stability states based on current board state to determine optimal placement</li>
</ol>

<p><br>TUNING TO HUMAN BEHAVIOR<br>
<em>We want each discrete, local AI to evolve uniquely, in conjunction with human play, and adapt itself to the preferences of their respective humans.</em>
<br><br>
Hardness: Most simply, we want AI hardness to be determined by a Win/Loss ratio against the human player.  Hard = Human never wins.  Easy = Human always wins.  This forms a spectrum, so you might have settings for 2/3 W/L or 1/3 W/L.  These don't have to be precise, but the AI should tailor it's play strength for a subsequent game based on the outcome of the previous game. <br><br>
Individuality: We want the AI's to learn not only by self-play, but by play with their humans.  It's ok if the process is glacial b/c the basic evaluation functions and perfect endgame will provide inherent AI strength. The main thing is that the discrete AI's develop uniquely.  AI's will be able to play other AI's.<br><br></p>

<p>STABILITY STATES <br>
<em>There are three simple stability states that arise out of the mechanics. These states can be strong or weak. These can be a hard T/F, or, since the game is quantitative, values between 1 and 0, based on the respective regional deltas (Neutrality). ""Flipping"" of a region refers to the changing of the dominant player in the region.</em> <br><br>
Stability (weak: can the region be flipped with a single placement? strong: can the polarity of a region be flipped by any number of placements?)<br><br>
Epistability (weak: will the region flip under prior resolution?  strong: such that the region cannot be epistabilized by further placements?)<br><br>
Metastability (weak: can the region's position in the resolution order be meaningfully altered with a single placement?  strong: can the position in the resolution order be meaningfully altered by any number of placements?)<br></p>

<p>COMPLEX STABILITY STATES <br>
<em>The simple stability states combine into 8 ""complex stability states"", based on True/False for metastability/epistability/stability:</em> <br>
TTT Superstability (""super-stably stable"")<br>
FTT Semistability  (""semi-stably stable"")<br>
TFT Mendaxastability (""super-unstably stable"")<br>
FFT Demistability (""semi-unstably stable"") <br> 
TTF Contrastability (""super-stably unstable"") <br>
FTF Nonstability (""semi-stably unstable"")  <br>
TFF Antistability (""super-unstably unstable"")<br>
FFF Khaotictivity (""semi-unstably unstable"")<br></p>

<p>There are two hierarchies, the ordering/weighting of metastability, epistability and stability in the complex states (here the order is reversed for ease of interpreting the linguistic description of these states,) and the ordering/weighting of the complex states themselves, currently with Superstability as the least important, and Khaotictivity as the most important.  (i.e. a strongly superstable region cannot be flipped, and does not need to be reinforced; a khaotic regions is very much ""in play"") </p>

<p>It seems to me that deep learning might be very usefully applied in determining the hierarchies based on any give board state.</p>
"
1152,"<p>I'm really new to neural networks. I'm trying to make a neural network with genetics algorithms which will make a snake learn to look for the food and avoid hitting his tail.</p>

<p>The thing is that I think that I've done it, but as there's no walls the snake learns to go one direction only without making a 180 turn <a href=""https://i.imgur.com/cdVmixk.gifv"" rel=""nofollow noreferrer"">[GIF HERE]</a>.</p>

<p>I've tried to incentivate mutations that make them turn by decreasing the score of the snakes that always takes the same directions, but it don't works. I've only made them dumber, needing more breeds to reach another ""smart"" linear snake.</p>

<p>I've made a network with 5 inputs:</p>

<ul>
<li>Food position relative to my position and direction (2 inputs. x and y)</li>
<li>Nearest wall (my tail) if I turn left</li>
<li>Nearest wall (my tail) if I do not turn</li>
<li>Nearest wall (my tail) if I turn right</li>
</ul>

<p>The output are 3, being the first one turning left, the second do not turn and the third going right. I make the snake go the highest one of the 3 outputs.</p>

<p>I've added 1 hidden layer of 8 neurons (inputs + outputs premise).</p>

<p>The way I calculate the score is:</p>

<ul>
<li>Each step, 1 point.</li>
<li>Each food eaten, 10 points.</li>
<li>If the snake lasts too much time without eating food, dies.</li>
<li>If hits his tail, dies.</li>
</ul>

<p>Then I save each time the direction this snake has gone (up, right, down, left) and increment them by one. When the snake dies, I weight the final score by the difference between the lowest and highest values. If the difference is high, they receive a big penalty (down to 0.25 of their score). This way if a snake is pretty much linear gets a high penalty and if a snake does a cool pattern, gets a low penalty.</p>

<p>Also, I keep a record of each time a direction change happens compared to the last direction change, so if a snake keeps going in circles don't gets a high score because of ""cool pattern method"" for going all 4 directions.</p>

<p>With all this, I don't understand why my best snakes are always the linear ones :-/</p>

<p>I spawn 20 snakes and get the best 4 of each generation when everyone dies.</p>

<p>For generations I use neataptics.js and for neural networks I use synaptics.js. I have a Fiddle here: <a href=""http://jsfiddle.net/Llorx/gunsct5r/"" rel=""nofollow noreferrer"">http://jsfiddle.net/Llorx/gunsct5r/</a></p>

<p>At line <code>10</code> you can see the network definition. At <code>211</code> you can see the snake ""view"" (food position and walls. Where it gets the inputs) and at <code>164</code> you can see the score weight calculation depending on steps taken that I mentioned before.</p>

<p>All inputs are normalized from 0 to 1.</p>

<p>I'm sure that I'm doing, not one, but a lot of things pretty bad, as I'm a newbie on this, but some light on this will be really cool.</p>
"
1153,"<p>Recently, DeepMind's AlphaZero chess algorithm did better than the prior best chess software Stockfish. I read an <a href=""https://arxiv.org/abs/1712.01815"" rel=""nofollow noreferrer"">arxiv paper</a> about it but I'm not sure if:</p>

<ul>
<li><p>is there a value given for each piece (e.g. 1 for pawn, 3 for knight, 9 for queen, etc.) to train the algorithm, or does the algorithm learn this by himself?</p></li>
<li><p>I read that the algorithm uses Monte Carlo Tree Search, but what are the key improvements to prior chess algorithms already using MCTS?</p></li>
<li><p>Is there a hope for being able to run it an average computer? They said it required 9 hours learning (starting with nearly 0 knowledge except rules (and maybe value for piece?)), and 24 millions of games. Is it something doable in maybe 1 month with a average computer?</p></li>
</ul>
"
1154,"<p>Do off-policy policy gradient methods exist?</p>

<p>I know that policy gradient methods themselves using the policy function for sampling rollouts. But can't we easily have a model for sampling from the environment? If so, I've never seen this done before.</p>
"
1155,"<p>Let say that we have four straight string with colors RED, GREEN, BLUE and YELLOW. These strings are tied up randomly. We know the current state of string (like starting point, where we should we start untying from) and the final state (where current string must look like after untying them).</p>

<p>Here is a very simple example of the problem:</p>

<p><a href=""https://i.stack.imgur.com/tI6ah.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tI6ah.png"" alt=""a sample of the puzzle""></a></p>

<p>At every move, we are allowed to replace only two nearby strings. For example, to solve the shown problem, we should do the following replacements:</p>

<p>1) Replace GREEN and YELLOW</p>

<p>2) Replace BLUE and GREEN</p>

<p>But in a programming environment, how can I calculate the movements to untie the strings? What will be the algorithm specifically to solve such type of problems? Here is another <a href=""https://math.stackexchange.com/questions/2581164/string-knots-unlocking-algorithm"">variation</a> of the question. Its not possible to solve this problem according to the answer.</p>
"
1156,"<p>Imagine this theoretical situation:</p>

<blockquote>
  <p>A group of people are asked to provide a solution for an imaginary
  problem via email. Then an AI service runs through their written
  solution, analyzing the kind of words and sentences they use,
  essentially exploring how they faced the problem. Then the service
  finds a personality portfolio of each person.</p>
</blockquote>

<p>Is there such an AI service which can find such personality portfolios by analyzing the text they use describing their solution to a problem? If there is not a current service, how might it be approached?</p>

<p>Regards,
Koppany</p>
"
1157,"<p>I am using HOG (Histogram of Oriented Gradients) for car detection from a video. I have used the Matlab function <code>extractHOGFeatures()</code> , it has given me a feature vector but how do I differentiate between different features of different objects. After this I extracted the corner points(313x1) and BRISK points (172x1) but they show nothing just empty vectors.</p>
"
1158,"<p>I have a large number of observations. Each observation contains:</p>

<ul>
<li>dependent variable: a scores ranging from 0 - 100</li>
<li>independent variable: a large article</li>
</ul>

<p>I want to know which words or phrases predict a higher score.</p>

<p>I know that for discrete dependent variables, Naive Bayes can be used for this task. How do I conduct this analysis with a continuous dependent variable?</p>
"
1159,"<p>I am thinking of making a simple and interesting research on training a RNN, to create source codes.</p>

<p>The objective is to have a set of simple REPL programs in java, and create a source code for a program that is not present in the databas.
Ex: database => [palindrom, prime, fibonacci] ai will be able to create a source code to print all the fibonacci numbers that are prime, or create a tribonacci number etc.</p>

<p><strong>I am wondering how to break a code into a training input!</strong></p>

<p>I have found some link </p>

<p><a href=""https://github.com/souparno/rnn2source"" rel=""nofollow noreferrer"">rnn2source</a> <br/> 
<a href=""https://github.com/souparno/deep-generation"" rel=""nofollow noreferrer"">deep generation</a><br/>
<a href=""https://github.com/souparno/AI-Programmer"" rel=""nofollow noreferrer"">AI-Programmer</a><br/>
<a href=""https://openreview.net/pdf?id=ByldLrqlx"" rel=""nofollow noreferrer"">deep coder</a></p>

<p><strong>Can anybody explain me what is the ideal way, to approach this problem, and how to break down the source code into a training set?</strong></p>
"
1160,"<p>I am trying to train Chess data through CNN. To proceed reinforcement learning, I had divided into two - ""current network"" and ""reinforcement network"". For each checkpoint file stored in different directory, would like to call the weight and proceed chess games and learn only ""reinforcement network.""</p>

<p>Thus I had made an instance for each network, current and reinforcement, however, regardless of its sequence, if a network stores saver.restore the other network returns error while same saver.restore process. </p>

<p>I doubt this happens because I had used same variable_scope.</p>

<p>Maybe this is because in tensorflow there already exists same scope_variable I guess.</p>

<p>Any chance I could use two different already-learned network simultaneously on tensorflow?</p>

<p>I use python/tensorflow1.1.0. </p>

<p>Any advice will be appreicated</p>
"
1161,"<p>I'm trying to learn more about AI by trying to program a neural network. First I'm trying to understand writing my own perceptron but I'm struggling to get a basic perceptron working correctly.</p>

<p>I've tried writing a few basic perceptrons to do very basic tasks for example trying to classify a point as above or below a line y = x.</p>

<p>The problem I'm facing is it seems that when training the perceptron my weights start increasing exponentially and it doesn't seem like I'm getting anywhere.</p>

<p>If I start with a perceptron with two weights representing an x and a y value of a point on a graph:
starting weight values of 1 and 1
Using the sign function of the output being more than or equal to 0 outputs +1 and less than 0 is -1.</p>

<p>I'm using the training data with examples (1,2), (2,1), (3,3), (2,1).
I think I'm correct in saying if the perceptron outputs the correct value then you don't need to adjust the weights, if it's incorrect then the weights can be updated using</p>

<p>new_weight = old_weight + ((expected_ouptut - false_output) * input)</p>

<p>By the first iteration I have weights of -4 and -2 respectively, the second passes, the third i.e (3,3) with weights (-4,-2) changes the weights to (53,55).</p>

<p>I'm fairly sure my math is correct, and it seems like the weights are increasing exponentially without making any difference to getting a working perceptron. Are there any errors I'm making with trying to design a working perceptron?</p>

<p>Thanks!</p>

<p>Sorry this may get slightly long!
EDIT:</p>

<p>I think I may have been making mistakes doing it manually, I’ve written the code now that seems to be partly working.</p>

<p>The way my code works is by running test firstly with test cases, followed by the results from those test cases in the same order within the lists. Followed by the weights and finally the test case to run it from. So running in ghci:</p>

<p>test [(5,2)]   [(-1)]   (1,1)   (5,2)</p>

<p>this works correctly, first it tests the coordinates (5,2) in the perceptron with weights (1,1) and checks to see if it is equal to -1, if it is not equal to one it will run it again and alter the weights with the alter function until it does equal -1, when this happens it then checks the perceptron again and outputs with the final tuple (5,2) which is the test case to check if it’s working. This works correctly, however when working with larger sets it doesn’t work.
 Next I tried</p>

<p>test [(1,2),(2,5),(3,1),(7,0),(6,5),(4,4)]   [1,1,(-1),(-1),(-1),1]   (1,1)   (5,1)</p>

<p>where the first two lists are the test cases and the results, then the weights (1,1) and finally after running all of the test cases trying to teach the perceptron it then runs the perceptron on the final tuple (5,1) and outputs the result, it should be (-1) as it is below y = x, however it outputs 1.</p>

<p>Pseudocode:</p>

<pre><code>testFunction
if(test cases remaining):
  if calling sign on the output of the perceptron != result for test case then:
    call same test case again but alter weights
  else:
    call testFunction again will same lists minus the first test arguments and results
else if(no more test cases, all have been tested and removed, or none specified):
  call sign on the output of the perceptron for the final tuple provided with the altered weights
</code></pre>

<p>Code in Haskell:</p>

<pre><code>module Main where

-- function to take training data, results from that data to test the perceptron
-- also takes weights and finally outputs the result of the test case
test :: [(Float,Float)] -&gt; [Float] -&gt; (Float,Float) -&gt; (Float,Float) -&gt; Float
test [] [] weights testCase = sign(perceptron testCase weights)
test (t:trainData) (r:trainResult) weights testCase
  | (output /= r) = test ([t] ++ trainData) ([r] ++ trainResult) (alter weights output t r) testCase -- add t and r
  | otherwise     = test trainData trainResult weights testCase
  where
    output = sign(perceptron t weights)

-- function that computes x1*w1 + x2*w2
perceptron :: (Float,Float) -&gt; (Float,Float) -&gt; Float
perceptron coordinate weights = ((((fst coordinate) * (fst weights))) + ((snd coordinate) * (snd weights)))

-- function that finds the new weights by doing w1 = w1 + (expected_result - perceptron output)*x1 and similar for w2
alter :: (Float,Float) -&gt; Float -&gt; (Float,Float) -&gt; Float -&gt; (Float,Float)
alter (w1,w2) output (x1,x2) result = ((w1 + ((result - output) * x1)), (w2 + ((result - output) * x2)))

-- activation function
sign :: Float -&gt; Float
sign n
  | n &gt;= 0 = 1
  | otherwise = (-1)
</code></pre>
"
1162,"<p>I am reading <a href=""https://arxiv.org/abs/1003.0120"" rel=""nofollow noreferrer"">Learning from Logged Implicit Exploration Data</a> </p>

<p>It says </p>

<blockquote>
  <p>Formally, given a dataset of the form S = (x, a, r_a)* generated by the <strong>interaction of an uncontrolled logging policy</strong></p>
</blockquote>

<p>What is such a policy? </p>
"
1163,"<p>Lets pretend we had a list of facts (similar to prolog tuples) that define some knowledge about some entities. e.g.</p>

<p>doing(clean, data)
done(collect, data)
todo(train, model)
todo(write, paper)</p>

<p>What methods could I use to generate sentences like:</p>

<p>You should be cleaning the data you collected, then you need to train your model and write your paper.</p>
"
1164,"<p>I was wondering if its possible to classify or learn to estimate the minimum value in a table if the values are integer and represented 32 bits, and we can input all variable at the same moment like in SoC or something.</p>
"
1165,"<p>Recently I have had the urge to make a chat-bot. I want to train my chat-bot on a large set of data. This raised my question on whether XML, SQLite, or  storing it in a raw JSON text file will be faster for storing questions with possible answers. 
What are your thoughts on this?</p>
"
1166,"<p>What is the <a href=""https://www.tensorflow.org/tutorials/image_retraining"" rel=""noreferrer"">concept</a> and how does one calculate Bottleneck values? How do these values help image classification? Please explain in simple words.</p>
"
1167,"<p>I started to study NN recently.
So I understand principles with which I should define input and output layers.</p>

<p>But I can't find any guide/directions how to build hidden layers: how many layers do I need, how many neurons per layer, what activation functions should I use etc for different types of tasks.</p>

<p>So I am searching for some guide like: <em>try to start with ... layers of ... neurons. If you get ... result, please increase number of neurons ..., but if you get... result, you should try to decrease number of neurons. Or something similar.</em></p>
"
1168,"<p>I need to have a full brief on recurrent neural network. With the explanation how to train recurrent neural network??</p>
"
1169,"<p>For instance, one task would be to detect if with an Android phone in hand, I'm panning the camera toward a circle shape in a 2D space.</p>

<p>What is the best technology set and embedded application approach can be used for these types of motion independent pattern recognition tasks?</p>

<p><a href=""https://i.stack.imgur.com/36kBFs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/36kBFs.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/XIAIms.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XIAIms.jpg"" alt=""enter image description here""></a></p>
"
1170,"<p>Is it possible with any of machine learning methods to train machine to tie shoe lace? If possible how data should be interpreted for the training? 
If we are using reinforcement learning, how will it learn to reach the best rewards?</p>
"
1171,"<p>For example I need to detect classes for MNIST data. But I want to have not 10 classes for digits but also I want to have 11th class ""not a digit"".
So that any letter (except ""O"" of course:) ), any other type of image or random noise would be classified as ""not a digit"".</p>

<p>Or with CIFAR-10 I want to have 11th ""unknown"" class to classify any image that contain something out of classes range.</p>

<p>So how to implement such feature?
Maybe there are some examples somewhere, preferable with Keras.</p>
"
1172,"<p>Machine learning and NN trainings as a part of ML is based on data that was gotten from real world and inserted into virtual space by humans.</p>

<p>Meanwhile NN are also used for data generation. Each year the more and more texts, images, sounds etc become more realistic and cannot be determinated from real world data even by a human.</p>

<p>So it is really possible that machines will start to learn with data that was generated by other machines, because it will look as real even for a human, but naturally will be not related to real world.</p>

<p>Q: Will it lead to some machine learning collapse? </p>

<p>Q: Might it lead to some changes in human's perception of the world, because people get a very big part of their knowledge using computers, connected to the Internet?</p>

<p>Q: Is anyone thinking about this potential problem? </p>
"
1173,"<p>Are there any algorithms, or any evidence to decide or to suggest it would be better to connect a neuron node in a layer <code>l</code>, in a neural network to particular nodes in the previous layer <code>l-1</code> of the neural network as well as to particular nodes in the next layer <code>l+1</code> of the neural network?</p>

<p>This is obviously contrived, but here is a illustration of what I mean.
<a href=""https://i.stack.imgur.com/Vy7FL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Vy7FL.jpg"" alt=""enter image description here""></a>
The thick line with arrow indecates an edge that leads to l-1 layer neuron</p>
"
1174,"<p>The basis of my question is that a CNN that does great on MNIST is far smaller than a CNN that does great on ImageNet. Clearly, as the number of potential target classes increases, along with image complexity (background, illumination, etc.), the network needs to become deeper and wider to be able to sufficiently capture all of the variation in the dataset. However, the downside of larger networks is that they become far slower for both inference and backprop.</p>

<p>Assume you wanted to build a network that runs on a security camera in front of your house. You are really interested in telling when it sees a person, or a car in your driveway, or a delivery truck, etc. Let's say you have a total of 20 classes that you care about (maybe you want to know minivan, pickup, and so on).</p>

<p>You gather a dataset that has plenty of nice, clean data. It has footage from lots of times of the day, with lots of intra-class variation and great balance between all of the classes. Finally, assume that you want this network to run at the maximum possible framerate (I know that security cameras don't need to do this, but maybe you're running on a small processor or some other reason that you want to be executing at really high speed).</p>

<p>Is there any advantage, computationally, to splitting your network into smaller networks that specialize? One possibility is having a morning, an afternoon/evening, and a night network and you run the one corresponding to the time of day. Each one can detect all 20 classes (although you could split even farther and make it so that there is a vehicle one, and a person one, and so on). Your other option is sharing base layers (similar to using VGGNet layers for transfer learning). Then, you have the output of those base layers fed into several small networks, each specialized like above. Finally, you could also have just one large network that runs in all conditions.</p>

<p><strong>Question: Is there a way to know which of these would be faster other than building them?</strong> </p>

<p>In my head, it feels like sharing base layers and then diverging will run as slow as the ""sub-network"" with the most additional parameters. Similar logic for the separate networks, except you save a lot of computation by sharing base layers. Overall, though, it seems like one network is probably ideal. Is there any research/experimentation along these lines?</p>
"
1175,"<p>Say I have 500 variables and I believe those variables can be shown in a 4-dimensional latent representation which I want to learn.</p>

<p>What I have for training is 100K samples, and those samples are coming mainly from 3 unbalanced groups: 1st group has 1K samples, 2nd group has 49K samples, and 3rd group has 50K samples.</p>

<p>Do you think I can learn a meaningful representation by training a (variational) autoencoder with this data? Is there a reason that requires all samples to come from the same distribution? If not, is there a reason that requires balanced classes?</p>
"
1176,"<p>I've seen data sets for classification / regressions tasks in domains such as credit default detection, object identification in an image, stock price prediction etc.  All of these data sets could simply be represented as an input matrix of size (n_samples, n_features) and fed into your machine learning algorithm to ultimately yield a trained model offering some predictive capability.  Intuitively and mathematically this makes sense to me.</p>

<p>However, I'm really struggling with how to think about the structure of an input matrix for game-like tasks (Chess, Go, <a href=""https://www.youtube.com/watch?v=Ipi40cb_RsI"" rel=""nofollow noreferrer"">Seth Blings Mario Kart AI</a>) specifically (using the Chess example):</p>

<ol>
<li>How would you encode the state of the board to something that a model could train on?  Is it reasonable to think about the board state as a 8x8 matrix (or 1x64) vector with each point being encoded by a numerical value dependent on the type of piece and color?</li>
<li><p>Assuming a suitable representation of the board state, how would the model be capable of making a recommendation given that each piece type moves differently?  Would it not have to evaluate the different move possibilities for each piece and propose which move it ""thinks"" would have the best long term outcome for the game?</p></li>
<li><p>A follow up on 2 - given the interplay between a moves made now and moves made n moves into the future how would the model be able to recognize and make trade-offs between moves which may offer a better position now vs those that offer a position n moves in the future - would one have to extend the board state input to a vector of length 1x64n where n is the total number of moves for expected for an individual player or is this a function of a different algorithm which should be able to capture historical information which training?</p></li>
</ol>

<p>I am unsure if I'm overthinking this and am missing something really obvious but I would appreciate any guidance in terms of how to approach thinking about this.</p>
"
1177,"<p>Suppose I have a classification problem with a stream of training-samples constantly arriving over time. I cannot keep all training-samples in memory, but I still want to train a classifier that will have the ""wisdom"" of all samples, and additionally, I want the classifier to become better whenever it gets new samples. </p>

<p>I thought of the following idea. Suppose we have enough memory to keep 100 samples. Then, for each run of 100 samples, we will train a different sub-classifier. We will have a meta-classifier that will classify based on voting between all existing sub-classifiers. Over time, we will have more and more sub-classifiers, so hopefully the meta-classifier will improve with time - it will have a ""wisdom of the crowds"" effect. </p>

<p>Has this method been tried before? Specifically, has it been tried in a deep-learning sequence-classification setting?</p>
"
1178,"<p>I was wondering any examples of the following;</p>

<ul>
<li><p>para generation: For eg, given X similar paragraphs,  are you able to build a model to learn the style and generate a new para that is a paraphrase of the X paras. Similar in meaning but diff wording.</p></li>
<li><p>drawing conclusions from X given articles. He has a list of conclusions, check the X articles can provide evidence to the conclusions. Eg, given conclusion “city is not safe”, look for evidence such as “murders” and “thefts”.</p></li>
</ul>

<p>Glady Appreciate,
Betty</p>
"
1179,"<p>The Intel 8080 had 4500 transistors and ran at 2-3.125 MHz. By comparison, the 18-core Xeon Haswell-E5 han 5,560,000,000 transistors and can run at 2 GHz. Would it be possible or prudent to simulate a neural network by backing a chip chock-full of a million interconnected, slightly modified intel 8080s (sped up to run at 2 GHz)? If each one modeled 100 neurons you could simulate a neural network with 100 million neurons on a single chip.</p>

<p><strong>Edit:</strong> I'm not proposing that you <em>actually</em> use a million intel 8080s; rather I'm proposing that you take a highly minimal programmable chip design <em>like</em> the intel 8080's design and pattern it across a wafer as densely as possible with interconnects so that each instance can function as one or a few dozen fully programmable neurons each with a small amount of memory. I'm <em>not</em> proposing that someone take a million intel 8080s and hook them together. </p>
"
1180,"<p>I'm a student, and currently into image processing project and coding using OpenCV. Recently, I watched Sebastian Thrun from Udacity in TedTalks talked about AlphaGo and I'm totally interested in the idea. I have read this question too : <a href=""https://ai.stackexchange.com/questions/4394/why-is-the-merged-neural-network-of-alpha-go-zero-more-efficient-than-two-separa/4654"">Merged Neural Network in AlphaGo</a>.
I was wondering if same approaches can be used in my project.</p>

<p>I'm going to perform color enhancement method for any natural images. And of course, color sampling is a tricky task now. It's a lot of work, I have to prepare condition for each key-color sampling given and also prepare &amp; pick the best enhancement function for it. I'm able to do it already using OpenCV.</p>

<p>But I was wondering if I could load tons of sample pictures instead, have my system test them against each other, and figure out its own enhancement rules from all testing.</p>

<p>I'm not that familiar with Deep Learning, we don't even have deep learning course at my university, but I'm interested in the idea and ready to learn. I'm not even sure if this can be done or not, but I wonder what kind of approaches should I learn to achieve my goal ? Is Deep Learning --> Neural Network a good start ? In my case, to which method in Deep Learning should I go with ? Any reference / advice will be highly appreciated. Thanks.</p>
"
1181,"<p>I coded a small RNN network with Tensorflow to return the total energy consumption given some parameters. There seem to be a problem in my code. It can't overfit the training data when I use a batch size > 1 (even with only 4 samples!). In the code below, the loss value reaches 0 when I set BatchSize to 1. However, by setting BatchSize to 2, the network fails to overfit and the loss value goes toward 12.500000 and gets stuck there forever.</p>

<p>I suspect this has something to do with LSTM states. I get the same problem if I don't update the state with each iteration. Or maybe the cost function? A help is appreciated. Thanks.</p>

<pre><code>import tensorflow as tf
import numpy as np
import os

from utils import loadData

Epochs = 10000
LearningRate = 0.0001
MaxGradNorm = 5

SeqLen = 1
NChannels = 28
NClasses = 1

NLayers = 2
NUnits = 256

BatchSize = 1
NumSamples = 4
#################################################################

trainingFile = ""./training.dat""

X_values, Y_values = loadData(trainingFile, SeqLen, NumSamples)

X = tf.placeholder(tf.float32, [BatchSize, SeqLen, NChannels], name='inputs')

Y = tf.placeholder(tf.float32, [BatchSize, SeqLen, NClasses], name='labels')

keep_prob = tf.placeholder(tf.float32, name='keep')

initializer = tf.contrib.layers.xavier_initializer()

Xin = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))

lstm_layers = []

for i in range(NLayers):

    lstm_layer = tf.nn.rnn_cell.LSTMCell(num_units=NUnits, initializer=initializer, use_peepholes=True, state_is_tuple=True)

    dropout_layer = tf.contrib.rnn.DropoutWrapper(lstm_layer, output_keep_prob=keep_prob)

    #[LSTM ---&gt; DROPOUT] ---&gt; [LSTM ---&gt; DROPOUT] ---&gt; etc...
    lstm_layers.append(dropout_layer)   

rnn = tf.nn.rnn_cell.MultiRNNCell(lstm_layers, state_is_tuple=True)

initial_state = rnn.zero_state(BatchSize, tf.float32)

outputs, final_state = tf.nn.static_rnn(rnn, Xin, dtype=tf.float32, initial_state=initial_state)

outputs = tf.transpose(outputs, [1,0,2])
outputs = tf.reshape(outputs, [-1, NUnits])

weight = tf.Variable(tf.truncated_normal([NUnits, NClasses]))
bias = tf.Variable(tf.constant(0.1, shape=[NClasses]))
prediction = tf.matmul(outputs, weight) + bias
prediction = tf.reshape(prediction, [BatchSize, SeqLen, NClasses])

cost = tf.reduce_sum(tf.pow(tf.subtract(prediction, Y), 2)) / (2 * BatchSize)

tvars = tf.trainable_variables()

grad, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), MaxGradNorm)

optimizer = tf.train.AdamOptimizer(learning_rate = LearningRate)

train_step = optimizer.apply_gradients(zip(grad, tvars))

sess = tf.Session()

sess.run(tf.global_variables_initializer())

iteration = 1

for e in range(0, Epochs):

    train_loss = []

    state = sess.run(initial_state)

    for i in xrange(0, len(X_values), BatchSize):
        x = X_values[i:i + BatchSize]
        y = Y_values[i:i + BatchSize]

    y = np.expand_dims(y, 2)

        feed = {X : x, Y : y, keep_prob : 1.0, initial_state : state}

        _ , loss, state, pred = sess.run([train_step, cost, final_state, prediction], feed_dict = feed)

        train_loss.append(loss)

        iteration += 1

    print(""Epoch: {}/{}"".format(e, Epochs), ""Iteration: {:d}"".format(iteration), ""Train average rmse: {:6f}"".format(np.mean(train_loss)))
</code></pre>

<p><a href=""https://i.stack.imgur.com/jbmh0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jbmh0.png"" alt=""Batch size = 1""></a></p>

<p><a href=""https://i.stack.imgur.com/d7Hmp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d7Hmp.png"" alt=""Batch size = 2""></a></p>
"
1182,"<p>I would like to ask community members what kind of knowledge is required to jump into the field of AI. </p>

<ul>
<li><p>Currently I have just started programming and development. What should I pursue in order to get into this field. </p></li>
<li><p>My second question is about the mathematics involved with Artificial Intelligence. How good I should be in mathematics?</p></li>
</ul>

<p>I am very interested in mathematics and logic. I would like to know the right direction. I would be grateful if you suggest me reading material too.  </p>
"
1183,"<p>There is more information recently that AlphaZero has been trained to be the best chess program after 4 hours of learning (in chess). I am wondering how the AI network could have been modeled for this program?</p>
"
1184,"<p>I am trying to understand if robotic process automation is a field which requires expertise in machine learning.
Does the algorithm behind rpa use machine learning expect from OCR?</p>
"
1185,"<p>Can <a href=""https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework"" rel=""nofollow noreferrer"">Viola Jones algorithm</a> can be used to detect the facial emotion. Actually it was used in creating harr-cascade file for object and facial detection. But what confused me is whether it can be used to train for emotion detection. </p>

<p>If not what algorithms can I use, and what are the mathematical bases (i.e. what mathematics should I be studying?) </p>
"
1186,"<p>I have a large dataset of skin images, each one associated with a hydration value (percentage).</p>

<p>Now I'm looking into predicting the hydration value from an image. My thinking: train a CNN on the dataset and evaluate the model with a mean square error regression.</p>

<p>First, does this sound like a sensible way to try this?</p>

<p>Second, I'd like to run the model on mobile. Can you recommend any examples with Caffe2 (or alternatively TensorFlow) or diagrams that might explain a similar task?</p>
"
1187,"<p>Can someone please point me to where I can read up on why non linearities that can produce values larger than 1 or smaller than 0 work. My understanding is that neurons can only produce values between 0 and 1 and that this assumption can be used in things like cross entropy. Are my assumptions just completely wrong?</p>
"
1188,"<p>Imagine we have 2 air conditioner systems (AA) and 2 ""free cooling"" systems which mix external and internal air (FC) in a closed box which always tends to warm up. For each system, we have to find turn on and off temperatures (for some hysteresis, let's say between the range 20-40 each one) to optimize the energy consumption.</p>

<p>As we don't know the relation between these parameters and the energy consumption (and we don't intend to know them), we treat the problem as a black-box function.</p>

<p>Till now, the problem would be solvable via a bayesian optimizer (eg. with gaussian process acquisition function).</p>

<p>But there is a problem: the best configuration may change between seasons, and even days! A simple bayesian optimizer maybe could deal with these changes limiting the data it takes into account by, for example, the last 15-30 days. But this would deal with the change AFTER the consumption increased.</p>

<p>So, the idea is introduce some contextual variables which would help the system prevent these changes (eg. the external and internal temperature, and the vectors of variation of external and/or internal temperature, the weather prediction, whatever).</p>

<p>Also, some of these variables we can take into account might be internal of the system, which means while these influence the best configuration, the actual configuration also influences these variables! and this becomes a reinforcement learning problem.</p>

<p>1) Is there a way (documented or experimental) to know which variables (both internal or external) influences the optimal configuration of these AA/FC systems?</p>

<p>2) Based on the first question, which would be the best approach?</p>

<p>2.1.) No features. This might be considered a <a href=""https://stats.stackexchange.com/questions/168718/multi-armed-bandit-for-continuous-rewards-extended-question?rq=1"">multiarmed bandit problem for continuous reward</a>. (FIX POSTERIOR TO SCENARIO CHANGE, IF THERE IS A SCENARIO CHAGNE)</p>

<p>2.2.) Only external features to predict the scenario change. This might be considered a contextual multiarmed bandit-problem. (FORESEE THE SCENARIO CHANGE)</p>

<p>2.3.) Consider only system-internal features. This can be considered a reinforcement-learning problem. (FIX IMMEDIATLY THE SCENARIO CHANGE)</p>

<p>2.4.) Consider both external and internal features. This can be considered a reinforcement-learning problem where some of the states are not influenced by the configuration. (FORESEE THE SCENARIO CHANGE, AND IF SOMETHING FAILS, FIX IMMEDIATLY).</p>
"
1189,"<p>I studied machine learning when I was in university, a couple years ago. I used it for my master thesis (decision trees, ensembles, svm and word embedding mostly) and for other projects either personal and academics (genetic algorithms, Q-learning). </p>

<p>Then I studied neural networks by myself, from the pure theory of the perceptron, backpropagation, gradient descent, etc... (with Bengio's deep learning book) to the various ramifications of convnets, deconvnets, auto encoders, lstm, deep reinforcement learning and other various papers. I took a several months break and there are already new types of networks that I don't know... so hard to keep up with the state of the art.</p>

<p>However, my knowledge is purely theoretical. I want to embed machine learning in my future career but getting a job in the field requires practical expertise, which I lack. I am working full time now as a software engineer, so what do you think may be a possible path to follow in order to gain expertise? Given that I have a strong mathematical background and more than ten years of coding on my back, is there any resource on practical rules of thumbs, suggestions, real case studies, etc... which I can invest time onto? I mean something beyond the vanilla ""introduction to machine learning"", something more real and concrete. </p>

<p>Do you think that attempting kaggle challenges from scratch would be stimulating? Or too confusing? Do you think delivering a personal project (i.e. not a byproduct of my current job) in which I employ one of such methods to solve a potentially real problem would be enough to be relevant in my cv?</p>
"
1190,"<p>I am making my speech recognition project for PC (working on Windows 8) and am new in this area.  The project should have basic functionality like dictation with accuracy in email, notepad, etc., and should respond to local commands of PC.</p>

<p>I am using <a href=""https://cmusphinx.github.io/wiki/tutorialsphinx4/"" rel=""nofollow noreferrer"">sphinx4</a> for my speech recognition project.  I'm trying to determine if there is there is a better open source API than CMU Sphinx in terms of accuracy and large vocabulary? </p>

<p>Does Kaldi (deep neural network based) perform better than CMU Sphinx (HMM based)?  Are the two platforms better for different kinds of applications?</p>

<p>Essentially:</p>

<blockquote>
  <p>I want to increase my speech recognition system accuracy and vocabulary.  Which might be most optimal: <a href=""http://kaldi-asr.org/doc/about.html"" rel=""nofollow noreferrer"">Kaldi</a> or <a href=""http://www.speech.cs.cmu.edu/sphinx/doc/Sphinx.html"" rel=""nofollow noreferrer"">CMU Sphinx</a>?</p>
</blockquote>

<p>Also wondering what is the difference between a speech API and a speech Engine, and, more generally, as a developer what I will require to develop my software?</p>

<p>Please help me to get a clear understanding about above questions and, if possible, provide some speech recognition developer or researcher community links.  </p>

<p>Any and all comments, suggestions and answers are welcome!</p>
"
1191,"<p>I am working on a problem where I need to determine whether two sentences are similar or not.  I implemented a solution using BM25 algorithm and wordnet synsets for determining syntactic &amp; semantic similarity.  The solution is working adequately, and even if the word order in the sentences is jumbled, it is measuring that two sentences are similar e.g. - <br></p>

<p>1) Python is a good language. <br>
2) Language a good python is.</p>

<p>My solution is determining that these two sentences are similar.  </p>

<ul>
<li>What could be the possible solution for Structural similarity? </li>
<li>How will I maintain structure of sentences?</li>
</ul>
"
1192,"<p>What is the difference between a histopathological image and a natural image when training a neural network? </p>
"
1193,"<p>I'm currently writing an Alpha-Beta-Pruning algorithm for a board game. Now I need to come up with a good evaluation function. The game is a bit like snakes and ladders (you have to finish the race first), so for a possible feature list I came up with following:</p>

<ul>
<li>field index should be high</li>
<li>in the lower fields my fuel should be high, when coming to the end it should be low (maximum of '10' required to enter the goal)</li>
<li>all 'power-ups' must be spent to enter the goal, so prioritize them</li>
<li>if it is possible to enter the goal (a legit move), do it!</li>
</ul>

<p>There could be some more for some special cases.<br>
I've read somewhere that it is the best (and easiest) to combine them in a linear function, for example:</p>

<pre><code>i = field index
p = power-ups
f = fuel

-&gt; 0.75 * i - 5 * p - 0.25 * |(f - MAX_FIELD_INDEX/i)|
</code></pre>

<p>Since I can't ask an expert and I'm not an expert by myself I have nobody to ask if those parameters are good, if I've forgot something or if I've combined the factors correctly.<br>
The parameters aren't that big of a deal because I could use a genetic algorithm or something else to optimize them.<br>
My problem and question is: What do I have to do to find out how to put together my features optimally (how can I optimize the function / parameter arrangement itself)?</p>
"
1194,"<p><strong>CIO NN</strong></p>

<p>CIO NN stands for <strong>C</strong>ontroller <strong>I</strong>nput <strong>O</strong>utput <strong>N</strong>erual <strong>N</strong>etwork</p>

<p>note due to a typo the ""<strong>nearon</strong>"" means ""<strong>neron</strong>""</p>

<p>For this we have to redefine the Nearon</p>

<ul>
<li>2 Inputs</li>
<li>2 Outputs</li>
<li>4 Weights (each input and output have their own weights)</li>
<li>Internal Memory Cell (any byte or bit or block size with variable size)</li>
<li>Activation Function (Defines what weights and what inputs activate this Nearon)</li>
<li>Memory Storage Function (Defines what and when this cell should store said memory or memory stream)</li>
<li>Memory Transpose Function (Once activated any stored memory that the activation function can trigger will be played/pushed into the Nerual Network)</li>
<li>Forget Function (Defines when and/or how and/or why these memories can be destroyed/removed based on Activation function with Memory states and any input stateses itself)</li>
</ul>

<p>How would I implement this in the form of Code?
please take note of the spec. (this is non profit/GNU v3)</p>

<p><strong>This would look something like these:</strong>
<a href=""https://i.stack.imgur.com/bo4We.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bo4We.png"" alt=""Basic CIO Nearon""></a></p>

<p>These would be arranges like this:
<a href=""https://i.stack.imgur.com/SDVmv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SDVmv.png"" alt=""enter image description here""></a></p>

<p>Which we can build it into this:
<a href=""https://i.stack.imgur.com/GdAra.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GdAra.png"" alt=""enter image description here""></a></p>

<p>it can be trained like this:</p>

<ul>
<li>do normal NN training from the inputs to the input outputs like a hidden layer NN</li>
</ul>

<p>then trained to be controlled like this:</p>

<ul>
<li>then set the known NN dataset (inputs) to be corrected to actual or correct values via the CI (Controller Input) which will be outputed on the ""Input + Controller Input"" Output</li>
</ul>

<p>by training like this:</p>

<ul>
<li>you have a normal NN with hidden layers which can be trainned (this can be done with CNNs) with backpropergation, and a cost function (use least amount of nerons) etc</li>
<li>now you can allow the CIO NN to retrain / teach itself with supervised or unsupervised learning.</li>
<li>you can combine the ""Input Output"" and the ""Input + Controller Input"" Output with another NN which can then connect with this NN in a similiar way that a Neron connects to a Neron</li>
</ul>
"
1195,"<p>At my work we're currently doing some research into data visualisation for highly inter connected data, basically graphs.</p>

<p>We've been implementing all sorts of different layouts and trying to see which fits best, but, due to the nature of the problem --it's a visual thing - we needed to come up with some automated way to analyse the result so welcome up with a bunch of metrics to analyse our layouts. </p>

<p>So far, the most important metrics have been information density, edge crossings, node overlap and edge length. This gives us some good results and has allowed us to fine tune our layout algorithms.   </p>

<p>However, when a new graph is loaded, we noticed that humans still tend to fiddle a lot with the structure of the layout. Moreover, it seems that our metrics do a good job of predicting where a user is likely to mess around. Graph layout is a tough problem, so <strong>after some discussion, the idea of just throwing data at a neural network and let it figure it out came up.</strong>  </p>

<p>None of us are experts, or even experienced in AI.  I'm the one with the most contact with AI methods.  <strong>All I've ever done were simple NN models, no convolution, feedback or feedforward or anything of the sorts, but it seems to me this should be doable.</strong> </p>

<p>Maybe it's my lack of expertise here but I haven't been able to find any good information on this sort of application for NNs, so I was hoping someone here could point me in the right direction.   </p>

<ul>
<li>What sort of model is best for such a situation? and why? Is this actually possible or would it be super complicated? Has anyone ever tried something like this before?</li>
</ul>

<p>If it helps, our input data (for v1, I guess) would be two arrays of variable length, one for the nodes and another for the relationships between them and the output data would be an array with the node XY coordinates.</p>

<p>Any help would be greatly appreciated!
Cheers!</p>
"
1196,"<p>I have some episodic datasets extracted from a turn-based RTS game in which the current actions leading to the next state doesn’t determine the final solution/outcome of the episode. </p>

<p>The learning is expected to terminate at a final state/termination condition (when it wins or losses) for each episode and then move on to the next number of episodes in the dataset. </p>

<p>I have being looking into Q learning, Monte Carlo and SARSA but I am confused about which one is best applicable. </p>

<p>If any of the mentioned algorithm is implemented, can a reward of zero be given in preliminary states before termination state of each episodes at which it will be rewarded with a positive/negative (win/loss) value?</p>
"
1197,"<p>I have a dataset with 2,23,586 samples out of which i used 60% for training and 40% for testing. I used 5 classifiers individually, SVM, LR, decision tree, random forest and boosted decision trees. SVM and LR performed well with close to 0.9 accuracy and recall also 0.9 but tree based classifiers reported an accuracy of 0.6. After a careful observation, I found out that SVM and LR did not predict the labels of 20,357 samples identically. So Can I apply voting and resolve this conflict wrt prediction outcome? Can this conflict be due to an imbalanced dataset?</p>
"
1198,"<p>Some argue that humans are somewhere along the middle of the intelligence spectrum, some say that we are only at the very beginning of the spectrum and there's so much more potential ahead.</p>

<p>This leads to a question: does the intelligence spectrum itself have a ceiling, could it be possible for a general intelligence to progress infinitely provided enough resources and armed with the best self-recursive improvement algorithms?</p>
"
1199,"<p>i'm quite new to neural network and i recently built neural network for number classification in vehicle license plate. It has 3 layers: 1 input layer for 16*24(382 neurons) number image with 150 dpi , 1 hidden layer(199 neurons) with sigmoid activation function, 1 softmax output layer(10 neurons) for each number 0 to 9. </p>

<p>I'm trying to expand my neural network to also classify letters in license plate. But i'm worried if i just simply add more classes into output, for example add 10 letters into classification so total 20 classes, it would be hard for neural network to separate feature from each class. And also, i think it might cause problem when input is one of number and neural network wrongly classifies as one of letter with biggest probability, even though sum of probabilities of all number output exceeds that.</p>

<p>So i wonder if it is possible to build hierchical neural network in following manner:</p>

<p>There are 3 neural networks: 'Item', 'Number', 'Letter'</p>

<ol>
<li><p>'Item' neural network classifies whether input is numbers or letters.</p></li>
<li><p>If 'Item' neural network classifies input as numbers(letters), then input goes through 'Number'('Letter') neural network.</p></li>
<li><p>Return final output from Number(Letter) neural network.</p></li>
</ol>

<p>And learning mechanism for each network is below:</p>

<ol>
<li>'Item' neural network learns all images of numbers and letters. So there are 2 output.</li>
<li>'Number'('Letter') neural network learns images of only numbers(letter).</li>
</ol>

<p>Which method should i pick to have better classification? Just simply add 10 more classes or build hierchical neural networks with method above?</p>
"
1200,"<p>What is the relation between back-propagation and reinforcement learning?</p>
"
1201,"<p>I play a racing game called Need For Madness ( some gameplay: <a href=""https://www.youtube.com/watch?v=NC5uFZ-t0A8"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=NC5uFZ-t0A8</a> ). NFM is a racing game, where the player can choose different cars and race and crash the other cars, and you can play on different tracks too. The game has a fixed frame rate, so you can assume that the same sequence of button presses will always arrive at the exact same position, rotation, velocity, etc. of the car.</p>

<p>I want to make a bot which could race faster than I can. What would be the best way to go about doing this? Is this problem even suited for deep learning?</p>

<p>I was thinking I could train a neural network where the input would be the current world state (position of the player, position of the checkpoints you have to through and all the obstacles), and the output would be an array of booleans, one for each button. During a race, I could then keep forward propagating from the input to the booleans. However, I'm not so sure what I would do after the race is over. How do I back propagate after the race to make the NN be less likely to make bad moves?</p>
"
1202,"<p>I don't understand why Google Translate translates the same text in different ways.</p>

<p><a href=""https://en.wikipedia.org/wiki/Enter_the_Dragon"" rel=""nofollow noreferrer"">Here is the Wikipedia page of the 1973 film ""Enter the Dragon""</a>. You can see that its traditional Chinese title is: 龍爭虎鬥. <a href=""https://translate.google.com"" rel=""nofollow noreferrer"">Google translates</a> this as ""Dragons fight"". </p>

<p>Then, if we go to <a href=""https://zh-yue.wikipedia.org/wiki/%E9%BE%8D%E7%88%AD%E8%99%8E%E9%AC%A5_(%E9%9B%BB%E5%BD%B1)"" rel=""nofollow noreferrer"">Chinese Wikipedia page of this film</a>, and search for 龍爭虎鬥 using Ctrl-F, it will be found on several places:</p>

<p><a href=""https://i.stack.imgur.com/cezxH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cezxH.png"" alt=""enter image description here""></a></p>

<p>But if we try to copy hyperlink of Chinese page into Google translate, it will be word ""tiger"" from somewhere:</p>

<p><a href=""https://i.stack.imgur.com/bsf5G.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bsf5G.png"" alt=""enter image description here""></a></p>

<p>Even more, if we try to translate Chinese page into English using build-in Chrome translate, it will be sometimes translated as ""Enter the Dragon"", in English manner:</p>

<p><a href=""https://i.stack.imgur.com/JIRQY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JIRQY.png"" alt=""enter image description here""></a></p>

<p>Why it gives different tanslations for the same Chinese text here?</p>
"
1203,"<p>I am new to neural networks, I've only started studying and learning about the subject a year ago, and I just started building my first neural network.</p>

<p>The project is a little bit ambitious: A browser extension for children's safety,  it checks for sexual or abusive content, so that it replaces that content with a placeholder, the user will have to insert a password to show original content.</p>

<p>I didn't find a dataset online so I decided to build my training dataset. So, I started by writing a web crawler, it starts collecting images, meanwhile implementing data augmentation techniques.</p>

<p>It basically resizes images (to 95x95), crops them, rotates, changes colors, adds blur, black and white, noise...etc.</p>

<p>The problem is that after applying these techniques, I noticed that some images are not even recognizable by a human subject.</p>

<p>I mean that even though I know that picture contains sexual content, it doesn't even appear to be sexual anymore. </p>

<p>So, do I have to label it as sexual or not sexual? </p>

<p>Notice that it's easier for me to consider it as sexual, if every image produces about 50 edited images, I'd only have to label the original image , what follows is that all 50 images get the same label. Is it okay to do just that?<a href=""https://i.stack.imgur.com/n6yQj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n6yQj.png"" alt=""enter image description here""></a></p>

<p>This is a sample of what I get after doing data augmentation, notice that some pictures are not recognizable by humans.</p>

<p>For example, look at the result after editing images hue and saturation, a human can't recognize this result, is it okay to label it : Not sexual?</p>

<p><a href=""https://i.stack.imgur.com/M5SPe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M5SPe.jpg"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/PjJWy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PjJWy.jpg"" alt=""enter image description here""></a></p>

<p>I wouldn't recognize the picture on the right if I didn't see the original one.</p>

<p><strong>Edit</strong> : I just tested this on human subjects (my brothers), they didn't recognize the squirrel on the right.</p>

<p>Thank you !</p>
"
1204,"<p>I was watching a documentary on Netflix about AlphaGo, and at one point (~1:10:16 from the end), one of the programmers uses the term ""heavy node,"" which I assume has to do with neural networks. I did a little bit of research but couldn't find anything on what that term means. The closest I could get was this wikipedia page on Heavy path decomposition: <a href=""https://en.wikipedia.org/wiki/Heavy_path_decomposition"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Heavy_path_decomposition</a>, which seemed like it could be somewhat related, but I wasn't sure how exactly. Has anyone heard of this term being used? Does anyone know what it means? </p>

<p>For context, in the documentary the line is that if it (the network/player) creates something new not in the heavy node, then they don't see it. </p>
"
1205,"<p>In recent years, China has made rapid progress in manufacturing and scientific research as evidenced by their successful teleportation of a single quantum entangled photon to a satellite in orbit. My question is, what major contributions have Chinese AI researchers made in the field of Artificial Intelligence?</p>
"
1206,"<p>I am trying to develop a machine learning algorithm to identify topological features within 3D CAD models (i.e. slots, pockets, holes, bosses etc)</p>

<p>For the input data I have decided to use the adjacency of the faces to identify the features within the model. I have developed a pre-processing algorithm which computes the adjacency between every face in the model and also whether the relationship is concave or convex in nature.</p>

<p>Below is an example of a 3D model and the relationships between each faces represented in a graph format.</p>

<p><a href=""https://i.stack.imgur.com/IU10S.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/IU10S.png"" alt=""Example of face adjacency within a part""></a> </p>

<p>I was thinking of using some sort of supervised learning training method where the training data would include all of the adjacency info for the model with labels defining the features that exist within the model such as:</p>

<p>Slot - made up of faces F7, F8, F9</p>

<p>Hole - made up of faces F15</p>

<p>Pocket - made up of faces F10, F11, F12, F13</p>

<p>And eventually once the model is sufficiently trained it would be able to identify the features in an un-seen model and determine which faces make up those features.</p>

<p>I am not sure how I would go about pre-processing the input data (i.e. it would be of variable length since the number of faces may not be the same from model to model). I am also struggling to understand which type of machine learning algorithm would be best suited to this application</p>

<p>Any help on this problem or even pointing to some resources to help understand would be appreciated</p>
"
1207,"<p>I am not looking for proof of AI (too broad, I know) concepts trying to solve a specific business problem, instead for a <strong>proof (or dis-proof) or a model</strong> (or a source of the same) where a machine is capable of </p>

<ul>
<li>General thinking and decision making.</li>
<li>Collecting data as it see fit (use any channel at their disposal).</li>
<li>Communicate as it see fit (with whom, how, where and when).</li>
<li>Develop and evolve an agenda.</li>
</ul>

<p><strong>Note</strong> - I wrote an article more than 5 years ago - <a href=""https://www.experts-exchange.com/articles/10609/Why-a-software-cannot-be-intelligent.html"" rel=""nofollow noreferrer"">why a software cannot be intelligent</a>. Since then I have not read a convincing argument against it.</p>

<p><strong>Edit</strong></p>

<p><em>Intelligence is the ability to take a decision in the unseen, unfamiliar, and unvisited circumstances</em>. So, my definition of Artificial intelligence is - <em>a machine <strong>doing</strong> essentially the same</em> (doing, not replicating or simulating). </p>

<p>I am looking for a proof or disproof that machine is capable of doing things (written above), not the following</p>

<ol>
<li><p>Instances where machine as simulated a certain aspect or property of intelligence.</p></li>
<li><p>Machine replicating a certain human behavior to achieve a goal. </p></li>
</ol>
"
1208,"<p>I have a pedestrian dataset and would like to estimate human height in a video survillance using person detection techniques like YOLO Darknet or SSD (Single Shot Detectors). Would this technique work? Also, the videos that I have are in a constrained environment with good illumination. The idea is to get the coordinates from the bounding box and try to estimate pixel height. After getting the pixel height, some correlation could be estimated between pixel height and real world height. Note that I won't be using camera calibration. </p>
"
1209,"<p>Due to recursive self-improvement, AI could lead to an intelligence explosion improving on itself year over year exponentially.</p>

<p>Assuming the proper environment was created to allow an AI to self-improve, how fast would this occur?</p>

<p>With human intervention we might say AI could improve similar to GDP or science growth say 3% per year. But this intervention would be additive to the self-recurring improvements made by the AI.</p>

<p><strong>What is a reasonable annual rate of self-growth and what would the processing power of the AI be after 10 years given an initial value of say 1PFLOP?</strong></p>

<p>Example:</p>

<p>human rate = 3% annually</p>

<p>AI rate = 1% of current processing power annually</p>

<p>Year 1. 1 PFLOP</p>

<p>Year 2. 1 PFLOP + (1 PFLOP * 3%) + (1 PFLOP * 1%) = 1.04 PFLOP</p>

<p>Year 3. 1.04 PFLOP + (1.04 PFLOP * 3%) + (1.04 PFLOP * 1.04%) = 1.082 PFLOP</p>

<p>Year 4. 1.082 PFLOP + (1.082 PFLOP * 3%) + (1.082 PFLOP * 1.082%) = 1.126 PFLOP</p>

<p>Year 5. 1.126 PFLOP + (1.126 PFLOP * 3%) + (1.126 PFLOP * 1.126%) = 1.172 PFLOP</p>
"
1210,"<p>For supervised learning, humans have to label the images computers use to train in the first place, so the computers will probably get wrong the images that humans get wrong. If so can computers beat humans?</p>
"
1211,"<p>Many of the architectures that do semantic segmentation like SegNet, DilatedNet (Yu and Koltun), DeepLab, etc. do not work on high resolution images. For such benchmarks like <a href=""https://www.cityscapes-dataset.com/benchmarks/#pixel-level-results"" rel=""nofollow noreferrer"">Cityscapes</a>, what is a standard/practical approach for such methods to perform on the benchmark?</p>

<p>I've tried to look into the paper, but I couldn't find such details. There's an <a href=""http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review"" rel=""nofollow noreferrer"">article</a> mentioning that they output at 1/8 of input images than do interpolation (usually 2, 4 or 8 times) from their results, but the article does not specify which upsampling techniques are the most reasonable one.</p>
"
1212,"<p>I've been studying a recommender system which uses a collaborative deep learning approach and bayesian learning having the following NN representation :</p>

<p><a href=""https://i.stack.imgur.com/p2veT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/p2veT.jpg"" alt=""sdae""></a></p>

<p>Need to know the working of stacked denoising autoencoders.</p>

<p>Here is the link to the paper:<a href=""http://www.wanghao.in/paper/KDD15_CDL.pdf"" rel=""nofollow noreferrer"">http://www.wanghao.in/paper/KDD15_CDL.pdf</a></p>
"
1213,"<p>What are the connections between <strong>ethics</strong> and <strong>artificial intelligence</strong>?</p>

<p>What are issues that have arisen, especially in the business context? What are issues that may arise?</p>
"
1214,"<p>Consider an environment, where an agent intends to move from cell ""A"" to cell ""G"", avoiding obstacles (cells marked with shading). The agent can move forward, rotate 90º to the left, or rotate 90º to the
right and can identify the type of cell in front of him.</p>

<p><a href=""https://i.stack.imgur.com/F07OU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F07OU.png"" alt=""enter image description here""></a></p>

<p>Classify the characteristics of the environment, according to: Acessible or non-acessible ;Deterministic or non‐deterministic; Episodic
or non‐episodic; Discrete or continuous; Static or dynamic.</p>
"
1215,"<p><a href=""https://i.stack.imgur.com/Nm8RW.jpgT"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nm8RW.jpgT"" alt=""enter image description here""></a></p>

<p>The above environment is <a href=""https://selfdrivingcars.mit.edu/deeptraffic/"" rel=""nofollow noreferrer"">DeepTraffic</a> </p>

<p>Now consider this situation in the above environment, the Red car (we control it with our RL agent) is on the extreme right lane.</p>

<p>During the exploration phase, we take a 'move right' action, which ofcourse will result in the car not moving right,but the other cars will be moving, state changes due to the rules of the environment.</p>

<p>I'm using CNN to solve this, the state representation is the image itself and its a Q-Learning algorithm as described in DQN paper from deepmind.</p>

<p>In the above situation I mentioned, wont the agent think due to 'move right' action the state has changed, which is not really the case?</p>

<p>and when remembering the state transition (s,a,r,s') should i remember the actual action 'move right'(invalid) or 'do nothing'(correct as per env) ?</p>
"
1216,"<p>I'm struggling to understand the underlying mechanics of CNNs so any help is appreciated. I have a network with a ReLU activation function which does perform signifigantly better than one with sigmoid. This is expected as <a href=""https://www.quora.com/How-does-the-ReLu-solve-the-vanishing-gradient-problem"" rel=""nofollow noreferrer"">ReLU solves the vanishing gradient problem</a>. However, my understanding was the reason we implement nonlinearities is to separate data which cannot be separated linearly. But if ReLU is linear for all values we care about it shouldn't work at all? </p>

<p>Unless, of course, neurons are defined for negative values but then my question becomes ""why does ReLU solve the vanishing gradient problem at all?"", since the derivative of ReLU for x&lt;0 = 0</p>
"
1217,"<p>In the book ""Reinforcement learning"" by Sutton there is a discussion of the k-armed bandit problem, where the expected reward from the bandits changes slightly over time (is non-stationary). Instead of updating the Q values by taking an average of all rewards, the book suggests using a constant step-size parameter, so as to give greater weight to more recent rewards. Thus:</p>

<p><span class=""math-container"">$$ Q_{n+1} = Q_n + \alpha (R_n - Q_n)$$</span></p>

<p>Where <span class=""math-container"">$\alpha$</span> is a constant between 0 and 1. </p>

<p>Until here I understand.</p>

<p>The book then asserts that:
We call this a weighted average because the sum of the weights = 1.</p>

<p>My Question: can someone please explain what that last line means, and why it is true?</p>
"
1218,"<p>I have a model that predicts sentiment of tweets. Are there any standard procedures to evaluate such a model in terms of its output?</p>

<p>I could sample the output, work out which are correctly predicted by hand, and count true and false positives and negatives but is there a better way?</p>

<p>I know about test and training sets and metrics like AUROC and AUPRC which evaluate the model based on known data, but I am interested in the step afterwards when we don't know the actual values we are predicting. I could use the same metrics, I suppose, but everything would need to be done by hand.</p>
"
1219,"<p>I am trying to do some experiments with some intelligent agents, but I'm not sure how significant they will be in the future. Can someone explain some interesting scenarios, maybe some use-cases of intelligent agents in the future? </p>

<p>For instance it can be used as a virtual assistant instead of a real call agent. But what can be a more appealing application in the future?</p>
"
1220,"<p>By open up I mean slightly open up so that a theoretical structure of panels with no width looks three-dimensional. The original structure being an ideal object where any number several panels can occupy the same region of space (plane).</p>

<p>To concretize what I mean by rigid structure of panels, let's take what's on my profile picture. I'm including here a larger version of that object:</p>

<p><a href=""https://i.stack.imgur.com/m9Uxz.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m9Uxz.jpg"" alt=""enter image description here""></a></p>

<p>An origami figure is folded from a square and, unlike this simple example in the image, it can be very convoluted, with layers upon layers.</p>

<p>Let's say that if I have an ideal, theoretical and flat model of an origami picture and by flat I mean the faces are on planes but not necessarily on a single plane. For example all of faces of the figure in the image would be in one plane, but there could be figures with more planes; think of animals with ears, flippers, etc.</p>

<p>I would like to open up those parts that hinge on a theoretical segment (relatively easy) or curve or make a triangle of the corners of those faces that have two adjacent faces with no connections, open up a set of several faces that allow such opening of which the image is a good example.</p>

<p>So far I have tried programming rules for different structures suchs as flaps, ends, wrap-arounds... However there are three no small problems. First, it's extremely challenging to take into account all the corner cases and possibilities. I suspect it sounds simpler that it really is, but I don't want to digress with explanations. Second, the code is not maintenable. It's difficult to put into words rules that have to be visualized. Third it's very difficult to unit test and debug.</p>

<p>I strongly suspect that there must be some Artificial Intelligence techniques for doing this. Would you be so kind as to point me in the right direction? Just in a general way, without needing to go much into detail.</p>

<p>Let me know if I should include more info or code.</p>
"
1221,"<p>I am currently writing an engine to play a card game and I would like for an ANN to learn how to play the game. The game is currently playable, and I believe for this game a deep-recurrent-Q-network with a reinforcement learning approach is the way to go. </p>

<p>However, I don't know what type of layers I should use, I found some examples of Atari games solved through ANN, but their layers are CNN (convolutional), which are better for image processing. I don't have an image to feed the NN, only a state composed of a tensor with cards in the player's own hand and cards on the table. And the output of the NN should be a card or the action 'End Turn'.</p>

<p>I'm currently trying to use TensorFlow but I'm open to any library that can work with NN. Any type of help or suggestion would be greatly appreciated!      </p>
"
1222,"<p>As far as I understand, the hill climbing algorithm is a local search algorithm that selects any random solution as an initial solution to start the search. Then, should we apply an operation (i.e., mutation) on the selected solution to get a new one or we replace it with the fittest solution among its neighbours? </p>

<p>This is the part of the algorithm where I am confused. </p>
"
1223,"<p>I do not have background in Artificial Intelligence related algorithms etc. But I have a scheduling problem that needs to be resolved. </p>

<p>For example, employees need to be scheduled for on call. They might be on vacation. </p>

<p>Can you guide as to what algorithms I need to look into, suitable for a beginner? </p>
"
1224,"<p>In the <a href=""https://en.wikipedia.org/wiki/Delta_rule"" rel=""nofollow noreferrer"">delta rule</a> the equation to adjust the weight with respect to error is :-</p>

<p><a href=""https://i.stack.imgur.com/YmYaD.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YmYaD.gif"" alt=""enter image description here""></a> </p>

<p><em>where <a href=""https://i.stack.imgur.com/VX3z3.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VX3z3.gif"" alt=""enter image description here""></a> is the <code>Learning Rate</code> and <strong>E</strong> is the <code>Error</code></em></p>

<p>The graph for <code>E</code> vs <code>w</code> would look like the one below with <code>E</code> in the <code>y</code> axis and <code>W</code> in the <code>x</code> axis</p>

<p><a href=""https://i.stack.imgur.com/Ju05S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ju05S.png"" alt=""enter image description here""></a></p>

<p>In other words we can write</p>

<p><a href=""https://i.stack.imgur.com/BRhJt.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BRhJt.gif"" alt=""enter image description here""></a></p>

<p>I want to know, what is the proof behind the gradient of a curve being equal/proportional to the distance between the two co-ordinates in the x-axis</p>

<p>-OR-</p>

<p><strong>(∂E/∂w) times step is a small shift on f(w) not w.so why does the difference between W(n+1) and Wn be equal to f(W)</strong></p>

<p>I found a similar question <a href=""https://stats.stackexchange.com/questions/305516/some-confusion-of-gradient-descent"">some-confusion-of-gradient-descent</a>, but the accepted answer doesnot have a proof.</p>
"
1225,"<p>Are there a finite set of computable functions constructing deep neural network which can form or implement any c.e. function or computable function? </p>

<p>Or does there exist a finite set of computable function by which every c.e. function can be implemented by combination and connection(like connection in DNN)?</p>
"
1226,"<p><a href=""http://bigthink.com/elise-bohan/the-most-human-ai-youve-never-heard-of-meet-luna"" rel=""nofollow noreferrer"">This AI</a> is really human-like and allegedly doesn't give pre-programmed responses. It's makers Robots without Borders say the project is open source but I couldn't find the code anywhere. </p>
"
1227,"<p>I am currently working on a defect detection algorithm but I only have a few samples of defects.I googled for defect detection datasets and I found this one: </p>

<p><a href=""http://resources.mpi-inf.mpg.de/conferences/dagm/2007/prizes.html"" rel=""nofollow noreferrer"">http://resources.mpi-inf.mpg.de/conferences/dagm/2007/prizes.html</a> </p>

<p>which has a few hundreds of original images of defects.</p>

<p>My idea is:
Imagenet => Defect dataset from internet => Own defect dataset</p>

<p>Step 1. Training a model with ImageNet initialization using the defect dataset  found in the internet (+ non-defect images + augmented data)</p>

<p>Step 2. Using the output model of step 1 (which will be more similar to my own data),do transfer learning using my own defect dataset (defects + non-defects + augmented).</p>

<p>Do you think this a good way to get good results? </p>

<p>Based on:
<a href=""https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0"" rel=""nofollow noreferrer"">https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0</a></p>

<p>Should defect images consider as low similar with imagenet's images? or similar to model because a both inputs are images? Some webpages said because they both are images, they are similar but some webpages said because these images are too different to the images used to train the imagenet model so I got confused about this.</p>

<p>If I skip step 1, I dont think I get anything good because I have less than 100 images.</p>

<p>Any advise or comment will be appreciated. </p>
"
1228,"<p>I am trying to build a neural network that takes in a single string, ex: ""dog"" as an input, and outputs 50 or so related hashtags such as, ""#pug, #dogsarelife, #realbff"".</p>

<p>I have thought of using a classifier, but because there is going to be millions of hashtags to choose the optimal one from, and millions of possible words from the english dictionary, it is virtually impossible to search up the probability of each </p>

<p>It is going to be learning information from analyzing twitter posts' text, and its hashtags, and find which hashtags goes with what specific words.</p>
"
1229,"<p>I have a sample set of data about Leads that gets generated every day. Leads are nothing but a user expressing request to be our partner or not. Sample data set is as shown below</p>

<pre><code>LEADID,CREATEDATE,STATUS,LEADTYPE
810029,24-DEC-17 12.00.00.000000000 AM,open,LeadType1
806136,30-DEC-17 12.00.00.000000000 AM,open,LeadType2
812134,31-DEC-17 12.00.00.000000000 AM,open,LeadType2
806147,31-DEC-17 12.00.00.000000000 AM,open,LeadType1
806166,01-JAN-18 12.00.00.000000000 AM,open,LeadType2
28002,04-MAR-16 12.00.00.000000000 AM,open,LeadType2
808156,01-JAN-18 12.00.00.000000000 AM,open,LeadType1
808162,01-JAN-18 12.00.00.000000000 AM,open,LeadType2
806257,07-JAN-18 12.00.00.000000000 AM,open,LeadType1
832091,17-JAN-18 12.00.00.000000000 AM,open,LeadType2
838079,17-JAN-18 12.00.00.000000000 AM,open,LeadType1
66001,26-MAR-16 12.00.00.000000000 AM,open,LeadType1
70001,28-MAR-16 12.00.00.000000000 AM,open,LeadType2
806019,23-DEC-17 12.00.00.000000000 AM,open,LeadType2
822064,12-JAN-18 12.00.00.000000000 AM,open,LeadType1
834043,14-JAN-18 12.00.00.000000000 AM,open,LeadType2
836053,16-JAN-18 12.00.00.000000000 AM,open,LeadType1
838119,19-JAN-18 12.00.00.000000000 AM,open,LeadType2
</code></pre>

<p>As you can see Lead types can be of LeadType1 or LeadType2 and this get generated every day. </p>

<p>In order to make sense of data I created the following plot using Python</p>

<p><a href=""https://i.stack.imgur.com/MXTc8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MXTc8.png"" alt=""enter image description here""></a></p>

<p>The supporting code is as follows. Note I am just a Noob to Python and AI but I want to check if this proves a valid use case for Machine Learning and what should be my approach</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#%matplotlib inline
in_file = 'lead_data.csv'
mydf = pd.read_csv(in_file,encoding='latin-1')

fig, ax = plt.subplots(figsize=(15,7))
#g = mydf.groupby(['R4GSTATE','LEADTYPE']).count()['STATUS'].unstack()
g = mydf.groupby(['R4GSTATE','STATUS']).count()['LEADTYPE'].unstack()
g.plot(ax=ax)
#ax.set_xlabel('R4GSTATE')
ax.set_xlabel('R4GSTATE')
ax.set_ylabel('Number of Leads')
ax.set_xticks(range(len(g)));
ax.set_xticklabels([""%s"" % item for item in g.index.tolist()], rotation=90);
</code></pre>

<p>Basically I just read the csv, curated the data( I have cleaned the original csv) to keep what is meaningful for me. I also created grouping of number of leads Month-Year wise so that I can see the historical lead generated every month. </p>

<p>I want to know if Machine Learning helps me to predict number of Lead generated in next coming months based on previous months data. </p>

<p>If the answer is yes then is Linear Regression the right path to explore further</p>
"
1230,"<p>Forgive what might be a basic question.  I'm just experimenting with ML / AL and I have a small problem set and I'd like to see if it can be solved with ML / AI.  Basically, given a set of objects with multiple features, I'd like to create a process for recommending one automatically to a user.  </p>

<p>I'm thinking that some sort of clustering algorithm may be the best approach. However, one main challenge I'm trying to wrap my head around is that I don't know in advance how many distinct clusters will evolve... There may be scenarios where we Feature X is really important, but other scenarios where a user will say Feature Y is important.  </p>

<p>Secondly, what is my input set?  For each training sample, I will have 1 selected object, and N-1 unselected objects.  But I don't want to ""train"" that the unselected objects are ""bad"" because they could be selected in a future training example.  </p>

<p>Finally, I don't have a large training set already, so I would like to use feedback (user input, ""This was a bad choice"" or ""Use this object instead."") from the process to further refine the algorithm.  Is this feasible?</p>

<p>Are there any established patterns for this sort of process?  Thanks in advance.</p>
"
1231,"<p>I am currently learning about CNN's and I am confused on how filter/kernels are initialized beside their size? Say if you want a filter of 3x3 how are the inner values initialized a the start? </p>

<p><a href=""http://setosa.io/ev/image-kernels/"" rel=""nofollow noreferrer"">http://setosa.io/ev/image-kernels/</a></p>

<p>Do you just use those predefined image-kernels as a start? Or are they randomly initialized and retrained from backpropagation? I am pretty confused on this matter because so far of all the lecture I have taken no one really talk about this yet and I want to know it now.</p>
"
1232,"<p>I am implementing neural network to train hand written digits. Following is the cost function,</p>

<p><a href=""https://i.stack.imgur.com/Q4Wzx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q4Wzx.png"" alt=""cost function""></a>
In <strong>log(1-(h(x))</strong>, if h(x) is 1, then it would result in log(1-1) (i.e) log(0).
So im getting math error. </p>

<p>Im initializing the weights randomly between 10-60. Im not sure where i have to change and what i have to change! Thanks for your help.</p>
"
1233,"<p>I'm reading the book <a href=""https://www.springer.com/la/book/9783642072857"" rel=""nofollow noreferrer"">Introduction to Evolutionary Computing</a> and on the chapter about Evolution Strategies said that we have to modify the strategy parameter sigma (standard deviation or mutation step size) before using in to modify the object parameter and I don't understand why.</p>

<p>Why do we have to modify sigma before the mutation of object parameters?</p>

<p>Maybe because if we do it we will have the mutated object parameters and the strategy parameters that have generated them.</p>
"
1234,"<p>I am trying to develop a neural network which can identify design features in CAD models (i.e. slots, bosses, holes, pockets, steps).</p>

<p>The input data I intend to use for the network is a n x n matrix (where n is the number of faces in the CAD model). A '1' in the top right triangle in the matrix represents a convex relationship between two faces and a '1' in the bottom left triangle represents a concave relationship. A zero in both positions means the faces are not adjacent. The image below gives an example of such a matrix.
<a href=""https://i.stack.imgur.com/Soj7K.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Soj7K.png"" alt=""enter image description here""></a></p>

<p>Lets say I set the maximum model size to 20 faces and apply padding for anything smaller than that in order to make the inputs to the network a constant size.</p>

<p>I want to be able to recognise 5 different design features and would therefore have 5 output neurons - [slot, pocket, hole, boss, step]</p>

<p>Would I be right in saying that this becomes a sort of 'pattern recognition' problem? For example, if I supply the network with a number of training models - along with labels which describe the design feature which exists in the model, would the network learn to recognise specific adjacency patterns represented in the matrix which relate to certain design features?</p>

<p>I am a complete beginner in machine learning and I am trying to get a handle on whether this approach will work or not - if any more info is needed to understand the problem leave a comment. Any input or help would be appreciated, thanks. </p>
"
1235,"<p>I am new to machine learning and AI, so forgive me if this is obvious. I was talking with a friend on how to solve this problem, and neither of us could figure out how to do it.</p>

<p>Say I have a grid area of 100x100 blocks, and I want a robot to build a horizontal 100x100 grid, and 3 blocks high. I am given a random, but known starting surface, always 100x100 but the height of the random surface can vary from 1 to 5 blocks. I have an extra reserve of blocks i can pick up, so dont have to worry about running out. The robot can move in any direction, even diagonally at some cost penalty. The robot can obviously move a 4 high block to fill in a 2 high, so each is at the design height of 3. 
This sounds like a reinforcement learning problem, but would any one be able to explain more detail how I would do this, to a) minimize the amount of moves, and b) to get to the design surface. </p>
"
1236,"<p>Trying to figure out what Google is offering in the article at this link
<a href=""https://www.nytimes.com/2018/01/17/technology/google-sells-ai.html"" rel=""nofollow noreferrer"">https://www.nytimes.com/2018/01/17/technology/google-sells-ai.html</a>.</p>

<p>I also need to know if their offerings are limited to visual scans and if so, the range of photographical types it can work with (heat maps for example, or continuous video feed).</p>

<p>We are a small road construction company looking for ways to start using AI on some of our projects so I am in the process of gathering information on exactly what it can do.</p>
"
1237,"<p>I am working with a project which is a agent based pedestrian simulation in Java and its is animated with the help of JavaFX. I've tried to read all the social force model papers but my understanding of those articles are none. 
So I tried an own approach which got trashed after failing time after time.</p>

<p>My approach was that each agent calculated its surrounding and first calculated the distance to each of the agents on the field and if that distance was below a constant then the agent calculates the angle of that agent which is too close to it and moves accordingly to the calculated angle. </p>

<p>This approach didnt work for me because the ""avoidance code"" is not efficient enough and the agents just dont know where to go when they meet and just stays in place.</p>

<p>I am asking for guidance to how I can approach this problem in a better way.</p>

<pre><code>double[] check(Vector&lt;Pedestrian&gt; peds, Pedestrian p1){
for (Pedestrian p : peds){
    if (p.getPedestrianId() != this.id){
        double distance = IPedestrian.distance_formula(getTranslateX(), getTranslateY(), p.getTranslateX(), p.getTranslateY());
        if (distance &lt;= DANGER){
            System.out.println(""DANGER"");
            return IPedestrian.angle(getTranslateX(), getTranslateY(), p.getTranslateX(), p.getTranslateY(), p1);
        }
    }
}
return new double[] {SPEED, 0};
</code></pre>

<p>}</p>

<pre><code>public void move(Vector&lt;Pedestrian&gt; peds, Pedestrian p) {
double[] new_steps = this.check(peds, p);
if (side == SideChooser.Left){
    setTranslateX(getTranslateX() + new_steps[0]);
    setTranslateY(getTranslateY() + new_steps[1]);
} else {
    setTranslateX(getTranslateX() - new_steps[0]);
    setTranslateY(getTranslateY() - new_steps[1]);
}
</code></pre>

<p>}</p>

<p>Math formulas: </p>

<pre><code>static double distance_formula(double thisX, double thisY, double otherX, double otherY){
return Math.sqrt(Math.pow(otherX - thisX, 2) + Math.pow(otherY - thisY, 2));
</code></pre>

<p>}</p>

<pre><code>static double[] angle(double x1, double y1, double x2, double y2, Pedestrian p){
double angle = Math.toDegrees(Math.atan2(y2-y1, x2-x1));
angle += Math.ceil(-angle/360) * 360;

//double angle = Math.toDegrees(Math.atan2(y2-y1, x2-x1));

if (p.getSideChoosen() == SideChooser.Left){//if the pedestrian is from the left side
    if (angle &lt; 45 || angle &gt; 315)//front
        return new double[]{-SPEED/5, 0};

    else if (angle &gt;= 135 || angle &lt;= 225 ) //back
        return new double[]{SPEED*1.4, 0};

    else if (angle &gt;= 45 || angle &lt;= 90)//North-East
        return new double[]{0, SPEED};

    else if (angle &gt; 90 || angle &lt;= 135) //North-West
        return new double[]{SPEED*1.2 , SPEED};

    else if (angle &gt;= 270 || angle &lt;= 315) //South-East
        return new double[]{0, -SPEED};

    else if (angle &gt; 225 || angle &lt;= 270) //South-West
        return new double[]{SPEED*1.2, -SPEED};

    else
        return new double[]{SPEED, 0};
} else {
    if (angle &lt; 45 || angle &gt; 315)//back
        return new double[]{SPEED*1.4, 0};

    else if (angle &gt;= 135 || angle &lt;= 225 ) //front
        return new double[]{-SPEED/5, 0};

    else if (angle &gt;= 45 || angle &lt;= 90)//North-West
        return new double[]{SPEED*1.2, -SPEED};

    else if (angle &gt; 90 || angle &lt;= 135) //North-East
        return new double[]{0 , -SPEED};

    else if (angle &gt;= 270 || angle &lt;= 315) //South-West
        return new double[]{SPEED*1.2, SPEED};

    else if (angle &gt; 225 || angle &lt;= 270) //South-East
        return new double[]{0, SPEED};

    else
        return new double[]{SPEED, 0};
}
</code></pre>

<p>}</p>
"
1238,"<p>Below is a quote from CS231n</p>

<blockquote>
  <p>Prefer a stack of small filter CONV to one large receptive field CONV
  layer. Suppose that you stack three 3x3 CONV layers on top of each
  other (with non-linearities in between, of course). In this
  arrangement, each neuron on the first CONV layer has a 3x3 view of the
  input volume. A neuron on the second CONV layer has a 3x3 view of the
  first CONV layer, and hence by extension a 5x5 view of the input
  volume. Similarly, a neuron on the third CONV layer has a 3x3 view of
  the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose
  that instead of these three layers of 3x3 CONV, we only wanted to use
  a single CONV layer with 7x7 receptive fields. These neurons would
  have a receptive field size of the input volume that is identical in
  spatial extent (7x7), but with several disadvantages.</p>
</blockquote>

<p>My visualized interpretation 
<a href=""https://i.stack.imgur.com/KZy6R.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KZy6R.png"" alt=""enter image description here""></a></p>

<p>How can you see through the first CNN layer from the second CNN layer and see a 5x5 sized receptive field? There were no previous comments stating all the other hyperparameters, like input size, steps, padding, which made this very confusing to visualize.</p>

<p>Edited:</p>

<p>I think I found the answer
<a href=""https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807"" rel=""nofollow noreferrer"">https://medium.com/@nikasa1889/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807</a></p>

<p>BUT I still don't understand it. In fact, I am more confused than ever.</p>
"
1239,"<p>I trained data for recognizing the fingerprint of my friends. I am feeding testing data to local machine by hardware. What should I do to run this as a server and get the data over network in least time and in most efficient way? </p>
"
1240,"<p>I want to develop an Eclipse voice plugin on a Mac that helps me jot down high level classes and stub methods like
create a class that inherits from X, add a method that returns String</p>

<p>Could somebody help me point out the right material to learn to achieve that?</p>

<p>I don't mind using an existing solution if it exists. As far as I understand, I would have to use some Siri interface and use nltk to convert the natural text into commands. Maybe there's some chatbot library that saves me some boilerpate NLP code to directly jump on to writing grammar or selecting sentence patterns.</p>
"
1241,"<p>I'm going through Andrew NG's course which talks about YOLO but he doesn't go into the implementation details of anchor boxes.</p>

<p>Look through the code, each anchor box is represented by two values, but what exactly are these values representing??</p>

<p>As for the need of Anchor boxes, I'm also a little confused about that --
As far as I understand, the ground truth labels have around 6 variables :</p>

<p>1)  P_o which check if it's an object or background,</p>

<p>2,3)   Bx, By (which are the center coordinates)</p>

<p>4,5) Bh, Bw which are the (Height and Width of the box)</p>

<p>6) C ( Object Class, which depends on how many class labels you have, so you can have multiple C)</p>

<p>As for creating the bounding box, </p>

<p>Bh is divided by 2, with one half from the center points (Bx, By) to the top, and the other half to the bottom.</p>

<p>If we train our classifier, wouldn't the prediction boxes be close to the ground truth labels as training progresses?  So if our ground truth label has a high height, small width as boxes for some images, and low hight and large width for other images, wouldn't our classifier automatically
learn to differentiate between when to use one over the other, as it is being trained? If so then what is the use of anchor boxes? And what are those numbers representing anchor boxes representing? </p>

<p>Thank you. </p>
"
1242,"<p>So I am currently trying to create a program that when provided a list of descriptive words or a passage of text, would create a piece of abstract art based on the feelings evoked by those words. I figured that AI would probably be the best way of achieving the results I am looking for. Even though I do have quite a bit of experience with programming, I have just about zero knowledge on the subjects of machine learning, artificial intelligence, neural nets, etc. Another issue is that I simply don't have much time to write this program and I simply don't have the luxury of thoroughly learning about this subject.</p>

<p>I came across a really cool python thingy called StackGAN that is specifically made for text to image generation. My plan was to take a bunch of pictures of abstract art and for each one write down a list of descriptive words or emotions that they evoke. I figure that there must be some way for me to then feed the pictures plus associated words/description into the neural net as training data but have no idea how to do that and the fact that there is very little documentation about the program doesn't help. Even after spending a few days trying to make sense of the code I am completely lost as to use StackGAN.</p>

<p>So.... my question is: how exactly do I set up the training data and train StackGAN to do what I am trying to do? The github's README.md mentions some sort of CNN-RNN text embedded in the images used for training, and I have no idea what this means, if its necessary, or how I would add such data because I can't find any sort of thorough instructions. Also if you have any suggestions on alternative more user friendly libraries that would be greatly appreciated.</p>

<p>StackGAN link: <a href=""https://github.com/hanzhanggit/StackGAN-v2"" rel=""nofollow noreferrer"">https://github.com/hanzhanggit/StackGAN-v2</a></p>
"
1243,"<p>Before I start I want to let you know that I am completely new to the field of deep learning! Since I need a new graphics card either way (gaming you know) I am thinking about buying the GTX 1060 with 6GB or the 1070 ti with 8GB. Because I am not rich, basically I am a pretty poor student ;), I don't want to waste my money. I don't need deep learning for my studies I just want to dive into this topic because of personal interest. What I want to say is that I can wait a little bit longer and don't need the results as quick as possible.</p>

<p>So here is my question:
Can I do deep learning with the 1060 (6GB seem to be very limiting according to some websites) or the 1070 ti? Is the 1070 ti overkill for a person hobby deep learner?</p>

<p>Or should I wait for the new generation Nvidia graphics card?</p>

<p>Thank you very much in advance!</p>
"
1244,"<p>Imagine two languages that have only these words:</p>

<pre><code>Man = 1,
deer = 2, 
eat = 3,
grass = 4 
</code></pre>

<p>And you would form all sentences possible from these words:</p>

<pre><code>Man eats deer.
Deer eats grass.
Man eats.
Deer eats.
</code></pre>

<p>German:</p>

<pre><code>Mensch = 5,
Gras = 6, 
isst = 7, 
Hirsch = 8
</code></pre>

<p>Possible german sentences:</p>

<pre><code>Mensch isst Hirsch.
Hirsch isst Gras.
Mensch isst.
Hirsch isst.
</code></pre>

<p>How would you write a program that would figure out which words have same meaning in english and german?</p>

<p>It is possible.</p>

<p>All words get their meaning from the information in which sentences they can be used. Connection with other words define their meaning.</p>

<p>We need to write a program that would recognize that a word is connected to other words in the same way in both language. Then it would know those two words must have the same meaning.</p>

<p>If we take word ""deer"" (2) it has this structure in english</p>

<pre><code>1-3-2
2-3-4
</code></pre>

<p>In german (8):</p>

<pre><code>5-6-8
8-6-7
</code></pre>

<p>We get the same structure (pattern) in both languages: 
both 8 and 2 lie in first and last position, and middle word is the same in both languages, the other word is different in both languages. So we can conclude that 8=2 because both elements are connected with other elements the same way.</p>

<p>Maybe we just need to write a very good program for recognizing analogies and we will be on the right track to creating AI?</p>
"
1245,"<p>As many papers point out, for better learning curve of a NN, it is better for a data-set to be normalized in a way such that values match a Gaussian curve.</p>

<p>Does this process of feature normalization apply only if we use sigmoid function as squashing function? If not what deviation is best for the tanh squashing function?</p>
"
1246,"<p>I got a 'homework' to solve from college but I have no idea how to do it ...<br>
Can anyone get it?</p>

<blockquote>
  <p>A neural network with the following structure is given: one input
  neuron, four elements in the hidden layer, one output neuron. The
  output neuron is bipolar, the neurons in the hidden layer are linear.
  The weights between the input neuron and the neurons in the hidden
  layer have the following values: w11 = -3, w12 = 2, w13 = -1, w14 =
  0.5, while between neurons in the hidden layer and the starting neuron:: w21 = +2, w22 = -0.5, w23 = -3, w24 = +1 (no threshold input
  in both layers). What will the network response be like if the number
  3 is given to the input neuron?</p>
</blockquote>
"
1247,"<p>I am writing my thesis in the field of (deep) metric learning (DML). I am training a network in the fashion of contrastive / triplet Siamese networks to learn similarity and dissimilarity of inputs. In this context, the ground truth is commonly expressed as a binary. Let's take an example based on the similarity of <a href=""https://en.wikipedia.org/wiki/Gray_wolf#Taxonomy_and_evolution"" rel=""nofollow noreferrer"">species</a>:</p>

<ul>
<li>Image A: german shepard (dog)</li>
<li>Image B: siberian husky (dog)</li>
<li>Image C: turkish angora (cat)</li>
<li>Image D: gray wolf (wolf)</li>
</ul>

<blockquote>
  <p>Image A and B are similar: same species, same sub-species (canis lupus) -> <strong>1.0</strong> == <code>TRUE</code></p>
  
  <p>Image A and C are dissimilar: different species (canis lupus vs. felis silvestris) -> <strong>0.0</strong> == <code>FALSE</code></p>
  
  <p>Image A and D ? same species, but different sub-species -> <strong>0.8</strong></p>
</blockquote>

<p><strong>Which metric learning approaches use a continuous ground truth for learning?</strong></p>

<p>I could imagine that there is a lot of research out there using a continuous ground truth in classification settings. For instance to learn that the expression of a face is ""almost (60%) happy"", or more controversial, an image of a person depicts a ""70% attractive person"". Also in this fields I would be happy for hints / links.</p>

<p><strong>Remarks:</strong></p>

<ul>
<li>I don't ask for opinions on whether this makes sense or not.</li>
<li>Could someone please attach the tag <em>metric learning</em>? I don't have enough reputation.</li>
</ul>
"
1248,"<p>Will it be possible to model the problem of odd-even distinction of an integer (not binary string representation) using neural networks? </p>
"
1249,"<p>So I am training an ANN for classification between 3 classes. The ANN has an input layer, one hidden layer and a 3 node output layer. </p>

<p>The problem I am facing is that the output being produced by the 3 output nodes are so close to 1 (for the first few iterations at least and so i am assuming the problem propagates to future outputs as well) the weights are not being updated (or hardly updated) due to overflow (about <code>10^-11</code>). Now I can fix the overflow problem (but I don't think it is the culprit). I think such low values of error is the main culprit, and I cannot figure what is causing such low values of error. </p>

<p>What will cause the network to behave more interactively like I will be actually able to grasp the weight updates and not something in the order of <code>10^-11</code>?</p>

<p>Note: Data set contain values in the order of <code>10's</code> and the weights randomly initialized are in the order of <code>0 &lt; w &lt; 1</code>. I have tried feature normalization but it is not that effective.</p>

<p>Any help is highly appreciated.</p>

<p>EDIT: I did not know the term was called vanishing gradient, so I added it for better readability</p>

<p>NOTE: Experienced users can freely edit the question to cater to a more general problem, since I believe the problem has many other variations.</p>
"
1250,"<p>I'm having trouble with accuracy evaluation at the end of Session.</p>

<p>Training process looks like this:</p>

<pre><code>with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    epochs = 10
    batch_size = 128

    for e in range(epochs):
        shuffle_indices = np.random.permutation(np.arange(len(inputx)))
        X_train = inputx[shuffle_indices]
        y_train = inputy[shuffle_indices]

        epoch_loss = 0
        for i in range(int(len(inputx) // batch_size)):
            start = i * batch_size
            batch_x = X_train[start:start + batch_size]
            batch_y = y_train[start:start + batch_size]

            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
            epoch_loss += c

        print(e + 1, '/', epochs, 'loss:', epoch_loss)
</code></pre>

<p>At the end of this session I want to evaluate reached accuracy by calculating mean value of percentual differences between <code>prediction</code> and <code>testy</code> for each of 3 output layer neurons.</p>

<pre><code>percentual_differences_for_neuron1 = # insert your advice here
accuracy_for_neuron1 = tf.reduce_mean(percentual_differences_for_neuron1)
print('Accuracy of neuron 1 is', '''another advice''')
</code></pre>

<p>What's the best way of doing this in <em>TensorFlow</em>?</p>
"
1251,"<p>My goal is to build a neural net that can find patterns between a hash and a word on it's own. So that it returns the word of any hash that I will input. </p>

<p>Unfortunatally my skill in the area of neural net isn´t advanced, and I want to use this project to learn more. So I use a German dictionary and encode it via <code>one_hot</code> encoding. Then I generate the <code>sha256</code> value of every word inside (before I have done this I cleaned the file and wrote every word in another line) it. So I got an big array with the shape of 20000x20000 for the words and another for the hashs. </p>

<p>So then I used the a example of the Keras homepage for <code>binary classification</code> because the one_hot values are represented by ones and zeros.</p>

<p>So if I want to predict a hashs I get these error: <code>Error when checking : expected dense_1_input to have shape (20000,) but got array with shape (1,)</code>. So I don't know if this model is working for my problem but I couldn't convert one hash into a size of 20000x20000. (The hash will <code>one_hot</code> encoded for that prediction). So how could I get it to accept different shaped hashs/one hash only?  </p>

<p>Is there a way to train the model with each hash after another for example with a for loop?!<br>
EDIT: So I figured out that I can convert a list of characters into a numpy.array with 2 dimensions. So I <code>hot_encoded</code> every character and create a list of them, these list I passed inside the <code>np.array(words,ndim=2)</code>. So this I have done for my hashs aswell. Then after I run the code I got this error: <code>ValueError: setting an array element with a sequence</code> So I tried to reshape the array with the <code>.reshape(20000)</code> command but nothing chaged. So what to do with that? EDIT2: I figured out now that the problem is that <code>enhot_encoding</code> generates diffrent sized ""arrays"" for each word, and if I fill this into a real array and this into a neuronal net it have to return this error. But still the question is: How to convert single words and hashs to a format that I can train a neuronal net with and get usefull output so I can enter any hash and it should return some kind of word(lable). If you need the actual code please inform me and I will upload it`s current state.
Code:  </p>

<pre><code>model = Sequential()
model.add(Dense(64, input_shape=20000, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dense(units=64, activation=""relu""))
model.add(Dropout(0.5))
model.add(Dense(19957, activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='rmsprop', metrics=['accuracy'])
print(""Fitting data..."")
model.fit(test_hashs,test_words ,epochs=10,batch_size=128, verbose=1)


train_y=input(""Input a hash that is not contained in the training data: "")
#train_x=pd.Series(hashlib.sha256(str.encode(train_y)).hexdigest())
train_y=pd.Series(train_y)
#test_x=pd.get_dummies(train_x)
test_y=pd.get_dummies(train_y)
model.save(""first_test"")
print(model.evaluate(test_y))
#score=model.evaluate(test_x, test_y, batch_size=128,)
print(""Score: ""+score)
prediction=model.predict(test_x,verbose=1)
for i in prediction:
    print(i)
</code></pre>
"
1252,"<p>I am new in Machine Learning. I have taken a course in vision and we are required to do a project. </p>

<p>I am thinking of  data mining medical lab report images. My code must take an image and jpg file and then extract important information from it like lab where test has been done, patient name, test type and more important various data like heamoglobin, RBC, etc in case of blood test report. </p>

<p>I can build an OCR, but, problem which I am stuck at is in case of data which generally forms a table like structure. So, I want to find that tabular structure on which I can just apply matrix extraction to find various datas. </p>

<p>I'm looking for assistance with two basic things:</p>

<ol>
<li><p>Is my approach of finding tables and then extracting data is correct? If yes, then can you point out some good papers or implementation to find tabular structure. (P.S.- Don't mention tabular)</p></li>
<li><p>Any approach which is state-of-the-art or good? (Paper or implementation)</p></li>
</ol>
"
1253,"<p>I'm relatively new to AI, and I've tried to create one that ""speaks"". Here's how it works:<br><br>
1. Get training data e.g 'Jim ran to the shop to buy candy'<br>
2. The data gets split into overlapping 'chains' of three e.g ['Jim ran to', 'ran to the', 'to the shop', 'the shop to'...]<br>
3. User enters two words<br>
4. Looks through the chains to find if the two words have been seen before.<br>
5. If they have, finds out which word followed it and how many times.<br>
6. Work out the probability e.g: if 'or' followed the two words 3 times, 'because' followed the two words 1 time and 'but' followed it 1 time it would be 0.6, 0.2 and 0.2<br>
7. Generate a random decimal<br>
8. If the random decimal is in the range of the first word (0 - 0.6) pick that one or if it's in the range of the second word (0.6 - 0.8) pick that word or if it's in the range of the third (0.8 - 1) pick that word<br>
9. Output the word picked<br>
10. Repeat from 4 but with the new last two words e.g if the last words had been 'to be' and it picked 'or' the new last two words would be 'be or'.</p>

<p>It does work, but it doesn't stick to a particular topic. For example, after training with 800 random Wikipedia articles:</p>

<blockquote>
  <p>In the early 1990s the frequency had a plastic pickguard and separate hardtail bridge with the council hoped that the bullet one replaced with the goal of educating the next orders could revert to the north island or string of islands in a new urban zone close to the west.</p>
</blockquote>

<p>As you can see the topic changes many times mid-sentence. I thought of increasing the number of words it considered from two to three or four, but I thought it might start simply quoting the articles. If I'm wrong please tell me.</p>

<p>Any help is greatly appreciated. If I haven't explained clearly enough or you have any questions please ask.</p>
"
1254,"<p>I have seen people using stacked softmax layers right at the output of neural networks designed for classification. I'm trying to understand this. Does it give any additional value? I think this could ""sharpen"" decisions on the boundaries.   </p>

<blockquote>
  <p>model.add(Dense(10, activation='sigmoid'))<br>
  model.add(Dense(1, 
  activation='sigmoid'))</p>
</blockquote>

<p>Seen <a href=""https://github.com/natbusa/deepcredit/blob/master/default-prediction.ipynb"" rel=""nofollow noreferrer"">here.</a></p>
"
1255,"<p>Is there any previous work on computing some sort of prominence score based on the prevalence of features in an image?</p>

<p>For example, let's say I am classifying images based on whether or not they have dogs in them. Is there a way to compute how prominent that feature is?</p>
"
1256,"<p>At some point in time during the evolution, because of some factors, some beings first started to become conscious of themselves and their surroundings. That conscious experience is beyond some mere sensory reflexive actions trained. Can that be possible with AI?</p>
"
1257,"<p>I have started to make a chatbot. It has a list of greetings that it understands and responds to with its own list of greetings.</p>

<p>How would a bot/script learn a new greeting or a synonym to a word it already knows?</p>
"
1258,"<p>A human player plays limited games compared to a system that undergoes millions of iterations. Is it really fair to compare AlphaGo with the world #1 player when we know experience increases with the increase in number of games played?</p>
"
1259,"<p>I read about minimax, then alpha-beta pruning and then about iterative deepening. Iterative deepening coupled with alpha-beta pruning proves to quite efficient as compared alpha-beta alone.</p>

<p>I have implemented a game agent that uses iterative deepening with alpha-beta pruning. Now I want to beat myself. What can I do to go deeper? Like alpha-beta pruning cut the moves, what other small change could be implemented that can beat my older AI?</p>

<p>My aim to go deeper than my current AI. If you want to know about the game, here is a brief summary:</p>

<p>There are two players, four game pieces and a 7-by-7 grid of squares. At the beginning of the game, the first player places both the pieces on any two different squares. From that point on, the players alternate turns moving both the pieces like a Queen in chess (any number of open squares vertically, horizontally, or diagonally). When the piece is moved, the square that was previously occupied is blocked. That square can not be used for the remainder of the game. The piece can not move through blocked squares. The first player who is unable to move any one of the queens loses.</p>

<p>So my aim is to cut the unwanted nodes and search deeper.</p>
"
1260,"<p>So, I have seen few pictures re-created by a Neural Network or some other Machine Learning algorithm after it has been trained over a data set.</p>

<p>How, exactly is this done? How are the weights converted back into a picture or a memory which a Neural Net is holding?</p>

<p>A real life example would be when we close our eyes we can easily visualize things we have seen. Based on that we can classify things we see. Now in a Neural Net classification part is easily done, but what about the visualization part? What does the Neural Net see when it closes its eyes? And how to represent it for human understanding?</p>

<p>For example a deep net generated this picture:</p>

<p><a href=""https://i.stack.imgur.com/1YJg6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1YJg6.jpg"" alt=""enter image description here""></a>
SOURCE: <a href=""http://fastml.com/deep-nets-generating-stuff/"" rel=""nofollow noreferrer"">Deep nets generating stuff</a></p>

<p>There can be many other things generated. But the question is how exactly is this done?</p>
"
1261,"<p><a href=""https://i.stack.imgur.com/oUWU8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oUWU8.png"" alt=""The log probability function""></a></p>

<p>Can anyone explain what information the formula gives us. What does the notations mean? Where i can find more material about what does the log probability function do?</p>
"
1262,"<p>What is the application of Generative Adversarial Networks having been successfully  trained? </p>

<p>splitted into two part as G and D, the G is for creation, and the D for a decider? Then there is a assumption that is the input of GAN  have to be continued?</p>
"
1263,"<p>I recently read that Google has developed a new AI that anyone can upload data to and it will instantly generate models i.e. an image recognition model based off that data. </p>

<p>Can someone explain to me in a detailed and intuitive manner how this AI works?</p>
"
1264,"<p>I would like to do some practical implementation of Artificial Intelligence Planning (of course something a bit simple and easy). Is there any website where I can pick an algorithm, say A* or hill climbing or calculate heuristic values, code it and visualize how it works? </p>

<p>Example: for machine learning, the above i.e. pick a learning method, say Linear Regression, code it and visualize how it works in <a href=""https://www.kaggle.com/"" rel=""nofollow noreferrer"">https://www.kaggle.com/</a></p>

<p><em>Note:</em> if you find the tags to be inappropriate for this question, then I am sorry. I couldn't find (or I don't know) the appropriate tag</p>

<p>Any input will be appreciated. Thank you :) </p>
"
1265,"<p>Let's say we have a cluster of 20-2000 heterogenous compute nodes.
Consider for example the parallel solution of the helmholtz equation:
Now we want to distribute the solution process and, to make things easier, we split the problem in a fine-grained way (partial solution of the system matrix).
We could train an Ai with the time taken to solve the subproblem depending on multiple factors (for example, size of the mesh, needed precision, etc) and let the Ai choose the optimal distribution and division of the problem based on the available data.</p>

<p>I'm new to the area of Artificial Intelligence.
Are there any open source frameworks which could accomplish this task?
How would you estimate the required amount of compute power to train the network?</p>
"
1266,"<p>number of layer of DNN and computational complexity of it are correlated after optimization, but how to estimate it before designing DNN? </p>
"
1267,"<p>Gold and Vapnik proved that what are learnable are only data of  finite demension or set of language without infinite language, that is only algorithms with computational complexity weaker than NP or much more weaker. But we know that DNN can implement any Turing Machine. Is deeplearning contradict to result  of Gold and Vapnik? Or are there difference between notion of learnability?</p>
"
1268,"<p>I'm trying to understand how to effectively plan and write a Neural Network but running into problems with understanding how they should be written. I'm working with classification with writing a backpropagation Neural Network however any other types/examples would be greatly appreciated. </p>

<p>One of my problems is understanding the possibilities of them, for example if you have a single neuron I assume you can have 2^1 possibilities, like in my example I can't imagine how I could predict something more than above or below a straight line. Similarly I assume with 3 neurons therefore 2^3 possibilities you could predict if something is one of the 4 quadrants of a graph and then predict if it's in the upper part or lower part of the quadrant so 8 classifications. Is this true or is a network capable of predicting more/less than that?</p>

<p>On to my specific problem.</p>

<p>I've written a single Neuron/Perceptron that can predict whether something is above or below a straight line graph given the correct training data and using a sign activation function.</p>

<p>Following from this I'm trying to write a Neural Network that can predict whether something is in the 1st, 2nd, 3rd or 4th quadrant of a graph. </p>

<p>One idea I have had is to have 2 input perceptrons, the first taking the x value, the 2nd taking the y value, these then try and predict individually whether the answer is on the right or left of the centre, and then above or below respectively. These then pass their outputs to the 3rd and final output neuron. The 3rd Neuron uses the inputs to try and predict which quadrant the coordinates are in. The first two inputs use the sign function.</p>

<p>The problems I'm having with this is to do with the activation function of the final neuron, one idea was to have a function that somehow scaled the output into a integer between 0 and 1, so 0 to 0.25 would be quadrant 1, and so on up to 1. Another idea would be to convert it to a value using sin and representing it as a sine wave as this could potentially represent all 4 quadrants.</p>

<p>Another idea would be to have a single neuron taking the input of the x and y value and predicting whether something was above or below a graph (like my perceptron example), then having two output neurons, which the 1st output neuron would be fired if it was above the line and then passed in the original x coordinate to that output neuron. The 2nd output neuron would be fired if it was below then pass in the original x value as well to determine if it was left or right.</p>

<p>Are these ideas adequate examples of writing a network? </p>

<p>Also my final question would be if you wanted your network to have 8 possible outputs would you need 8 output neurons or could you represent the 8 values with a single more advanced activation function?</p>

<p>Thanks!</p>
"
1269,"<p>I had this idea of training for example a CNN on images, and having output branches at several of its intermediate layers. The early layers' output branch might then predict high-level class of detected objects (supposedly able to do this because less info is needed for a high-level classification than a very specialised one), and the later layers giving more detailed labels of the sub-class of the earlier high level class.</p>

<p>I have been searching for research on this type of setup but couldn't really find anything. Is there a name for this idea, or is this an open question/idea?</p>
"
1270,"<p>If I train a speech recognition model using data collected from N different microphones, but deploy it on an unseen (test) microphone - does it impact the accuracy of the model? </p>

<p>While I understand that theoretically an accuracy loss is likely, does anyone have any practical experience with this problem? </p>
"
1271,"<p>The match got a lot of press, and I doubt anyone is surprised that Alpha Zero crushed Stockfish.</p>

<p>See: <a href=""https://www.chess.com/news/view/google-s-alphazero-destroys-stockfish-in-100-game-match"" rel=""nofollow noreferrer"">AlphaZero Destroys Stockfish in 100 Game Match</a> </p>

<p>To me, what's really salient is that <em>""much like humans, AlphaZero searches fewer positions that its predecessors. The paper claims that it looks at ""only"" 80,000 positions per second, compared to Stockfish's 70 million per second.""</em>  </p>

<p>For those who remember Matthew Lai's <a href=""https://arxiv.org/pdf/1509.01549.pdf"" rel=""nofollow noreferrer"">GiraffeChess</a>:</p>

<blockquote>
  <p>However, it is interesting to note that the way computers play chess is very different from how
  humans play. While both humans and computers search ahead to predict how the game will go on,
  humans are much more selective in which branches of the game tree to explore. Computers, on the
  other hand, rely on brute force to explore as many continuations as possible, even ones that will be
  immediately thrown out by any skilled human. In a sense, the way humans play chess is much more
  computationally efficient - using Garry Kasparov vs Deep Blue as an example, Kasparov could not
  have been searching more than 3-5 positions per second, while Deep Blue, a supercomputer with
  480 custom ”chess processors”, searched about 200 million positions per second <a href=""https://arxiv.org/pdf/1509.01549.pdf"" rel=""nofollow noreferrer"">1</a> to play at
  approximately equal strength (Deep Blue won the 6-game match with 2 wins, 3 draws, and 1 loss).</p>
  
  <p>How can a human searching 3-5 positions per second be as strong as a computer searching 200
  million positions per second? And is it possible to build even stronger chess computers than what
  we have today, by making them more computationally efficient? Those are the questions this
  project investigates.</p>
</blockquote>

<p><em>[Lai was tapped by DeepMind as a researcher last year]</em></p>

<p>But what I'm interested in at the moment is the decision speed in these matches:</p>

<p><strong>- What was the average time to make a move in the AlphaZero vs. Stockfish match?</strong></p>
"
1272,"<p>I have tried several environment libraries like OpenAI gym/gridworld but now I am trying to create a toy environment for experimentation. The environment I've created is as follows: </p>

<ol>
<li><p>State: grid with n rows by m columns, represented by a boolean matrix. Each grid cell can be empty or filled and the grid starts empty.</p></li>
<li><p>Action: one of the m columns to be filled, which must have at least the top row empty. </p></li>
<li><p>Next state: Once a column is chosen, the lowest unfilled cell in that column is filled. This works from bottom up like a very simple version of Tetris. </p></li>
<li><p>Reward: after every action, a reward equal to the number of empty columns is awarded. </p></li>
</ol>

<p>Therefore in a sample world of 5 rows by 3 column, starting off with an empty grid, the maximum attainable reward would be by filling column wise first. This policy will give a maximum total reward of 2*5 + 1*5 = 15. (2 free columns by 5 row action, once first column is filled then 1 free column by 5 row action.) </p>

<p>This very simple environment is trained using DQN with a single ff layer. The agent only took a few episodes to converge and is able to produce the maximum attainable reward. </p>

<p>In a next toy environment, I've made it a little more complex. I modified the very first action to be random choice of any column. I have retrained the RL model with the new environment modification. However, after convergence, the agent does not attain max score of 15 for all possible starting columns. I.e. If column 1 was randomly chosen first, max score might be 15, however column 2 or 3 was randomly chosen first, max score might only reach 11 or 9. In theory, the optimum policy would be for the agent to fill column that was randomly chosen first - i.e. repeat the first randomly chosen action.</p>

<p>I have tried several ways to tweak my input parameters (e.g. episilon_decay_rate, learning_rate, batch_size, number of hidden nodes) to see if the agent could act optimally for all possible starting columns. I also tried DDQN and Sarsa. The only way I could make the agent perform optimally is by reducing gamma (discount factor) to 0.5 or below. Are there any explanations to why the agent only works for small discount factors in this example? Also, are there alternative ways to obtain the optimum policy?</p>
"
1273,"<p>Which freely available code/ software is the fastest way to train and test data on?</p>

<p>I'm looking for a GUI software/code that let me test data-sets without investing too much time in coding. I found some projects online but I wanted to hear your suggestions in order to avoid investing time in testing many different ones.</p>

<p>Thanks</p>
"
1274,"<p>How to prove and what is the mathematical condition in terms of probability that a MEASURABLE  FUNCTION is a bayesian decision function ? Can you give an example with standard or weighted binary calssification ?</p>
"
1275,"<p>It was <a href=""https://ai.stackexchange.com/questions/5220/what-was-the-average-decision-speed-pf-alpha-zero-in-the-recent-stockfish-match/5222?noredirect=1#comment7698_5222"">recently brought to my attention</a> that Chess experts took the outcome of this now famous match as something of an upset.  </p>

<p>See: <a href=""https://fivethirtyeight.com/features/chesss-new-best-player-is-a-fearless-swashbuckling-algorithm/"" rel=""noreferrer"">Chess’s New Best Player Is A Fearless, Swashbuckling Algorithm</a></p>

<p>As as a non-expert on Chess and Chess AI, my assumption was that, based on the performance of AlphaGo, and the validation of that type of method in relation to combinatorial games, was that the older AI would have no chance.  </p>

<ul>
<li>Why was AlphaZero's victory surprising?</li>
</ul>
"
1276,"<p>Which problems are considered to be the toughest problems in Artificial Intelligence/Machine Learning?</p>
"
1277,"<p>So this is an introductory question. Whenever I read any book about Neural Nets or Machine Learning, their introductory chapter says that we haven't been able to replicate the brain's power due to its massive parallelism. </p>

<p>Now, in the modern times transistors have been reduced to the size of nano-meters, much smaller than the nerve cell. Also we can easily build very large supercomputers. </p>

<ul>
<li>Computers have much larger memories than brain.</li>
<li>Can communicate faster than brain (clock pulse in nanoseconds).</li>
<li>Can be of arbitrarily large size.</li>
</ul>

<p>So my question is why cannot we replicate the brain's parallelism if not its information processing ability (since brain is still not well understood) even with such advanced technology? What exactly is the obstacle we are facing?</p>
"
1278,"<p>For instance, the title of <a href=""https://arxiv.org/abs/1611.01224"" rel=""nofollow noreferrer"">this paper</a> reads: ""Sample Efficient Actor-Critic with Experience Replay"".</p>

<p>What is <em>sample efficiency</em>, and how can <em>importance sampling</em> be used to achieve it?</p>
"
1279,"<p>A lot of people seem to be under the impression that combining <a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow noreferrer"">GOFAI</a> and contemporary AI will make models more general. I'm particularly interested in reasoning through analogy or case-based reasoning.</p>
"
1280,"<p>So, I am trying to create an AI to handle the construction layout of a real time strategy game like Age of Empires II. The process has too many steps to be handled effectively by brute force, but also has enough structural requirements that it also cannot just be done randomly.</p>

<p>Assuming a limited area to work with, which can be represented by tiles, the AI must be able to place several structures within the area. A layout once fully created can be given a score based on the pathable distance between certain structures as well as a few other factors. If paths between certain structures become completely blocked, the AI has failed.</p>

<p>The starting area that is defined by tiles is also random in nature, containing a few predefined elements that the AI cannot control, which include randomly placed terrain and resource nodes.</p>

<p>I know this problem case has been quite vague, but the actual question relates to the type of AI that would best fit solving this issue. I currently have an algorithm that runs through what it believes are the best few results in a series of steps trying to create the most optimal layout, but it fails with many starting layouts, which require manual adjustments to the AI before it can process them properly. Even then, it is just an approximation at best, since it starts out assuming that the steps I gave it to check contain the most optimal layout.</p>

<p>Here is an example of what a finished layout might look like. It doesn't state which colors are which objects, but it might help in determining what type of AI I should be looking at using.</p>

<p><a href=""https://i.stack.imgur.com/oyJ62.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oyJ62.png"" alt=""screeps example""></a></p>
"
1281,"<p>So I read somewhere there are 2 different views of Artificial Intelligence.</p>

<ul>
<li>One is the normal conventional approach where we use ML and AI logical inference programs to mimic a human brain.</li>
<li>Another view is that we cannot create an intelligent agent just by the above procedure. We humans or animals are only intelligent due to our social interactions.</li>
</ul>

<p>If we think about it, the second view has a lot of credibility since a brain in a jar is not at all intelligent and will not evolve things that make an animal possess intelligent traits like language, logical reasoning, etc. Whereas an ant or bee colonies are apparently intelligent due to their collective information gathering even if their brain are basically hard-wired logical sensors which just perform an action on a given stimulus.</p>

<p>So my questions are: </p>

<ol>
<li>Are all this artificially intelligent agents/programs being created or will be created in near future, just a set of rules and probabilities?</li>
<li>What is the current progress and views on collective/swarm intelligence?</li>
<li>And what is the level of intelligence or brain capabilities required by an agent which if works collectively (like bees/ants broadly super-organisms) can exceed normal human intelligence?</li>
</ol>

<p>EDIT: If we look at it nodes in a neural net are also kind of ants/bees. They just perform a hardwired calculation and output the result. Can we think of an ANN as swarm intelligence?</p>

<p>A collective answer explaining the reasoning for the views is highly appreciated. Thanks in advance!!</p>
"
1282,"<p>Once the artificially intelligent machines are able to identify objects, we might want to teach them how to value different things differently based on their utility, demand, life, etc. How can we accomplish this and how did we start to value things?</p>
"
1283,"<p>I'm trying to implement some Image super-resolution models on medical images. After reading a set of papers, I found that none of the existing models use any activation layer for the last layer. 
What's the rationale behind that? </p>
"
1284,"<p>I'm wondering if these 2 specific programs already exist and if not how hard would it be to write them:</p>

<ol>
<li><p>A program that would figure out (by only ""reading"" large amounts of texts in human language 1 and 2) which words in second language have the same meaning as a word in first language.  You would give for input texts in both languages and for output you would get for every word in first language a list of words in second language that are most similar to it with a probability that they mean the same thing. </p></li>
<li><p>A program that would figure out which words have the most similar meaning by analyzing large amounts of texts in one human language.</p></li>
</ol>

<p>I'm planning on writing these two programs and it would be nice if I could get existing programs that do this so that I could compare results of my program to those of existing programs.</p>
"
1285,"<p><strong>Introduction</strong></p>

<p>Tracking control is a technique which is similar to PID control and has the aim to implement a robust feedback loop for generating action-signals. In contrast to a line following robot, tracking control is oriented in a time-action space. On the x-axis the timecode is presented, for example 0 seconds, 0.5 seconds and 1 seconds, while on the y-axis the value of the signal is plotted. The value can be the position of a robot, the angle of an inverted pendulum or the x-position of an object. Even in simple problems like the inverted pendulum, the number of parameters on the y-axis is greater than 1. The spline of all parameters of the time are equal to a movement pattern.</p>

<p>The problem is how to summarize the different values to a single signal. This is needed for determine the similarity in a “Learning from demonstration” experiment. The human-operator is doing an action, and the aim of the robot is to reproduce the movement pattern. I've searched a bit the topic in Google Scholar and found a paper: <a href=""https://miguelgfierro.com/docs/gonzalez-fierro2013humanoid.pdf"" rel=""nofollow noreferrer"">A humanoid robot standing up through learning from demonstration using a multimodal reward function</a> but I'm a bit unsure, because there is so much math and it is also dedicated to ZMP biped walking. What I'm searching is more a general idea of how to compress different splines into one reward function.</p>

<p><strong>Description of the bug</strong></p>

<p>Tracking a single spline which is plotted in a diagram is easy. The difference between the current value and the desired value is measured and the feedback-controller reduces the difference. If the number of splines is 2 this concept fails. For example, spline #1 represents the angle and spline #2 the velocity over time. The current state has 2 variables and the desired state has 2 variables. How can I program a steering-controller for a multi-spline?</p>
"
1286,"<p>Is most development or theory geared towards the idea that consciousness is an emergent phenomenon? That once we put enough complexity into our system, it will become self-aware? Or is this even a problem that people are attempting to tackle right now? </p>
"
1287,"<p>I'm developing a multi-armed bandit which learns the best information to display to persuade someone to donate to charity.</p>

<p>Suppose I have treatments A, B, C, D (which are each one paragraph of text). The bandit selects one treatment to show to a person. The person is given $1 and has to decide how much (if any) to donate, in increments of one cent. The donation decision is recorded and fed to the multi-armed bandit, who will then re-optimize before another person is shown a treatment selected by the bandit.</p>

<p>How should I program the bandit if my objective is to maximize total donations? For example, can I use Thompson sampling, and if a participant donates $0.80, I count that as 80 successes and 20 failures?</p>
"
1288,"<p>I'm reading <em>""<a href=""http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf"" rel=""nofollow noreferrer"">Recurrent neural network based language model</a>""</em> of Mikolov et al. (2010). Although the article is straight forward, I'm not sure how word vector w(t) is obtained (printscreen from PDF article):</p>

<p><a href=""https://i.stack.imgur.com/6i54I.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6i54I.png"" alt=""enter image description here""></a></p>

<p>The reason I wonder is that in the classic ""<em><a href=""http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"" rel=""nofollow noreferrer"">A Neural Probabilistic Language Model</a></em>"" Bengio et al. (2003) - they used separate embedding vector for representing each word and it was somehow ""semi-layer"", meaning - it haven't contains non-linearity but they do updated embedded-word vectors during the backropagation.</p>

<p>In Mikolov approach though, I assume they used simple one-hot vector where each feature represent presence of each word. If we represent that's way single word input (like was in the Mikolov's paper) - that vector become all-zeros except single one.</p>

<p>Is that correct?</p>
"
1289,"<p>The way I understand it is that hidden units are added to capture higher order interactions offering more capacity to the model. Now, the BM family are energy based undirected networks, meaning there's no forward computations; instead, for each input configuration <code>x</code>, a scalar energy is calculated to asses this configuration. (the higher the energy, the less likely for <code>x</code> to be sampled from the target distribution.)</p>

<p>The probability distribution is defined through the energy function by summing over all possible states of the hidden part <code>h</code>. So, my question is: how do we calculate the values of these hidden units? Or do we not explicitly compute these valus and instead approximate the marginal ""free energy"" which is the negative log of the sum over all possible states of <code>h</code>?</p>
"
1290,"<p>In order for the generalized bell membership function to retain its defined shape and domain, two restrictions must be placed on the b parameter: 1) b must be positive and 2) b must be an integer. Using backpropagation to tune the membership parameters (a,b and c in the bell), it appears to be possible that the correction to b will break one or both of these restrictions on b. Can someone please explain to me how we can use backpropagation to tune the b parameter (as well as a and c) without violating 1) and 2)?</p>
"
1291,"<p>I have purchasing history data for grocery shopping. I am trying to get abnormally frequently purchased items under certain conditions. For instance, I am trying to find frequently purchased items, when customers shop online and are willing to pay an extra shipping fee.</p>

<p>In order to find items that are particularly (or abnormally) frequently purchased under that situation (through online stores by paying shipping fee), how and what Machine Learning Algorithm  should I apply and identify those items?</p>

<p>I found <code>arules</code> R package which is using the association rules with purchasing history and tried to apply it. But it seems the package might be based on different principle from my idea.</p>

<p>Anyone has an idea about my problem? If there is an R package related to the problem, it would be perfect.</p>
"
1292,"<p>In Box2D is a platform given with a ball on top. The platform can be moved to left and right, and the object is moving according to standard gravity of 9.8f which is set in the Box2D setting. To make the task a bit harder, there are two platforms at the same time. Left in the image is the demonstration and right is the repetition. The repetition platform gets the same input signals but starts with a slightly different position of the object. The position of the ball can be retrieved in realtime from the program.</p>

<p>The human operator is sending a “left” command, the platform is moving and the ball is rolling. How the controller looks like for the repetition platform if both balls should be in the same position?</p>

<p><a href=""https://i.stack.imgur.com/Mpqh7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mpqh7.png"" alt=""enter image description here""></a></p>

<p><strong>My efforts to solve the problem</strong></p>

<ol>
<li>I've created the sourcecode in C++ which is 420 lines long</li>
<li>I've programmed a manual controller for testing if the physics engine works</li>
<li>I read some papers about Dynamic movement primitives, Learning from demonstration and Tracking control</li>
</ol>

<p>But it seems, that something is missing for solving the problem. Any advice is welcome.</p>

<p><strong>Sourcecode</strong></p>

<p>The controller is executed in the mainloop. It works currently as a normal pid-controller, which measures the difference between the x-positions of both balls and executes a left or right counteraction. The idea is, that the human-operator controls the demonstration and this modifies the pid-control loop in the agent. But in most cases, it works not very well. The controller is not very intelligent.</p>

<pre><code>mainloop:
  myagent.automode();

class Agent {
public:
  void automode() {
    int posA=myphysics.mybox[6].positioncenter.x-200;
    int posB=myphysics.mybox[9].positioncenter.x-400;
    int diff = posB-posA;
    std::cout &lt;&lt; mysettings.framestep &lt;&lt; "" ""&lt;&lt;diff &lt;&lt; ""\n"";
    if (diff&gt;0) {
      std::thread t1;
      t1=std::thread(&amp;Agent::autoleftrun, this);
      t1.detach();
    }
    if (diff&lt;-0) {
      std::thread t1;
      t1=std::thread(&amp;Agent::autorightrun, this);
      t1.detach();
    }
};
</code></pre>
"
1293,"<p><a href=""https://en.wikipedia.org/wiki/Greedy_algorithm"" rel=""noreferrer"">Greedy algorithms</a> are well known, and although useful in a local context for certain problems, and even potentially find general, global optimal solutions, they nonetheless trade optimality for shorter-term payoffs.  </p>

<p>This seems to me a good analogue for human greed, although there is also the <a href=""https://en.wikipedia.org/wiki/Grey_goo"" rel=""noreferrer"">grey goo</a> type of greed that is senseless acquisition of material (think plutocrats who talk about wealth as merely a way of ""keeping score"".)</p>

<p><a href=""https://en.wikipedia.org/wiki/Technical_debt"" rel=""noreferrer"">Technical debt</a> is an extension of development practices that fall under the algorithmic definition of greed (short-term payoff leads to trouble down the road.)  This may be further extended to any non-optimized code in terms of energy waste (flipping of unnecessary bits) which will only increase as everything becomes more computerized.</p>

<p>So my question is:</p>

<ul>
<li>What are other vices that can arise in algorithms?</li>
</ul>
"
1294,"<p>I'm interested in working on challenging AI problems, and after reading this article (<a href=""https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/"" rel=""nofollow noreferrer"">https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/</a>) by DeepMind and Blizzard, I think that developing a robust AI capable of learning to play Starcraft 2 with superhuman level of performance (without prior knowledge or human hard-coded heuristics) would imply a huge breakthrough in AI research.</p>

<p>Sure I know this is an extremely challenging problem, and by no means I pretend to be the one solving it, but I think it's a challenge worth taking on nonetheless because the complexity of the decision making required is much closer to the real world and so this forces you to come up with much more robust, generalizable AI algorithms that could potentially be applied to other domains.</p>

<p>For instance, an AI that plays Starcraft 2 would have to be able to watch the screen, identify objects, positions, identify units moving and their trajectories, update its current knowledge of the world, make predictions, make decisions, have short term and long term goals, listen to sounds (because the game includes sounds), understand natural language (to read and understand text descriptions appearing in the screen as well), it should probably be endowed also with some sort of attention mechanism to be able to pay attention to certain regions of interest of the screen, etc. So it becomes obvious that at least one would need to know about Computer Vision, Object Recognition, Knowledge Bases, Short Term / Long Term Planning, Audio Recognition, Natural Language Processing, Visual Attention Models, etc. And obviously it would not be enough to just study each area independently, it would also be necessary to come up with ways to integrate everything into a single system.</p>

<p>So, does anybody know good resources with content relevant to this problem? I would appreciate any suggestions of papers, books, blogs, whatever useful resource out there (ideally state-of-the-art) which would be helpful for somebody interested in this problem.</p>

<p>Thanks in advance.</p>
"
1295,"<p>I often develop bots and I need to understand what some people are saying.</p>

<p>Examples:<br>
- I want an apple<br>
- I want an a p p l e</p>

<p>How do I find the object (<em>apple</em>)? I honestly don't know where to start looking. Is there an API that I can send the text to which returns the object? Or perhaps I should manually code something that analyses the grammar?</p>
"
1296,"<p>I have been trying to use CNN for a regression problem. I followed the standard recommendation of disabling dropout and overfitting a small training set prior to trying for generalization. With a 10 layer deep architecture, I could overfit a training set of about 3000 examples. However, on adding 50% dropout after the fully-connected layer just before the output layer, I find that my model can no longer overfit the training set. Validation loss also stopped decreasing after a few epochs. This is a substantially small training set, so overfitting should not have been a problem, even with dropout. So, does this indicate that my network is not complex enough to generalize in the presence of dropout? Adding additional convolutional layers didn't help either. What are the things to try in this situation? I will be thankful if someone can give me a clue or suggestion. </p>

<p>PS: For reference, I am using the learned weights of the first 16 layers of Alexnet and have added 3 convolutional layers with ReLU non-linearity followed by a max pooling layer and 2 fully connected layers. I update weights of all layers during training using SGD with momentum. </p>
"
1297,"<p>This is a kind of biological and philosophical question. So, the recent concern in AI is that an AI agent may go rogue with prominent people voicing their concerns.</p>

<p>Now say, we have created an AI (you are free to use your own definition of what makes an AI intelligent) which has gone rogue with powers given in this <a href=""https://ai.stackexchange.com/questions/2274/what-would-be-the-best-way-to-disable-a-rogue-ai"">question</a>.</p>

<p>Now, the broad view of today's biology is that everything we do is to further our genes down the future (leaving aside small technical details). It is even widely accepted that we are just machines whose controller are the genes. Everything we do is controlled/hardwired by the genes with some avenue of learning from experiences. Also genes only further their own interest. Scientist <a href=""https://en.wikipedia.org/wiki/George_R._Price"" rel=""nofollow noreferrer"">George Price</a> even wrote a mathematical equation proving all our acts are selfish and only furthering the interest of our genes (<a href=""https://motherboard.vice.com/en_us/article/bmjanm/george-price-altruism"" rel=""nofollow noreferrer"">article</a>). Also <a href=""https://en.wikipedia.org/wiki/Richard_Dawkins"" rel=""nofollow noreferrer"">Richard Dawkins</a> is a pioneer of this idea (this is only to show I haven't pulled the idea out from air).</p>

<p>Now, my question is that what will possibly be the motivation of an AI agent to go rogue? It doesn't have genes whose interest it needs to further. We all do something for an end result. What is the end result a rogue AI might try to achieve/attain and why?</p>
"
1298,"<p>I was reading Gary Marcus's a Critical Appraisal of Deep Learning. And one of his criticisms is that neural networks don't incorporate prior knowledge in tackling a problem. My question is, has there been any attempts at encoding prior knowledge in deep neural networks?</p>
"
1299,"<p>I have completed week 1 of Andrew Ng's course. I understand that the cost function for linear regression is defined as <span class=""math-container"">$J (\theta_0, \theta_1) = 1/2m*\sum (h(x)-y)^2$</span> and the <span class=""math-container"">$h$</span> is defined as <span class=""math-container"">$h(x) = \theta_0 + \theta_1(x)$</span>. But I don't understand what <span class=""math-container"">$\theta_0$</span> and <span class=""math-container"">$\theta_1$</span> represent in the equation. Is someone able to explain this?</p>
"
1300,"<p>I'm looking for an industry standard framework for joining multiple neural networks in a modular way.</p>

<p>Assume we have two or more neural networks trained to perform certain tasks. By feeding the outputs of some networks into the inputs of others we might obtain higher functionality, but to test multiple hypotheses, we would need a way to rapid prototype those configurations.</p>

<p><a href=""https://i.stack.imgur.com/JF9wW.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JF9wW.gif"" alt=""Modular Neural Networks""></a></p>

<p><a href=""https://www.teco.edu/~albrecht/neuro/html/node32.html"" rel=""nofollow noreferrer"">The source of this image is here &mdash; a proposition of this modular ANN architecture in a thesis</a></p>

<p>I'd be interested in knowing:</p>

<ul>
<li><p>Do any frameworks or libraries like this even exist?</p></li>
<li><p>If so, do they support distributed models, so that models don't have to be hosted in the same process or on the same machine?</p></li>
<li><p>Do they allow hosting models to be generated from different deep learning frameworks?</p></li>
</ul>
"
1301,"<p>I'm trying to implement Q-learning (state-based representation and no neural / deep stuff) but I'm having a hard time getting it to learn anything.</p>

<p>I believe my issue is with the exploration function and/or learning rate. Thing is, I see different explanations in the sources I am following so I'm not sure what's the right way anymore.</p>

<p>What I understand so far is that Q-learning is TD with q-val iteration.</p>

<p>So a time-limited q-val iteration step is:</p>

<pre><code>Q[k+1](s,a) = ∑(s'): t(s,a,s') * [r(s,a,s') + γ * max(a'):Q[k](s',a')]
</code></pre>

<p>Where:</p>

<pre><code>Q = q-table: state,action -&gt; real
t = MDP transition model
r = MDP reward func
γ = discount factor.
</code></pre>

<p>But since this is a model-free, sample-based setting, the above update step becomes:</p>

<pre><code>Q(s,a) = Q(s,a) + α * (sample - Q(s,a))
</code></pre>

<p>Where:</p>

<pre><code>sample = r + γ * max(a'):Q(s',a')
r  = reward, also coming from percept after taking action a in step s.
s' = next state coming from percept after taking action a in step s. 
</code></pre>

<p>Now for example, assume the following MDP:</p>

<pre><code>    0    1    2    3    4  
0 [10t][ s ][ ? ][ ? ][ 1t]

Discount: 0.1 | 
Stochasticity: 0 | 
t = terminal (only EXIT action is possible)
s = start
</code></pre>

<p>With all of the above, my algo (in pseudo code) is:</p>

<pre><code>input: mdp, episodes, percept
Q: s,a -&gt; real is initialized to 0 for all a,s
α = .3

for all episodes:
    s = mdp.start

    while s not none:
        a  = argmax(a): Q(s,a) 
        s', r = percept(s,a)
        sample = r + γ * max(a'):Q(s',a')
        Q(s,a) = Q(s,a) + α * [sample - Q(s,a)]
        s = s'
</code></pre>

<p>As stated above, the algorithm will not learn. Because it will get greedy fast. </p>

<p>It will start at 0,1 and choose the best action so far. All q-vals are 0 so it will choose based on arbitrary order on how the qvals are stored in Q. Asume 'W' (go west) is chosen. It will go to 0,0 with a reward of 0 and a q-val update of 0 (since we don't yet know that 0,0, EXIT yields 10)</p>

<p>In the next step it will take the only possible action EXIT from 0,0 and get 10.</p>

<p>At this point the q-table will be:</p>

<pre><code>0,1,W:      0
0,0,Exit:   3 (reward of 10 averaged by learning rate of .3)
</code></pre>

<p>And the episode is over because 0,0 was terminal.
On the next episode, it will start at 0,1 again and take W again because of the arbitrary order. But now 0,1,W will be updated to 0.09. Then 0,0,Exit will be taken again (and 0,0,Exit updated to 5.1). Then the second episode will be over.</p>

<p>At this point the q-table is:</p>

<pre><code>0,1,W:      0.09
0,0,Exit:   5.1
</code></pre>

<p>And the sequence 0,1,W->0,0,Exit will be taken ad infinitum.</p>

<p>So this takes me to learning rates and the exploration functions.</p>

<p>The book 'Artificial Intelligence: A Modern Approach' (3ed, by Russell) first mentions (pages 839-842) the exploration function as something to put in the val update (because it is discussing a model-based, value iteration approach instead).</p>

<p>So extrapolating from the val update discussion in the book, I'd assume the q-val update becomes:</p>

<pre><code>Q(s,a) = ∑(s'): t(s,a,s') * [r(s,a,s') + γ * max(a'):E(s',a')]
</code></pre>

<p>Where E would be an exploration function which according to the book could be something like:</p>

<pre><code>E(s,a) = &lt;bigValue&gt; if visitCount(s,a) &lt; &lt;minVisits&gt; else Q(s,a)
</code></pre>

<p>The idea being to artificially pump up the vals of actions which have not been tried yet and so now they'll be tried out at least <code>minVisits</code> times.</p>

<p>But then, in page 844 the book shows pseudo code for Q-learning and instead does not use this E in the q-val update but rather in the argmax of the action selection. I guess makes sense? Since exploration amounts to choosing an action...</p>

<p>The other source I have is the UC Berkeley CS188 lecture videos/notes.
In those (Reinforcement Learning 2: 2016) they show the exploration function in the q-val update step. This is consistent with what I extrapolated from the book's discussion on value iteration methods but not with what the book shows for Q-Learning (remember the book uses the exploration function in the argmax instead).</p>

<p>I tried placing exploration functions in the update step, the action selection step and in both at the same time.. and still the thing eventually gets greedy and stuck.</p>

<p>So not sure where and how this should be implemented.</p>

<p>The other issue is the learning rate. The explanation usually goes ""you need to decrease it over time."" Ok.. but is there some heuristic? Right now, based off the book I am doing:</p>

<p><code>learn(s,a) = 0.3 / visitCount(s,a)</code>. But no idea if it is too much or too little or just right. </p>

<p>Finally, assuming I had the exploration and learn right, how would I know how many episodes to train for? </p>

<p>I'm thinking I'd have to keep 2 versions of the Q-table and check at which point the q-vals do not change much from previous iterations (similar to value iteration for solving known MDPs).</p>
"
1302,"<p>I'm working on a project related to machine Q&amp;A, using the SQuAD dataset. I've implemented a neural-net solution for finding answers in the provided context paragraph, but the system (obviously) struggles when given questions that are unanswerable from the context. It usually produces answers that are nonsensical and of the wrong entity type.</p>

<p>Is there any existing research in telling whether or not a question is answerable using the info in a context paragraph? Or whether a generated answer is valid? I considered textual entailment but it doesn't seem to be exactly what I'm looking for (though maybe I'm wrong about that?)</p>
"
1303,"<p>I am fairly a newbie to Neural Networks.</p>

<p>I wanted to ask if it is possible to train a NN to identify only one type object? For instance, a table from a large set of images, where the NN should be able to identify if new images are tables.</p>

<p>If yes can you please guide me in the direction to get started?</p>

<blockquote>
  <p><strong>EDIT:</strong></p>
  
  <p>I wanted to ask if it is possible to train a NN to identify only one type object?...</p>
  
  <p>so my understanding as of now is that if i train an NN on one class and one class alone with a fairly large amount of data then i can get it trained in a small amount of time rather than training it on a huge amount of data and consuming a larger amount of data...</p>
  
  <p>Objective is to train a large no of such smaller NNs to create an ensemble of NNs.</p>
</blockquote>
"
1304,"<p>Any small application based on real world application of AI which can be done easily at home for a beginner who is trying to make use of his basic programming skills into AI at the beginning level. </p>
"
1305,"<p>So i really don't get this question because i always thought the agent program is the same as agent's function, but i read somewhere that this is statement might be true</p>

<p>so is this statement actually true? if so then why? if i have an agent function why i cannot implement it in an program?!</p>

<p>This is the definition of function:
The function is implemented as the agent program.</p>
"
1306,"<p>I was reading a paper recently about improving the learning of an ANN using weight normalization: </p>

<p><a href=""https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf"" rel=""nofollow noreferrer"">Weight Normalization: A Simple Reparameterization
to Accelerate Training of Deep Neural Networks</a></p>

<p>I wanted to check my understanding/interpretation of it with the experts here.</p>

<p>Normally the output of a neuron is equal to the sum of every incoming neuron multiplied by their respective weights and then pushed through an activation function:</p>

<p>output = activation ( SUM (x multiplied by w))</p>

<p>In the example in this paper it seems that instead of using each individual weight they are actually calculating a normalized weight to substitute by taking the square root of all summed weights (The Euclidean Norm?) - we'll call that Z - and then plugging it into the calculation as:</p>

<p>normalized_weight = ( scalar / z ) multiplied by w.</p>

<p>They never saw what they used for scalar btw...</p>

<p>Can anyone confirm if I am correct in my understanding and if not could they correct me.  The maths goes a little over my head so any pseudocode is welcome.</p>
"
1307,"<p>I have been reading a lot lately about some very promising work coming out of Uber's AI Labs using mutation algorithms enhanced with NOVELTY SEARCH to evolve deep neural nets.</p>

<p><a href=""https://www.arxiv-vanity.com/papers/1712.06563/"" rel=""nofollow noreferrer"">https://www.arxiv-vanity.com/papers/1712.06563/</a></p>

<p>I'd like to ask the community if anyone is able to understand how this is done or has any ideas on how to do this?  </p>

<p>I am unclear on if the novelty should reward novel structures within the ANN or novel behavior.  I am guessing the latter but would love to hear from others on this topic.  </p>

<p><em>I checked the other items in ai.stackexchange for novelty searches but they didn't answer this question so I do not believe this is already answered.</em></p>
"
1308,"<p>What is the difference between AI architecture and AI models. Are both of them same? if not please distinguish both of them and give example of each. And also suggest books/ papers that delve on AI architecture and models. </p>
"
1309,"<p>i'm trying to identify numbers and letters in license plate. License plate images are taken at different lighting condtion and converted to gray image. My concern with type of data for training is:</p>

<p>Gray Image:</p>

<ul>
<li>Since they are taken at different lighthing condition, gray image have different pixel intensity for same number. Which means, i have to get many training data for different lighting condition to train.</li>
</ul>

<p>Edge Image:</p>

<ul>
<li>They lack enough pixel information since only edge is white while others(background) are black. So i think they will be very weak for translational difference like shearing or shifting.</li>
</ul>

<p>I want to get some information about which type of image is better for training number in different lighting condition. I wish to use edge image if they don't  differ much since i can prepare edge image right now.</p>
"
1310,"<p>The more I think about machine learning the more I realize the importance of finding similarities by using analogies as a way of learning.</p>

<p>If I want to categorize words into hierarchical tree this method would work I think, if not tell me why?</p>

<p>Find two sentences that contain same words but their order is different and some words can also be different.</p>

<p>Find another two such sentences.</p>

<p>Evaluate strength of analogy between these 4 sentences.</p>

<p>The stronger it is the more probability that words mean similar thing, that they belong to same category. </p>

<p>When you learn categories you can do analogies on abstract sentences that contain these found categories and in this manner you can build hierarchy of categories.</p>

<p>For example:</p>

<p>Deer eats grass.
...is to...
Man eats deer.</p>

<p>as 
Cow eats grass
is to
?</p>

<p>If we can find sentence Man eats cow.
being used in texts that we analyze then we have confirmed that cow and deer belong to category
Animals that eat grass and that human eats them.</p>
"
1311,"<p>I'm studying a Master's Degree in Artificial Intelligence and I need to learn how to use the <a href=""http://www.ra.cs.uni-tuebingen.de/software/JavaNNS/welcome_e.html"" rel=""nofollow noreferrer"">Java Neural Network Simulator</a>, JavaNNS, program.</p>

<p>In one practice I have to build a neural network to use backpropagation on it.</p>

<p>I have created a neural network with one input layer with 12 nodes, one hidden layer with 6 nodes and one output layer with 1 node.</p>

<p>I'm using Kaggle's titanic competition data with this format following Dataquest course <a href=""https://www.dataquest.io/course/kaggle-fundamentals"" rel=""nofollow noreferrer"">Getting Started with Kaggle for Titanic competition</a>:</p>

<pre><code>Pclass_1,Pclass_2,Pclass_3,Sex_female,Sex_male,Age_categories_Missing,Age_categories_Infant,Age_categories_Child,Age_categories_Teenager,Age_categories_Young Adult,Age_categories_Adult,Age_categories_Senior,Survived
0,0,1,0,1,0,0,0,0,1,0,0,0
1,0,0,1,0,0,0,0,0,0,1,0,1
0,0,1,1,0,0,0,0,0,1,0,0,1
1,0,0,1,0,0,0,0,0,1,0,0,1
0,0,1,0,1,0,0,0,0,1,0,0,0
0,0,1,0,1,1,0,0,0,0,0,0,0
1,0,0,0,1,0,0,0,0,0,1,0,0
0,0,1,0,1,0,1,0,0,0,0,0,0
</code></pre>

<p>If you want see the same data better in an Spreadsheet:
<a href=""https://i.stack.imgur.com/Ab2aX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ab2aX.png"" alt=""enter image description here""></a></p>

<p>But they preprocess the data to use it with linear regression and I don't know if I can use these data with backpropagation</p>

<p>I think something is wrong because when I run backpropagation in JavaNNS I get these data:</p>

<pre><code>opened at: Sat Feb 17 17:29:40 CET 2018
Step 200 MSE:   0.5381023044692738  validation: 0.11675894327003862
Step 400 MSE:   0.5372328944712378  validation: 0.11700781497209432
Step 600 MSE:   0.5370386219557437  validation: 0.11691717861750939
Step 800 MSE:   0.5370348711919518  validation: 0.11696104763606407
Step 1000 MSE:  0.5369724294992798  validation: 0.11697568840154722
Step 1200 MSE:  0.5369697016710676  validation: 0.11665485957481342
Step 1400 MSE:  0.5370053339270906  validation: 0.11684215268609244
Step 1600 MSE:  0.5370121961199371  validation: 0.11670833992558485
Step 1800 MSE:  0.5370200812483633  validation: 0.11673550099633925
Step 2000 MSE:  0.5367923502149529  validation: 0.11675956129361797
</code></pre>

<p>Nothing changes, it is like it doesn't learn anything.</p>

<p>How many hidden layers does the network have with how many nodes on each hidden layer?</p>

<p>Maybe the problem is that the data have been prepared to be used in Linear regression and I using it with Backpropagation.</p>

<p>I have only created the neural network, I haven't implemented the backpropagation algorithm because it is already implemented in JavaNNS.</p>
"
1312,"<p>I am new to neural networks. I am trying to model the run-off vs. time in a water channel after a storm event given that I know the permeability of the material in the channel, total precipitation, and some other single valued parameters for a particular event..</p>

<p>I have a database of run off histories, and the values of the associated parameters (permeability, total precipitation, etc)</p>

<p>I want my model to give me a runoff vs. Time history when I enter the associated parameters.</p>

<p>I do not know how to train my model. Do i just stack all the time histories in my database together and feed them together? All examples in books use one time history to train the model. Im confused.</p>
"
1313,"<p>From Russell-Norvig:</p>

<blockquote>
  <p>A CSP is strongly k-consistent if it is k-consistent and is also (k − 1)-consistent, (k − 2)-consistent, . . . all the way down to 1-consistent.</p>
</blockquote>

<p>How can a CSP be k-consistent without being (k - 1)-consistent? I can't think of any counter example for this case. Any help would be appreciated.</p>
"
1314,"<p>The problem: I want to classify a trajectory if it has some properties, for example I want to create a simple 0/1 classifier for circular trajectories. If a target is moving in a circular trajectory the network should produce 1, if not it should produce 0.</p>

<p>My input and data set: what I have is data set with cartesian coordinates in 2d so x,y,Vx,Vy. I have a dataset of 10000 trajectory, 5000 circular, 5000 rectilinear. So I feed the network with a tensor [10000, 4, 1]</p>

<p>The question: I'm trying to use a network with three layers, input layer with 4 neurons, hidden layer with 2 LTSM and one fc layer with sigmoid activation function. Is it possible to feed the network with a tensor [4x1] each time? Or do I need to provide the information in batches? Or what? Is the design of my basic network correct?</p>
"
1315,"<p>What AI Algorithms and processes would be involved to simulate a personal trainer to provide a virtual solution so people, athletes and schools can have custom workouts to lessen injuries, optimize athletic performance, lose weight, gain weight, and improve strength?</p>
"
1316,"<p>I am in the process of getting back into AI programming after some time out and have been building my neural net in C#.NET.  I managed to get all of the feed-forward stuff working very eloquently but I am not using Sigmoid as the activation function; instead I am using Leaky RELU as I heard it is best for deep learning.</p>

<p>I began working through the back-propagation material today and ran into a road-block as I need to know the derivative of Leaky RELU in order to calculate the changes in weights.  My calculus is lacking so I was hoping someone might be able to help.</p>

<p>Here is the code for the leaky RELU function which I got from <a href=""https://www.codeproject.com/Articles/1220276/ReInventing-Neural-Networks"" rel=""nofollow noreferrer"">https://www.codeproject.com/Articles/1220276/ReInventing-Neural-Networks</a>:</p>

<pre><code>private double ReLU(double x)
{
    if (x &gt;= 0)
        return x;
    else
        return x / 20;
}
</code></pre>

<p>Thanks in advance.</p>

<p>-Mike</p>
"
1317,"<p>I'm new to this, know only the theory part of the stuff. </p>

<p>I want to train a neural network for detection of currently single class(will be extending to detect more classes)</p>

<p>I came across <a href=""http://cs231n.github.io/transfer-learning/"" rel=""nofollow noreferrer"">transferred learning</a>, that explains as how to use a pre trained NN to train new data.</p>

<p>I selected a framework pytorch which has a nice tutorial explaining <a href=""http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#training-the-model"" rel=""nofollow noreferrer"">Transfer Learning</a></p>

<p>We have a <a href=""https://github.com/amdegroot/ssd.pytorch"" rel=""nofollow noreferrer"">pytorch implemented Single Shot Detector</a> as well.</p>

<p>This is my current situation</p>

<ul>
<li>The data I want to train is different from the once trained already i.e 20 classes those have already been trained.</li>
<li>I currently have a very limited labeleb data training set.</li>
</ul>

<p>The solution is to freeze the weights of the initial few layers, and then train the NN.</p>

<p>I am confused as what exactly is meant by initial few layers?</p>

<p>This is a useful post I found online <a href=""https://towardsdatascience.com/learning-note-single-shot-multibox-detector-with-pytorch-part-1-38185e84bd79"" rel=""nofollow noreferrer"">[Learning Note] Single Shot MultiBox Detector with Pytorch — Part 1</a> explaining how the Single Shot Detector works.</p>

<p>Can anyone help me how to perform these two tasks </p>

<ol>
<li>What are the initial few layers here in this case? How exactly can I freeze them?</li>
<li>What are the changes I need to make while training the NN to classify one or more new classes? </li>
</ol>
"
1318,"<p>Lately I've been wondering.
Is there's a way to locate redundant/unnecessary/misleading inputs by analysis of weights in the first layer?</p>
"
1319,"<p>In the <a href=""https://arxiv.org/pdf/1503.03832.pdf"" rel=""nofollow noreferrer"">FaceNet paper</a> there mentions an gradient algorithm called 'AdaGrad'(Adaptive Gradient) referenced to <a href=""http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf"" rel=""nofollow noreferrer"">this paper</a> which has apparently been used to calculate the gradient of the Triplet Loss function. After referring to the paper also I find it hard to understand how to calculate this adaptive gradient.  </p>

<p>Any ideas regarding this matter? Would love to hear any explanations or ideas towards understanding this concept.</p>

<p>Thank you. </p>
"
1320,"<p>I have a problem in which the dimensions of the input are increasing in row and column at each timestep. What method for preprocessing could be done or are there any architectures used for solving such a case?</p>
"
1321,"<p>I have a map. I need to colour it with <span class=""math-container"">$k$</span> colours, such that two adjacent regions do not share a colour. </p>

<p>How can I formulate the map colouring problem as a hill climbing search problem?</p>
"
1322,"<p>Two Stanford University researchers, Dr. Michal Kosinki and Yilun Wang have published a paper that claims that AI can predict sexuality from a single facial photo with startling accuracy. This research is obviously disconcerting since it exposes an already vulnerable group to a new form of systematized abuse.</p>

<p>The research can be found here <a href=""https://osf.io/zn79k/"" rel=""nofollow noreferrer"">https://osf.io/zn79k/</a> ,here  <a href=""https://psyarxiv.com/hv28a/"" rel=""nofollow noreferrer"">https://psyarxiv.com/hv28a/</a> and has even been highlighted by Newsweek magazine here <a href=""http://www.newsweek.com/ai-can-tell-if-youre-gay-artificial-intelligence-predicts-sexuality-one-photo-661643"" rel=""nofollow noreferrer"">http://www.newsweek.com/ai-can-tell-if-youre-gay-artificial-intelligence-predicts-sexuality-one-photo-661643</a> </p>

<p><a href=""https://i.stack.imgur.com/wRwrA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wRwrA.jpg"" alt=""enter image description here""></a></p>

<p>Above is an image of composite heterosexual faces and composite gay faces from the research. (Image courtesy of Dr Michal Kosinki and Yilun Wang)</p>

<p>My question is, as knowledgable members of the AI community, how can we scientifically debunk/discredit this research?</p>
"
1323,"<p>I have a customer purchasing dataset and the data set is from a retailer having an online store and offline stores. So, customers have two options in their shopping channel, online or offline. In an online shopping, there is a shipping fee however if a basket size is larger than $50 there is no shipping fee. </p>

<p>I found pieces of evidence that customers are trying to add some of items to make their basket size larger than $50 when their baskets are near and a little bit below the $50, because their shipping fee can be waived by doing that.</p>

<ul>
<li>In this situation, I am trying to identify and characterize items that were purchased only because of the shipping threshold by using a machine learning algorithm. </li>
</ul>

<p>If there is no shipping threshold, $50, the customers would not purchase the items, but they purchased some items to make their basket size larger than $50. I have not observed those kinds of items (added items because of the shipping threshold). </p>

<ul>
<li>Is there any machine learning algorithm that I can identify those kinds of items? </li>
</ul>

<p>I think I need to use some of unsupervised machine learning algorithm.</p>

<p>Another challenging part is that each customer has different characteristics so I probably need to consider it as well. How can I detect those kinds of items??</p>
"
1324,"<p>I am absolutely new in AI area.</p>

<p>I would like to know how to mathematically/logically represent the <strong>sense</strong> of sentences like:</p>

<pre><code>The cat drinks milk.

Sun is yellow.

I was at work yesterday.
</code></pre>

<p>So that it could be converted to computer understandable form and analysed algorithmically.</p>

<p>Any clue?</p>
"
1325,"<p>I am trying to perform classification task using Keras and tensorflow. However, the learning converges after achieving an accuracy of 57%. All my inputs and outputs are categorical data. Am I using the correct approach to train? Are there any other example in the web which is trying to solve the similar problem?</p>

<p><strong>data.csv</strong></p>

<pre><code>religion,caste,qualification,marital_status,sex,nature_activity
2,20,5,1,1,10
2,20,5,1,1,10
2,20,5,1,1,10
2,20,5,1,1,10
1,3,5,1,1,13
1,4,4,1,2,3
1,3,4,2,1,1
1,3,3,2,1,1
</code></pre>

<p><strong>Source code</strong></p>

<pre><code>from keras.models import Sequential
from keras.layers import Dense
import pandas
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
import numpy
from sklearn.model_selection import train_test_split
from keras.wrappers.scikit_learn import KerasClassifier
import sys
import matplotlib.pyplot as plt
# fix random seed for reproducibility
numpy.random.seed(7)

# load pima indians dataset
dataframe = pandas.read_csv(""data.csv"",header=None)
dataset=dataframe.values
# split into input (X) and output (Y) variables
X = dataset[:,0:4].astype(float)
Y = dataset[:,4]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.0015, random_state=42)
encoder = LabelEncoder()
encoder.fit(Y_train)
encoded_Y = encoder.transform(Y_train)
one_hot_enc_Y=np_utils.to_categorical(encoded_Y)
model=Sequential()
model.add(Dense(500,input_dim=4,activation='relu'))
model.add(Dense(100,activation='relu'))
model.add(Dense(50,activation='relu'))
model.add(Dense(3,activation='softmax'))

# Compile model
model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
# Fit the model
history=model.fit(X_train, one_hot_enc_Y, validation_split=0.33,epochs=300, batch_size=5)
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
# summarize history for loss
plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'], loc='upper right')
plt.show()
# evaluate the model
scores = model.evaluate(X_train, one_hot_enc_Y)
print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
sys.exit()
</code></pre>
"
1326,"<p>In some Atari games in the Arcade Learning Environment (ALE), it is necessary to press <code>FIRE</code> once to start a game. Because it may be difficult for a Reinforcement Learning (RL) agent to learn this, they may often waste a lot of time executing actions that do nothing. Therefore, I get the impression that some people hardcode their agent to press that <code>FIRE</code> button once when necessary. </p>

<p>For example, in OpenAI's <a href=""https://github.com/openai/baselines"" rel=""nofollow noreferrer"">baselines repository</a>, this is implemented using the <a href=""https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py#L37"" rel=""nofollow noreferrer""><code>FireResetEnv</code></a> wrapper. Further down, in their <a href=""https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py#L223"" rel=""nofollow noreferrer""><code>wrap_deepmind</code></a> (which applies that wrapper among others), it is implied that DeepMind tends to use this functionality in all of their publications. I have not been able to find a reference for this claim though.</p>

<hr>

<p><strong>My question is</strong>: is it common in published research (by DeepMind or others) to use the functionality described above? I'd say that, if this is the case, it should be explicitly mentioned in these papers (because it's important to know if hardcoded domain knowledge was added to a learning agent), but I have been unable to explicitly find this after looking through a wide variety of papers. So, based on this, I'd be inclined to believe the answer is ""no"". The main thing that confuses me then is the implication (without reference) in the OpenAI baselines repository that the answer would be ""yes"".</p>
"
1327,"<p>It is suggested that the number of hidden units in a layer should be in powers of 2 because it helps converge faster. Is it a fact and if it is, how this helps the NN learn faster. Does it have to do something with how the memory is laid down?</p>
"
1328,"<p>When we augment data for training are we also changing the distribution of data and if its a different distribution why do we use it to train a model for original distribution ?</p>
"
1329,"<p>How is it that word embedding layer (say word2vec) brings more insights to the network compared to a simple one hot encoded layer? </p>

<p>I understand how word embedding carry some semantic meaning but it seems that this information would get ""squashed"" by the activation function, leaving only a scalar value and as many different vector could yield the same result, I would guess that the information is more or less lost.</p>

<p>Could anyone bring me insights as to why a network may utilize the information contained in a word embedding ?</p>
"
1330,"<p>For people who are not in academia, could you please provide some insight into the current stage of developments in AGI area? </p>

<p>Are there any projects that had breakthroughs recently? </p>

<p>Maybe some news source to follow on this topic?</p>
"
1331,"<p>Artificial networks model systems with a set of inputs and outputs and expected behavior. To train a network for modeling such systems, hundreds, thousands, or millions of example inputs-output pairs may be required. This is called a labelled data set, and the network and its optimization algorithm are meant to find a set of network parameters that best match the I/O of the artificial network with the I/O of the system.</p>

<p>Are there any systems, for which sufficient labelled data sets exist, that have yet to be successfully modeled with artificial networks of any type (recurrent, deep, convolution, etc)?</p>
"
1332,"<p>I have a game application with characters that have to cross mazes. The game can generate thousands of different mazes and the characters can move according to users choice and cross the maze manually. We needed to add the possibility to show a correct way out of each maze. Therefore we added the possiblity to move the characters according to an xml file. </p>

<p>This XML file is very complex, usually around thirty-fifty thousands of rows. lets say its in the following structure (but much more complex):</p>

<pre><code>  &lt;maze-solution&gt;
  &lt;part id=""1""&gt;
  &lt;sector number=""1""&gt;
    &lt;action&gt;
        &lt;equipment&gt;heavy&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;1250&gt;&lt;/start-position&gt;
            &lt;angle&gt;23.43&lt;/angle&gt;
            &lt;duration&gt;0.44&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;run&lt;/action-type&gt;
        &lt;character&gt;1&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;
    &lt;action&gt;
        &lt;equipment&gt;light&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;4223&gt;&lt;/start-position&gt;
            &lt;angle&gt;233.43&lt;/angle&gt;
            &lt;duration&gt;0.32&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;walk&lt;/action-type&gt;
        &lt;character&gt;1&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;
    &lt;action&gt;
        &lt;equipment&gt;heavy&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;1231&gt;&lt;/start-position&gt;
            &lt;angle&gt;84.134&lt;/angle&gt;
            &lt;duration&gt;0.454&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;run&lt;/action-type&gt;
        &lt;character&gt;2&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;
    &lt;action&gt;
        &lt;equipment&gt;heavy&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;932&gt;&lt;/start-position&gt;
            &lt;angle&gt;34.43&lt;/angle&gt;
            &lt;duration&gt;0.50&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;duck&lt;/action-type&gt;
        &lt;character&gt;1&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;   
  &lt;/sector&gt;
  &lt;sector number=""2""&gt;
    &lt;action&gt;
        &lt;equipment&gt;heavy&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;1250&gt;&lt;/start-position&gt;
            &lt;angle&gt;23.43&lt;/angle&gt;
            &lt;duration&gt;0.44&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;run&lt;/action-type&gt;
        &lt;character&gt;1&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;
    &lt;action&gt;
        &lt;equipment&gt;light&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;4223&gt;&lt;/start-position&gt;
            &lt;angle&gt;233.43&lt;/angle&gt;
            &lt;duration&gt;0.44&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;walk&lt;/action-type&gt;
        &lt;character&gt;1&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;
    &lt;action&gt;
        &lt;equipment&gt;heavy&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;1231&gt;&lt;/start-position&gt;
            &lt;angle&gt;84.134&lt;/angle&gt;
            &lt;duration&gt;0.454&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;run&lt;/action-type&gt;
        &lt;character&gt;2&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;
    &lt;action&gt;
        &lt;equipment&gt;heavy&lt;/equipemnt&gt;
        &lt;movement&gt;
            &lt;start-position&gt;932&gt;&lt;/start-position&gt;
            &lt;angle&gt;23.43&lt;/angle&gt;
            &lt;duration&gt;0.44&lt;/duration&gt;
        &lt;/movement&gt;
        &lt;action-type&gt;duck&lt;/action-type&gt;
        &lt;character&gt;1&lt;/character&gt;
        &lt;protection&gt;none&lt;/protection&gt;       
    &lt;/action&gt;   
  &lt;/sector&gt;
  &lt;sector number=""3""&gt;   
  &lt;/maze-solution&gt;
</code></pre>

<p>At the moment, we have the ability to analayze each maze using a CNN algorithm for image classification and generate an xml that represents a way out of the maze - meaning that if the characters will be moved according to that file, they will cross the maze. That algorithm has been tested and can not be changed by any means.</p>

<p>The problem is that most of the times the generated file is not the best one possible (and quite often it is very noticeable). There are different, faster, better ways to cross the maze.</p>

<p>We also have thousands (and we can get as many as needed) files that were created manually for saved mazes and therefore they are representing an elegant and a fast way out of the maze.  The ideal goal is that someday, our program will learn how to generate such a file without people creating them manually.</p>

<p>To conclude, we have plenty of XML files generated by a program compared to the hard-coded XML files. There are thousands of pairs - The file the program generated, and the ""ideal"" file version that a person created. (and we can get infinite number of such pairs)
Is there a way, using those thousands of pairs, to make a second step algorithm that will ""learn"" what adjustments should be made in the generated XML files to make them more like the hard-coded ones?</p>

<p>I'm not looking for a specific solution here but for a general idea that will get me going. I hope i made myself clear but if I missed some info let me know and I will add it.</p>
"
1333,"<p>I've been trying to puzzle our ResNet and I think I have <a href=""https://medium.com/@CommonGibbon/explaining-resnet-with-crayons-74740bc8a519"" rel=""nofollow noreferrer"">an intuitive explanation</a> for it, but I want to make sure it doesn't stray too far from the actual theory to be misleading. Again, since I started out on this from a point of relative ignorance, I could be mistaking/missing some fundamental concept which would prevent my metaphorical explanation from accurately modeling reality. </p>

<p>Here again is the link to the medium article I've written. 
<a href=""https://medium.com/@CommonGibbon/explaining-resnet-with-crayons-74740bc8a519"" rel=""nofollow noreferrer"">https://medium.com/@CommonGibbon/explaining-resnet-with-crayons-74740bc8a519</a></p>

<p>I haven't yet published it, so I'd welcome any feedback.</p>

<p>Thank you!</p>

<p><strong>EDIT:</strong> After reading this through a few more times and comparing against the literature, I'm pretty confident I have it right. I'm still more than open to any feedback you might have!</p>
"
1334,"<p>I trained my model <code>ResNet-50</code> for <a href=""http://weegee.vision.ucmerced.edu/datasets/landuse.html"" rel=""nofollow noreferrer"">UCMerced_LandUse dataset</a> and this my loss graph
<img src=""https://i.stack.imgur.com/ZGzqR.png"" alt=""enter image description here"">
but I have a problem when I try my first picture <code>river</code> give me the right class <code>river</code>
then give me the first class <code>river</code> predicted for all the next picture I test </p>

<p><strong>for example :</strong><br>
  input picture is => <code>pic A</code> result <code>class A</code><br>
  input picture is => <code>pic B</code> result <code>class A</code><br>
  input picture is => <code>pic C</code> result <code>class A</code><br>
  ....</p>

<p>this is my <a href=""https://github.com/SakhriHoussem/RestNet"" rel=""nofollow noreferrer"">code repository</a></p>

<p>my test code that give me always the first class</p>

<pre><code>import tensorflow as tf
import numpy as np
from os import path as PATH
from os import makedirs
import cv2


classes =['agricultural', 'airplane', 'baseballdiamond', 'beach', 'buildings',
             'chaparral', 'denseresidential', 'forest', 'freeway', 'golfcourse', 'harbor',
             'intersection', 'mediumresidential', 'mobilehomepark', 'overpass',
             'parkinglot', 'river', 'runway', 'sparseresidential', 'storagetanks',
             'tenniscourt']


def testing(sess,path,classes=classes, save_dir = ""Save/"",save_file =""data""):

    data = cv2.imread(path)
    labels = np.zeros((1,len(classes)))

    # get image height, width, channels
    height, width, channels = data.shape

    data =np.array([data])

    print(data.shape)
    print(""Input image size :"", height, width, channels)

    if not PATH.isdir(save_dir):
        if makedirs(save_dir):
            print(save_dir,""is created"")
    with sess:
        if PATH.isdir(save_dir) and PATH.isfile(save_dir+save_file+"".meta"") and PATH.isfile(save_dir+""checkpoint""):
            print(""files are exist"")
            saver = tf.train.import_meta_graph(save_dir+save_file+"".meta"")
            saver.restore(sess, tf.train.latest_checkpoint(""Save/""))
            print(""data are restored"")
            graph = tf.get_default_graph()
            x = graph.get_tensor_by_name(""t_picture:0"")#vrai
            y = graph.get_tensor_by_name(""t_labels:0"")#vrai
            #train = tf.get_collection('train_op')#vrai
            #loss = tf.get_collection('loss_op')#vrai
            logists = tf.get_collection('logits_op')#vrai
            #errors = tf.get_collection('errors')#vrai
        else:
            exit(""data are not exist"")
        print(""start"")
        curr_logists = sess.run([logists], {x: data, y: labels})
        curr_logists = np.array(curr_logists)[0,0,0]
        softmax = sess.run(tf.nn.softmax(curr_logists))
        print(""logists : "", curr_logists)
        print(""softmax : "", softmax)
        print(""class : "",classes[np.argmax(softmax)])
        #print(""loss:\n%s"" % ( curr_loss))
        print(""test is finished"")
        sess.close()

if __name__ == '__main__':

    # path=""UCMerced_LandUse/Images/""
    path=""runway.tif""
    save_dir = ""Save/""
    save_file = ""dataSaved""
    testing(tf.Session(),path,classes,save_dir,save_file)
</code></pre>
"
1335,"<p>If possible consider the relationship between implementation difficulty and accuracy in voice examples or simply chat conversations.</p>

<p>And currently, what are the directions on algorithms like Deep Learning or others to solve this.</p>
"
1336,"<p>I am trying to produce Decision Tree from Feed Forward Neural Network . </p>

<p>The input to the feed forward neural network is <strong>Condition Action Statement</strong>
for example, if airthrusthold > 90 , power up the engine else rotate shaft 5 degree wide</p>

<p>Above statement is the input to the FFNN . How do i feed the statement ? 
Either converting into word2vec ? (or) there is any other format to do ?</p>

<p>And i need to produce <strong>decision tree</strong> from the <strong>outcome</strong> of neural network </p>

<p>Can we do this using reinforcement learning using Markov Decision Process? </p>

<p>Thanks!</p>
"
1337,"<p>I've been working with vanilla feed forward neural networks and have been researching the convolutional neural network literature. Thus far I've have not encountered how often the model is executed in order to classify objects. For example if a camera is capturing video at a rate of 15 frames per second is the classification model being trained / executed iteratively in order to maintain non time delayed classifications ?</p>
"
1338,"<p>Is there a way for people outside of core research community of AGI to contribute to the cause?</p>

<p>There are a lot of people interested in supporting the field, but there is no clear way to do that. Is there something like BOINC for AGI researches, or open projects where random experts can provide some input? Maybe Kickstarter for crazy AI projects? </p>
"
1339,"<p>If the game had a variable speed and was essential in evolution/gaining score(IDK AI terminologies). Would the AI be able to figure out when to slow down and speed up?</p>

<p>If it is able to solve the problem or complete the level, will it have an equation to relating acceleration, or perhaps a number on when to speed up and down. What if the game environment was dynamic?</p>

<p>Can you even teach math to an AI?</p>

<p>PS: I'm not sure if I should ask separate question?</p>
"
1340,"<p>In the <a href=""https://arxiv.org/pdf/1503.03832.pdf"" rel=""nofollow noreferrer"">Facenet</a> paper, Under section <strong>3.2</strong> The authors mention that:</p>

<blockquote>
  <p>The embedding is represented by f(x) ∈ R<sup>d</sup> . It embeds an image x into
  a d-dimensional Euclidean space. Additionally, we constrain this
  embedding to live on the d-dimensional hypersphere, i.e. ||f(x)||<sub>2</sub> = 1.</p>
</blockquote>

<p>I don't quite understand how the above equation holds! As far as I understand L2 Norm is same as euclidean distance but I don't quite understand how this imposes
<strong>||f(x)||<sub>2</sub> = 1</strong> criteria.</p>
"
1341,"<p>Many of you have probably seen the turtle from LabSix that gets mistaken for a rifle in Google's InceptionV3 image classifier. I read <a href=""https://arxiv.org/pdf/1707.07397.pdf"" rel=""nofollow noreferrer"">the paper</a> and I understand how they apply EOT to 2d images and on the individual pixel values, but I am still unsure how they implement the EOT algorithm to the 3d model. </p>

<ol>
<li>Are they using EOT to perturb the individual coordinates in the 3d model's mesh? Or are they perturbing images of a turtle and then printing the turtle from the images?</li>
<li>How do they check the InceptionV3 output iteratively without having to 3d print the object each time and check the probabilities given?</li>
</ol>

<p>Any examples that someone can point to would also be very helpful.</p>
"
1342,"<p>Recently my friend asked me a question: having two input matrices X and Y (each size NxD) where D >> N, and ground truth matrix Z of size DxD, what deep architecture shall I use to learn a deep model of this representation?</p>

<ul>
<li>N ~ is in the order of tens</li>
<li>D ~ is in the order of tens of thousands</li>
</ul>

<p>The problem is located in the domain of bioinformatics, however, this is more of an architectural problem. All matrices contain floats.</p>

<p>I tried first a simple model based on a CNN model in keras. I've stacked input X and Y into an Input Matrix of size (number of training examples, N, D, 2). Outputs are of size (number of training examples, D, D, 1)</p>

<ol>
<li>Conv2D layer

<ul>
<li>leaky ReLU</li>
</ul></li>
<li>Conv2D layer

<ul>
<li>leaky ReLU</li>
</ul></li>
<li>Dropout</li>
<li>Flattening layer</li>
<li>Dense (fully connected) of size D

<ul>
<li>leaky ReLU</li>
<li>droout</li>
</ul></li>
<li>Dense (fully connected) of size D**2 (D squared)

<ul>
<li>leaky ReLU</li>
<li>droout</li>
</ul></li>
<li>Reshaping output into (D,D,1) (for single training set)</li>
</ol>

<p>However, this model is untrainable. It has over billion parameters for emulated data. </p>

<p>(Exactly 1,321,005,944 for my randomly emulated dataset)</p>

<p>Do you find this problem solvable? What other architectures I might try to solve this problem?</p>

<p>Best.</p>
"
1343,"<p>I recently came across a <a href=""https://www.quora.com/I-found-many-of-your-answers-on-reinforcement-learning-extremely-insightful-Im-at-Google-and-you-mentioned-that-you-were-giving-a-talk-about-imagination-learning-I-would-love-to-attend-if-possible"" rel=""nofollow noreferrer"">Quora post</a>, where I saw the term ""Imagination Learning"". It seems to be based on something called ""<a href=""https://people.cs.umass.edu/~mahadeva/Site/About_Me.html"" rel=""nofollow noreferrer"">Imagination Machines</a>"" (the link is based on a guy's work profile as of now; subject to change).</p>

<p>The only thing that I could find on Internet about it is this paper: <a href=""https://arxiv.org/pdf/1707.06203.pdf"" rel=""nofollow noreferrer"">Imagination-Augmented Agents for Deep Reinforcement Learning</a>. (But I'm not sure if it's related to that concept.)</p>

<p>Any ideas on this would be appreciated.</p>
"
1344,"<p>For a multi (4xTitan Xp) GPU deep learning setup what kind of CPU is preferable?</p>

<p>Specifically I am comparing:</p>

<ul>
<li>Intel Xeon E5-2620 with 8x2.1GHz 20MB L3 Cache</li>
<li>Intel Xeon E5-1620K with 4x3.5Ghz 10MB L3 Cache</li>
<li>Intel Xeon E5-1650K with 6x3.6GHz 15MB L3 Cache</li>
<li>Intel i7-6850K with 6x3.6GHz 15MB L3 Cache</li>
</ul>

<p>I wonder if the higher clock rates are important or is it better to have more number of cores in this use case.</p>
"
1345,"<p>I have a data-set with <code>m</code> observations and <code>p</code> categorical variables (nominal), each variable <code>X1,X2...Xp</code> has several different possible values. Ultimately, I am looking for a way to find anomalies i.e to identify rows for which the combination of values seems incorrect with respect to the data I saw so far. So far, I was thinking about building a model to predict the value for each column and then build some metric to evaluate how different the actual row is from the predicted row. I would greatly appreciate any help!</p>
"
1346,"<h1>The situation</h1>

<p>I am referring to the paper <a href=""https://arxiv.org/abs/1509.02971"" rel=""nofollow noreferrer"">T. P. Lillicrap et al, ""Continuous control with deep reinforcement learning""</a> where they discuss deep learning in the context of continuous action spaces (""Deep Deterministic Policy Gradient"").</p>

<p>Based on the DPG approach (""Deterministic Policy Gradient"", see <a href=""http://proceedings.mlr.press/v32/silver14.pdf"" rel=""nofollow noreferrer"">D. Silver et al, ""Deterministic Policy Gradient Algorithms""</a>), which employs two neural networks to approximate the actor function <code>mu(s)</code> and the critic function <code>Q(s,a)</code>, they use a similar structure.<br>
However one characteristic they found is that in order to make the learning converge it is necessary to have two additional ""target"" networks <code>mu'(s)</code> and <code>Q'(s,a)</code> which are used to calculate the target (""true"") value of the reward:</p>

<pre><code>y_t = r(s_t, a) + gamma * Q'(s_t1, mu'(s_t1))
</code></pre>

<p>Then after each training step a ""soft"" update of the target weights <code>w_mu', w_Q'</code> with the actual weights <code>w_mu, w_Q</code> is performed:</p>

<pre><code>w' = (1 - tau)*w' + tau*w
</code></pre>

<p>where <code>tau &lt;&lt; 1</code>. According to the paper</p>

<blockquote>
  <p>This means that the target values are constrained to change slowly, greatly improving the stability of learning.</p>
</blockquote>

<p>So the target networks <code>mu'</code> and <code>Q'</code> are used to predict the ""true"" (target) value of the expected reward which the other two networks try to approximate during the learning phase.</p>

<p>They sketch the training procedure as follows:</p>

<p><a href=""https://i.stack.imgur.com/BBNRP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BBNRP.png"" alt=""DDPG Sketch""></a></p>

<h1>The question</h1>

<p>So my question now is, after the training is complete, which of the two networks <code>mu</code> or <code>mu'</code> should be used for making predictions?</p>

<p>Equivalently to the training phase I suppose that <code>mu</code> should be used without the exploration noise but since it is <code>mu'</code> that is used during the training for predicting the ""true"" (unnoisy) action for the reward computation, I'm apt to use <code>mu'</code>.</p>

<p>Or does this even matter? If the training was to last long enough shouldn't both versions of the actor have converged to the same state?</p>
"
1347,"<p>I have a cancer patient database from mass spectrometry on patients which consists of more than half million features. My task is to apply a feature selection algorithm to extract the most relevant features from it. My question is, which feature selection model would be the most appropriate in this case? Any suggestion from practical experience for these types of data is appreciated. </p>
"
1348,"<p>The region proposal network (RPN) in Faster-RCNN models contains a classifier and a regressor network. Why does the classifier network output two scores (object and background) for each anchor instead of just a single objectness-probability? Aren't the two classes considered exclusive?</p>

<p>Source: Figure 3 of the <a href=""https://arxiv.org/abs/1506.01497"" rel=""nofollow noreferrer"">original Faster-RCNN paper</a></p>
"
1349,"<p>Neural networks have the problem, that they are not turing-complete. That means, it is not possible to express any function with it. Instead, logicgate networks which are consisting of AND, OR, NOT are turing complete. It is possible to implement a primenumber generator with boolean algebra: <a href=""https://www.youtube.com/watch?v=e2JOm-zJstY"" rel=""nofollow noreferrer"">A circuit to find prime numbers</a></p>

<p>In the hope that logicgates can be trained like a neural network, i implemented a prototype:</p>

<pre><code>/*
I0I1| A B C D
0 0 | 0 0 0 0
0 1 | 0 1 1 1
1 0 | 0 1 0 1
1 1 | 1 1 1 1

A And
B OR
C i1 or A
D I0 or C
*/
class Logicgate {
public:
  void addrandomgate() {
    int id1 = std::rand() % mylogicgate.size(); // 0..mylogicgate.size()
    int id2 = std::rand() % mylogicgate.size(); 
    int relation=std::rand() % 3; // 0=AND, 1=OR, 2=NOT
    bool value=0;
    mylogicgate.push_back({id1,id2,relation,value});
  }
  void calc() {
    for (auto i=numberinput;i&lt;mylogicgate.size();i++) {
      // get value
      bool temp1=mylogicgate[mylogicgate[i].id1].value;
      bool temp2=mylogicgate[mylogicgate[i].id2].value;
      // logic operation
      if (mylogicgate[i].relation==0)
        mylogicgate[i].value = temp1 &amp;&amp; temp2;
      if (mylogicgate[i].relation==1)
        mylogicgate[i].value = temp1 || temp2;
      if (mylogicgate[i].relation==2)
        mylogicgate[i].value = ! temp1;
    }
  }
  void train() {
    for (auto trial=0;trial&lt;100000000;trial++) {
      if (error&lt;minerror) 
        std::cout&lt;&lt;""trial ""&lt;&lt;trial&lt;&lt;"" error ""&lt;&lt;error&lt;&lt;""\n""; 
    }
  }
};
</code></pre>

<p>The prototype works fine. He calculates for the input the output, and it is possible to set the logicgates via a random-generator. The idea is to use a brute-force-solver for testing out all possible logicgates and find a mapping from input to output. In the literature this concept is called binary decision tree. But I found a bug. The same problem is happening like in neural networks learning too. The CPU consumption in testing out all possibilities is very high, but the solver don't find the correct weight. Weight means here, the boolean algebra which is the program of the network. How can I improve the performance of the solver?</p>

<p><strong>Update</strong>
McCulloch Pitts neuron is the correct term. It describes a neuron which is equal to a logicgate. The idea was to use some kind of genetic programming on that model, but the state-space of all possible connections seems heavy large. For example, if 10 neurons are possible, that means every neuron can connected with two other neurons: 10x10 and has either the AND, OR, NOT function. So the overall number of possibilities is (10*10*3)^10=3.4867844e+49?</p>
"
1350,"<p>I'd like to generate subtitles for a silent film. Is there an open source project out there capable of creating captions based on a series of images (such as a scene from a movie)?</p>

<p>EDIT: thanks for the comments below. To clarify, what i'm looking for is an algorithm which can generate a caption for a sequences of images within a movie  describing what happens in the sequence. This is for preliminary research, so accuracy is less important. </p>
"
1351,"<p>Usually, in binary classification problems, we use <strong>sigmoid as activation function of last layer plus the binary cross-entropy as cost function</strong>.</p>

<p>However, I have already experienced (more than once) that <strong>tanh as activation function of last layer + MSE as cost function</strong> worked slightly better for binary classification problems.</p>

<p>Using a binary image segmentation problem as an example, we have the two scenarios:</p>

<ol>
<li><strong>sigmoid (last layer) + cross entropy</strong>: the output of the network will be a probability for each pixel and we want to maximize it according to the correct class. </li>
<li><strong>tanh (last layer) + MSE</strong>: the output of the network will be a normalized pixel value [-1, 1] and we want to make it as close as possible the original value (normalized too).</li>
</ol>

<p>We all know the problems associated with sigmoid (vanish of gradients) and the benefits of cross-entropy cost function. We also know tanh is slightly better than sigmoid (zero-centered and little less prone to gradient vanishing), but when we use MSE as the cost function, we are trying to minimize a completely different problem - regression instead of classification.</p>

<p>Anybody else has already faced the same results? 
<br>Is there any intuition about why ""tanh + MSE"" worked better than ""sigmoid + cross entropy""?</p>
"
1352,"<p>I am looking for an algorithm to transform an input data to a goal data using a series of operations. The shorter the series the better.</p>

<p>The following is known:</p>

<ul>
<li>the input data</li>
<li>the goal data</li>
<li>input and goal data does not stand in any correlation</li>
<li>operations (and there impact to the current data) which can be endless combined</li>
<li>different input data for the same goal data could have same, similar or totally different operation series</li>
<li>for some data states not all operations are possible</li>
</ul>

<hr>

<p>I thought of a pathfinding algorithm, since I can calculate the distance between current data and goal data. So each edge would be an operator and each node the current data. But I am unsure about variety and combination amount of possible operations.</p>

<p>What approach could I try?</p>
"
1353,"<p>I want to plot a schedule of races based on rules. Rules like ""each team needs at least 2 races between their next race"" and some teams (e.g. collegiate) need to be clumped near each other.</p>

<p>What would be the best algorithm to approach this? So far, all I've found is genetic algorithm. Are there any other alternatives I could look into? </p>
"
1354,"<p>I am currently trying to solve a regression problem using neural networks. I want to detect movement patterns in images over time (video) and output a continuous value. 
During the training process I noticed a strange behaviour for the validation loss curve and I was wondering if anyone has noticed this kind of <strong>periodic pattern</strong> on some of their own work. <strong>What might cause this?</strong> </p>

<p>The model looks like the following:  </p>

<pre><code>- TimeDistributed(Conv2D(32, (3,3)))
- TimeDistributed(Conv2D(16, (3,3)))
- TimeDistributed(Flatten())
- GRU(64, stateful=True)
- Dropout(0.5)
- Dense(64, activation='relu')
- Dense(1)
</code></pre>

<p>I trained the model using the mean squared error as the loss function, a batch size of 1 and the AdamOptimizer with an initial learning rate of 10^(-6). Obviously, the loss curve for the training data is not very good, but I am currently just wondering about the pattern of the val_loss. The plots below represent the loss of 65 epochs.</p>

<p>Thanks!</p>

<p><a href=""https://i.stack.imgur.com/ebRvf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ebRvf.png"" alt=""Validation Loss""></a></p>

<p><a href=""https://i.stack.imgur.com/HK88g.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HK88g.png"" alt=""Training Loss""></a></p>

<p><strong>Edit:</strong>
The way I try to solve my task relies on a sliding window approach where I try to predict a continuous value for the next second based on the last 20 seconds (400 frames) of the time-series input data. But I don't think this information is needed to solve my initial question since the periodic patterns appear over several epochs (one ""peak"" for about every 15 epochs) which is strange. Although the stateful-version of the GRU is used (btw: using TensorFlow and Keras), the internal state of the GRU is reset after every epoch to maintain a clean start. The stateful keyword is used to indicate a dependency between batches.</p>
"
1355,"<p>I am doing a project on Visual Place Recognition in Changing Environments.  The CNN used here is mostly AlexNet, and a feature vector is constructed from Layer 3.  Does anyone know of similar work using other CNN's e.g. VGGnet (which I am trying to use) and the corresponding layers please?</p>

<p>I have been trying out the different layers of VGGnet-16.  I am trying to get the nearest correspondence to the query image by using the cosine difference between query image and database images.  So far no good results.</p>

<p>Thanks.</p>
"
1356,"<p>As discussed <a href=""https://ai.stackexchange.com/questions/2980/how-to-handle-invalid-moves-in-reinforcement-learning"">in this thread</a>, you can handle invalid moves in Reinforced Learning by re-setting the probabilities of all illegal moves to zero and renormalising the output vector.</p>

<p>In back-propagation, which probability matrix should we use? The raw output probabilities, or the post-processed vector?</p>
"
1357,"<p>Recently I came across this website which is a year old: <a href=""https://affinelayer.com/pixsrv/"" rel=""nofollow noreferrer"">https://affinelayer.com/pixsrv/</a></p>

<p>On Desktop we can draw and download the trained model in browser and see the corresponding image generated on the right side.</p>

<p>Wondering how it works and with some investigation I have the following 2 questions:</p>

<ol>
<li>Why are we only downloading one file per “showcase”? Because GAN should need 1 model for converting the image and 1 model for verifying the image from what I have read.

<ol start=""2"">
<li>Why are the models in pict extension? Which framework did the author use to create the pict files? In linked repositories I have found TensorFlow, PyTorch etc which none of them produces pict files...</li>
</ol></li>
</ol>

<p>Thanks in advance.  I am very curious why such JavaScript client side demo is lacking on the internet... or I just haven’t found the correct keyword to find them out.</p>
"
1358,"<p>As far as I understand, neural networks aren't good at classifying 'unknowns', i.e. objects that do not belong to a learned class. But how do face detection/recognition approaches usually determine that no face is detected/recognised in a region? Is the predicted probability somehow thresholded?</p>

<p>I'm asking because my application will involve identifying unknown objects. In fact, most of the input objects are unknown and only a fraction is known.</p>
"
1359,"<p>I was reading an interesting book about the role of AI in Cybersecurity, and the author mentioned there being 3-4 types. Each one is dependent on its abilities and understanding. For example, a ‘narrow artificial intelligence’ is only capable of one such ability.</p>

<p>What are these “levels” and how are they gauged?</p>
"
1360,"<p>What will be the difference when used for video classification? Will they yield different results or are they the same fundamentally?</p>
"
1361,"<p>In order to model a card game as an exercise I was thinking an elementary setting as a multiarmed bandit, each lever being the distribution of expected rewards of an specific card.</p>

<p>But of course the player only have some cards in the hand each round, or equivalently for a given round it has available a number $n$ of arms randomly selected from the total number $N$ of levers. Is this just a ""contextual bandit"" or has it some specific, narrower, name that I could use to look up in the literature?</p>
"
1362,"<p>It is said that activation functions in neural networks help introduce <strong><em>non-linearity</em></strong>.</p>

<ul>
<li>What does this mean?</li>
<li>What does <strong><em>non-linearity</em></strong> mean in this context?</li>
<li>How does introduction of this <strong><em>non-linearity</em></strong> help?</li>
<li>Are there any other purposes of <strong><em>activation functions</em></strong>?</li>
</ul>
"
1363,"<p>Does NEAT requires only connection genes to be marked with a global innovation number?<br>
From the NEAT paper</p>

<blockquote>
  <p>Whenever a new
  gene appears (through structural mutation), a global innovation number is incremented
  and assigned to that gene.</p>
</blockquote>

<p>it seems that any gene (both node genes and connection genes) requires an innovation number, however I was wondering what was the node gene innovation number for.<br>
Is it to provide the same node ID across all elements of the population?
Isn't the connection gene innovation number sufficient?<br>
Besides, the NEAT paper includes the following image which doesn't show any innovation number on node genes.</p>

<p><a href=""https://i.stack.imgur.com/0xkhX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0xkhX.png"" alt=""enter image description here""></a></p>
"
1364,"<p>In the add node mutation, the connection between two chosen nodes (e.g A and B) is first disabled and then a new node is created between A and B with their respective two connections.<br>
I guess that the former A-B connection can be re-enabled via crossover (is it right?).<br>
Can the former A-B connection also be re-enabled via mutation (e.g. ""add connection"")?</p>
"
1365,"<p>I am very new to ML can you please point me to some good tutorials/examples on the application of Incremental(Online) Learning Novelty Detection Algorithms.</p>

<p>Thank You</p>
"
1366,"<p>Is there any rule to follow when it comes to total amount of NN parameters (weights and biases) taking into account the amount of training data? Is there any recommended ratio between the two, for example 10 training vectors for each NN parameter? </p>
"
1367,"<p>I have a general question about the updating of the network/model in the PPO algorithm (<a href=""https://arxiv.org/abs/1707.06347"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1707.06347</a>). If I understand it correctly there are multiple iterations of weight updates done on the model with data that is created from the environment (with the model before the update). Now I think that the updates of the model weights are not correct anymore after the first iteration/optimization step because the model weights changed and therefore the training data is outdated (since the model would now give different actions in the environment and therefore different rewards).</p>

<p>Basically in the pseudo code of the algorithm I don't understand the line ""Optimize surrogate L ... with K epochs..."". If the update is done for multiple epochs the data that is learned on is outdated already after the first iteration of optimization since the model weights changed. In other algorithms like A2C there is only one opimization step done instead of K epochs.</p>

<p><a href=""https://i.stack.imgur.com/Fpcbx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fpcbx.png"" alt=""enter image description here""></a></p>

<p>So is this some form of approximation or augmentation on the data by using the data that was created by an older model for multiple iterations or am I missing something here? And if yes where was this idea first introduced or better described? And where is a (empirical) proof that this still leads to a correct weight updating?</p>

<p>I am new to the RL field so I am sorry if this is an obvious question.</p>
"
1368,"<p>As someone who knows basics of machine learning, I have doubts that mathematical models are answer for general AI. I am not sure if it is possible to represent emotions, intuition, knowledge and so on with mathematical models? Do we need a new approach in order to solve this problem?</p>
"
1369,"<p>I have installed jFuzzylite and now I want to use it in java but I could not find any documentation or examples to learn how to use this package please could you kindly tell me what should I do?</p>
"
1370,"<p>I am seeking the information for this kind of chatbot architecture : There are two chatbots. One plays the role of teacher, and another is a student who is learning. The goal is to test the student's quality, and to improve the student's ability.</p>

<p>I didn't find much reference. There are :</p>

<p><a href=""http://Bottester:%20Testing%20Conversational%20Systems%20with%20Simulated%20Users"" rel=""nofollow noreferrer"">Bottester: Testing Conversational Systems with Simulated Users</a></p>

<p>And the <a href=""http://parl.ai/static/docs/basic_tutorial.html#"" rel=""nofollow noreferrer"">ParlAI</a>, a python-based platform for enabling dialog AI research has the notion of ""Teacher agent"", which seems to be what I am looking for.</p>

<p>Of course, we also have deep reinforcement learning which might be related.</p>

<p>I prefer to have some classical references for this approach to chatbots.
Currently, reinforcement learning is not in my consideration.</p>

<p>Constructing two chatbots talking to each other, like what Facebook did, is not what I want. Because in this case, both of them are student agents.</p>
"
1371,"<p>What is supposed to happen first: Strong AI or Technological Singularity? </p>

<p>Meaning which option is more likely, that the Strong AI that will bring as to the state of technological singularity or achieving technological singularity will allow us to construct strong AI?</p>
"
1372,"<p><strong>Summary:</strong>
I am teaching bots to pick food on a playing field. Some food is poisonous and some is good.</p>

<p><strong>Food Details:</strong>
Poisonous food  subtracts score points and good food adds.
Food points vary based on it's size.
There is about 9:1 ratio of poisonous food to good food, so a lot more chances to end up in negative numbers.
Food grows in points overtime.
Food spoils after some predetermined size becoming poisonous.</p>

<p><strong>Fitness Function:</strong>
The fitness function I use is simply counting points by the end of iterations. Bot's might choose to eat it or skip it.</p>

<p><strong>The Problem:</strong>
The problem I am having is that first generation, most bots eat a lot of bad crap and the curious ones end up in negative numbers. So mostly the ones that make it are the ones that are lazy and didn't eat or didn't head towards the food and most of the time the fittest for first few generations comes out with 0 points and 0 eats of any kind of food. When trained for long time they just end up waiting for the food instead of eating multiple times. Often while they wait food goes bad and they just end up going to another food. This way in the end of the iteration, I have some winners but they are nowhere near the potential they could have been at.</p>

<p><strong>Question:</strong>
I somehow need to weight the importance of eating food. I want them to eventually learn to eat.</p>

<p><strong>So I thought of this:</strong></p>

<pre><code>brain.score += foodValue * numTimesTheyAteSoFar
</code></pre>

<p>But this blows up the score too much and now the food quality is not respected and they just gulp on anything slightly above 0. </p>

<p>Please help.</p>
"
1373,"<p>I'm trying to implement a custom version of YOLO neural network. Originally it was described in this <a href=""https://arxiv.org/pdf/1506.02640.pdf"" rel=""noreferrer"" title=""paper"">paper</a>. I have some problems understanding the loss function they used. </p>

<p>Basic information:</p>

<ul>
<li>An input image is divided into S by S grid (that gives the total of S^2 cells) and each cell predicts B bounding boxes and c conditional class probabilities. Each bbox predicts 5 values: x,y,w,h,C (center of bbox, width and height and confidence score). This makes the output of yolo a SxSx(5B*c) tensor.</li>
<li>The (x,y) coordinates are calculated relative to the bounds of the cell and (w,h) is relative to the whole image.</li>
<li>I understand that the first term penalizes the wrong prediction of the center of a bbox; 2-nd term penalizes wrong width and height prediction, 3-rd term - wrong confidence prediction, 4-th is responsible for pushing confidence to zero when there is no object in a cell; the last term penalizes wrong class prediction.</li>
</ul>

<p>My problem:</p>

<p>I don't understand when 1^obj_ij should be 1 or 0. In the paper, they write: ""1^obj_ij denotes that the j-th bbox predictor in i-th cell is responsible for that prediction"" and also ""Loss function only penalizes bounding box coordinate error if that predictor is responsible for ground truth box"". </p>

<p>So is it right that for every object in the image there should be exactly one pair of ij such taht 1^obj_ij=1? And if this is correct, this means that the center of the ground truth bbox should fall into i-th cell, right? </p>

<p>If this is not the case, what are other possibilities when 1^obj_ij=1 and what ground truth labels x_i and y_i should be in these cases?</p>

<p>Also, I assume that ground truth p_i(c) should be 1 if there is an object of class c in the cell i, but what ground truth p_i(c) should be equal to in case there are several objects of different classes in the cell?</p>

<p><a href=""https://i.stack.imgur.com/MPXKM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MPXKM.png"" alt=""YOlO loss function""></a></p>
"
1374,"<p>I'm new to this AI/Machine Learning and was playing around with OpenAI Gym a bit. When looking through the environments I came across ""Blackjack-v0"" which is a basic implementation of the game where the state is the hand count of the player and the dealer and if the player has an useable ace. The actions are only hit or stand and the possible rewards 1 if the player wins, -1 if the player loses and 0 when draw.</p>

<p>So that got me thinking what a more realistic environment/model for this game would look like, taking into account the current balance and other factors and has multiple actions like betting 1-10€ and hit or stand.</p>

<p>This brings me to my actual question:</p>

<ul>
<li>As far as I understand neural networks (and I do not very well yet, I guess) the input will be the state and the output the possible actions and how good the network thinks they are/will be. But now there are two different action spaces which apply to different states of the game (betting or playing), so some of the actions are useless. How would be the right way to approach this scenario?</li>
</ul>

<p>I'm guessing one answer would be to give some kind of negative reward if the network guesses an useless action but in this case I think the reward should be the actual stake (negative reward) and the actual win if any. Therefor this would cause some bias in how the game proceeds as it should start with some amount of balance and end if the balance is 0 or after a specified amount of rounds.</p>

<p>Limiting timesteps wouldn't be an option either I guess because it should be limited to rounds so it won't end after a betting step e.g.</p>

<p>Therefore, for a useless step the reward would be 0 and the state would stay the same but for the network it doesn't matter how many useless steps it takes because it'll make no difference to the actual outcome.</p>

<p>Corollary question:</p>

<ul>
<li>Should be split up into two neural networks? One for betting and one for playing?</li>
</ul>
"
1375,"<p>In the diagram below, although the flow of information happens from the input to output layer, the labeling of weights appears reverse. Eg: For the arrow flowing from X3 to the fourth hidden layer node has the weight labeled as W(1,0) and W(4,3) instead of W(0,1) and W(3,4) which would indicate data flowing from the 3rd node of the 0'th layer to the 4th node of the 1st layer.  </p>

<p><a href=""https://i.stack.imgur.com/YyD9E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YyD9E.png"" alt=""enter image description here""></a></p>

<p>One of my neural networks teachers did not emphasize on this convention at all. Another teacher made it a point to emphasize on it.  </p>

<p>Is there a reason there is such an un-intuitive convention and is there really a convention?</p>
"
1376,"<p>I'm training Seq2Seq model on OpenSubtitles dialogs - <a href=""http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html"" rel=""nofollow noreferrer"">Cornell-Movie-Dialogs-Corpus</a>. </p>

<p>My work based on the following papers (but currently I'm not implemented Attention yet):</p>

<ul>
<li><a href=""https://arxiv.org/abs/1409.3215"" rel=""nofollow noreferrer""><em>Sequence to Sequence Learning with Neural Networks, Sutskever et al. 2014</em></a></li>
<li><a href=""https://arxiv.org/abs/1506.05869"" rel=""nofollow noreferrer""><em>A Neural Conversational Model, Vinyals, Le, 2015</em></a></li>
</ul>

<p>The <code>loss</code> I received is quite high and sucked in variation <code>~6.4</code> after 3 epoches. The model predicts the most common words with some times other not significant words (but 99.99% is just 'you'):</p>

<ul>
<li>I’ve experimented with 128 - 2048 hidden units and with 1 or 2 or 3 LSTM layers per <code>encoder</code> and <code>decoder</code>. The outcomes are more or less the same.</li>
</ul>

<blockquote>
  <p>SEQ1: yeah man it means love respect community and the dollars too the package the unk end</p>
  
  <p>SEQ2: but how did you get unk 82 end</p>
  
  <p>PREDICTION: promoting 16th dashboard be of the the the you you you you you you you you you you you you you you you you you you you you you you you you</p>
</blockquote>

<p>I'm using here <code>greedy</code> prediction, meaning - after I receive <code>logit</code> I do <code>argmax(..)</code> on all its value for first-3 mini-batch-elements (here I present only first element). For convenient - <code>SEQ1</code> and <code>SEQ2</code> are also printed - to know the actual dialog which was presented to the model.</p>

<p>The pseudo-code of my architecture looks like this (I'm using Tensorflow 1.5):</p>

<pre><code>seq1 = tf.placeholder(...)
seq2 = tf.placeholder(...)

embeddings = tf.Variable(tf.random_uniform([vocab_size, 100],-1,1))

seq1_emb = tf.nn.embedding_lookup(embeddings, seq1)
seq2_emb = tf.nn.embedding_lookup(embeddings, seq1)

encoder_out, state1 = tf.nn.static_rnn(BasicLSTMCell(), seq1_emb)
decoder_out, state2 = tf.nn.static_rnn(BasicLSTMCell(), seq2_emb,
                                                        initial_state=state_1)
logit = Dense(decoder_out, use_bias=False)

crossent = tf.nn.saparse_softmax_cross_entropy_with_logits(logits=logit, 
                                                         labels=target)
crossent = mask_padded_zeros(crossent)
loss = tf.reduce_sum(crossent) / number_of_words_in_batch

train = tf.train.AdamOptimizer(learning_rate=0.00002).minimize(loss) 
</code></pre>

<p>I'm also wonder if I pass well <code>state1</code> to <code>decoder</code>, which in general looks like this:</p>

<pre><code># reshape in pseudocode: state1 = state[1:]
new_state1 = []
for lstm in state1:
    new_lstm = []
    for gate in lstm:
        new_lstm.append(gate[1:])
    new_state1.append(tuple(new_lstm))
state1 = tuple(new_state1)
</code></pre>

<ul>
<li>Should I use some projection layer between states of <code>encoder</code> and <code>decoder</code> ?</li>
</ul>

<p>So if <code>seq1</code> has 32 words, <code>seq2</code> has 31 (since we will not predict nothing after the last word, which is the tag <code>&lt;END&gt;</code>).</p>
"
1377,"<p>Google Analytics allows me to collect data about every web-session. For simplicity, let's assume for each user, we collect the number of pages and time spent on site for each session:</p>

<pre><code>user_id visit_id page_views time_spent result
1       1        10         100        0
1       2        31         510        0
1       3        1          10         1
</code></pre>

<p>How would you model this data? What I would like the <strong>ML algorithm to
    do</strong>: </p>

<ol>
<li>Extract as much information as possible</li>
<li>Have a flexible number of
inputs (e.g. the number of sessions can go to infinity)</li>
</ol>

<p><strong>What I can think of</strong>:</p>

<ol>
<li>Aggregate the data per user e.g. average page_views or total page_views and feed it into a general algorithm e.g. random forrest (but I lose information with aggregation)</li>
<li>Use LSTM and feed at most last 3 visits (will also lose information, but would this perform better than aggregation?)</li>
</ol>

<p><strong>Goal:</strong>
To build a predictive model to analyse all user sessions and make a prediction whether the person will convert or not.</p>
"
1378,"<p>Please if you know good research about limitations of neural network give me a link. There are a lot papers about expressive power of neural networks but I need researches about limitations. I know about the Universal approximation theorem, but I need research that enplane - what function neural network can't learn computational efficient or can't learn by gradient algorithms.</p>
"
1379,"<p>I've seen these terms thrown around this site a lot, specifically in the tags <a href=""/questions/tagged/convolutional-neural-networks"" class=""post-tag"" title=""show questions tagged &#39;convolutional-neural-networks&#39;"" rel=""tag"">convolutional-neural-networks</a> and <a href=""/questions/tagged/neural-networks"" class=""post-tag"" title=""show questions tagged &#39;neural-networks&#39;"" rel=""tag"">neural-networks</a>.</p>

<p>I know that a Neural Network is a system based loosely on the human brain. But what's the difference between a <em>Convolutional</em> Neural Network and a regular Neural Network? Is one just a lot more complicated and, ahem, <em>convoluted</em> than the other? </p>
"
1380,"<p>I am reading through the NEAT paper <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">here</a>. On page 14 of the PDF, there is this quote about mutation:</p>

<blockquote>
  <p>There was an 80% chance of a genome having its connection weights mutated, in which case <strong>each weight had a 90% chance of being uniformly perturbed</strong> and a 10% chance of being assigned a new random value.</p>
</blockquote>

<p>What exactly does it mean to perturb weights? What is uniform vs. nonuniform perturbation?</p>

<p>Is there an established method to do this? I am imagining the process as multiplying each connection weight by a random number, but I'm unfamiliar with the term.</p>
"
1381,"<p>Let us for these purposes say with are working with any feed forward neural network.</p>

<p>Let us also say, that we know beforehand that certain portion of our dataset arsignificantly more impactful or important to our underlying representation. Is there anyway to add that “weighting” to our data?</p>
"
1382,"<p>As we all know, there has been tons of GAN variants featuring different aspects of the image generation task such as stability, resolution or the ability to manipulate images. However, it is still confusing to me that how do we determine that images generated by one network are more plausible than images generated by another?</p>

<p>PS: could someone with higher reputation create more tags like image generation?</p>
"
1383,"<p>I have a fully connected network that takes in a variable length input padded with 0.</p>

<p>However the network doesn't seem to be learning and I am guessing that the high number of zeros in the input might have something to do with that. </p>

<p>Are there solution for dealing with padded input in fully connected layers or should I consider a different architecture?</p>

<p><strong>UPDATE (to provide more details)</strong>:</p>

<p>The goal of the network if to clean full file paths:
i.g.: </p>

<ul>
<li><code>/My document/some folder/a file name.txt &gt; a file name</code></li>
<li><code>/Hard drive/book/deeplearning/1.txt &gt; deeplearning</code></li>
</ul>

<p>The constraint is that the training data labels have been generated using a regex on the file name itself so it's not very accurate. </p>

<p>I am hopping that by treating every word equally (without sequential information) the network would be able to generalize as to which type of word is usually kept and which is usually discarded. </p>

<p>Then network takes in a sequence of word embedding trained on paths data and output a logits that corresponds to probabilities of each word to be kept or not.    </p>
"
1384,"<p>I have an idea about how to use neural networks but I'm not sure if it is possible or not.</p>

<p>In supervised learning we have a set of attributes labeled with an output value. I can use these set to train my network.</p>

<p>Now I have a network trained to get an output value from an random set of attributes but, <strong>can I use this trained network to get the input attributes using only the desired output?</strong></p>

<p>I will have N input values and only 1 output value. I've thought that I can use the weights for that network into a new one with 1 input value and N output values but I'm not sure if I can do that.</p>
"
1385,"<p>I used to work with 'traditional' layered neural network and I evaluated the output given certain inputs by processing layer-by-layer.<br>
With NEAT, a neural network may assume any topology and they are no longer layered, so how to evaluate the output? I understand time-steps must be taken into account, but how?<br>
Should I keep the inputs until all hidden neurons are processed and output is produced? Should I wait for output to stabilize?</p>
"
1386,"<p>In two player games, the exact value of the evaluation function doesn't matter as long as it's bigger for better positions. However, for learning, it's customary when it does change when the best move gets made. This way, the learning can minimize the difference between the directly computed value <code>f(0, p)</code> of a position <code>p</code> and the value obtained from <code>n</code> step minimax <code>f(n, p)</code>.</p>

<p>What I'm missing here is a way to direct the evaluation function to actually winning. For example, a <em>perfect</em> evaluation function for a won position in chess would always return <code>+1</code> without any hint how to progress towards checkmate. In a chess variant without the fifty-move limit, it could play useless turns forever.</p>

<p>I guess, this is a rather theoretical problem as we won't ever have such a good function, but I wonder <em>if there's a way to avoid it</em>?</p>
"
1387,"<p>How does one prove the uniqueness of the value function obtained from <a href=""http://artint.info/html/ArtInt_227.html"" rel=""nofollow noreferrer"">value iteration</a> in the case of bounded and undiscounted rewards? I know that this can be proven for the discounted case pretty easily using the <a href=""https://en.wikipedia.org/wiki/Banach_fixed-point_theorem"" rel=""nofollow noreferrer"">Banach fixed point theorem</a>.</p>
"
1388,"<p>I am working on a js library which focuses on error handling. A part of the lib is a stack parser which I'd like to work in most of the environments. </p>

<p>The hard part that there is no standard way to represent the stack, so every environment has its own stack string format. The variable parts are message, type and frames. A frame usually consists of called function, file, line, column. </p>

<p>In some of the environments there are additional variable regions on the string, in others some of the variables are not present. I can run automated tests only in the 5 most common environments, but there are a lot more environments I'd like the parser to work in. </p>

<ul>
<li>My goal is to write an adaptive parser, which learns the stack string format of the actual environment on the fly, and after that it can parse the stack of any exception of that environment. </li>
</ul>

<p>I already have a plan how to solve this in the traditional way, but I am curious, <strong>is there any machine learning tool (probably in the topic of unsupervised learning) I could use to solve this problem?</strong></p>

<p>According to the comments I need to clarify the terms ""stack string format"" and ""stack parser"". I think it is better to write 2 examples from different environments:</p>

<p>A.)</p>

<p><em>example stack string:</em></p>

<pre><code>Statement on line 44: Type mismatch (usually a non-object value used where an object is required)
Backtrace:
  Line 44 of linked script file://localhost/G:/js/stacktrace.js
    this.undef();
  Line 31 of linked script file://localhost/G:/js/stacktrace.js
    ex = ex || this.createException();
  Line 18 of linked script file://localhost/G:/js/stacktrace.js
    var p = new printStackTrace.implementation(), result = p.run(ex);
  Line 4 of inline#1 script in file://localhost/G:/js/test/functional/testcase1.html
    printTrace(printStackTrace());
  Line 7 of inline#1 script in file://localhost/G:/js/test/functional/testcase1.html
    bar(n - 1);
  Line 11 of inline#1 script in file://localhost/G:/js/test/functional/testcase1.html
    bar(2);
  Line 15 of inline#1 script in file://localhost/G:/js/test/functional/testcase1.html
    foo();
</code></pre>

<p><em>stack string format (template):</em></p>

<pre><code>Statement on line {frames[0].location.line}: {message}
Backtrace:
{foreach frames as frame}
  Line {frame.location.line} of {frame.unknown[0]} {frame.location.path}
    {frame.calledFunction}
{/foreach}
</code></pre>

<p><em>extracted information (json):</em></p>

<pre><code>{
    message: ""Type mismatch (usually a non-object value used where an object is required)"",
    frames: [
        {
            calledFunction: ""this.undef();"",
            location: {
                path: ""file://localhost/G:/js/stacktrace.js"",
                line: 44
            },
            unknown: [""linked script""]
        },
        {
            calledFunction: ""ex = ex || this.createException();"",
            location: {
                path: ""file://localhost/G:/js/stacktrace.js"",
                line: 31
            },
            unknown: [""inline#1 script in""]
        },
        ...
    ]
}
</code></pre>

<p>B.)</p>

<p><em>example stack string:</em></p>

<pre><code>ReferenceError: x is not defined
    at repl:1:5
    at REPLServer.self.eval (repl.js:110:21)
    at repl.js:249:20
    at REPLServer.self.eval (repl.js:122:7)
    at Interface.&lt;anonymous&gt; (repl.js:239:12)
    at Interface.EventEmitter.emit (events.js:95:17)
    at Interface._onLine (readline.js:202:10)
    at Interface._line (readline.js:531:8)
    at Interface._ttyWrite (readline.js:760:14)
    at ReadStream.onkeypress (readline.js:99:10)
</code></pre>

<p><em>stack string format (template):</em></p>

<pre><code>{type}: {message}
{foreach frames as frame}
{if frame.calledFunction is undefined}
    at {frame.location.path}:{frame.location.line}:{frame.location.column}
{else}
    at {frame.calledFunction} ({frame.location.path}:{frame.location.line}:{frame.location.column})
{/if}
{/foreach}
</code></pre>

<p><em>extracted information (json):</em></p>

<pre><code>{
    message: ""x is not defined"",
    type: ""ReferenceError"",
    frames: [
        {
            location: {
                path: ""repl"",
                line: 1,
                column: 5
            }
        },
        {
            calledFunction: ""REPLServer.self.eval"",
            location: {
                path: ""repl.js"",
                line: 110,
                column: 21
            }
        },
        ...
    ]
}
</code></pre>

<p>The parser should process the stack strings and return the extracted information. The stack string format and the variables are environment dependent, the library should figure out on the fly how to parse the stack strings of the actual environment.</p>

<p>I can probe the actual environment by throwing exceptions with well known stacks and check the differences of the stack strings. For example if I add a whitespace indentation to the line that throws the exception, then the column and probably the called function variables will change. If I detect a number change somewhere, then I can be sure that we are talking about the column variable. I can add line breaks too, which will cause line number change and so on...</p>

<p>I can probe for every important variables, but I cannot be sure that the actual string does not contain additional unknown variables and I cannot be sure that all of the known variables will be added to it. For example the frame strings of the ""A"" example contain an unknown variable and do not contain the column variable, while the frame strings of the ""B"" example do not always contain the called function variable.</p>
"
1389,"<p>I am trying to understand backpropagation. I used a simple neural network with one input <strong><em>x</em></strong>, one hidden layer <strong><em>h</em></strong> and one output layer <strong>y</strong>, with weight <strong><em>w1</em></strong> connecting <strong><em>x</em></strong> to <strong><em>h</em></strong>, and <strong><em>w2</em></strong> connecting <strong><em>h</em></strong> to <strong><em>y</em></strong>.</p>

<blockquote>
  <p>x--[w1]--> h --[w2]-->y</p>
</blockquote>

<p>In my understanding these are the steps happening while we train a neural network:</p>

<p><a href=""https://i.stack.imgur.com/sGToV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sGToV.png"" alt=""enter image description here""></a></p>

<p>I understood most parts of backpropogation, but how do we get the gradients for the middle layer weights <code>dL/dw1</code>?</p>

<p>EDIT</p>

<pre><code>Latex

\\
Feed \ forwarding \\
h=\sigma (xw_{1}+b) \\ 
{y}'=\sigma (hw_{2}+b) \\ \\
Loss \ function \\ 
L=\frac{1}{2}\sum(y-{y}')^{2} \\ \\ 
Gradient \ calculation \\ \\
\frac{\partial L}{\partial w_{2}}=\frac{\partial {y}'}{\partial w_{2}}\frac{\partial L }{\partial {y}'} \\ \\ 
\frac{\partial L}{\partial w_{1}}= \frac{\partial h}{\partial w_{1}} \frac{\partial {y}'}{\partial h} \frac{\partial L}{\partial {y}'}   \\ \\ % DuttaA's solution
Weight \ update \\ 
w_{i}^{t+1} \leftarrow w_{i}^{t}-\alpha \frac{\partial L}{\partial w_{i}}
</code></pre>

<p>How should we calculate gradient of a network similar to this?
<a href=""https://i.stack.imgur.com/P5fce.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P5fce.png"" alt=""enter image description here""></a></p>

<p>is this the correct equation?</p>

<p><a href=""https://i.stack.imgur.com/aOjdc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aOjdc.png"" alt=""enter image description here""></a></p>

<p>Latex format</p>

<pre><code>\frac{\partial L}{\partial w_{1}}=\frac{\partial h_{1}}{\partial w_{1}}\frac{\partial w_7}{\partial h_{1}}\frac{\partial o_2}{\partial w_{7}}\frac{\partial L}{\partial o_{2}}  + \frac{\partial h_{1}}{\partial w_{1}}\frac{\partial w_5}{\partial h_{1}}\frac{\partial o_1}{\partial w_{5}}\frac{\partial L}{\partial o_{1}}
</code></pre>
"
1390,"<p>So, as you may already know, in multivariate linear regression (linear regression with more than one variable) the model is <code>yi = b0 + b1x1i + b2x2i + ...</code> and so on.  But my question is how is the w_n value calculated iteratively? Can it be calculated non-iteratively? What is the intuition behind using that method to calculate <code>b2</code>?</p>
"
1391,"<p>I am wondering, when one uses NEAT to evolve best fitting network for the job, does training take place in each epoch as well?</p>

<p>If I understand correctly, training is adjustment of weights in the gates via back propagation process. During NEAT say a generation runs through 1000 iterations. During that time, is there any training involved, or does each genome randomly poke around and the winner takes it to the next stage?</p>

<p>I am wondering because I use NEAT and somehow not training networks in the process is not logical to me, but at the same time I can't find any code in my framework(Neataptic.js) that would train the generation during the epoch.</p>
"
1392,"<p>I know that this question is very common but i am very confused about where to start?</p>

<p>first let me introduce my self I am php developer and I have 3 years of experience in programming now i want to move my career into AI for that i have also learned basic of Python and also started learning linear algebra.</p>

<p>I have also research many posts to start career in AI but most of posts path are not same like:
-In 1 post I have found that i should learn python first, then linear algebra, probability, statistics and then i need to take any course of ML from Udemy or any other study platform.</p>

<p>-In another post I found that math is really no required at beginning phase so i need to jump directly to ML course.</p>

<p>Different people to different views that's common thing I know they all are right for different perspective.</p>

<p>can anyone give a guideline from where <strong>I</strong> should start because i am a programmer i don't need to learn analytics and all other non programming field.</p>

<p>Thanks.</p>
"
1393,"<p>What are the advantages/ strengths and disadvantages/weakness of programming languages like Common Lisp, Python and Prolog? Why are these languages used in the domain of artificial intelligence? What type of problems related to AI are solved using these languages? </p>

<p>Please, give me link to papers or books regarding the mentioned topic.</p>
"
1394,"<p>So, currently the most commonly used activation functions are Re-Lu's. So I answered this question <a href=""https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks/5521#5521"">What is the purpose of an activation function in Neural Networks?</a> and while writing the answer it struck me, how exactly can Re-Lu's approximate non-linear function? </p>

<p>By pure mathematical definition, sure, its a non-linear function due to the sharp bend, but if we confine ourselves to the positive or the negative portion of the x-axis only, then its linear in those regions. Let's say we take the whole x-axis also, then also its kinda linear (not in strict mathematical sense) in the sense that it cannot satisfactorily approximate curvaceous functions like sine wave (<code>0 --&gt; 90</code>) with a single node hidden layer as is possible by a sigmoid activation function. </p>

<p>So what is the intuition behind the fact that Re-Lu's are used in NN's, giving satisfactory performance (I am not asking the purpose of Re-lu's) even though they are kind of linear? Or are non linear functions like sigmoid and tanh thrown in  the middle of  the network sometimes?</p>

<p>EDIT: As per @Eka's comment Re-Lu derives its capability from discontinuity acting in the deep layers of Neural Net. Does this mean that Re-Lu's are good as long as we use it in Deep NN's and not a shallow NN?</p>
"
1395,"<p>I'm a software developer who keeps trying (and failing) to get my head around AI and neural networks. There is one area that sparked my interest recently - simulating a mouse ""homing in"" on a piece of cheese by following the smell. Based on the rule that moving closer to the cheese = stronger smell = good, then it feels like it should be quite a simple problem to solve - in theory at least!</p>

<p>My thought process was to start by placing the mouse and cheese in random positions on the screen. I would then move the mouse one step in a random direction and measure its distance to the cheese, and if it's closer than before (stronger smell) then that's good. This is where I come unstuck on the theory - this ""feedback"" somehow needs to modify the mechanism used to move the mouse, gradually refining it until the mouse is able to head straight towards the cheese.
Once ""trained"", I should be able to reposition the cheese and expect the mouse to travel to it more quickly. <em>Note I'm also keeping things simple by not having obstacles for the mouse to negotiate around.</em></p>

<p>How on earth would this be implemented with a NN? I understand the basic concepts, but I find that things unravel once I start looking at real code! The examples I've seen typically start by training the NN from a data set, but this doesn't seem to apply here as it feels like the only training available is ""on the fly"" as the mouse moves around (i.e. closer = good, further away = bad). I'm assuming the brain has some kind of ""reward mechanism"" triggered by a stronger smell of cheese.</p>

<p>Am I barking up the wrong tree - either with my thought process, or NN not being a good fit for this problem? This isn't homework btw, just something that I've been puzzling over in the back of my mind.</p>
"
1396,"<p>I'm having trouble wrapping my head around some details of neural nets and back prop.</p>

<p>For example's sake, consider the following net, where I have separated the 'neurons' into linear nodes plus activation (in this case sigmoid) nodes, more like a general computation graph. L2 is the squared loss function.</p>

<p><a href=""https://i.stack.imgur.com/xz0CA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xz0CA.png"" alt=""enter image description here""></a></p>

<p>This is a 3 part question (parts 1 and 2 disregarding linear algebra / vectorization). </p>

<p><strong>1.</strong> I want to confirm that I cannot just apply chain rule all the way from the L2 to the inputs and get the derivatives for weights at different layers.</p>

<p>For example:</p>

<p><a href=""https://i.stack.imgur.com/wE4t7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wE4t7.png"" alt=""enter image description here""></a></p>

<p>But </p>

<p><a href=""https://i.stack.imgur.com/E1PG9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/E1PG9.png"" alt=""enter image description here""></a></p>

<p>For w5, at o1 I took the derivative with respect to w5. But for w1, at o1 I had to instead take the derivative with respect to ah1 to keep moving backwards.</p>

<p><strong>2.</strong> I want to confirm that at any node that branches into more than one node at forward time (i.e. ah1), when I back prop, I have to add the derivatives of all it's branches: in this case do1/dah1 and do2/ah1.</p>

<p><strong>3.</strong> If these 2 things are as I say, then how can I implement backprop in vectorized + linear algebra way without branching or using conditional logic.</p>

<p>The forward pass is easy:</p>

<ul>
<li>x1 + x2 are a single input vector.</li>
<li>h1 + h2 are a matrix of 2 rows (number of output / units) and 2 columns (number of inputs).</li>
<li>ah1 + ah2 are a single function that can operate on vectors (i.e. 1/(1 + np.exp()))</li>
</ul>

<p>The same for the next layer. So to do a forward pass, I just do: 
L2(y, a_o(W_o @ a_h(W_h @ x))) (@ = matrix mul)</p>

<p>But for the back pass not so much, since I'd have to check which derivative to use at o1 and I'd have to sum o1 and o2 at ah1 and ah2.</p>

<p><a href=""http://neuralnetworksanddeeplearning.com/chap2.html"" rel=""nofollow noreferrer"">http://neuralnetworksanddeeplearning.com/chap2.html</a> this shows that in theory it can be done by always transposing the previous weight matrix, but I haven't fully understood it yet.</p>
"
1397,"<p>I was trying to build a prediction system where I have the input data arranged in multiple columns. The input data would be of the type where I have weather, service type (bronze, silver, gold), size(xs, s, m, l, xl, xxl), time, availability, pin code and the result (target). Each of the data types is arranged in columns with a specific code. I have read <a href=""https://stackoverflow.com/questions/48385376/multi-label-prediction-using-dnn"">this</a>, <a href=""https://stackoverflow.com/questions/33369706/multi-label-cross-entropy"">this</a>, <a href=""https://stackoverflow.com/questions/44613753/prediction-using-svm-regression"">this</a> , <a href=""https://stackoverflow.com/questions/29625263/prediction-using-linear-regression-with-sklearn"">this</a>, and <a href=""https://stackoverflow.com/questions/3712863/continuous-prediction-in-google-prediction-api"">this</a>.  </p>

<p>They are helpful but do not give me a clear picture. I would like to achieve multi-vs-one prediction. Most of the schemes available are one-vs-one where the data is a 1*1 entity. </p>

<p>Here is a sample code that I was working with: </p>

<pre><code>regressionModel = linear_model.LinearRegression()
    """""" 3. Processing is not necessary for current concept """"""
    y = pd.DataFrame(modifiedDFSet['Code'])
    print(y.shape)
    drop2 = ['Code']
    X = modifiedDFSet.drop(drop2)
    print(X.shape)
    """""" 4. Data Scaling, Data Imputation is not necessary. Training and Test data is prepared using train-test-split """"""
    train_data, test_data = train_test_split(X, test_size=0.20, random_state=42)
    """""" 5. the Regression Model """"""
    # h = .02  # step size in the mesh
    # logreg = linear_model.LinearRegression()
    # we create an instance of Neighbours Classifier and fit the data.
    regressionModel.fit(X, y)
    d_predictions = regressionModel.predict(y)
</code></pre>

<p>X.shape and y.shape would yield 
(500, 6) &amp; 
(500, 1) respectively. 
Which would obviously cause a dimensional error in the <strong><em>d_predictions</em></strong> meaning the regression model does not take multiple column inputs.</p>

<p>I have a hypothesis that I can create a scoring scheme that will take into account the importance of each of the columns and create a scheme that creates a score and the end result would be a one-vs-one regression problem. <strong>Looking for some direction with respect to my hypothesis. Is it correct, wrong or halfway?</strong> </p>

<p>Any help and I am grateful already. Cheers people. </p>
"
1398,"<p>I have hundreds of .txt files. I need to get into each one of them and remove certain paragraphs that start with specific words but as a whole, are not exactly the same every time.
Is there an automatic way that can help me clean these parts out? If yes, what is it? 
If not, is it easy/quick to create my own AI tool for this job? Assuming that I need to get this done very soon, does it take a lot of time to learn how to create an AI tool to get the job done for me?</p>

<p>Thanks in Advance!</p>
"
1399,"<p>I'm developing a millitary game and I want soldiers to place themselves in specific order, when commander orders. </p>

<p>How can I do this? Should agents learn somehow how to achieve given order or there exist other way?</p>

<p>I'm new in AI, so I do not really know how can I accomplish my goal. 
Thanks for every advice.</p>
"
1400,"<p>I'm trying to design an orbital rendezvous program for Kerbal Space Program.  I'm focusing on when to launch my spacecraft so that I can end up in the general vicinity of the target.  If I can control the ascent profile, the remaining dependent variables are the ship's twr and the target's altitude.  I want to try a computer learning solution.</p>

<p>What is the best way to formulate the problem of learning the time to launch based on some twr?  </p>

<p>How can I make an algorithm to compute the general equation of launch time to a altitude based on my ability to accelerate?  What type of learning problem could this be classified as?  What are some approaches to solve problems with known dependent variables?</p>

<p>This may be an obvious question, I kind of expect the answer is regression?  But it seemed a general enough question to be sure about to get a solid foothold in computer learning with this type of problem, which seems to come up a lot.</p>
"
1401,"<p>It's an academic point but according to the definition of a fully observable environment in Russell &amp; Norvig, AIMA (2nd ed) p41-44, an environment is only fully observable if it requires zero memory for an agent to perform optimally. I.e. all relevant information is immediately available from sensing the environment. From this definition and from the definition of a ""episodic"" environment in the same book, it's implied that all fully observable environments are in-fact episodic or can be treated as episodic. Which doesn't seem intuitive, but logically follows from the definitions. Also no stochastic environment can be fully observable, even if the entire state space at a given point in time can be observed because rational action may depend on previous observation that must be remembered. Am I wrong?</p>
"
1402,"<p>Came across this line while reading the <a href=""https://arxiv.org/abs/1506.02025"" rel=""nofollow noreferrer"">original paper</a> on Spatial Transformers by Deepmind in the last paragraph of Sec 3.1:</p>

<blockquote>
  <p>The localisation network function floc() can take any form, such as a fully-connected network or a convolutional network, but should include a final regression layer to produce the transformation parameters θ.</p>
</blockquote>

<p>I understand what regression is but what is meant by a regression layer?</p>
"
1403,"<p>I used to treat back propagation as a black box but lately I want to understand much more about it. I have used <a href=""https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"" rel=""nofollow noreferrer"">mattmuzr's</a> and <a href=""https://ai.stackexchange.com/a/5620/39"">DuttA's</a> explanaiton as a guide to hand compute a simple neural network. I have computed feed forward and back propagation to a network similar to this one with one input, one hidden and one output</p>

<p><a href=""https://i.stack.imgur.com/hapD0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hapD0.jpg"" alt=""enter image description here""></a></p>

<p>Here are my computations</p>

<p><a href=""https://i.stack.imgur.com/DxTjP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DxTjP.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/PNCZv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PNCZv.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/fe5iP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fe5iP.png"" alt=""enter image description here""></a></p>

<p>Is my computations correct?</p>

<p><a href=""https://pastebin.com/YgjywbCr"" rel=""nofollow noreferrer"">*Full latex code</a></p>
"
1404,"<p>I'm working on a q-learning project that involves a ""robot"" solving a maze, and there is a problem with how I update the Q values (every time the robot ends up switching between two squares instead of actually learning) but I'm not sure where: I am at my wits end. Any pointers are welcome, here is the minimal viable example (I really can't condense it much more).. Thanks!</p>

<pre><code>from enum import Enum
import numpy as np
from random import randrange
import string
import random

class Direction(Enum):
    up=0
    down=1
    left=2
    right=3

stepsTaken=0
nbMaxSteps=500
Q = {}
gamma=0.95
strat=1
epsilon=0.99
maze=[]
penalty=0
#values of each movement
step=-1
stepTrap=-20
stepExit=500
stepWall=-100
#current position of the robot
position=[0, 0]

#funciton that checks if a certain place in the Q matrix is empty, returns 1 if it is
def currentQEmpty():
    global Q
    global position
    moves=[]
    if (position[0]!=0):
        moves.append(Direction.left)
    if (position[0]!=cols-1):
        moves.append(Direction.right)
    if (position[1]!=0):
        moves.append(Direction.down)
    if (position[1]!=rows-1):
        moves.append(Direction.up)
    for d in moves:
        if (Q.get((position[0],position[1],d),'A')=='A'):
            return 1
    return 0

#intialise the Q matrix
cols=10
rows=10
values=np.zeros((rows,cols))
for x in range(rows):
        for y in range(cols):
            for dir in Direction:
                Q[(x, y, dir)] = 0

#fills the Q matrix (replaces empty values only)
def QFill(moves):
    global maze
    global position
    global Q
    global step
    global stepTrap
    global stepWall
    global stepExit
    global gamma
    for d in moves:
        reward=0
        newpos=position
        if d==Direction.up:
            newpos=[position[0], position[1]+1]
        if d==Direction.down:
            newpos=[position[0], position[1]-1]
        if d==Direction.left:
            newpos=[position[0]-1, position[1]]
        if d==Direction.right:
            newpos=[position[0]+1, position[1]]
        reward=reward+values[newpos[0],newpos[1]]
        if(Q.get((position[0],position[1],d),0)==0):
            Q[position[0],position[1],d]=reward

#Qmove: decides which move to make depending on current Q values
#this is where the issue is!
def Qmove(moves):
    global position
    global Q
    global step
    global stepTrap
    global stepWall
    global stepExit
    global gamma
    bestd=0
    newd=moves[random.randint(0,len(moves)-1)]
    for d in moves:
        newpos=position
        if d==Direction.up:
            newpos=[position[0], position[1]+1]
        if d==Direction.down:
            newpos=[position[0], position[1]-1]
        if d==Direction.left:
            newpos=[position[0]-1, position[1]]
        if d==Direction.right:
            newpos=[position[0]+1, position[1]]
        #update value to best value of new position
        if Q.get((newpos[0],newpos[1],d),0)&gt;=Q.get((newpos[0],newpos[1],bestd),0):
            bestd=d
        Q[position[0],position[1],d]=Q.get((position[0],position[1],d),0)+ (values[newpos[0]][newpos[1]] + gamma * Q.get((newpos[0],newpos[1],bestd),1) - Q.get((position[0],position[1],d),0))      
        #update arrow
        if Q.get((position[0],position[1],d),0)&gt;Q.get((position[0],position[1],newd),0):
            newd=d
    return newd

#create maze
ch=['0', '1', '3']
for i in range(cols):
    maze.append([0]*(cols))
    for j in range(cols):
        random_index = randrange(0,len(ch))
        maze[i][j]=ch[random_index]
        if i==cols-1 and j==cols-1:
            maze[i][j]='5'
        if i==0 and j==0:
            maze[i][j]='0'
        if(maze[i][j]==""1""):
            values[i][j]=step
        elif(maze[i][j]==""0""):
            values[i][j]=stepWall
        elif(maze[i][j]==""3""):
            values[i][j]=stepTrap
        else:
            values[i][j]=stepExit
#move
while(stepsTaken&lt;nbMaxSteps):
    moves=[]
    #if he finishes he starts over
    if(position[0]==rows-1 and position[1]==cols-1):
        position[0]=0
        position[1]=0
        penalty=0
    #identify the moves he can legally make
    if (position[0]!=0):
        moves.append(Direction.left)
    if (position[0]!=cols-1):
        moves.append(Direction.right)
    d=moves[0]
    if (position[1]!=0):
        moves.append(Direction.down)
    if (position[1]!=rows-1):
        moves.append(Direction.up)
    dest=[]
    #choose epsilon value
    rand=random.uniform(0, 1)
    if(rand&lt;epsilon**stepsTaken):
        strat=1
        #explore
    else:
        strat=2
        #exploit
    #print(epsilon**stepsTaken)
    if(currentQEmpty() or strat==1):
        QFill(moves)
        d=moves[random.randint(0,len(moves)-1)]#how and why he moves
        print('dumb')
    else:
        d=Qmove(moves)
        print('smart')
    if(d==Direction.left):
        dest.append(position[0]-1) #x decreases by 1 place
        dest.append(position[1]) #y does not change
    if(d==Direction.right):
        dest.append(position[0]+1) #x increases by 1 place
        dest.append(position[1]) #y does not change
    if(d==Direction.up):
        dest.append(position[0]) #x does not change&amp;&amp;
        dest.append(position[1]+1) #y increases by 1
    if(d==Direction.down):
        dest.append(position[0]) #x does not change
        dest.append(position[1]-1) #y decreases by 1
    #penalty is calculated
    penalty=penalty+values[dest[0]][dest[1]]
    if(maze[dest[0]][dest[1]]!='0'): #not a wall
        position=dest
    stepsTaken=stepsTaken+1
    #show Q matrix
    x=position[0]
    y=position[1]
    print(""x:"",x,"" y:"",y)
    print("" UP:%s"" % Q.get((x,y, Direction.up)))
    print("" DOWN:%s"" % Q.get((x,y, Direction.down)))
    print("" LEFT:%s"" % Q.get((x,y, Direction.left)))
    print("" RIGHT:%s\n"" % Q.get((x,y, Direction.right)))
</code></pre>
"
1405,"<p>How could we express the following as STRIPS operators?</p>

<p>A computer has memory cells M1, ..., Mn and registers R1, ..., Rm. Two computer instructions could be:</p>

<p>LOAD(M,R) (copy contents of M into R, overwriting what’s there)</p>

<p>ADD(M,R) (add contents of M to contents of R, leaving result in R)</p>
"
1406,"<p>I'm new to AI development and am looking for a quality algorithm (potentially nlp?) implementation proved against US legal texts.</p>

<p>Obviously some training would need to be done, but I've found little to no online references to go on when it comes to running assessment against US legal documents.</p>

<p>My goal is to use an algorithm to discover potential issues in long and complex legal texts, or associated (groups) of legal texts which bind one or more related entities (people or corporations) to potentially conflicting clauses.</p>

<p>Just a pointer in some kind of direction would be helpful.</p>
"
1407,"<p>I am currently learning c #</p>

<blockquote>
  <p>Any advice with this language about courses or should use another programming language,Preferred languages ​​Spanish or English</p>
</blockquote>
"
1408,"<p>This is a theoretical question. I am newbie to artificial intelligence and machine learning, and the more I read the more I like this. So far, I have been reading about evaluation of language models (I am focused on ASR), but I still don't get the concept of development test. The clearest explanation I have come across is the following</p>

<pre><code>""Sometimes we use a particular test set so often that we implicitly tune to its 
characteristics. We then need a fresh test set that is truly unseen. In such cases, 
we call the initial test set the development test set or, devset""
</code></pre>

<p>Nevetheless I have not found sense as for why an additional test has to be used. In other words, why aren't training and test sets enough?</p>

<p>Thanks in advance!</p>
"
1409,"<p>Interested to know if there was any use or interest in activation functions with more than one output value to the next column instead of  single firing.</p>

<p>I'm interested to know if this would have any use or would be almost identical to a single value firing.</p>

<p><a href=""https://i.stack.imgur.com/kmGh2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kmGh2.png"" alt=""enter image description here""></a></p>
"
1410,"<p>I want to learn a policy network for a <a href=""https://www.jasondavies.com/domineering/#5x5"" rel=""nofollow noreferrer"">Domineering game</a></p>

<ul>
<li><p>For each position I have to recall the input (i.e. the board, the flipped board,
and the turn) and the output of the same size as the board with only
the best move found by the Monte Carlo evaluation marked as 1.</p></li>
<li><p>For instance csv lines for a 2x2 board : </p>

<p>0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0</p></li>
</ul>

<p>which corresponds to board, flipped board, player plane, move to learn</p>

<p>Corresponding input tensor :</p>

<pre><code>0 0 1 1 1 1
0 0 1 1 1 1
</code></pre>

<p>Corresponding output tensor :</p>

<pre><code>1 0
0 0
</code></pre>

<p>From <a href=""http://www.lamsade.dauphine.fr/~cazenave/domineering.csv.zip"" rel=""nofollow noreferrer"">here</a> I have a 8*8 board games database. With this tutorial I already developed a failing neural network. Indeed I did 12 nodes for 128 inputs there seems to be a problem with the first layer <code>model.add(Dense(12, input_dim=128, activation='relu'))</code>.</p>

<pre><code># model construction

model = Sequential()
model.add(Dense(12, input_dim=128, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(128, activation='sigmoid'))

print(""compile"")
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print(""fit"")
model.fit(X_test, Y_test, epochs=3, batch_size=10)
</code></pre>

<p>In effect it gives me the following error value:</p>

<pre><code>ValueError: Error when checking target: expected dense_27 to have shape (128,) but got array with shape (127,)
</code></pre>

<p>And when I try to replace with 127, just to see, it says:</p>

<pre><code>ValueError: Error when checking input: expected dense_28_input to have shape (127,) but got array with shape (128,)
</code></pre>

<p>Here is the entire code, which you can get <a href=""https://github.com/antoinecomp/NestedMonteCarlo"" rel=""nofollow noreferrer"">on GitHub</a> as well. It's the ipython notebook.</p>

<pre><code>#!/usr/bin/env python3
from timeit import default_timer as timer

import csv 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use(""ggplot"")

from keras.models import Sequential
from keras.layers import Dense

# we divide data.csv into train and tests 

with open(""data.csv"", 'r') as f:
    plays = np.array(list(csv.reader(f, delimiter="","")))
    print(plays.shape)    
# We take the 126 first columns as input
df = pd.DataFrame(data=plays[0:28961,1:256])
# We take the 126 last columns as output
Y = pd.DataFrame(data=plays[0:28961,129:256])

#plays.reshape((64,64))

#board = np.reshape(plays, (8, 8))

df['split'] = np.random.randn(df.shape[0], 1)
msk = np.random.rand(len(df)) &lt;= 0.7

train_df = df[msk].fillna(""sterby"")
test_df = df[~msk].fillna(""sterby"")

# we take the 128 first columns has input
X_train = train_df.iloc[:,0:128].values
# we take the 128 last columns has input
y_train = train_df.iloc[:,129:].values
X_test = test_df.iloc[:,0:128].values
Y_test = test_df.iloc[:,129:].values

# Necesary Keras Importations

from keras.preprocessing import sequence
from keras.models import Model, Input
from keras.layers import Dense, Embedding, GlobalMaxPooling1D
from keras.preprocessing.text import Tokenizer
from keras.optimizers import Adam

# model construction

model = Sequential()
model.add(Dense(12, input_dim=128, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(128, activation='sigmoid'))

print(""compile"")
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

print(""fit"")
model.fit(X_test, Y_test, epochs=3, batch_size=10)

print(""evaluate"")

# evaluate the model
scores = model.evaluate(X_test, Y)
print(""\n%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
</code></pre>

<p>And the error :</p>

<pre><code>compile 
fit
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-60-8b0dc569bd70&gt; in &lt;module&gt;()
      3 
      4 print(""fit\n"")
----&gt; 5 model.fit(X_test, Y_test, epochs=3, batch_size=10)
      6 
      7 # evaluate the model

/usr/local/lib/python3.5/dist-packages/keras/models.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
    961                               initial_epoch=initial_epoch,
    962                               steps_per_epoch=steps_per_epoch,
--&gt; 963                               validation_steps=validation_steps)
    964 
    965     def evaluate(self, x=None, y=None,

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)
   1628             sample_weight=sample_weight,
   1629             class_weight=class_weight,
-&gt; 1630             batch_size=batch_size)
   1631         # Prepare validation data.
   1632         do_validation = False

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)
   1478                                     output_shapes,
   1479                                     check_batch_axis=False,
-&gt; 1480                                     exception_prefix='target')
   1481         sample_weights = _standardize_sample_weights(sample_weight,
   1482                                                      self._feed_output_names)

/usr/local/lib/python3.5/dist-packages/keras/engine/training.py in _standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    121                             ': expected ' + names[i] + ' to have shape ' +
    122                             str(shape) + ' but got array with shape ' +
--&gt; 123                             str(data_shape))
    124     return data
    125 

ValueError: Error when checking target: expected dense_12 to have shape (128,) but got array with shape (127,)
</code></pre>
"
1411,"<p>The result of gradient descent algorithm is a vector. So how does this algorithm decide the direction for weight change? We Give hyperparameters for step size. But how is the vector direction for weight change, for the purpose of reducing the Loss function in a Linear Regression Model, determined by this algorithm?</p>
"
1412,"<p>The lithographs of dutch artist M.C. Escher have been used in the study of artificial Intelligence. How can the human mind Incorporate these optical illusions into abstract thought? Is this reverse artificial intelligence?</p>
"
1413,"<p>I am trying to understand the difference between the workings biological evolution and  artificial evolution. If we look at it in terms of genetics, in both of them, selection is the key term, natural selection in biological way and selection as in genetic algorithms than what's the difference in between artificial and biological evolution?</p>
"
1414,"<p>I am not clear with the concept that an unsupervised model learns. We are giving an input and output to the supervised model so that it can generate a particular value, pattern or something out of it which can be used to categorize something in the future. By contrast, in unsupervised learning we are clustering and all so why do we need learning?</p>

<p>Can anyone detail me with some real world examples?</p>
"
1415,"<p>I asked my self this simple question while reading <a href=""https://web.stanford.edu/class/cs224n/reports/2762092.pdf"" rel=""nofollow noreferrer"">""Comment Abuse Classification with Deep Learning""</a> by Chu and Jue. Indeed, they say at the end of the that </p>

<blockquote>
  <p><em>It is clear that RNNs, specifically LSTMs, and CNNs are state-of-the-art  architectures for sentiment analysis</em></p>
</blockquote>

<p>To my mind CNNs were only neurons arranged so that they correspond to overlapping regions when paving the input field. It wasn't that recurrent at all.</p>
"
1416,"<p>I'm reading <a href=""https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks"" rel=""nofollow noreferrer"">the AlexNet paper</a> . In the section 4 where the authors explain how they prevent overfitting, they mention "" Although the 1000 classes of ILSVRC make each training example impose 10 bits of constraint on the mapping from image to label, ~"". What does this mean?</p>
"
1417,"<p>I was reading AI For Humans Vol. 1 by Jeff Heaton when I came across the terms ""equilateral encoding"" and ""one-of-n encoding."" The explanations unfortunately made no sense to me and the reddit threads on the Web are blocked by my Internet provider (I use a high-school machine). Is anyone here able to provide basic explanations regarding the two procedures for me? Thanks in advance.</p>
"
1418,"<blockquote>
  <p>UPDATE: The tables look messed up so i put them on pastebin for better
  visibility. <a href=""https://pastebin.com/gDX28uVF"" rel=""nofollow noreferrer"">https://pastebin.com/gDX28uVF</a></p>
</blockquote>

<p>I am using Neural Network with different learning types (for example Standard Backpropagation) to classify trends in time series. As stated in several papers, data normalization is a very important factor for successful / efficient learning. I am trying to be clear and precise as possible in the description.</p>

<p><strong>Problem / Learning Goal:</strong></p>

<p>The network gets trained with time series and 2 indicators to predict a specific cluster.  Here is a very simple (madeup) example of raw data to understand the problem:</p>

<p><strong>Example RAW Data</strong>
Timestamp;DensityX;WaveLengthY;Temperature (K)</p>

<p>1;0.1;2;200</p>

<p>2;0.9;3;150</p>

<p>3;-0.5;1;175</p>

<p>4;0;6;154</p>

<p>5;1;8;155</p>

<p>6;1.3;1.5;220</p>

<p>7;-0.5;3.4;250</p>

<p>8;0.2;2;255</p>

<p>9;0.1;1;180</p>

<blockquote>
  <p>see <a href=""https://pastebin.com/gDX28uVF"" rel=""nofollow noreferrer"">https://pastebin.com/gDX28uVF</a> for better visual</p>
</blockquote>

<p>I use the following process to generate suitable sample data for training:</p>

<p>The neural network receives n time slices with the indicators and tries to check if a future trend in the temperature occurs (for x future time slices).
For example n = 2; x=3. </p>

<p>The input and output are defined as follows:</p>

<p><strong>Input vector:</strong></p>

<ul>
<li>In1 =  Density_(t-2) </li>
<li>In2 =  Wavelength_(t-2) </li>
<li>In3 = Density_(t-1) </li>
<li>In4 = Wavelength_(t-1)</li>
</ul>

<p><strong>Output vector:</strong></p>

<p>Output Vector is a classification encoded by Effects Encoding or Dummy Encoding (Details in “Neural Networks using C# Succinctly”)</p>

<p>Calculation:</p>

<ul>
<li>Classification “Down” :  Temperature drops 3 times in a row (Encoded as 0;1)</li>
<li>Classification: “Stable”:    Temperature does neither drop nor raises 3 times in a row (1;0)</li>
<li>Classification: “Up”:    Temperature raises 3 times in a row. (-1;-1;)</li>
</ul>

<p>So the “processed” training sample would look like this:</p>

<p><strong>Processed Data</strong></p>

<p>Pattern;I1;I2;I3;I4;O1;O2;Class;Used TS</p>

<p>1;0.1;2;0.9;3;0;1;Down;1 to 5</p>

<p>2;0.9;3;-0.5;1;-1;-1;Up;2 to 6</p>

<p>3;-0.5;1;0;6;-1;-1;Up;3 to 7</p>

<p>4;0;6;1;8;1;0;Stable;4 to 8</p>

<p>5;1;8;1.3;1.5;1;0;Stable;5 to 9</p>

<blockquote>
  <p>see <a href=""https://pastebin.com/gDX28uVF"" rel=""nofollow noreferrer"">https://pastebin.com/gDX28uVF</a> for better visual</p>
</blockquote>

<p>As you can see due to the different indicators ranges I want to normalize the data.</p>

<p>Basically I found the following propositions in literature and research:</p>

<p><strong>Min/Max Normalization</strong></p>

<p>Requires the following values to calculate 
 - dataHigh: The highest unnormalized observation.</p>

<ul>
<li><p>dataLow: The lowest unnormalized observation.</p></li>
<li><p>normalizedHigh: The high end of the range to which the data will be normalized.</p></li>
<li><p>normalizedLow: The low end of the range to which the data will be normalized.</p></li>
</ul>

<p><strong>Reciprocal normalization</strong></p>

<p>Every value is processed to its reciprocal (x=1/x). Calculated values for density x would be:</p>

<p>Timestamp;Reciprocal Density</p>

<p>1;10</p>

<p>2;1.111111111</p>

<p>3;-2</p>

<p>4;#DIV/0!</p>

<p>5;1</p>

<p>6;0.769230769</p>

<p>7;-2</p>

<p>8;5</p>

<p>9;10</p>

<blockquote>
  <p>see <a href=""https://pastebin.com/gDX28uVF"" rel=""nofollow noreferrer"">https://pastebin.com/gDX28uVF</a> for better visual</p>
</blockquote>

<p><strong>Percentage normalization</strong></p>

<p>The percentual delta is calculated using the value from the previous time stamp.</p>

<p>The starting point was Timestamp 1 where the Delta equals 0. 
For each timestamp the delta percentage is calculated evaluating the previous value. So calculating the time series delta percentages would turn out to:</p>

<p>Timestamp;""Delta Density X""</p>

<p>1;0</p>

<p>2;0.9</p>

<p>3;-0.555555556</p>

<p>4;0</p>

<p>5;#DIV/0!</p>

<p>6;1.3</p>

<p>7;-0.384615385</p>

<p>8;-0.4</p>

<p>9;0.5</p>

<blockquote>
  <p>see <a href=""https://pastebin.com/gDX28uVF"" rel=""nofollow noreferrer"">https://pastebin.com/gDX28uVF</a> for better visual</p>
</blockquote>

<p><strong>As you can see there are errors with handling zero values and the range is still a problem in my opinion. The Min/Max approach is generally leads to a good normalization but I think there is a problem as well, because live data may breach the max and min values of the training set.</strong></p>

<p>My questions are:</p>

<ul>
<li>What are your thoughts about the general idea how I process the raw data?</li>
<li><p>How would you normalize the given data – if at all?</p>

<p>a) Does it make sense for MinMax Normalization to propose a min max value which will include live data (And throw some error in case it happens)</p>

<p>b) How to handle 0 values (maybe convert it to a small positive or negative value?</p></li>
<li><p>Are there other ideas or concepts to conduct this problem?</p></li>
</ul>

<p>I am looking forward to your input. Everything is appreciated. Thanks in advance! I also apologize for errors in the example values. Anyways, thanks for your time.</p>

<p>Cheers, hob.</p>
"
1419,"<p>Could you implement code into an AI that can't be modified? Like if you place a code that shuts-down the program/machine would they be able to rewrite/ reinterpret the ideas?</p>
"
1420,"<p>I'm new to neural network, I study electrical engineering, and i just started working with <a href=""https://en.wikipedia.org/wiki/ADALINE#Learning_algorithm"" rel=""noreferrer"">ADALINEs</a>.</p>

<p>I use Matlab, and in their <a href=""https://www.mathworks.com/help/nnet/ug/adaptive-neural-network-filters.html"" rel=""noreferrer"">Documentation</a> they cite :</p>

<blockquote>
  <p>However, here the <strong>LMS (least mean squares) learning rule</strong>, which is
  much more powerful than the perceptron learning rule, is used. The
  LMS, or Widrow-Hoff, learning rule minimizes the mean square error and
  thus moves the decision boundaries as far as it can from the training
  patterns.</p>
</blockquote>

<p>The LMS algorithm is the default learning rule to linear neural network in Matlab, but few days later i came across another algorithm which is : <a href=""https://en.wikipedia.org/wiki/Recursive_least_squares_filter"" rel=""noreferrer"">Recursive Least Squares (RLS)</a> in a 2017 Research Article by <strong>Sachin Devassy</strong> and  <strong>Bhim Singh</strong> in the journal : <strong>IET Renewable Power Generation</strong>, under the title : <a href=""http://ieeexplore.ieee.org/document/8049622/"" rel=""noreferrer"">Performance analysis of proportional resonant and ADALINE-based solar photovoltaic-integrated unified active power filter</a> where they state :</p>

<blockquote>
  <p>ADALINE-based approach is an efficient method for extracting
  fundamental component of load active current as no additional
  transformation and inverse transformations are required. The various
  adaptation algorithms include <strong>least mean square</strong>, <strong>recursive least</strong>
  <strong>squares</strong> etc.</p>
</blockquote>

<p><strong>My questions are:</strong></p>

<ul>
<li>Is RLS just like LMS (i mean can it be used as a learning
algorithm too) ? </li>
<li>If yes, how can i customize my ADALINE to use RLS instead of LMS as
a learning algorithm (preferably in Matlab, if not in Python) because i want to do a comparative study between the two Algorithm !</li>
</ul>
"
1421,"<p>I want to use a custom loss function which is a weighted combination of l1 and DSSIM losses. The DSSIM loss is limited between 0 and 0.5 where as the l1 loss can be orders of magnitude greater and is so in my case. How does backpropagation work in this case? For a small change in weights, the change of the l1 component would obviously always be far greater than the SSIM component. So, it seems that only l1 part will affect the learning and the SSIM part would almost have no role to play. Is this correct? Or I am missing something here. I think I am, because in the DSSIM implementation of Keras-contrib, it is mentioned that we should add a regularization term like a l2 loss in addition to DSSIM (<a href=""https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py"" rel=""nofollow noreferrer"">https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py</a>); but I am unable to understand how it would work and how the SSIM would affect the backpropagation being totally overshadowed by the large magnitude of the other component. It will be a great help if someone can explain this. Thanks. </p>
"
1422,"<p>I am (planning on) using the Microsoft bot framework to build a chatbot I am working on.  However I simply cannot wrap my head around why the chatbot needs to be registered with Azure.</p>

<p>My line of expectation going into this was the same as any other .NET project.  I build it, I deploy it to my own server and I live happily ever after.</p>

<p>I have bought two books on the subject and yet after going through online videos, the books and articles I still cannot see a good reason for why one is forced to register the bot.   I feel like Lisa Simpson in that episode of The Simpsons where there is a puzzle that is super easy and everyone understands it but her.  I know I am being blinded by my own assumptions and expectations going into this.</p>

<p>If anyone can explain the 'why' of the registration that will really help.  I am also wondering if Azure went down like AWS did if that would stop the bot working?</p>

<p>My backup plan is just to write the bot using ASP.NET web API and then connect to it from a web container and facebook.  Neither of which needs the registration.  </p>

<p>Thanks,</p>

<p>-Mike</p>
"
1423,"<p>I've seen numerous mathematical explanations of reward, V(s) value functions, and returns functions. The reward provides an immediate return for being in a specific state. The better the reward, the better the state. </p>

<p>As I understand it, it can be better to be in a low-reward state sometimes because we can accumulate more long term which is where the expected returns function comes in. An expected return, return or cummulative reward function effectively adds up the rewards from the current state to the goal state. This implies it's model-based. However it seems a Value function does exactly the same?</p>

<p>Is a Value function a Return function? Or are they different?</p>
"
1424,"<p>Suppose that a NN contains <span class=""math-container"">$n$</span> hidden layers, <span class=""math-container"">$m$</span> training examples, <span class=""math-container"">$x$</span> features, and <span class=""math-container"">$n_i$</span> nodes in each layer. What is the time complexity to train this NN using back-propagation? </p>

<p>I have a basic idea about how they find the time complexity of algorithms, but here there are 4 different factors to consider here i.e. iterations, layers, nodes in each layer, training examples, and maybe more factors. I found an answer <a href=""https://www.researchgate.net/post/What_is_the_time_complexity_of_Multilayer_Perceptron_MLP_and_other_neural_networks"" rel=""nofollow noreferrer"">here</a> but it was not clear enough.</p>

<p>Are there other factors, apart from those I mentioned above, that influence the time complexity of the training algorithm of a NN?</p>
"
1425,"<p>I have seen weka j48 classifier, I want to build a classifier similar to it but I don't know how to go about it. 
Can anyone advice me on how to create a classifier algorithm for decision tree? </p>
"
1426,"<p>In a convolutional neural network (CNN), since the RGB values get multiplied in the first convolutional layer, does this mean that color is essentially only extracted in the very first layer?  </p>

<p>Snippets from <a href=""http://cs231n.github.io/understanding-cnn/"" rel=""nofollow noreferrer"">Stanford CS231n Chapter on CNN</a>:</p>

<blockquote>
  <p>[...] One dangerous pitfall that can be easily noticed with this
  visualization is that some activation maps may be all zero for many
  different inputs, which can indicate dead filters, and can be a
  symptom of high learning rates [...] Typical-looking activations on
  the first CONV layer (left), and the 5th CONV layer (right) of a
  trained AlexNet looking at a picture of a cat. Every box shows an
  activation map corresponding to some filter. Notice that the
  activations are sparse (most values are zero, in this visualization
  shown in black) and mostly local.</p>
</blockquote>
"
1427,"<p>Say you follow a tutorial on the tensorflow website for a wide and deep model (<a href=""https://www.tensorflow.org/tutorials/wide_and_deep"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/wide_and_deep</a>)</p>

<p>I create a model based on the US census data to predict whether or not an individual will make more or less than \$50k given a number of features like age, education, profession, etc. I've been able to create the model as well as create a predictor that uses the model just fine. But is there a way to see what features tensorflow is ""weighing"" more than others? For instance, does it weigh a higher education more than someones age? (i.e. if someone has a PHD and is 26 years old, is the model more likely to say they make more than \$50k vs someone who has an associates degree and is 55 years old?)</p>

<p>I'm using a DNNLinearCombinedClassifier if it matters to this question.</p>
"
1428,"<p>Given a dataset with no noisy examples (i.e., it is never the case that for 2 examples, the attribute values match but the class value does not), </p>

<p>Is the training error for the ID3 algorithm is always equal to 0?</p>
"
1429,"<p>Right now I'm planning to make a <strong>DNN</strong> for classifying taste of crystals with their molecular structure which include information like no of atoms mass of each atoms atomic no of atoms. How should I make a data set for training testing and validation?</p>
"
1430,"<p>I'm new to the world of machine learning. My question is how can I determine the size of the biases in a neural network (with backpropagation algorithm)? Currently, I have a 2-layer neural network (1 hidden and 1 output layer). Here's the code:</p>

<pre><code>import numpy as np
from matplotlib import pyplot as plt 

sigmoid = lambda x : 1 / (1 + np.exp(-x))
dsigmoid = lambda y: y * (1 - sigmoid(y))

# This function performs the given function (func) to the whole numpy array
def mapFunc(array, func) :
    newArray = array.copy()
    for element in np.nditer(newArray, op_flags=['readwrite']) :
        element[...] = func(element)
    return newArray

class NeuralNetwork :

def __init__(self, input_nodes, hidden_nodes, output_nodes) :
    self.input_nodes = input_nodes
    self.hidden_nodes = hidden_nodes
    self.output_nodes = output_nodes

    self.W_ih = np.random.rand(hidden_nodes, input_nodes)
    self.W_ho = np.random.rand(output_nodes, hidden_nodes)

    self.B_ih = np.random.rand(hidden_nodes, 1)
    self.B_ho = np.random.rand(output_nodes, 1)

    self.learningRate = 0.1

def predict(self, inputs) :
    # Calculate hidden's output
    H_output = np.dot(self.W_ih, inputs)
    H_output += self.B_ih
    H_output = mapFunc(H_output, sigmoid) # Activation

    # Calculate output's output
    O_output = np.dot(self.W_ho, H_output)
    O_output += self.B_ho
    O_output = mapFunc(O_output, sigmoid) # Activation

    return O_output

def train(self, inputs, target) :
    # Calculate hidden's output
    H_output = np.dot(self.W_ih, inputs)
    H_output += self.B_ih
    H_output = mapFunc(H_output, sigmoid) # Activation

    # Calculate output's output
    O_output = np.dot(self.W_ho, H_output)
    O_output += self.B_ho
    O_output = mapFunc(O_output, sigmoid) # Activation

    # Calculate output error :
    O_error = O_output - target

    # Calculate output delta
    O_gradient = mapFunc(O_output, dsigmoid)
    O_gradient = np.dot(O_gradient, np.transpose(O_error)) * self.learningRate

    W_ho_delta = np.dot(O_gradient, np.transpose(H_output))

    self.W_ho -= W_ho_delta
    self.B_ho -= O_gradient

    # Calculate hidden error :
    W_ho_t = np.transpose(self.W_ho)
    H_error = np.dot(W_ho_t, O_error)

    # Calculate hidden delta :
    H_gradient = mapFunc(H_output, dsigmoid)
    H_gradient = np.dot(H_gradient, np.transpose(H_error)) * self.learningRate

    W_ih_delta = np.dot(H_gradient, inputs)

    self.W_ih -= W_ih_delta
    self.B_ih += H_gradient

    return O_output


n = NeuralNetwork(2, 2, 1)

inputs = np.matrix([[1], [0], [1], [1], [0], [1], [0], [0]])

input_list = []
input_list.append([[1], [0]])
input_list.append([[0], [1]])
input_list.append([[1], [1]])
input_list.append([[0], [0]])

target = np.matrix([[0], [0], [1], [1]])

outputs = []
for i in range(50000) :
    ind = np.random.randint(len(input_list))
    inp = input_list[ind]
    out = n.train(inp, target[ind]).tolist()
    outputs.append(out[0][0])

print outputs
plt.plot(outputs)

plt.show()

newInput = [[1], [1]]
print (n.predict(newInput))
</code></pre>

<p>In the train function, the line <code>self.B_ih += H_gradient</code> throws me an error about their sizes not being equal. I even tried to make the biases only a single number but that didn't help as it gets changed by H_gradient to a matrix. So, is there something wrong in the bias itself or I did some other step(s) wrong?</p>
"
1431,"<p>My goal is to take an image and return another image that looks as if the scene was viewed from another angle.  The difference in angle can be small — let's say as if the hand holding the camera moved slightly sideways.</p>
"
1432,"<p>I did my Master's thesis on Deep Generative Models and I'm currently looking for a new subject.</p>

<p><strong>Q:</strong> What are the ""hottest"" research topics that are taking a lot of attention of the deep learning community lately?</p>

<p>A few clarifications:</p>

<ul>
<li>I did look through similar questions and none of them answered my
question.</li>
<li>I come from a pure mathematical background, I only
transitioned into deep learning a year ago, and my research on
generative models was mostly theoretical. Which means, most of my
work revolved around structured probabilistic models, and approximate
inference. That said, I have yet to explore real world applications of deep learning.</li>
<li>I did my homework before posing the question. My goal was to get ai SE's input on the matter and see what people are working on.</li>
</ul>
"
1433,"<p>How would you design and handle training NN where the label is 1000++ bit binary (50% ones, 50% zeros). The number of labels can be small OR big in different situations.</p>

<p>My question:  is such scenario well suited for NN ? If that seem like OK how will you design it.</p>

<p>For the sake of example let say I'm training the NN with MINST db. I.e. 10 labels.</p>

<p>Again the important thing is that the LABEL is with the specification I described. I can drop 50% ones, 50% zeros requirement if absolutely necessary,but the size of the binary have to stay.</p>
"
1434,"<p>My understanding is that the convolutional layer of a convolutional neural network has four dimensions: input_channels, filter_height, filter_width, number_of_filters.  Furthermore, it is my understanding that each new filter just gets convoluted over ALL of the input_channels (or feature/activation maps from the previous layer).  </p>

<p>HOWEVER, the graphic below from CS231 shows each filter (in red) being applied to a SINGLE CHANNEL, rather than the same filter being used across channels.  This seems to indicate that there is a separate filter for EACH channel (in this case I'm assuming they're the three color channels of an input image, but the same would apply for all input channels).  </p>

<p>This is confusing - is there a different unique filter for each input channel? </p>

<p><a href=""https://i.stack.imgur.com/3m7mW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3m7mW.png"" alt=""enter image description here""></a></p>

<p>Source: <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""noreferrer"">http://cs231n.github.io/convolutional-networks/</a></p>

<p>The above image seems contradictory to an excerpt from O'reilly's <a href=""https://rads.stackoverflow.com/amzn/click/1491925612"" rel=""noreferrer"">""Fundamentals of Deep Learning""</a>: </p>

<blockquote>
  <p>""...filters don't just operate on a single feature map. They operate
  on the entire volume of feature maps that have been generated at a
  particular layer...As a result, feature maps must be able to
  operate over volumes, not just areas""</p>
</blockquote>

<p>...Also, it is my understanding that these images below are indicating a <strong>THE SAME</strong> filter is just convolved over all three input channels (contradictory to what's shown in the CS231 graphic above):</p>

<p><a href=""https://i.stack.imgur.com/VdqER.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VdqER.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/kczF0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/kczF0.png"" alt=""enter image description here""></a></p>
"
1435,"<p>I wanted to start developing a project with image recognition. I want to know the difference between the Intel Movidius Neural Compute Stick and TensorFlow to develop this project Any help would greatly be appreciated. What is the difference between them and what is better for accuracy? Thanks, Aditya</p>
"
1436,"<p>When recording audio for screencasts or similar, very often the keyboard is clearly visible and can start to annoy listeners after a while. </p>

<p>NN are quiet good at recognizing patterns. Image classification is all over the place these days. There is also some work on audio, so that seems to work as well. Could the following approach therefore work to eliminate (or greatly reduce) the sounds of the keyboard in a recording whilst leaving the voice quality largely untouched?</p>

<ol>
<li>Train a NN to recognize the clicking sounds of the keys. Lots of labeled data can be created by just recording and tracking key clicks in the millisecond range. That way markers can be placed on the recording automatically that ""label"" clicks from non clicks. Let's say a click has on average a 10ms range in the audio, the audio feed could be cut into snippets of 10ms and those that have a click sound in it are labelled as such. </li>
<li>A adversarial network is trained to modify an input stream so as to fool the first one into thinking there are no clicks while also being punished for large changes in the stream data. So the better it removes the clicks sounds the better but if it just gives out nothing (technically no clicks then), it's of course bad so there needs to be some reward for being ""close to input""</li>
</ol>

<p>Would this be a good approach? Are there other ways to filter this? I know there is an ""ehm detector"" that uses MDP to warn speakers whenever they are likely to say ""ehm"". This wouldn't apply to this though, because it's not that I want to guess when the next click comes but rather I want to manipulate the input stream without running a constant filter on the entire stream such as a lowpass filter for removing unwanted <em>constant</em> noise. So ideally the algorithm would learn to apply a ""correction stamp"" whenever a click is detected to remove a range of frequencies during small windows in the overall recording but leaving most of it untouched. </p>
"
1437,"<p>I read that to compute the derivative of the error with respect to the input of a convolution layer is the same to make of a convolution between deltas of the next layer and the weight matrix rotated by <span class=""math-container"">$180°$</span>, i.e. something like</p>

<p><span class=""math-container"">$$\delta^l_{ij}=\delta^{l+1}_{ij} * rot180(W^{l+1})f'(x^l_{ij})$$</span></p>

<p>with <span class=""math-container"">$*$</span> convolution operator; This is valid with <span class=""math-container"">$stride=1$</span>; but what happens when stride is greater than <span class=""math-container"">$1$</span>? is still a convolution with a kernel rotation or I can't make this simplification?</p>
"
1438,"<p>There are many machine learning api for scanning images but they just return a bunch of tags. </p>

<p><a href=""https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/"" rel=""nofollow noreferrer"">https://azure.microsoft.com/en-gb/services/cognitive-services/computer-vision/</a></p>

<pre><code>{ ""tags"": [ ""train"", ""platform"", ""station"", ""building"", ""indoor"", ""subway"", ""track"", ""walking"", ""waiting"", ""pulling"", ""board"", ""people"", ""man"", ""luggage"", ""standing"", ""holding"", ""large"", ""woman"", ""yellow"", ""suitcase"" ],  ""confidence"": 0.833099365 } ] }
</code></pre>

<p>Are there any apis for combining these into a sentence?
MS Cognitive Vision is the only one that produces a full caption</p>

<pre><code>""captions"": [ { ""text"": ""people waiting at a train station"",
</code></pre>

<p>Google sentiment analysis can split a sentence into grammar parts but is there any api that does the reverse?</p>

<p><a href=""https://cloud.google.com/natural-language/docs/basics"" rel=""nofollow noreferrer"">https://cloud.google.com/natural-language/docs/basics</a></p>

<p>INPUT:
""train"", ""platform"", ""station"", ""building"", ""indoor"", ""subway"", ""track"", ""walking"", ""waiting"", ""pulling"", ""board"", ""people"",
 ""man"", ""luggage"", ""standing"", ""holding"", ""large"", ""woman"", ""yellow"", ""suitcase""</p>

<p>OUTPUT:
""people waiting at a train station""</p>
"
1439,"<p>I have a large set of simulation logs for a market simulation of which I want to learn from. The market includes:</p>

<ul>
<li>customers</li>
<li>products (subscriptions)</li>
</ul>

<p>The customers choose products and then stick with them until they decide on a different one. Examples could be phone, electricity or insurance contracts. </p>

<p>For every simulation I get the data about the customers (some classes and metadata) and then for each round I get signups/withdrawals and charges for the use of the service. </p>

<p>I am trying to learn a few things</p>

<ul>
<li>competitiveness of an offering (in relation to the environment/competition)</li>
<li>usage patterns of customers (the underlying model is a statistical simulation) depending on their chosen tariff, time of day and their metadata + historical usage</li>
<li>ability to forecast customer numbers for each product</li>
</ul>

<p>The use cases are all very applicable to real world data although my case is all a (rather large) simulation.</p>

<p>My problem is this: What kind of learning is this? Supervised? Unsupervised? I have come up with various hypotheses and cannot find a definite answer for either.</p>

<ul>
<li>Pro Supervised: For the usage patterns of the customers I have historical data of actual usage so I can do something similar to time-series forecasting. However, I don’t want to forecast simply off of their previous usage but also off of their metadata and their tariff choice (so also metadata in a way). </li>
<li>Pro Unsupervised: The forecasting of the “competitiveness” of a randomly chosen product configuration is hard to label even with historical data. The exact reason why a product has performed in a certain way is very high-dimensional. I do get subscription records about every product for every time slot though, so I guess some “feedback” could be generated. This might also be a RL problem though?</li>
</ul>

<p>So obviously I need help pulling these different concepts apart so as to map them on this kind of problem which is not the classical “dog or cat” problem or the classical “historical data here, please forecast” timeseries issue. It’s also not a “learn how to walk” reinforcement problem as it’s based on historical data. The end goal is however to write an agent that generates these products and competes in the market so that will be a reinforcement problem.</p>
"
1440,"<p>In Section 1.1 of Artificial Intelligence: A Modern Approach, it is stated that a computer which passes the Turing Test would need 4 capabilities, and that these 4 capabilities comprise most of the field of Artificial Intelligence:</p>

<ol>
<li>natural language processing: to enable it to communicate successfully in English</li>
<li>knowledge representation: to store what it knows and hears</li>
<li>automated reasoning: to use the stored information to answer questions and to draw new conclusions</li>
<li>machine learning: to adapt to new circumstances and to detect and extrapolate patterns</li>
</ol>

<p>Did Alan Turing discern the requirements for the field of artificial intelligence (the necessary subfields) and purposefully design a test around these requirements, or did he simply design a test that is so general that the subfields which developed within artificial intelligence happen to be what is required to solve it? That is, was he prescient or lucky? Are these Turing's subdivisions, or Peter Norvig's and Stuart Russell's?</p>

<p>If Turing did foresee these 4 requirements, what did he base them on? What principles of intelligence allowed him to predict the requirements for the field?</p>
"
1441,"<p>I have the following question.</p>

<p><a href=""https://i.stack.imgur.com/PVzCd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PVzCd.png"" alt=""enter image description here""></a> </p>
"
1442,"<p>I've just started learning Grammatical Evolution and I'm reading the paper <a href=""https://e098039d-a-62cb3a1a-s-sites.googlegroups.com/site/epscomputacionevolutiva/introduccion-a-la-programacion-automatica-evolutiva-evolucion-gramatical/grammatica_evolution_ieee_tec_2001.pdf?attachauth=ANoY7cp1rM9vbk-Cm-VUxPgX8XplxzM47859L5_Yj-F9tGdfKxfJFnqwcH2FZen6olXYuuAb0xBoKglZSuvGGcz7egrwOGeS1FixES3b1cpmhlKEm8cdgPp2P1umm6qcUfFJHwMvpybnfmSklvj0-3JAfaR1KY868iOln5iB3iUBjGP5EnBbnJsRdoXutpovtGhQxm8ZlToKmXXi7z7k95PRQGstABLy6KbDJNm8lWw_8tDxJsRsSa_Xav6N4jO0csNvhG8Vib45Oh8DvGKjurmyB-aS5D-BiRFuhX3Fdz_jHl5QIVXrHsH605WoIc10lhYxcJbCmNoYscl64cm5UKVHD3GcygrPntpJakrliiYb0GOb1WswY64%3D&amp;attredirects=1"" rel=""nofollow noreferrer"">Grammatical Evolution from Michael O’Neill</a>.</p>

<p>On page three said:</p>

<blockquote>
  <p>During the genotype-to-phenotype mapping process it is possible for
  individuals to run out of codons and in this case, we wrap the
  individual and reuse the codons.</p>
</blockquote>

<p>I'm not English speaker and I don't understand the meaning of the word <code>wrap</code> here. What does it mean?</p>

<p>I understood that if not of the symbols are terminals, we have to start from the begging of the genotype again and replace the nonterminal symbols until we have only terminal symbols. But, if I'm correct, when I have to stop? In the paper said also about non valid individuals.</p>
"
1443,"<p>I have following planning problem 
<a href=""https://i.stack.imgur.com/m83rG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/m83rG.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/agVUW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/agVUW.png"" alt=""enter image description here""></a></p>
"
1444,"<p>I have implemented multiple MCTS based AI players for the Love Letter game (<a href=""https://en.wikipedia.org/wiki/Love_Letter_(card_game)"" rel=""nofollow noreferrer"">rules</a>). It is a 2-4 players zero sum card game where players make alternating moves. I am struggling with how to properly conduct experiments for estimating AI player strength against human players: </p>

<ol>
<li>In 2 player game where one of the players is AI bot</li>
<li>In 4 player game where one (or multiple) of players is AI bot</li>
</ol>
"
1445,"<p>Basically, I want a system that reads blobs of text, parses it and stores related chunks and when the user asks a question, it returns with an in-context answer which groups together a bunch of facts. <em>The topic and the initial blob set is hardcoded.</em></p>

<p>I am noober than a newbie and I don't understand how the chunks would form relationships amongst themselves by logic and not by rule when the inference engine would run and also, how they would be stored together.</p>

<p>Also, if I am trying to reinvent the wheel, I'd be grateful if you could point me to open-source systems that already exist which just take in data and produce answers to questions after having performed some logical operations on it.</p>

<p>Thanks in advance.</p>
"
1446,"<p>I have started reading <a href=""https://books.google.com/books/about/Fundamentals_of_Deep_Learning.html?id=SL0BvgAACAAJ"" rel=""nofollow noreferrer"">Fundamentals of Deep Learning by Nikhil Buduma</a> and I have a question regarding tanh neurons. In the book, it is stated:</p>

<blockquote>
  <p>""When S-shaped nonlinearities are used, the tanh neuron is often preferred over the sigmoid neuron because it is zero-centered.""</p>
</blockquote>

<p>Can anyone explain me why exactly??</p>
"
1447,"<p>Suppose one trains a CNN to determine if something was either a cat/dog or neither (2 classes), would it be a good idea to assign all cats and dogs to one class and everything else to another? Or would it be better to have a class for cat, a class for dog and a class for everything else (3 classes)? My colleague argues for 3 classes because dogs and cats have different features, but I wonder if he's right.</p>
"
1448,"<p>I'm currently having troubles to win against a random bot playing the Schieber Jass game. It is a imperfect card information game. (famous in switzerland <a href=""https://www.schieber.ch/"" rel=""nofollow noreferrer"">https://www.schieber.ch/</a>) </p>

<p>The environement I'm using is on Github <a href=""https://github.com/murthy10/pyschieber"" rel=""nofollow noreferrer"">https://github.com/murthy10/pyschieber</a></p>

<p>To get a brief overview of the Schieber Jass I will describe the main characteristics of the game.
The Schieber Jass consists of four players building two teams.
At the beginning every player gets randomly nine cards (there are 36 cards).
Now there are nine rounds and every player has to chose one card every round. Related to the rules of the game the ""highest card"" wins and the team gets the points.
Hence the goal is to get more points then your opponent team.</p>

<p>There are several more rules but I think you can image how the game should roughly work.</p>

<p>Now I'm trying to apply a DQN approach at the game.</p>

<p>To my attempts:</p>

<ul>
<li>I let two independent reinforcement player play against two random players</li>
<li>I design the input state as a vector (one hot encoded) with 36 ""bits"" for every player and repeated this nine times for every card you can play during a game.</li>
<li>The output is a vector of 36 ""bits"" for every possible card.</li>
<li>If the greedy output of the network suggest an invalid action I take the action with the highest probability of the allowed actions</li>
<li>The reward is +1 for winning, -1 for losing, -0.1 for a invalid action and 0 for an action which doesn't lead to a terminal state</li>
</ul>

<p>My question:</p>

<ul>
<li>Would it be helpful to use a LSTM and reduce the input state?</li>
<li>How to handle invalid moves?</li>
<li>Do you have some good ideas for improvements? (like Neural-Fictitious Self-Play or something similar)</li>
<li>Or is this the whole approach absolute nonsense?</li>
</ul>
"
1449,"<p>Suppose a CNN is trained to detect bounding box of a certain type of object (people/cars/houses/etc.)</p>

<p>If each image in the training set contains just one object (and its corresponding bounding box,) how well can a CNN generalise to pick up all objects if the input for prediction contains multiple objects? </p>

<p>Should the training images be downsampled in order for the CNN to pick out multiple objects in the prediction?</p>

<p>EDIT: I don't have a specific one in mind. I was just curious about the general behaviour.</p>
"
1450,"<p>I am new to deep learning and computer vision. I have a problem where i use yolo algorithm (<a href=""https://pjreddie.com/"" rel=""nofollow noreferrer"">https://pjreddie.com/</a>) to detect objects. In the original paper, they define the output to recognize 80 classes, but for my problem i just want to recognize human only. </p>

<p>So i change the final layer to only 1 neuron, and do the training process with transfer learning techniques (used pretrained weights for the cases of 80 classes, of course not use the final layer weights and these weights becomes random number for my problems). I feed only human data to the algorithm. However, i realize that after longer training, the model becomes worse. It starts to recognize other objects as human. </p>

<p>I would like to hear any advice from you guys, should i also feed non-human data to the model.</p>

<p>Thanks</p>
"
1451,"<p>I need to efficiently align characters vertically using Multi Objective PSO. Alignment is achieved by adding spaces in between a given set of characters.</p>

<pre><code>a b c d e f
b b d h g
c a b f
</code></pre>

<p>Might be</p>

<pre><code>- a b - c d e f - -
- - b b - d - - h g
c a b - - - - f - -
</code></pre>

<p>Now this is a multi objective solution.
I need to maximize the characters that get aligned vertically and minimize the amount of spaces in between the characters.</p>

<p>I wanted to focus firstly on how to get a set of characters to represent a position of a particle. This would mean that I need to somehow transform a possible set of characters into a position of a particle. If I can somehow achieve this then the rest should fall into place.</p>

<ul>
<li>How do I transform these set of characters into a position of a particle? </li>
<li>Also is this the best approach or are there better ways to approach this problem?</li>
</ul>
"
1452,"<p>Is there AI open source software or service that can blur things like license plate number, house numbers and smears people faces (or better yet remove people and fills the background) automatically from image and video of outdoor shooting? </p>
"
1453,"<p><a href=""https://i.stack.imgur.com/Ko4R5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ko4R5.png"" alt=""ELU with α=5""></a></p>

<p>An exponential linear unit (as proposed by Clevert et al.) uses the function:</p>

<p>(Sorry--would have used MathJax were it available.)</p>

<p>ELU<sub>α</sub>(x) = α(e<sup>x</sup> - 1) (if x&lt;0), or x (if x≥0)</p>

<p>Now, this is continuous at x=0, which is great. It's differentiable there too if α=1, which is the value that the paper used to test ELU units.</p>

<p>But if α≠1 (as in the above diagram), then it's no longer differentiable at x=0. It has a crook in it, which seems weird to me. Having your function be differentiable at all points seems advantageous. Further, it seems that if you just make the linear portion evaluate to αx rather than x, that it <em>would</em> be differentiable there. Is there a reason that the function wasn't defined to do this? Or did they not bother, because α=1 is definitely the hyperparameter to use?</p>
"
1454,"<p>When Google researchers created AlphaGo, how did they simulate the game of Go? If I wanted to take the same approach to other games, like Risk, how would I go about coding the rules of the game? Is there a programming package, book, or general technique for coding the rules of a game for deep learning?</p>
"
1455,"<p>The Q function uses the (current and future) states to determine the action that gets the highest reward. </p>

<p>However, in a stochastic environment, the current action (at the current state) does not determine the next state.</p>

<p>How does Q learning handle this? Is the Q function only used during the training process, where the future states are known? And is the Q function still used afterwards, if that is the case?</p>
"
1456,"<p>What is the output value of the network for these inputs respectively, and why?
(Linear activation function is fine.)</p>

<p>[2, 3][-1, 2][1, 0][3, 4]</p>

<p>My main question is how you take the 'backwards' directed paths into account.</p>

<p><a href=""https://i.stack.imgur.com/H1xj8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H1xj8.png"" alt=""enter image description here""></a></p>
"
1457,"<p>Let us suppose I have a <code>NxN</code> matrix and I want to classify in <code>M</code> classes each entry of the matrix using a fuzzy classifier. The output of my classifier will be, for each matrix entry, an <code>M</code>-dimensional vector containing the probabilities for the entry to be classified in each class. 
A naive way to build a confusion matrix would be to select the highest probability in each vector and use it as a crips classification. However, I would like to take into account all the probabilities associated to each entry and compute a ""fuzzy"" confusion matrix. Is this possible?</p>
"
1458,"<p>How can I train a neural network to recognize sub-sequences in a sequence flow?</p>

<p>For example: Given the sequence <strong>111100002222</strong> as an input sample from a stream, the neural network would recognize that <strong>1111</strong> , <strong>0000</strong> , <strong>2222</strong> are sub sequences (so <strong>111100</strong> would not be a valid subsequence) and so on for ~ 50 to 100 different subsequences.</p>

<p>There is no particular order in which the subsequence would appear in the flow.
No network architecture restriction.
Subsequences are of variable length.</p>

<p>General concepts, ideas, and theory are welcome. </p>
"
1459,"<p>I've recently read the original <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">paper</a> about NeuroEvolution
of Augmenting Topologies by Kenneth O. Stanley and am now trying to prototype it myself in JavaScript. I stumbled across a few questions I can't answer.</p>

<hr>

<h2>My questions:</h2>

<ol>
<li><p>What is the definition of ""structural innovation"", and how do I store these so I can check if an innovation has already happened before?</p>

<blockquote>
  <p>However,
  by keeping a list of the innovations that occurred in the current generation, it
  is possible to ensure that when the same structure arises more than once through independent
  mutations in the same generation, each identical mutation is assigned the
  same innovation number</p>
</blockquote></li>
<li><p>Is there a reason for storing the type of a node (input, hidden, output)?</p></li>
<li><p>In the original paper, only connections have an innovation number, but in <a href=""http://www.automatonsadrift.com/neat/"" rel=""nofollow noreferrer"">other sources</a>, nodes do as well. Is this necessary for crossover? (This has already been asked <a href=""https://ai.stackexchange.com/questions/5496/neat-innovation-for-connection-genes-only"">here.</a>)</p></li>
<li><p>How could I limit the mutation functions to not add recurrent connections?</p></li>
</ol>

<p>I think that's it for now. All help is appreciated.</p>

<hr>

<h2>The relevant parts of my code:</h2>

<h3>Genome</h3>

<pre><code>class Genome {
    constructor(inputs, outputs) {
        this.inputs = inputs;
        this.outputs = outputs;

        this.nodes = [];
        this.connections = [];

        for (let i = 0; i &lt; inputs + outputs; i++) {
            this.nodes.push(new Node());
        }

        for (let i = 0; i &lt; inputs; i++) {
            for (let o = 0; o &lt; outputs; o++) {
                let c = new Connection(this.nodes[i], this.nodes[inputs + o], outputs * i + o);
                this.connections.push(c);
            }
        }

        innovation = inputs * outputs;
    }

    weightMutatePerturb() {
        let w = this.connections[Math.floor(random(this.connections.length))].weight;
        w += random(-0.5, 0.5);
    }

    weightMutateCreate() {
        this.connections[Math.floor(random(this.connections.length))].weight = random(-2, 2);
    }

    connectionMutate() {
        let i = this.nodes[Math.floor(random(this.nodes.length))];
        let o = this.nodes[Math.floor(random(this.inputs, this.nodes.length))];

        let c = Connection.exists(this.connections, i, o);

        if (c) {
            c.enabled = true;
        } else {
            this.connections.push(new Connection(i, o, innovation));
            innovation++;
        }
    }

    nodeMutate() {
        let oldCon = this.connections[Math.floor(Math.random(this.connections.length))];
        oldCon.enabled = false;

        let newNode = new Node();
        this.nodes.push(newNode);

        this.connections.push(new Connection(oldCon.input, newNode, innovation, 1));
        innovation++;
        this.connections.push(new Connection(newNode, oldCon.output, innovation, oldCon.weight));
        innovation++;
    }
}
</code></pre>

<h3>Node</h3>

<pre><code>class Node {
    constructor() {
        this.value = 0;
        this.previousValue = 0;
    }
}
</code></pre>

<h3>Connection</h3>

<pre><code>class Connection {
    constructor(input, output, innov, weight) {
        this.input = input;
        this.output = output;
        this.innov = innov;

        this.weight = weight ? weight : random(-2, 2);
        this.enabled = true;
    }

    static exists(connections, i, o) {
        for (let c = 0; c &lt; connections.length; c++) {
            if (connections[c].input === i &amp;&amp; connections[c].output === o) {
                return connections[c];
            }
        }
        return false;
    }
}
</code></pre>
"
1460,"<p>So I came across these 2 questions:</p>

<ul>
<li><a href=""https://ai.stackexchange.com/questions/5838/ideas-on-how-to-make-a-neural-net-learn-how-to-split-sequence-into-sub-sequences"">Ideas on how to make a neural net learn how to split sequence into sub sequences</a></li>
<li><a href=""https://ai.stackexchange.com/questions/4859/search-minimum-value-with-learning-machine-algorithm/4860#4860"">Search minimum value with learning machine algorithm</a></li>
</ul>

<p>For me both problems could be solved easily using traditional algorithmic techniques (as in coding in your typical programming language. I assume that training a NN (or any other machine learning technique) for such sorts of problems will be more time consuming, resource intensive and pointless.</p>

<p>My question is: If I want to solve a problem, how to decide whether it is better to solve algorithmically or by using NN/ML techniques? What are the pros and cons? How can this be done in a systematic way? And if I have to answer someone why I chose a particular domain, how should I answer?</p>

<p><strong>Summary:</strong> Choosing between normal computational approach vs abstract approach used in Neural Nets or ML or AI.</p>

<p>Example problems are appreciated :)</p>
"
1461,"<p>I have a neural network with 2 inputs and one output, like so:</p>

<pre><code>input    | output
____________________
 a    | b   |  c       
 5.15 |3.17 | 0.0607
 4.61 |2.91 | 0.1551
</code></pre>

<p>etc..</p>

<p>I have 75 samples and I am using 50 for training and 25 for testing. </p>

<p>However, I feel that the training samples are not enough. Because I can't provide more real samples (due to time limitation), I would like to train the network using fake data:</p>

<p>For example, I know that the range for the <code>a</code> parameter is from 3 to 14, and that the <code>b</code> parameter is ~65% of the <code>a</code> parameter. I also know that <code>c</code> is a number between 0 and 1 and that it increases when a &amp; b increase. </p>

<p>So, what I would like to do is to generate some data using the above restrictions (about 20 samples). For example, assume <code>a = 13</code> , <code>b = 8</code> and <code>c= 0.95</code>, and train the network with these samples before training it with the real samples.</p>

<p>Has anybody studied the effect of doing this on the neural network? Is it possible to know if the effect will be better or worse on the networks? and are there any recommendations/guidelines if I want to do this?</p>
"
1462,"<p>I created and operate a social network for meeting new people.  As a result of the recent FOSTA legislation, it’s imperative that I implement an automated system to prevent users from posting advertisements relating to prostitution. I do not have much expierence with AI/Machine learning.  What library, algorithm, method should I look into to solve this problem?</p>
"
1463,"<p>I am looking to learn Ai/Machine learning during my spare and need advice on tools used / What to read / How one can integrate machine learning with a simple website for user feedback</p>

<p><strong><em>Scenario:</em></strong><br>
<em>.Net Web Application in C#</em><br>
A simple website that a user can use to to track their fitness information... i.e One inputs weight (at the end of the day), diet (food eaten, calorie counting), exercises done for the day (calories burned). kind of like myfitnesspal.com in a way. From then on wards a user can view charts of their weight tracking to check/analyse progress of weight gain / weight loss etc. 
for example on user creation, one will insert a target goal to reach   </p>

<p>Target - 69kg<br>
Current - 85kg  </p>

<p>So they most likely want to track how far they are from their target weight loss on a day to day / weekly / monthly basis. So chart info in user's dashboard will show progress towards that goal. You get the idea... simple enough right?</p>

<p><a href=""https://i.stack.imgur.com/A1PwA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A1PwA.png"" alt=""Image example of information relayed to user from their daily input""></a></p>

<p><strong><em>Expansion:</em></strong><br>
Now say I want to attach a more intelligent hands off system that can analyse the user's data and provide feedback / advice on how improve or change habits from all the user's input over a period. Determine positive consistent patterns (or negative consistent patterns) with regards to them achieving their desired goal. Be it weight loss / bulking up / weight maintenance. </p>

<p>PYTHON seems to be coming up a lot when I google this, but none specific information on how to exactly go about this 
as in to attach python data analysis script/application to the .net site (could be PHP site or whatever).</p>

<p>What are my options here? Am I looking for Natural Language Processing, Machine Learning or Data Analysis. Whats my starting point or more popular tools to use with plenty of resources that one can have a look at. Creating the site isn't an issue.. its the more intelligent data analysis side that I want to dive into. F# has been mentioned here and there, but more posts seem to point out that there's more support or people using PYTHON which keeps popping up again and again. </p>

<p><strong><em>At the most basic level, what I hope to learn/achieve if for the intelligent side of the system to give feedback like</em></strong></p>

<p><strong>Machine response:</strong>  (from weekly/monthly view)<br>
<strong><em>""Your most efficient days of losing weight were on day X when you did exercise A and B, but you could maximize your progress if you ate food X from day C.""</em></strong>   </p>

<p>I do realize there there would be some Data Science involved to let the application know about healthy habits / healthy foods for weight loss etc.</p>

<p>So in general, if one has a .net site and learns machine learning / ai.. how do I attach this to my site. What tools are mostly used? Do I learn machine learning in Python and roll with the flow? There doesn't seem to be information that can guide a learner diving into this sort of implementation. Any advice on the path to take will be greatly appreciated.</p>

<p>Help me Obi-Wan-Stackexchange, you are my only hope. </p>
"
1464,"<p>Deep networks notoriously take a long time to train. What is the most time consuming aspect of training them? Is it the matrix multiplications? Is it the forward pass? Is it some component of the backward pass?</p>
"
1465,"<p>For example, hidden layer 1's outputs would be fed to the perceptrons in layer 2, 3, 4, ... etc.</p>

<p>Beyond computational power considerations, wouldn't this be better than only connecting layers 1 and 2, 3 and 4, etc? </p>

<p>My intuition is that humans combine simple decisions with more complex ones to form an answer. </p>

<p>Also, wouldn't this solve the vanishing gradient problem?</p>

<p>If computational power is the concern, perhaps you could connect layer 1 only to the next N layers.</p>
"
1466,"<p>I have absolutely no experience with any kind of AI and really want to create this:</p>

<p>A program that can train on a set of images to determine if an image is showing a fire flame or not (for fire detection). It should train on a set of ""flame images"".</p>

<p>I heard about the Keras python library which apparently allows to do this in 11 lines of code (<a href=""http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/"" rel=""nofollow noreferrer"">http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/</a>)...</p>

<p>Can anyone explain how I can make this work?</p>

<p>Thank you very much in advance!</p>
"
1467,"<p>Can we define the feeling of the human through conversations with an AI? Something like a ""confessional,"" disregarding human possibilities to lie.</p>

<p>Below, I have the categories joyful, sadness, anger, fear and affection. For each category, there are several words that can be in the texts that refer to it.</p>

<ul>
<li><p><strong>Joy:</strong> <strong>(</strong> cheerful, happy, confident, happy, satisfied, excited, interested, dazzled, optimistic, relieved, euphoric, drunk, witty, good <strong>)</strong></p></li>
<li><p><strong>Sadness:</strong> <strong>(</strong> sad, desperate, displeased, depressed, bored, lonely, hurt, desolate, meditative, defrauded, withdrawn, pitying, concentrated, depressed, melancholic, nostalgic <strong>)</strong></p></li>
<li><p><strong>Anger:</strong> <strong>(</strong> aggressive, critical, angry, hysterical, envious, grumpy, disappointed, shocked, exasperated, frustrated, arrogant, jealous, agonized, hostile, vengeful <strong>)</strong></p></li>
<li><p><strong>Fear:</strong> <strong>(</strong> shy, frightened, fearful, horrified, suspicious, disbelieving, embarrassed, embarrassed, shaken, surprised, guilty, anxious, cautious, indecisive, embarrassed, modest <strong>)</strong></p></li>
<li><p><strong>Affection:</strong> <strong>(</strong> loving, passionate, supportive, malicious, dazzled, glazed, homesick, embarrassed, indifferent, curious, tender, moved, hopeful <strong>)</strong></p></li>
</ul>

<p><strong>Flow Example</strong></p>

<p><strong>Phrase 1:</strong> ""I'm very happy! It concludes college.""</p>

<p><strong>Categorization 1:</strong>
 - Joy <strong>(+1)</strong>
 - Sadness <strong>(-1)</strong></p>

<hr>

<p><strong>Phrase 2:</strong> ""I'm sad, my mother passed away.""</p>

<p><strong>Categorization 2:</strong>
 - Sadness <strong>(+1)</strong>
 - Joy <strong>(-1)</strong></p>

<hr>

<p><strong>Phrase 3:</strong> ""I met a girl, but I was ashamed.""</p>

<p><strong>Categorization 3:</strong>
 - Fear <strong>(+1)</strong></p>

<p>Is this a clever way to follow and / or improve, or am I completely out of the way?</p>

<p>I see that there is a Google product that creates parsing according to the phrases. I do not know how it works, because I like to recreate the way I think it would work.</p>

<p>Remembering that this would not be the only way to categorize the phrase. This would be the first phase of the analysis. I can also identify the subject of the sentence, so we would know if the sadness is from the creator of the message or from a third party, in most cases.</p>

<ul>
<li><a href=""http://www.nltk.org/book/ch08.html"" rel=""nofollow noreferrer"">NLTK</a> </li>
<li><a href=""https://github.com/text-machine-lab/sentimental"" rel=""nofollow noreferrer"">Sentiment Analysis Python Example</a></li>
</ul>
"
1468,"<p>I'm working with acoustic data (filterbank features) and I want to build a neural network to detect claps using an LSTM (or a GRU) with a binary output (present/abscent), and I'm wondering about how I should prepare my data before feeding them to the RNN. </p>

<p>If I have 20 seconds of claps (separate claps separated by ~ 0.1 seconds) what is the difference between : </p>

<ol>
<li>Feeding the network a series of N claps as one example (with variable N : 1, 2, .., 10, ..) + padding with zeros to fit the longest sequence. </li>
<li>Feeding the network multiple examples of 1 clap. </li>
</ol>

<p>My problem is not restricted to claps but covers patterns that can be observed as separated occurrences, periodic sequence of occurrences, variable-length-period ""quasi-periodic"" sequence of occurrences, etc. </p>

<p>Unlike an ergodic HMM, an RNN doesn't have any loops to ""jump back"" to a previous ""acoustic state"", so what should-I do with this kind of data ?</p>
"
1469,"<p><a href=""https://ai.stackexchange.com/questions/5258/swarm-intelligence-vs-normal-human-intelligence"">A question about swarm intelligence as a potential method of strong general AI</a> came up recently, and yielded some useful answers and clarifications regarding the nature of swarm intelligence. But it got me thinking about group intelligence in general.  </p>

<p>Here organism is synonymous with algorithm, so a complex organism is an algorithm made up of component algorithms, based on a set of instructions in the form of a string. </p>

<p>Now consider the <a href=""https://en.wikipedia.org/wiki/Portuguese_man_o%27_war"" rel=""noreferrer"">Portuguese man o' war</a>, not a single animal, but a <a href=""https://en.wikipedia.org/wiki/Colony_(biology)"" rel=""noreferrer"">colonial organism</a>. In this case that means a set of animals connected for mutual benefit.  </p>

<p>And <em>physalia physalis</em> are pretty smart as a species in that they've been around for a while, I'm not finding them on any endangered lists, and based on their habitat it looks like global warming will be a jackpot for them.  And they don't even have brains.</p>

<p>Each component of the <em>physalia</em> has a narrow function, colony organism organism itself has a more generalized function, which is the set of functions necessary for maintenance and reproduction. </p>

<p>{Man o' War} ⊇ { {pneumatophore}, {gonophores, siphosomal nectophores,vestigial siphosomal nectophores}, {free gastrozooids, tentacled gastrozooids, gonozooids, gonopalpons}, {dactylozooids}, {gonozooids}, {gastrozooids} }   </p>

<ul>
<li>What types of applications qualify as ""compound intelligences""? What is the thinking on groups of neural networks comprising generally stronger or simply more generalized intelligence?</li>
</ul>

<p>I recognize the underlying problem is ultimately complexity and that ""strong narrow AI"" is, by definition, limited, so I use ""generalized"" and omit ""strong"" because because human-like and superintelligence are not conditions.  Compound intelligence is defined as a colony of dependent intelligences.*  </p>

<p>Utility software is often a form of expert system that manages a set of functions of varying degrees of complexity. There's currently a great deal of focus on autonomous vehicles, which would seem to require sets of functions.    </p>

<p>Links to research papers on this or related subjects would be ideal. </p>

<hr>

<p><a href=""http://oceana.org/marine-life/corals-and-other-invertebrates/portuguese-man-o-war"" rel=""noreferrer"">Portuguese Man o' War (oceana.org)</a><br></p>

<p><a href=""https://fivethirtyeight.com/features/the-bugs-of-the-world-could-squish-us-all/"" rel=""noreferrer"">The Bugs Of The World Could Squish Us All</a><br></p>
"
1470,"<p>It is possible to use case-based reasoning for forward simulation in Mario AI, this is explained by <a href=""https://www.cc.gatech.edu/~riedl/pubs/ijcai17.pdf"" rel=""nofollow noreferrer"">Game engine learning from video</a> They are using features: </p>

<p>distance, velocity and position</p>

<p>to predict following game-frames like in a physics engine. What my problem with the paper is, that the “learned game engine” seems to be working autonomously. That means, the human operator is out of the loop, he is not changing the cases manually and he is not involved in predicting the future. Is it possible to make an interactive forward simulation? For example, the game-engine asks the operator what will happen if Mario jumps over the wall.</p>

<p>What i do not understand is how to design the GUI-interface in such a situation, because the number of possible reactions of the game-engine to a situation is endless, and to make the simulation interactive the user would click very fast on some buttons. For example, if the desired framerate is 30fps, should the user define all the parameters in a timestep of 1/30 seconds?</p>
"
1471,"<p>I am trying to find out what are some good learning strategies for Deep Q-Network with opponents. Let's consider the well known game Tic-Tac-Toe as an example:</p>

<ul>
<li>How should an opponent be implemented to get good and fast improvements?</li>
<li>Is it better to play against a random player or a perfect player or should the opponent be a DQN player as well?</li>
</ul>
"
1472,"<p>To provide a bit of context, I'm a software engineer &amp; game enthusiast (card games specially). The thing is I've always been interested in AI oriented to games. In college, I programmed my own Gomoku AI so i'm a bit familiar with the basic concepts of AI game oriented and have read books &amp; articles about Game Theory as well. </p>

<p>My issue comes when I try to analyze AI's for <strong>Imperfect Information</strong> games like (Poker, Magic the gathering, Hearthstone, etc). In most cases when I found an AI for Hearthstone, it was either some sort of Monte Carlo or MinMax strategy. I honestly think although it might even provide some decent results it will still be always quite flat and linear since it doesn't take into account what deck the opponent is playing and almost always tries to follow the same game-plan, since it will not change based on tells your opponent might give away via cards played (hint that a human would catch). </p>

<p>I would like to know if using Neural Networks would be more better than just using a raw evaluation of board state + hands + Hp each turn without taking into account learning about possible threads the opponent might have, how to deny the opponent the best plays he could make, etc.</p>

<p>My intuition tells me that this is way harder and far more complex.
Is that the only reason the NN method is not used?Has there been any research to prove how much efficiency edge would be between those 2 approaches? </p>
"
1473,"<p>I found <a href=""http://www.asimovinstitute.org/neural-network-zoo/"" rel=""nofollow noreferrer"">this nice-ish-looking diagram</a>, but it has a wholly inadequate descriptions for each of the cell types, aside from including names.</p>

<p>What is the definition/description of each of these cell types?</p>

<ul>
<li>Input Cell</li>
<li>Backfed Input Cell</li>
<li>Noisy Input Cell</li>
<li>Hidden Cell</li>
<li>Probablistic Hidden Cell</li>
<li>Spiking Hidden Cell</li>
<li>Output Cell</li>
<li>Match Input Output Cell</li>
<li>Recurrent Cell</li>
<li>Memory Cell</li>
<li>Different Memory Cell</li>
<li>Kernel</li>
<li>Convolution or Pool</li>
</ul>
"
1474,"<p>In the search tree below, there are 11 nodes, 5 of which are leaves. There are 10 branches.</p>

<p>Is the average branching factor given by 10/6, or 10/11?</p>

<p>Are leaves included in the calculation? Intuitively, I would think not, since we are interested in nodes with branches. However, a definition given to me by my professor was ""The average number of branches of all nodes in the tree"", which would imply leaves are included.</p>

<p><a href=""https://i.stack.imgur.com/MCRn3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MCRn3.png"" alt=""Search tree""></a></p>
"
1475,"<p>In a nutshell: I want to understand why a one hidden layer neural network converges to a good minimum more reliably when a larger number of hidden neurons is used. Below a more detailed explanation of my experiment:</p>

<p>I am working on a simple 2D XOR-like classification example to understand the effects of neural network initialization better. Here's a visualisation of the data and the desired decision boundary:
<a href=""https://i.stack.imgur.com/4jCnc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4jCnc.png"" alt=""enter image description here""></a></p>

<p>Each blob consists of 5000 data points. The minimal complexity neural network to solve this problem is a one-hidden layer network with 2 hidden neurons. Since this architecture has the minimum number of parameters possible to solve this problem (with a NN) I would naively expect that this is also the easiest to optimise. However, this is not the case. </p>

<p>I found that with random initialization this architecture converges around half of the time, where convergence depends on the signs of the weights. Specifically, I observed the following behaviour:</p>

<pre><code>w1 = [[1,-1],[-1,1]], w2 = [1,1] --&gt; converges
w1 = [[1,1],[1,1]],   w2 = [1,-1] --&gt; converges
w1 = [[1,1],[1,1]],   w2 = [1,1] --&gt; finds only linear separation
w1 = [[1,-1],[-1,1]], w2 = [1,-1] --&gt; finds only linear separation
</code></pre>

<p>This makes sense to me. In the latter two cases the optimisation gets stuck in suboptimal local minima. However, when increasing the number of hidden neurons to values greater than 2, the network develops a robustness to initialisation and starts to reliably converge for random values of w1 and w2. You can still find pathological examples, but with 4 hidden neurons the chance that one ""path way"" through the network will have non-pathological weights is larger. But happens to the rest of the network, is it just not used then? </p>

<p>Does anybody understand better where this robustness comes from or perhaps can offer some literature discussing this issue?</p>

<p>Some more information: this occurs in all training settings/architecture configurations I have investigated. For instance, activations=Relu, final_activation=sigmoid, Optimizer=Adam, learning_rate=0.1, cost_function=cross_entropy, biases were used in both layers. </p>
"
1476,"<p>I have recently gone about and made a <a href=""https://jsbin.com/vijawuleyo/edit?html,console,output"" rel=""nofollow noreferrer"">simple AI</a>, one that gives responses to an input (albeit completely irrelevant and nonsensical ones), using Synaptic.js. Unfortunately, this is not the type of text generation I am looking for. What I am looking for would be a way to get connections between words and generate text from that. (What would be preferable would be to also generate at least semi-sensible answers also.)</p>

<p>This is part of project Raphiel, and can be checked out in the room associated with this site. What I want to know is what layer combination would I use for text generation?</p>

<p>I have been told to avoid retrieval-based bots.</p>

<p>I have the method to send and receive messages, I just need to figure out what combination of layers would be the best. </p>

<p>Unless I have the numbers wrong, this will be SE's second NN chatbot.</p>
"
1477,"<p>I'm looking to perform two tasks:</p>

<ul>
<li><p>Train a classifier to classify code as serial or parallel</p></li>
<li><p>Train a generative algorithm to generate parallel code from serial </p></li>
</ul>

<p>For the first task a simple scraper can scrape random C and C++ code from git, however for the second step I would need a decently large source of examples of serial to parallel code. Any ideas or pointers for existing or creating this type of dataset would be greatly appreciated.</p>
"
1478,"<p>When training on large neural network, how to deal with the case that the gradients are too small to have any impact?</p>

<p>FYI, I have an RNN, which has multiple LSTM cells and each cell has hundreds of neurons. Each training data has thousands of steps, so the RNN would unroll thousands of times. When I print out all gradients, they are very small, like e-20 of the variable values. Therefore the training does not change the variable values at all. </p>

<p>BTW, I think this is not an issue of vanishing gradients. Note that the gradients are uniformly small from the beginning to the end.</p>

<p>Any suggestion to overcome this issue?</p>

<p>Thank!</p>
"
1479,"<p>I'm detecting objects on images. I want to detect up to 10 objects, however, I'm not sure how to deal with the situation, where only one object is present.</p>

<p>Should I fill the remaining spaces in the label input data with vectors filled with 0? E.g:</p>

<pre><code>[[xmin,ymin,xmax,ymax],[0,0,0,0]...]
</code></pre>

<p>Or is there any better way? Thanks for help!</p>
"
1480,"<p>I am trying to create a fixed-topology MLP network from scratch (C#) which can classify some simple problems such as XOR and <a href=""http://yann.lecun.com/exdb/mnist/"" rel=""nofollow noreferrer"">MNIST</a> (Handwriting). The network will be trained purely with <strong>genetic algorithms</strong> instead of backprop.</p>

<p>Here are the details:</p>

<ul>
<li>Population size: 50</li>
<li>Activation function: SIGMOID</li>
<li>Fixed topology</li>
<li>XOR: 2 inputs, 1 output. Tested w/ different numbers of hidden layers/nodes.</li>
<li>MNIST: 28*28=784 inputs for all pixels, will be either ON(1) or OFF(0). 10 outputs to represent digits 0-9</li>
<li>Initial population will be given random weights between 0 and 1</li>
<li>10 ""Fittest"" networks survive each iteration, and performs crossover to reproduce 40 offspring</li>
<li>For all weights, mutation occurs to add a random value between -1 to 1, with a 5% chance</li>
</ul>

<p>With 2 hidden layers of 4 and 3 neurons respectively, XOR managed to achieve 97-99.9% accuracy in around 100 generations. Biases were not used here.</p>

<p><strong>However</strong>, trying out MNIST revealed a pretty glaring issue - the 784 inputs; a large increase of nodes compared to XOR, multiplied with weights and added up results in HUGE values of 50 to even 100, way beyond the typical domain range of the activation function (SIGMOID).</p>

<p>This just renders all layers' outputs as 1 or 0.99999-something, which breaks the entire network. Also, since this makes all individuals in a population extremely similar to one other, the genetic algorithm seems to have no clue on how to improve. Crossover will produce an offspring almost identical to its parents, and some lucky mutations are simply ignored my the sheer amount of other neurons!</p>

<p>What can be a viable solution to this? It's my first time studying NNs, and this is really... challenging. Please help!</p>
"
1481,"<p>I have been searching for information regarding industry accepted certifications in the area of Artificial Intelligence, Neural Networks, Machine Learning, NLP, deep learning. I couldn't find much information after searching. Could anyone please share, if you are aware of any certifications in these areas. </p>

<p><strong>P.S:</strong> I am not looking for course certifications but rather something along the lines of AWS Certifications, OCJP, etc.</p>
"
1482,"<p>With the growing ability to cheaply create fake pictures, fake soundbites, and fake video there becomes an increasing problem with recognizing what is real and what isn't. Even now we see a number of examples of applications that create fake media for little cost (see <a href=""https://en.wikipedia.org/wiki/Deepfake"" rel=""nofollow noreferrer"">Deepfake</a>, <a href=""https://en.wikipedia.org/wiki/FaceApp"" rel=""nofollow noreferrer"">FaceApp</a>, etc.).</p>

<p>Obviously, if these applications are used in the wrong way they could be used to tarnish another person's image. Deepfake could be used to make a person look unfaithful to their partner. Another application could be used to make it seem like a politician said something controversial.</p>

<p>What are some techniques that can be used to recognize and protect against artificially made media? </p>
"
1483,"<p>I am working on a project which maps to a variant of path finding problem. I am new to this area and I would be very grateful if you could give suggestions/ point to libraries for relevant algorithms. </p>

<p>A simplified version of my problem statement is as follows- </p>

<p>Goal: On a 2D grid, starting from a fixed point reach the destination in exactly N steps.</p>

<p>Allowed actions: 
1. At every position, you have a choice of up to three moves (i.e. straight, curve left, curve right). 
2. You cannot collide with the path traveled so far (just like in the snake game).</p>

<p>Dimension of the grid: N x N where N is between 100-1000</p>

<p>Scalable: Later on, the problem will be scaled to have multiple such snakes going between different pairs of points on the grid. The ultimate goal is to get ALL snakes to reach their respective destinations in a fixed number of steps without any collisions.</p>

<p><strong>TL;DR:</strong>
Essentially I have to find a fixed length path on a dynamically generated directed graph. Is there a better choice than a A* / greedy heuristic? Is it worth taking a Q-
learning approach?</p>

<p>A rudimentary one snake version written in python can be found here - 
<a href=""https://github.com/pranavm1502/Curves"" rel=""nofollow noreferrer"" title=""Github link"">Github Link</a>
Thanks in advance!</p>
"
1484,"<p>I have two classes in the training set: one that has images with a feature and the other of images without that feature.
Can there be a LOT more images with ""no feature"" so I can fit in all possible false positives?</p>
"
1485,"<p>In time Series prediction, we have a stream of vectors. There are different approaches for accounting for the temporal patterns between these vectors.</p>

<p>There's two that I'm considering. An LSTM or augmenting the feature space. What's the difference between the two? The most obvious to me is that an LSTM is more expressive and can get superior accuracy if modelled properly. </p>
"
1486,"<p>I encountered the algorithm below, which is the general tree search algorithm.</p>

<pre><code>function TREE-SEARCH(problem, fringe) returns a solution or failure

fringe &lt;-- INSERT(MAKE-NODE(INITIAL-STATE[problem]), fringe)
loop do
    if fringe is empty then return failure 
    node &lt;- REMOVE-FRONT(fringe)
    if GOAL-TEST[problem](STATE[node]) then return SOLUTION(node)
        fringe &lt;- INSERTALL(EXPAND(node, problem), fringe)

function EXPAND(node,problem) returns a set of nodes
    successors &lt;- the empty set
    for each action, result in SUCCESSOR-FN[problem](STATE[node]) do
        s &lt;- a new NODE
        PARENT-NODE[s] &lt;- node; 
        ACTION[s] &lt;- action; 
        STATE[s] &lt;- result
        PATH-COST[s] &lt;- PATH-COST[node] + STEP-COST(node, action, s)
        DEPTH[s] &lt;- DEPTH[node] + 1
        add s to successors
    return successors
</code></pre>

<p>What is the ""fringe"" in the context of search algorithms?</p>
"
1487,"<p>I've heard multiple times that ""Neural Networks are the best approximation we have to model the human brain"", and I think it is commonly known that Neural Networks are modelled after our brain.</p>

<p>I strongly suspect that this model has been simplified, but how much? </p>

<p>How much does, say, the vanilla NN differ from what we know about the human brain? Do we even know?</p>
"
1488,"<p>I want to train my computer to recognize texts from websites, copy them, insert them somewhere else, and other things like this, but I dont even know where to start my research and what tools should I use. Can you give me an advice?</p>
"
1489,"<p>The problem to solve is non-linear regression of a non-linear function. My actual problem is to model the function ""find the max over many quadratic forms"": max(w.H.T<em>Q</em>w), but to get started and to learn more about neural networks, I created a toy example for a non-linear regression task , using Pytorch. The problem is that the network never learns the function in a satisfactory way, even though my model is quite large with multiple layers (see below). Or is it not large enough or too large? How can the network be improved or may be even simplified to get a much smaller training error?</p>

<p>I experimented with different network architectures, but the result is never satisfactory. Usually the error is quite small within the input interval around around 0, but the network is not able to get good weights for the regions at the boundary of the interval (see plots below). The loss does not improve after a certain number of epochs.  I could generate even more training data, but I have not yet understood completely, how the training can be improved (tuning parameters such as batch size, amount of data, number of layers, normalizing input (output?) data,  number of neurons, epochs etc.)</p>

<p>My neural network has 8 layers with the following number of neurons: 1, 80,70,60, 40,40,20, 1</p>

<p>For the moment I do not care too much about overfitting, my goal is to understand, why a certain network architecture/certain hyper parameters need to be chosen. Of course, avoiding overfitting at the same time would be a bonus.</p>

<p>I am especially interested in using neural networks for regression tasks or as function approximators. In principle my problem should be able to be approximated to arbitrary accuracy by a single layer neural network, according to the universal approximation theorem, isn’t this correct?</p>

<p><a href=""https://i.stack.imgur.com/oqXXN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqXXN.png"" alt=""Difference between trained model and original data""></a></p>

<p><a href=""https://i.stack.imgur.com/l2U6j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l2U6j.png"" alt=""Loss value vs iterations""></a></p>

<p><a href=""https://i.stack.imgur.com/sHAct.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sHAct.png"" alt=""Difference between trained model and original data ""></a></p>
"
1490,"<p>We store supplier information for many companies. Each supplier database is logically separated, making it possible for a supplier to be common across logical separations.</p>

<p>At the moment, we have analysts go through the system manually to try identify common suppliers by name and address.</p>

<p>Is this a problem for a certain domain of AI? If so, what is the right hammer for this nail?</p>
"
1491,"<p>YouTube has a huge amount of videos, many of which also containing various spoken languages. This would presumably provide something like the data that a ""challenged"" baby would experience - ""challenged"" meaning a baby without arms or legs (unfortunately many people are born that way). </p>

<p>Would this not allow unsupervised learning in a deep learning system that has both vision and audio capabilities? The neural network would presumably learn correlations between words and images, and could perhaps even learn rudimentary language skills, all without human supervision. I believe that the individual components to do this already exist.</p>

<p>Has this been tried, and if not, why?</p>
"
1492,"<p>I wanted to use the visualization of the activation maximization of the filters that is described in the following keras tutorial/blog:</p>

<p><a href=""https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html"" rel=""nofollow noreferrer"">https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html</a></p>

<p>I'd like to know what is the intention behind the decision that filters that produce a loss &lt;= 0 are skipped. I know for 0 that would be reasonable since their would be no gradient flowing then (I think) but what about negative values? And is it also reasonable to use the mean of the outputs of the filters as a loss? What if there are weights of a filter that have high negative and positive numbers. Would that be a problem?</p>
"
1493,"<p>Are decision trees able to be used with time-related data?</p>

<p>I've read that decision trees are based on matrices and that ARRAYS of input matrices can be used to factor in time however I can't find an example of this.</p>

<p>Say for example, I'm monitoring the progress of students taking exams. Each day I ask them questions related to their mental state (fatigued, positivity, ability to concentrate, expectations for coming exam, confidence, etc). I have twenty days worth of questions. Day 1 for student A may see them studying for an exam the following day, while Day 1 for student B may see them actually doing the exam. There will be a relation between student's fatigue (for example) and the results they give the following day.</p>

<p>The examples when provided as input to a matrix will be used to show that IF on any given day, the student has an exam, and has breakfast, and does x,y,z THAT day then the outcome will be y. </p>

<p>However, short of encoding ""had exam previous day"" and ""had exam two days ago"" for each day, I can't see how I can include time dependency in decision trees.</p>
"
1494,"<p>I'm studying reinforcement learning. It seems that ""state"" and ""observation"" mean exactly the same thing. They both capture the current state of the game. </p>

<p>Is there a difference between the two terms? Is the observation maybe the state after the action has been taken?</p>
"
1495,"<p>I am trying to do <strong>3d image deconvolution</strong> using <strong>convolution neural network</strong>. But I cannot find many famous 3d convnets. Can any one point out some for me?  </p>

<p>Background: I am using PyTorch, but any language is OK. What I want to know most is the network structure. I can't find papers on this topic. </p>

<p>Links to research papers would be especially appreciated.</p>
"
1496,"<p>For a school project, I would like to investigate a paper on either reinforcement learning or computer vision. I am particularly interested in DQN, RNNs, CNNs or LSTMs. I would eventually like to implement any of these. However, I also need to take into account the computing resources required to train and analyse any of these algorithms. I understand that, in computer vision, the data sets can be quite large, but I am not so sure regarding the resources needed to implement and train a typical state-of-the-art RL algorithm (like DQN).</p>

<p>Would a ""standard PC"" be able to run any of these algorithms decently to achieve some sort of analysis/results? </p>
"
1497,"<p>I want to be able to input a block of text and then have it guess a string within a predefined range (i.e. a string that starts with three letters and ends with five numbers like ""XXX12345"", etc).  Ideally, the string it will be guessing will be somewhere in the block of text, but sometimes it won't be.</p>

<p>I have been struggling where to begin on this or if I am even going in the right direction for considering Machine/Deep learning to try to do this.</p>

<p>Help!</p>
"
1498,"<p>I want to create a neural network and train it on some data, however I want to be able to create a new model without retraining it from the start.</p>

<p>An example, I have 1000 data points in my training data</p>

<ol>
<li>model - trained on 0-99</li>
<li>model - trained on 1-100</li>
<li>model - trained on 2-101</li>
<li>and so forth</li>
</ol>

<p>So I'm wondering if I can use the first model to train the second model, essentially forgetting the first data point.</p>

<p>You can view it as a sliding window over the 1000 data points, sliding one data point to the right for each new model.</p>

<p>Does it make sense?
Is there any easy way to solve this problem?</p>
"
1499,"<p>There are two textbooks that I most love and am most afraid of in the world: <em>Introduction to Algorithms by Cormen</em> et al. and <em>Artificial Intelligence: A Modern Approach by Norvig</em> et al. I have started the ""AI: a modern approach"" more than once, but the book is so dense and full of theory that I get discouraged after a couple of weeks and stop.</p>

<p><strong>I am looking for a similar AI book but with an equal emphasis on theory and practice.</strong> Some examples of what I am looking for:</p>

<ul>
<li>The Elements of Statistical Learning by Tibshirani et al. <strong>(detailed theory)</strong></li>
<li>An Introduction to Statistical Learning: With Applications in R by
Tibshirani et al. <strong>(theory+practical)</strong></li>
<li>Digital Image Processing by Gonzalez et al. <strong>(detailed theory)</strong></li>
<li>Digital Image Processing Using MATLAB by Gonzalez et al.
<strong>(theory+practical)</strong></li>
</ul>
"
1500,"<p>I'm currently reading <a href=""https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner&#39;s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/"" rel=""nofollow noreferrer"">this</a> explanation of convolutional neural networks and there's a part around strides that I don't quite understand. I'm just starting with this so apologies if this is a really basic question. But I'm trying to develop and understanding and some of these images have thrown me off.</p>

<p>specifically in this image 
<a href=""https://i.stack.imgur.com/zI3nb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zI3nb.png"" alt=""enter image description here""></a></p>

<p>The stride has been increased to 2 and it's using a 3x3 filter (represented by the red,green and blue outline squares in the first picture)</p>

<p>Why is the blue square below the red one and not shifted to the side of the green one ending at the edge of the 7x7 volume? Should it not move left to right then down 2 squares when it reaches the next line?</p>

<p>I'm not sure if the author is just trying to show the stride moving down as it goes, but I think my confusion stems from the fact that the 1 stride image example is only moving in the horizontal direction (as seen below).</p>

<p><a href=""https://i.stack.imgur.com/P0r8c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P0r8c.png"" alt=""enter image description here""></a></p>

<p>Is there something fundamental I haven't grasped here?</p>
"
1501,"<p>I use Sigmoid activation function for neurons at output layer of my Multi-Layer Perceptron also, I use cross-entropy cost function. As I know when activation functions like Tanh is used in output layer it's necessary to divide outputs of output layer neurons by sum of them like what is done for softmax, is such thing necessary for sigmoid activation function?
If it's necessary to normalize outputs of neurons, does it affect derivations?</p>
"
1502,"<p>I have been reading quite a <a href=""http://dces.essex.ac.uk/staff/poli/technical-reports/tr-ces-475.pdf"" rel=""nofollow noreferrer"">few papers (chapter 7)</a> on genetic programming and its applications. Unfortunately, I can not wrap my head around how one could  apply genetic programming to robotics, for example, robot's path planning or robot's maneuvers. </p>

<p>Can anyone explain this in the most simple manner? I know that it all depends on the fitness function. </p>
"
1503,"<p>I am reading the cornerstone book, ""Artifical Intelligence, A Modern Approach"" by Stuart Russel, and Peter Norvig and there is a passage in the book on page 98: </p>

<blockquote>
  <p>The complexity results depend very strongly on the assumptions made
  about the state space. The simplest model studied is a state space
  that has a single goal and is essentially a tree with reversible
  actions. (The 8-puzzle satisfies the first and third of these
  assumptions.)</p>
</blockquote>

<p>What are the ""assumptions"" in that context?</p>
"
1504,"<p>The situation I encountered here is that I have two inputs(for instance, image embedding, etc.) into the first lstm of a series of lstms to predict the next word to generate sentence(from the second lstm, it started to predict the next word from the current input word). The length of each of the two inputs is 512. Merely for the first input, it increases the measurement, say, for instance, perplexity, by about 3 from no this input at all. Merely for the second input, it increases the measurement, say, for instance, perplexity, by about 1 from no input at all. The problem is: Is it possible to combine these two inputs into a model that can produce a result of increasement more than 3 or the larger amount of increasement of the former two inputs models? If it is, how to build a model or what model should I build to combine them to do so?</p>
"
1505,"<p>Suppose I have a Boolean function that maps N bits to one bit. If I understand correctly, this function will have 2^2^N possible configurations of its truth table.</p>

<p>What is the minimum number of neurons and hidden layers I would need in order to have a multi-layer perceptron capable of learning all possible truth tables? I found one set of lecture notes <a href=""http://deeplearning.cs.cmu.edu/slides/lec2.universal.pdf"" rel=""nofollow noreferrer"">here</a> that suggests that ""checkerboard"" shaped truth tables (which is something like an N-input XOR function) are hard to learn, but do they represent the worst-case scenario? The notes suggest that such tables require 3(N-1) neurons arranged in 2 log_2(N) hidden layers</p>
"
1506,"<p>Suppose that there are six type of problems to be handled when reading a book,<br>
I elaborate it as follow:</p>

<pre><code>while True:
    if type A happens :
        handle A
    #during handling the problem, it might reproduce new problems of kinds
    #A, B, C, D, E,
        produce  (A,B..E or null)
        continue

    if B occurs:
        handle B
    #during hanlding the problem, it might spwan new problems
    #A, B, C, D, E,
        produce  (A,B..E or null)
        continue

    if C happens:
        handle C
        produce (A,B..E or null)
        continue
    ...

    if there are no problmes:
        break
</code></pre>

<p>Suppose I have 3 problems on hand<br>
the above program might run endless on first one and never touch the second.</p>

<p>Take an example that I am reading a book,<br>
'Problem A' could  defined as encountering a 'new word', handling it is to look up dictionary.<br>
When looking up, I might come acorss another new word, another and another.<br>
In this case, I might end up reading one sentence of a book in years.</p>

<p>As a solution,<br>
I introduce a container to collect problems,sort them then determine which one to execute.    </p>

<pre><code>def solve_problems(problems)
    problem_backet = list(problems)
    while True:
        if problem_backet not is null:
            #value-weighted all the problem 
            #and determine one to implement
            value_weight to sort problems
            problem = x

        if problem == A:
            handle A
            delet A from problem_backet
            add new problems to problem_backet
            continue (back to determine process)

        if problem == B:
            handle B
            problem_backet.remove(B)
            problem_backet.append(new_problem)
            continue
        ...
        if problem_backet is null:
            return 
</code></pre>

<p>I tried alternatively to  improve efficiency  to get most from a book in least time.</p>

<p>but the value_weighted process consume most efforts and time. </p>

<p>How to solve such a problem in a proper algorithms or pattern?  </p>
"
1507,"<p>I'm writing neural network based on neural gas algorithm (university assignment) and I remember that lecturer said that, when you generate random neuron weights at the beginning, it's worth to generate them multiple times  and choose the best set of them.</p>

<p>The problem: I don't know <strong>what's the criteria of choosing the best set of weights for neurons</strong>?</p>
"
1508,"<p>I have Read much about the programming languages being used in AI specially LISP and PROLOG but just wanted to know about the current situation of the both these languages in AI.</p>

<p>Can somebody tell me which language is being more used these days? <strong>LISP</strong> or <strong>PROLOG</strong>.</p>
"
1509,"<p>I understand that a heuristic is <strong>admissible</strong> if it <em>never overestimates</em> the true cost to reach the goal node from n. I also understand that if a heuristic is <strong>consistent</strong> then the heuristic value of n is never greater than the cost of its successor, n', plus the successor's heuristic value. </p>

<p>However, I am having a hard time understanding the theorems stated in my lecture slides in regard to them: </p>

<ul>
<li>If h(n) is admissible, A* using TREE-SEARCH is optimal</li>
<li>If h(n) is consistent, A* using GRAPH-SEARCH is optimal</li>
</ul>

<p>Can someone please clarify why these are the case.</p>
"
1510,"<p>I was coding a CGAN model using Keras along with the paper (<a href=""https://arxiv.org/pdf/1411.1784.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1411.1784.pdf</a>) and I wanted to try and match the models to exactly what the paper says. I knew the models presented in the paper would be primitive but just wanted to replicate those and see. For example the generator model in the paper mentions this:</p>

<blockquote>
  <p>In the generator net, a noise prior z with dimensionality 100 was
  drawn from a uniform distribution within the unit hypercube. Both z
  and y are mapped to hidden layers with Rectified Linear Unit (ReLu)
  activation [4, 11], with layer sizes 200 and 1000 respectively, before
  both being mapped to second, combined hidden ReLu layer of
  dimensionality 1200. We then have a final sigmoid unit layer as our
  output for generating the 784-dimensional MNIST samples.</p>
</blockquote>

<p>So for this I had the code like this:</p>

<pre><code>def build_generator(self):

       model = Sequential()

       model.add(Dense(200, input_dim=self.latent_dim))
       model.add(Activation('relu'))
       model.add(BatchNormalization(momentum=0.8))

       model.add(Dense(1000))
       model.add(Activation('relu'))
       model.add(BatchNormalization(momentum=0.8))

       model.add(Dense(1200, input_dim=self.latent_dim))
       model.add(Activation('relu'))
       model.add(BatchNormalization(momentum=0.8))

       model.add(Dropout(0.5))

       model.add(Dense(np.prod(self.img_shape), activation='sigmoid'))
       model.add(Reshape(self.img_shape))


       model.summary()

       noise = Input(shape=(self.latent_dim,))
       label = Input(shape=(1,), dtype='int32')
       label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))

       model_input = multiply([noise, label_embedding])

       img = model(model_input)

       return Model([noise, label], img)
</code></pre>

<p>But still I think this not exactly what the paper means. What I understand from the paper is the noise and labels are first fed into two different layers and then combined to one layer.</p>

<p>Does this mean that there should be three separate models inside the generator? Or am I mistaken thinking that? Would like to hear any thoughts on this.</p>
"
1511,"<p>I have similar architecture like in image:<a href=""https://i.stack.imgur.com/Zvne1.png"" rel=""nofollow noreferrer"">CNN</a>.</p>

<p>I don't understand how to calculate gradient of filter <strong>F</strong>. 
I found these equations(<a href=""http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/"" rel=""nofollow noreferrer"">source</a>):
<a href=""https://i.stack.imgur.com/E97A9.png"" rel=""nofollow noreferrer"">Gradient and delta</a>,
where first equation calculate gradient of weight <strong>F<sub>ab</sub></strong> from filter <strong>F</strong> and second equation calculate <strong>deltas</strong> of input layer and<br>
<strong>E</strong> - total error,<br>
<strong>N</strong> - input width,<br>
<strong>M</strong> - input height,<br>
<strong>l</strong> - layer. 0 - first, l = L last<br>
<strong>x</strong> - input<br>
<strong>y</strong> - output<br>
<strong>o</strong> - activation function<br>
<strong>o<sup>'</strong> - derivate of activation function,<br>
but I don't understand how to calculate <strong>∂E/∂y<sub>ij</sub>l</sup></strong>.<br>
How can I can calculate gradient of <strong>F</strong>?</p>
"
1512,"<p>Not sure if this is the correct forum, but I have been working with a large (non-image) dataset that will eventually be used to train a neural network.  I have been puzzling over how to manage wide data sets.  For this application ""wide"" is maybe 10,000 or 20,000 points wide.  It is not really possible to store this as a row in a conventional RDMS (which are usually limited to several hundred columns).  Is it better to use a huge CSV file or maybe a no-sql technology like Cassandra (the data is originally in JSON format)?</p>
"
1513,"<p>The Random-MMP algorithm is used as a multi-modal planner for sampling the state space. <a href=""https://www.researchgate.net/profile/Victor_Ng-Thow-Hing2/publication/220122891_Randomized_Multi-Modal_Motion_Planning_for_a_Humanoid_Robot_Manipulation_Task/links/0deec53c61677a32dc000000/Randomized-Multi-Modal-Motion-Planning-for-a-Humanoid-Robot-Manipulation-Task.pdf"" rel=""nofollow noreferrer"">Multi-modal motion planning for a humanoid robot manipulation task</a> The robot has different modes like walk and push which has to be combined to a higher task. The idea is to create a mode graph which stores all possibilities. In the paper a probabilistic roadmap is utilized, which is similar to the RRT algorithm (Rapidly-exploring random tree). What i'm not understand is the expansion strategy. According to the RRT paradigm, at first a random point outside of the graph is generated and then the nearest node inside the graph is extended to that node. But how can we do this in a multi-modal setup? </p>
"
1514,"<p>I wondered what the difference was, ever since I tried the two and compared them. The LSM gave me better results at first, but had holes in it, as if some outputs were missing. The feedforward network gave me worse answers, but without the holes. </p>

<p>What I mean by 'holes' is shown below (The predicted result is ""How are you""):</p>

<p>LSM: ""Hw are yo""</p>

<p>FFN: ""Hnw brf ypu""</p>

<p>I was wondering what the difference was, as far as how they are connected and such.</p>
"
1515,"<p>I have been reading a bit about networks where deep layers able to deal with a bunch of features (be it edges, colours, whatever). </p>

<p>I am wondering: how can possibly a network based on this 'specialised' layers be fooled by adversarial images? Wouldn't the presence of specialised feature detectors be a barrier to this? (as in: this image of a gun does share one feature with 'turtles' but it lacks 9 others so: no, it ain't a turtle).
thanks!</p>
"
1516,"<p>If we model the game '2048' using a max-min game tree, what is the maximal path from a start state to a terminal state? (Assume the game ends only when the board is full</p>

<p>This is one of the sub-questions that should prepare us to actually modeling the game as a max-min game tree. However I'm failing to understand the question.</p>

<p>Is it actually the path to receiving 131072 as an endgame?</p>
"
1517,"<p><em>Warning: This question takes us into <a href=""https://en.wikipedia.org/wiki/VALIS"" rel=""nofollow noreferrer"">VALIS</a> territory, but I wouldn't underestimate the profundity of that particular philosopher.</em></p>

<p><a href=""https://en.oxforddictionaries.com/definition/intelligence"" rel=""nofollow noreferrer"">There is a non-AI definition of intelligence which is simply ""information"" (see definition 2.3)</a>.  If that information is <em>active</em>, in terms of utilization, I have to wonder if it might qualify as a form of algorithmic intelligence, regardless of the qualities of the information.</p>

<p>What I'm getting at is that fields such as recreational mathematics often produce techniques and solutions that don't have immediate, real world applications.  But there's an adage that pure math tends to find uses. </p>

<p>So you might have algorithms applied to a problems that outside of the problems from which it originated, or that couldn't initially be implemented in a computing context. (Minimax in 1928 might be an example.)</p>

<p>Goal orientation has been widely understood as a fundamental aspect of AI, but in the case of an algorithm designed for one problem, that it subsequently applied to a different problem, the goal may simply be a function of the algorithm's structure. (To understand the goal of minimax precisely, you read the algorithm.)</p>

<p>If you regard this form of information as intelligence, then intelligence can be general, regardless of strength in relation to a given problem.</p>

<ul>
<li>Can we consider this form of codification of information to be algorithmic intelligence?</li>
</ul>

<p><em>And, just for fun, if a string that encodes a cutting-edge AI is not being processed, does it still qualify as artificial intelligence?</em> </p>
"
1518,"<p>I have a rather basic question about YOLO for bounding box detection. My understanding is that it effectively associates each anchor box to a 8-dimension output. During testing, does YOLO take each anchor box and classify on it alone? What happens if the object is big and spans over several anchor boxes (e.g., covering 70% of the image)? How can YOLO classify and detect objects spanning over many anchor boxes?</p>
"
1519,"<p>I'm trying to predict grades within a course at my university. At the moment I manually extracting features, but I'm curious if it's possible to somehow use my entire dataset with a deep learning approach?</p>

<p>I have data from throughout the course of students solving mandatory exercises. All students uses a plug-in within the editor that takes a snapshot of code-base each time the student saves the project (exercise). I also have data from when the students run the debugger. All exercises include tests which determine what score the student will receive on a given exercise. The students are free to execute the tests as many times as they like wile solving the exercise (the final score is given when the students presents the final result to a teaching assistant). Execution and results of these tests are also included in the data. Timestamps exists for all data. I also have the final grade of each student (which is determined 100% by the final exam).</p>

<p>Does anyone know of an approach to use this kind of data with a deep learning approach?</p>
"
1520,"<p>Which specific performance evaluation metrics are used in training, validation and testing and <strong>why</strong>? I am thinking error metrics (RMSE, MAE, MSE) are used in validation, and testing should use a wide variety of metrics? I don't think performance is evaluated in training, but not 100% sure. </p>

<p>Specifically I am actually after deciding when to use (i.e. in training, validation or testing) correlation coefficient, RMSE, MAE and others for numeric data (e.g. Willmott's Index of Agreement, Nash-Sutcliffe coefficient, etc.)</p>

<p>Sorry about this being broad - I have actually been asked to define it generally (i.e. not for a specific dataset). But datasets I have been using have all numeric continuous values with supervised learning situations. </p>

<p>Generally I am using performance evaluation for environmental data where I am using ANN to model. I have continuous features and am predicting a continuous variable.</p>
"
1521,"<p>As I've thought about AI, and what I understand of the problems that we face in the creation of it, I've noticed a recurring pattern: we always seem to be asking ourselves, ""how can we better simulate the brain?"" </p>

<p>Why are we so fascinated with <strong>simulating</strong> it? Isn't our goal to create intelligence, not create intelligence in a <strong>specific medium</strong>? Isn't growing and sustaining living brains more inline with our goals, albeit a bit of an ethical controversy?</p>

<p>Why is this exchange's description: ""For people interested in conceptual questions about life and challenges in a world where 'cognitive' functions can be mimicked in a <strong>purely digital environment</strong>?""</p>

<p>To condense these feelings in a more concise question: Why are we trying to create AI in a computer?</p>
"
1522,"<p>How would you design a neural network that generates the positions of comparators in a <a href=""https://en.wikipedia.org/wiki/Sorting_network"" rel=""nofollow noreferrer"">sorting network</a> given a set of numbers. I've tried to modify some already implemented networks that given a set of numbers it sorts the number. My goal is that given an unsorted sequence of numbers to generate a sorting network that will sort those numbers. I am not asking for the complete solution, just a starting point.</p>
"
1523,"<p>My objective is simple...classify the given sequence of images(video) as either moving or staying still from the perspective of the person inside the car.</p>

<p>Below is an example of the sequence of 12 images animated.</p>

<p>1.Moving from the point of the person inside the car.</p>

<p><a href=""https://i.stack.imgur.com/n2kj7.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/n2kj7.gif"" alt=""Class 0: Moving""></a></p>

<p>2.Staying still from the point of the person inside the car. </p>

<p><a href=""https://i.stack.imgur.com/jUM4x.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jUM4x.gif"" alt=""Class 1: still""></a></p>

<p>Methods I tried to achieve this:</p>

<ol>
<li><p>simple CNN (conv2d) with those 12 images(greyscaled) stacked in the channels dimension.(like Deepmind's DQN). 
Input to the net is (batch_size, 200, 200, 12)</p></li>
<li><p>3D CNN (conv3d) . Input to the net is (batch_size, 12, 200, 200 ,1)</p></li>
<li><p>CNN+LSTM (timedistributed conv2d). Input to the net is (batch_size, 12, 200, 200 ,1)</p></li>
<li><p>Late fusion method , which is taking 2 frames from the sequence that are some time steps apart and passing them into 2 CNNs (with same weights) separately and concatenating them in dense layer <a href=""https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf"" rel=""nofollow noreferrer"">As mentioned in this paper</a>. This is also like CNN+LSTM without the lstm part.
Input to this net is (batch_size, 2, 200, 200, 1) -> the 2 images are first and last frames in the sequence</p></li>
</ol>

<p>All the methods I tried failed to achieve my objective. Tried tuning various hyperparameters like learning rate, no of filters in CNN layers etc. and nothing worked.</p>

<p>All the methods had a batch_size of 8 (due to memory constraint) and all images are greyscaled.Used ReLUs for activations and softmax in the last layer. No pooling layer was used.</p>

<p>Any help on why my methods are failing or any pointers to a related work</p>

<p>Many Thanks</p>
"
1524,"<p>In a neural network for chess (or checkers), the output is a piece or square on the board and an end position.</p>

<p>How would one encode this?</p>

<p>As far as I can see choosing a starting square is 8x8=64 outputs and an ending square is 8x8=64 outputs. So the total number of possible moves is 64x64 4096 outputs. Giving a probability for every possible move.</p>

<p>Is this correct? This seems like an awful lot of outputs!</p>
"
1525,"<p>If you have a game and you are training an AI there seems to be two ways to do it.</p>

<p>First you take the game-state and a possible move and evaluate whether this move would be good or bad:</p>

<p>(1) GAME_STATE + POSSIBLE_MOVE  --> Good or bad?</p>

<p>The second is to take the game state and get probabilities of every conceivable move:</p>

<p>(2) GAME_STATE ---> Probabilities for each move</p>

<p>It seems that both models are used. I.e. in language and RNN might use (2) to find the probabilities for each next word or letter. But AlphaZero might use (1). Noting also that in a game like chess GAME_STATE + POSSIBLE_MOVE = NEW_GAME_STATE. Whereas in some games you might not know the result of your move.</p>

<p>Which do you think is the best method? Which is the best way to do AI? Or some combination of the two?</p>
"
1526,"<p>Using a neural network the method seems to be that you end up with a probability for each possible outcome.</p>

<p>To predict the next frame in a monochrome movie of size 400x400 with 8 shades of gray, it seems like there seems to be: 8^(160000) possibilities.</p>

<p>On the other had if you just predicted the probability for each pixel individually you would end up with some kind of image which gets progressively blurred.</p>

<p>Perhaps what you want is to generate a few possibilities that are none-the-less quite sharp. In a similar way to weather prediction(?)</p>

<p>So how would you go about designing a neural network that takes reads a movie and tries to predict the next frame?</p>
"
1527,"<p>I am interested to see what advantages a Loop Network (Feed Forward Network that takes its output as input, I think it's called an RNN, not sure). The only result I found was that it was extremely sensitive to context, but only the context behind it. Other than that, I did not notice any changes. </p>

<p>I figured this would be better for a language processing unit, or one used to make inferences based upon it.</p>

<p>What are the shortcomings of each? Advantages?</p>
"
1528,"<p>I'm a mechanical engineer by training, so please forgive my ignorance in this area.</p>

<p>There is a popular story regarding the back-of-the-envelope calculation performed by a British physicist named G. I. Taylor. He used <a href=""http://www.atmosp.physics.utoronto.ca/people/codoban/PHY138/Mechanics/dimensional.pdf"" rel=""nofollow noreferrer"">dimensional analysis</a> to estimate the power released by the explosion of a nuclear bomb, simply by analyzing a picture that was released in a magazine at the time.</p>

<p><strong>My question</strong></p>

<p>I believe many of you know some nice back-of-the-envelope calculations performed in machine learning (more specifically Neural-Networks). Can you please share them?</p>
"
1529,"<p>I am looking to train a chatbot that can help me relieve stress and deal with my negative emotions. I would like for the chatbot to be like the ones that pass the Turing test, remain professional, yet caring, and still be useful. I do not want my chatbot to become too general and unable to take the specific actions like recommending resources and providing general advice that I need it to. My two approaches are to manually train the chatbot with expected intents and expressions it would need to understand or use a custom deep learning model on conversational data. I think I have the right data to train the chatbot on, but I am not sure if this is the right approach. Any help on the direction to take the chatbot in would be helpful.</p>
"
1530,"<p>I'm new to CNNs and am wondering if I understand the relationship between the following terms:</p>

<p>In image analysis, receptive fields group ""input neurons"" to reduce the connection 
to the next layer. Convolution (using kernels or filters) reduces the scale but produces depth in the form of feature maps which isolate shapes for example. Each subsequent process reduces scale but produces greater depth. </p>

<p>Pooling is a process used further reduce data sets by (in the case of max pooling) taking the ""brightest"" or strongest features and generalizing. </p>

<p>The stride is the ""resolution"" and a longer stride will reduce accuracy but could act like pooling</p>

<p>To what extent is my understanding of the relationships accurate?
Thank you</p>
"
1531,"<p>I am searching for a publishable research project in AGI. I have previous experience with machine learning(via Andrew Ng's course), but I want to explore other aspects of intelligence apart from pattern recognition. </p>

<p>Given my undergraduate computer science background, any suggestions as to what I can work on? 
No constraint on the programming language(I have comfortable with python). Mathematical background: Linear algebra, will learn probability if required.</p>
"
1532,"<p>I am trying to understand how genetic programming can be used in autoencoders. 
I am going through a few papers (<a href=""https://www.ijcai.org/Proceedings/89-1/Papers/122.pdf"" rel=""nofollow noreferrer"">the classic one</a> and <a href=""http://www.springer.om/cda/content/document/cda_downloaddocument/9783319700953-c2.pdf?SGWID=0-0-45-1623165-p181221526"" rel=""nofollow noreferrer"">another</a>) but they dont help me to even grasp the concept of genetic programming in autoencoders. I understand that autoencoders are supposed to reconstruct the instances of the particular classes they have been trained on. If another fed instance is not reconstructed as expected, then it could be called an anomaly. </p>

<p>How does one use genetic programming in autoencoders? You are still required to create a neural network just instead of feed forward you use autoencoders? I would appreciate any tutorials or explanations. </p>
"
1533,"<p>Researchers at Stanford University released this paper in 2012:</p>

<p><a href=""http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf"" rel=""nofollow noreferrer"">http://cs229.stanford.edu/proj2012/BernalFokPidaparthi-FinancialMarketTimeSeriesPredictionwithRecurrentNeural.pdf</a></p>

<p>It goes on to discuss how they used echo state networks to predict things such as Google's stock prices. However to do this once trained, the network's inputs are a day's stock price, and the output is the day's predicted stock price. The way the paper is worded is like this could be used to predict future stock prices for example. However, to predict tomorrows stock price, you need to give the neural network tomorrows stock price...</p>

<p>All this paper seems to show is that the neural network is converging on a solution where it simply modifies its inputs a minimal amount, hence the output of the ESN is just a small alteration of its input.</p>

<p>Here are some Python implementations of the work shown in this paper:</p>

<ul>
<li><p><a href=""https://github.com/kimanalytics/Recurrent-Neural-Network-to-Predict-Stock-Prices"" rel=""nofollow noreferrer"">https://github.com/kimanalytics/Recurrent-Neural-Network-to-Predict-Stock-Prices</a></p></li>
<li><p><a href=""https://github.com/europa502/RNN-based-Bitcoin-Value-Predictor"" rel=""nofollow noreferrer"">https://github.com/europa502/RNN-based-Bitcoin-Value-Predictor</a></p></li>
</ul>

<p>In particular, I was playing with the latter which produces the following graph:</p>

<p><a href=""https://i.stack.imgur.com/OtqC0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OtqC0.png"" alt=""Figure 1""></a></p>

<p>If I take the same trained network, and alter the 7th's day's ""real"" stock price to say something extreme like $0, this is what comes out: </p>

<p><a href=""https://i.stack.imgur.com/DblgY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DblgY.png"" alt=""Figure 2""></a></p>

<p>As you can see, it basically regurgitates its inputs. So, what is the significance of this paper? It has no use in any financial predictions like the network shown in this paper: </p>

<ul>
<li><a href=""https://arxiv.org/pdf/1603.08604.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.08604.pdf</a></li>
</ul>
"
1534,"<p>I am having a difficult time translating this pseudocode into functional C++ code.</p>

<p><a href=""https://i.stack.imgur.com/aJjyc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aJjyc.png"" alt=""enter image description here""></a></p>

<ul>
<li>At line 10: The value function is represented as V[s], which has a bracket notation like arrays. Is this a separate method or just a function of the value with a given state? Why is S inside the brackets? Is this supposed to be an array with as many elements as S?</li>
<li>At line 12: Vk would be the element in index k inside of array V?</li>
<li>At line 16: I'm interpreting this as the start of a do-while loop that ends at line 20.</li>
<li>Line 19: I'm finding the action that maximizes the sum, for all states, of the equation following the sigma?</li>
<li>Line 20: I'm interpreting this a the end of the do-while. But what is this condition? Am I checking if there is an s such that this condition applies? So Would I would have to loop between all states and stop if any state satisfies the condition? (Basically a loop with a break, instead of a while)</li>
</ul>
"
1535,"<p>I have order data, here's a sample:</p>

<pre><code>Ninety-six (96) covered pans, desinated mark cutlery.
5 vovered pans by knife co.
(SEE SCHEDULE A FOR NUMBERS). 757 SOUP PANS
115 10-quart capacity pots.
Thirteen (13), 30 mm thick covered pans. 
</code></pre>

<p>I have over 50k rows of data such as this. In a perfect world, the above would need to be tabulated as such:</p>

<pre><code>count, type
96, covered pan
5, covered pan
757, soup pan
115, pot
13, covered pan
</code></pre>

<p>Could machine learning be the correct approach for a problem such as this?</p>
"
1536,"<p>I'm trying to implement a tree sort for 8 numbers. I've created 15 tree nodes agents and one manager agent. What I'm trying to achieve is to synchronize leaves in the tree and send their generated numbers to manager node. My problem is that in my for cycle, the leaves are not synchronized so manager is waiting for next leaf but that leaf may have sent it's number already. Is there a way to synchronize these leaves, so that manager waits for each leaf and then prints out received number?</p>

<p>Here are the agents:</p>

<p>treeSort.mas2j</p>

<pre><code>MAS treeSort {
    infrastructure: Centralised
    agents:
        manager;
        agent#15;
}
</code></pre>

<p>manager.asl:</p>

<pre><code>!start.
+!start : true &lt;-
    for ( .range(I,8,15) ) {
        .concat(""agent"",I,TempAgent);
        .print(""waiting from "",TempAgent);
        .term2string(YT,TempAgent);
        .wait(recievedNum(YT,X));
        .print(""from "",YT,"" recvd "",X);    
    };
    .println("" done"").
</code></pre>

<p>agent.asl</p>

<pre><code>!start.

@p1 +!start : .my_name(agent8) | .my_name(agent9)| .my_name(agent10)|.my_name(agent11)
        | .my_name(agent12) | .my_name(agent13) | .my_name(agent14) |.my_name(agent15) &lt;- +myNum(math.round(math.random(100)));
    ?myNum(X);
    +iam(list);
    .my_name(Y);
    .send(manager, tell, recievedNum(Y,X)).

@p2 +!start : .my_name(agent7) | .my_name(agent6)|  .my_name(agent5) |.my_name(agent4)
        | .my_name(agent3) | .my_name(agent2) &lt;- +iam(node).

@p3 +!start : .my_name(agent1) &lt;- +iam(root).
</code></pre>

<p>This is the output of program: <a href=""https://imgur.com/a/ewOAuxv"" rel=""nofollow noreferrer"">https://imgur.com/a/ewOAuxv</a> . As you can see, the manager is waiting for number from agent9 bu because he already got that number from him waiting for agent8, he will now wait forever. There is managers database: <a href=""https://imgur.com/a/ZZ35pD9"" rel=""nofollow noreferrer"">https://imgur.com/a/ZZ35pD9</a> . </p>
"
1537,"<p>Does the human brain use a specific activation function? I've tried doing some research, and as it's a treshold for whether the signal is sent through a neuron or not, it sounds a lot like ReLU. However, I can't find a single article confirming this. Or is it more like a step function (it sends 1 if it's above the treshold, instead of the input value).</p>
"
1538,"<p>In a recent paper the term “learned sampler” is used in the context of Gaussian processes and STRIPStream. <a href=""https://arxiv.org/pdf/1803.00967"" rel=""nofollow noreferrer"">Active model learning and diverse action sampling for task and motion planning</a> I didn't ever before heard that noun, but i can imagine that it could have something to do with procedural animation in which the parameters are determined by machine learning. And yes, the paper has a well formatted bibliography at the end, which might help to understand what they are talking about. But the idea of combining STRIPS, sampling, machine learning and motion primitive goes a bit ahead of my knowledge in Artificial Intelligence. Can somebody explain in easy English what a “Learned sampler” is doing?</p>

<p>As far as i can see from the bibliography – only as an addition – is, that the paper is citing other works from 2012 until 2017 from diverse fields. On one hand i see in the titles high-level symbolics planning a la STRIPS, on the other hand also papers about low level kernels from machine learning. Reading all the papers in linear fashion seems straight forward. So I won't criticize the work of the authors itself but only want to mention that the requirement to the readership is high.</p>
"
1539,"<p>Imagine a game where it is a black screen apart from a red pixel and a blue pixel.</p>

<p>Given this game to a human, they will first see that pressing the arrow keys will move the red pixel.</p>

<p>The next thing they will try is to move the red pixel onto the blue pixel.</p>

<p>Give this game to an AI and it will randomly move the red pixel until a million tries later it accidentally moves onto the blue pixel to get a reward.</p>

<p>If the AI had some concept of distance between the red and blue pixel it might try to minimise this distance.</p>

<p>Without actually programming in the concept of distance, if we take the pixels of the game can we calculate a number(s) such as ""entropy"" or suchlike that would be lower when pixels are far apart than when close together? It should work with other configurations of pixels. Such as a game with three pixels where one is good and one is bad. Just to give the neural network more of a sense of how the screen looks?</p>

<p>Then give the NN a goal such as try and minimise the entropy of the board as well as try to get rewards.</p>

<p>Is there anything akin to this in current research?</p>
"
1540,"<p>I need a word database to train from. I found a word2vec JS word vector database, but I need a method to teach it which words go in which patterns.</p>

<p>Please note that I am not asking to have you recommend a database, just asking for some qualities that I should look for in a database.</p>

<p>I want it to be able to run at a notice, and not take 10-ish minutes to set up. </p>

<p>I am using Word2Vec word vectors, provided in JSON form by <a href=""https://igliu.com/2016/01/09/word2vec-json/"" rel=""nofollow noreferrer"">Anthony Liu</a>. I am using the 1000 word list.</p>

<h3>TL;DR</h3>

<p>What are some qualities I should look for in a word database to train from?</p>
"
1541,"<p><code>Deep Learning</code> refers to <strong>set of techniques for learning in <code>Neural Nets</code></strong>. So does the statement ""reimplementation of Deep Learning algorithms replicating performance from papers""<br>
mean implementing these algorithms using which <code>Neural Networks</code> learn their weights and measuring do they perform as well as said in the research paper? or implementing a <code>model</code> that some research paper suggests and replicating its results?</p>

<p><strong>Context:</strong> OpenAI Machine Learning Fellow position demands this in an applicants profile state exactly in the following words:  </p>

<p><strong>""We look for candidates with one or more of the following credentials:<br>
Open-source reimplementations of deep learning algorithms which replicate performance from the papers""</strong></p>

<p>Therefore, What is it that they are technically looking for?</p>
"
1542,"<p>I am working on an anti-fraud project. In the project, we are trying to predict the fraud user in the out time data set. But the fraud user has a very low ratio, only 3%. We expect a model with a precision more than 15%.</p>

<p>I tried Logistic Regression, GBDT+LR, xgboost. All models are not good enough. Step wise Logistic Regression performs best, which has a precision of 9% with recall rate 6%.</p>

<p>Is there any other models that I can use for this problem or any other advise ?</p>
"
1543,"<p>I tried to build a neural network from scratch to build a cat or dog binary classifier using a sigmoid output unit. I seem to get the output value around 0.5(+/- 0.002) for every input. This seems really weird to me. Here's my code, Please let me know if there is a mistake in the implementation.</p>

<pre><code>def initialize_parameters_deep(layer_dims):
    l=len(layer_dims)
    parameters={}
    for l in range(1,len(layer_dims)):
        parameters['W'+str(l)]=np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
        parameters['b'+str(l)]=np.zeros((layer_dims[l],1))
    return parameters

def linear_forward(A,W,b):
    Z=np.dot(W,A)+b
    cache=(A,W,b)
    return Z,cache


def sigmoid(Z):
    A = 1/(1+np.exp(-Z))
    cache=Z
    return A, cache


def relu(Z):
    A = np.maximum(0,Z)

    assert(A.shape == Z.shape)

    cache = Z 
    return A, cache

def relu_backward(dA, cache):
    Z = cache
    dZ = np.array(dA, copy=True) # just converting dz to a correct object.

    # When z &lt;= 0, you should set dz to 0 as well. 
    dZ[Z &lt;= 0] = 0

    assert (dZ.shape == Z.shape)

    return dZ

def sigmoid_backward(dA, cache):
    Z = cache

    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)

    assert (dZ.shape == Z.shape)

    return dZ


def linear_activation_forward(A_prev,W,b,activation):
    if(activation=='sigmoid'):
        Z,linear_cache=linear_forward(A_prev,W,b)
        A,activation_cache=sigmoid(Z)
    elif activation=='relu':
        Z,linear_cache=linear_forward(A_prev,W,b)
        A,activation_cache=relu(Z)
    cache=(linear_cache,activation_cache)
    return A,cache

def L_model_forward(X,parameters):
    A=X
    L=len(parameters)//2
    caches=[]
    for l in range(1,L):
        A,cache=linear_activation_forward(A,parameters['W'+str(l)],parameters['b'+str(l)],'relu')
        caches.append(cache)
    AL,cache=linear_activation_forward(A,parameters['W'+str(L)],parameters['b'+str(L)],'sigmoid')
    caches.append(cache)
    return AL,caches

def compute_cost(AL,Y):
    m=Y.shape[1]
    cost=-1/m*np.sum(np.multiply(np.log(AL),Y)+np.multiply(np.log(1-AL),1-Y))
    return cost

def linear_backward(dZ,cache):
    A_prev,W,b=cache
    m=A_prev.shape[1]
    dW = np.dot(dZ,A_prev.T)/m
    db = np.sum(dZ,axis=1,keepdims=True)/m
    dA_prev = np.dot(W.T,dZ)
    return dA_prev,dW,db

def linear_activation_backward(activation,dA_prev,cache):
    linear_cache,activation_cache=cache
    if activation=='sigmoid':

        dZ=sigmoid_backward(dA_prev,activation_cache)
        dA_prev,dW,db=linear_backward(dZ,linear_cache)
    if activation=='relu':
        dZ=relu_backward(dA_prev,activation_cache)
        dA_prev,dW,db=linear_backward(dZ,linear_cache)
    return dA_prev,dW,db

def L_model_backward(AL,Y,caches):
    L=len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    grads={}
    current_cache=caches[-1]
    grads['dA'+str(L-1)],grads['dW'+str(L)],grads['db'+str(L)]=linear_activation_backward('sigmoid',dAL,current_cache)

    for l in reversed(range(L-1)):
        current_cache=caches[l]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward('relu',grads['dA'+str(l+1)],current_cache)
        grads[""dA"" + str(l)] = dA_prev_temp
        grads[""dW"" + str(l + 1)] = dW_temp
        grads[""db"" + str(l + 1)] = db_temp
    return grads
def Grad_Desc(parameters,grads,learning_rate):
    L=len(parameters)//2
    for l in range(L):
        parameters['W'+str(l+1)]=parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]
        parameters['b'+str(l+1)]=parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)] 
    return parameters

def L_layer_model(X,Y,learning_rate,num_iter,layer_dims):
    parameters=initialize_parameters_deep(layer_dims)
    costs=[]
    for i in range(num_iter):
        AL,caches=L_model_forward(X,parameters)
        cost=compute_cost(AL,Y)
        grads=L_model_backward(AL,Y,caches)
        parameters=Grad_Desc(parameters,grads,learning_rate)
        if i%100==0:
            print(cost)
            costs.append(cost)
    plt.plot(np.squeeze(costs))
def predict(X,parameters):
    AL,caches=L_model_forward(X,parameters)
    prediction=(AL&gt;0.5)
    return AL,prediction

L_layer_model(x_train,y_train,0.0075,12000,[12288,20,7,5,1])
prediction=predict(x_train,initialize_parameters_deep([12288,20,7,5,1])) 
</code></pre>
"
1544,"<p>How can we attach numerical value to the measurement of soft skill? Is there any book, site or materials that can teach me how to do this?</p>

<p>I want to know if there is any way we can measure soft skill as listed  <a href=""https://training.simplicable.com/training/new/87-soft-skills"" rel=""nofollow noreferrer"">here</a> and say a person  has (over 100) 70% or 60% in critical thinking ability or his critical thinking ability is over 72% and be accurate with the score.</p>
"
1545,"<p>One of my friends built a version of ""Achtung, Die Kurve!"", or ""Curve Fever"" in Python. I was starting to study ML and decided to tackle the game from a learning perspective - write a bot that would crush him in the game. Did some research and found Deep Q learning. Decided to go with that and after a whole lot of throwing around different hyperparameters and layers, I decided I need some help on this. I am new to Deep and Machine Learning in general, so I may have missed things. I was kinda discouraged when I saw that Deep Q is SO impractical currently in the field.</p>

<p>how would you guys tackle this problem? I need some guidance/help building it if someone is up to the task.</p>
"
1546,"<p><a href=""https://i.stack.imgur.com/IzJ5Q.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IzJ5Q.jpg"" alt=""Picture""></a></p>

<p>So I know that 'h' and 'f' will be pruned, but I'm not sure about 'k' and 'l'.</p>

<p>When we visit 'j', technically there is no need for us to visit 'k' and 'l' because there are 2 options:</p>

<ol>
<li>one or two of them might be higher than 8 ('j') </li>
<li>both of them less than 8 </li>
</ol>

<p>But no matter what, the decision of the max(root) will not change, the max will choose the right side no matter what 'k' and 'l' are, because the right side will either be 8 or 9, which is still higher than 4 (returned value from left side)</p>

<p>so will alpha beta prune 'k' and 'l' or not? if not, then it means alpha beta is not ""optimal"" overall right? considering it will not prune all the unnecessary paths.</p>
"
1547,"<p>In MCTS, we start at root node R.
Then we select some leaf node L.
And we expand it by one or more child nodes and simulate from the child to end of game.<a href=""https://commons.wikimedia.org/wiki/File:MCTS_(English)_-_Updated_2017-11-19.svg"" rel=""nofollow noreferrer"">image link</a></p>

<p>My question is when to expand? and when to simulate? 
So why not expand 2 or 3 levels, and simulate from there, then backup values to top? 
Should we just expand 1 level? why not more? what is the criteria? </p>
"
1548,"<p>I am training <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""nofollow noreferrer"">pre-trained SSD-InceptionV2-Coco</a> to detect the ""car"", </p>

<p>which is one of the classes in <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt"" rel=""nofollow noreferrer"">mscoco label</a>.</p>

<p>I train the model with ~50k sample from KITTI, 500k iteration with batch size 2.</p>

<p>I followed <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/dataset_tools/create_kitti_tf_record.py"" rel=""nofollow noreferrer"">this script</a> to generate tfrecord file.</p>

<p>Then I test both original pre-trained model and my trained model with one video.</p>

<p>The performance of my trained model is worse. More missing detected results.</p>

<p>One thing I found recently is the classification_loss/localization_loss increases when AvgNumGroundtruthBoxesPerImage increases.</p>

<p><a href=""https://i.stack.imgur.com/EjMbi.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EjMbi.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/g8Sr1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g8Sr1.jpg"" alt=""enter image description here""></a></p>

<p><strong>EDIT</strong></p>

<p><a href=""https://i.stack.imgur.com/PLbux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PLbux.png"" alt=""enter image description here""></a></p>

<p>Another thing I found is the more ground truth boxes per image I have, </p>

<p>the less average num positive anchors per image I have.</p>

<p>This bothers me because if the number of anchors generated per image is fixed,</p>

<p>more ground truth boxes should provide more positive anchors per image.</p>

<p>So I wonder where to find the root cause.</p>

<p>Any suggestion is welcome.</p>

<p>Thank you for precious time on my question.</p>
"
1549,"<p>Everything from facial recognition to the google home is coming equiped with A.I and it is being widely used , If autonomously connected to the internet , will A.I pose a threat to privacy or will it endanger free will if used for surveillance with facial recognition , like in the movie 'Minority Report'</p>
"
1550,"<p>a number has randomly been chosen from 1 to 3. in each step we can make a guess and we will be told if our guess is equal, bigger or smaller than the chosen number. we're trying to find the number with the least number of guesses.</p>

<p>I need to draw the MDP model for this question with 7 states but i don't know how the states are supposed to be defined. can anyone help?</p>
"
1551,"<p>Suppose one is using a multi-armed bandit, and one has relatively few ""pulls"" (i.e. timesteps) relative to the action set. For example, maybe there are 200 timesteps and 100 possible actions. </p>

<p>However, you do have information on how similar actions are to each other. For example, I might want to rent a car, and know the model, year, and mileage of each car. (Specifically, I want to rent a car on a daily basis for each day in a 200 day period; on each day, I can either continue with the existing car or rent a new one. There are 100 possible cars.)</p>

<p>How can I exploit this information to choose actions that maximize my payoff.</p>
"
1552,"<p>Previously I had trained a <code>Neural Network</code>upon 20,000 character images. This <code>Neural Net</code> generally works well, it uses <code>RGB- Hue, Saturation, Intensity</code> feature set for training. However, there can be certain character images which have <code>RGB-HSI</code> values that this <code>neural net</code> has not seen before. Therefore I am looking forward to converting training data to <code>grayscale</code> and use some feature set well suited for grayscale images.</p>

<p>So are there any good suggestion for extracting a feature set out of <code>grayscale</code> images.</p>
"
1553,"<h1>Problem</h1>

<p>Given a collection of  pairs (X, y) where X belongs to R^n and y belongs to R, find the X such that the associated y would be maximum.</p>

<h1>Example</h1>

<p>Given:</p>

<ul>
<li>(X=(1, 2), y=-9)</li>
<li>(X=(-2, 4), y=-36)</li>
<li>(X=(-4, 2), y=-24)</li>
<li>...</li>
</ul>

<p>The algorithm should be able to detect that the function being applied to X is y=-(X[0]^2+2*(X[1]^2)) and find the input that maximizes this function, in this case X=(0,0) because y=0^2+2*0^2=0 and 0 is the maximum possible value, as all the other values are negative.</p>

<h1>How I've tried to solve it</h1>

<p>My first guess has been to create a neural network that predicts y given X, but, after that is done, I don't know how to go about optimizing the input.</p>

<h1>Questions</h1>

<p>Is there any algorithm that would help in this situation?</p>

<p>Also, would some other supervised learning algorithm fit better here than a neural network?</p>
"
1554,"<p>This is AI: A Modern Approach, 3.17c. The solution manual gives the answer as $\frac{d}{\epsilon}$, where $d$ is the depth of the shallowest goal node.</p>

<p>Iterative lengthening search uses a path cost limit on each iteration, and updates that limit on the next iteration to the lowest cost of any rejected node.</p>

<p>I have seen this question posted elsewhere as, ""What is the number of iterations with a continuous range $[0, 1]$ and a minimum step cost $\epsilon$?"" In that case, I agree that the minimum number of iterations is $\frac{d}{\epsilon}$ because you would need to increase the path cost limit by a minimum of $\epsilon$ with each iteration.</p>

<p>However, with a continuous range of $[\epsilon, 1]$, it seems there is an infinite range and that the number of iterations is potentially infinite, since there is no minimum step cost. Should this solution actually be infinite?</p>
"
1555,"<p>Word2vec assigns an N-dimensional vector (a dimensional reduction, really) to given words. It turns out that, at least with a number of canonical examples, vector arithmetic seems to work intuitively. </p>

<p><code>king + woman - man = queen</code></p>

<p>These terms are all N-dimensional vectors. So, what we really might have (numbers made up and N=3 here) is:</p>

<p><code>king(0,1,2) + woman(1,1,0) - man(2,2,2) = queen(-1,0,0)</code></p>

<p>In this (contrived) example, the last dimension (king/man=2, queen/woman=0) suggests a semantic concept of gender. Aside from semantics, a given dimension could ""mean"" a part of speech, first letter, or really any feature or set of features that the algorithm might have latched onto. However, any perceived ""meaning"" of a single dimension might well just be a  simple coincidence.</p>

<p>The question: if we picked out only a single dimension, does that dimension itself convey some predictable or determinable information? Or is this purely a ""random"" artifact of the algorithm, with only the full N-dimensional vector distances mattering?</p>
"
1556,"<p>I have implemented a neural network (NN) using python and numpy only for learning purposes. I have already coded learning rate, momentum, and L1/L2 regularization and checked the implementation with gradient checking. </p>

<p>A few days ago, I implemented batch normalization using the formulas provided by the <a href=""https://arxiv.org/pdf/1502.03167.pdf"" rel=""nofollow noreferrer"">original paper</a>. However, in contrast with learning/momentum/regularization, the batch normalization procedure behaves differently during <strong>fit</strong> and <strong>predict</strong> phases - both needed for gradient checking. As we fit the network, batch normalization computes each batch mean and estimates the population's mean to be used when we want to predict something.</p>

<p>In a similar way, I know we may not perform gradient checking in a neural network with <strong>dropout</strong>, since dropout turns some gradients to zero during <em>fit</em> and is not applied during <em>prediction</em>.</p>

<p>Can we perform gradient checking in NN with batch normalization? If so, how?</p>
"
1557,"<p>Theoretically speaking, when the singularity happens, could we aptly describe this event as the creation of GOD? I am agnostic in the sense that I don't believe any of the Gods of the present or past are real in any way shape or form, however when the singularity happens, is this even essentially the birth of a GOD? I mean a superhuman being, machine or spirit worshipped as having power over nature or human fortunes is by definition a GOD.</p>
"
1558,"<p>To start, let me just say that I am very new to tensorflow and Machine Learning in general. But, as part of my learning project I am trying to adapt the tensorflow wide and deep model to generate movie recommendations. However, the part I'm getting stuck on is handling multiple values for a categorical column. Below is a sample of how a couple of rows in my CSV look like.</p>

<pre><code>    ID,Genres,Tags,Rating,Recommendations
    ----------------------------------------
    1,genre1:genre2:genre3,tag1:tag3,4.3,44
    2,genre2:genre3,tag1:tag5,3.7,22
</code></pre>

<p>The Genres and Tags column have multiple categorical values. I have looked at <a href=""https://www.tensorflow.org/api_docs/python/tf/string_split"" rel=""nofollow noreferrer""><code>tf.string_split</code></a> to parse the strings and return a <code>SparseTensor</code>.</p>

<p>Once I have parsed my delimited string values into <a href=""https://www.tensorflow.org/api_docs/python/tf/SparseTensor"" rel=""nofollow noreferrer""><code>SparseTensor</code></a>, what do I do with? If I want to create <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file"" rel=""nofollow noreferrer""><code>categorical_column_with_vocabulary_file</code></a>, how does the <code>SparseTensor</code> interact with it? Is that even the correct step? Should the <code>SparseTensor</code> be converted into a something before I can create a <code>categorical_column_*</code>? </p>

<p>I am just not sure how to train the Wide and Deep model when you have multiple categorical values for a single column. Some people have suggested that I use each 'Genre' as its own column and encode it using One-Hot, but it is not realistic for me to do this, because there could be 100s of genres and tags in the data.</p>

<p>Any help on this matter would be welcome. Thank you!!</p>
"
1559,"<p>With so much innovation, with so much manual labor being replaced tasks performed in minutes or seconds by an artificial intelligence, one day man will put the survival and propagation of his species above his ideologies and cultures ...</p>

<p>I am worried because we are living the fourth industrial revolution, and this will generate millions of unemployment, even if new jobs are created in the future. The problem is that a lot of humans worry about their own job, and not about their own children's future. This is completely retrograde.</p>

<p>Will one day Artificial Intelligence be able to direct us towards an intelligent path as a propagation of the species, or else center the focus of humanity on something that it adds?</p>
"
1560,"<p>Semi gradient methods work well in <strong>Reinforcement Learning</strong>, but what is the reason of <strong><em>not using</em></strong> the true gradient if it can be computed? I tried it on the cart pole problem with a deep Q-Network and it performed much worse than traditional semi gradient, is there a concrete reason for this?</p>
"
1561,"<p>I am developing PDA like Google assistant on Android. So far, so good.
But now, I want to add contextual follow up like Google assistant so it can keep the train of thought.
As demonstrated here- <a href=""https://www.youtube.com/watch?v=xYRENGuwwCA"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=xYRENGuwwCA</a></p>

<p>Can anyone guide me or hint how to design the algorithm?
Thank you very much!</p>
"
1562,"<p>I am unable to identify general temrs or specific source of information for the below proposed problem. I would appreciate if the community can guide me to journal articles/books and keywords to look for in literature.</p>

<p><strong>Problem:</strong></p>

<p>There is a non-linear dynamic system taking input and producing 1D time series as output. I would like to use NN to find parameters of the dynamic system, according to the time series output. That is, mapping the features of the time series (after transformation, likely Fourtier Transform or Wavelet) to the parameters governing the dynamics of the system.</p>

<p><strong>Research so far:</strong></p>

<p>I have found a few journal papers mostly processing sounds of rolling bearings or hearbeat but only for error/failure classification. </p>

<ol>
<li>Rolling Bearing Fault Diagnosis Based on STFT-Deep Learning and SOund Signals</li>
<li>Deep Learning Enabled Fault Diagnosis Using Time-Frequency Image Analysis of Rolling Element Bearings</li>
<li>Deep Learning Based Approach for Bearing Fault Diagnosis</li>
<li>Detecting atrila fibrillation be deep convolutional neural networks</li>
</ol>

<p>(the above are classification problems, my problem is about parameter identification)</p>

<p><strong>Reason to address this on StackExchange:</strong></p>

<p>I think I am missing overview about the topic (identification of dynamic systems using NN), because I am not able to reach more profound information. Also, I think that NN would be more beneficial to my current application than lets say optimization by evolutionary algorithms, threfore I am specifically asking for NN.</p>
"
1563,"<p>I'm trying to understand what would be the best neural network for implementing a XOR gate. I'm considering a neural network to be good if it can produce all the expected outcomes with the lowest possible error.</p>

<p>It looks like my initial choice of random weights has a big impact on my end result after training. The accuracy (i.e. error) of my neural net is varying a lot depending on my initial choice of random weights.</p>

<p>I'm starting with a 2 x 2 x 1 neural net, with a bias in the input and hidden layers, using the sigmoid activation function, with a learning rate of 0.5. Below my initial setup, with weights chosen randomly:</p>

<p><a href=""https://i.stack.imgur.com/4tv2V.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4tv2V.jpg"" alt=""enter image description here""></a>  </p>

<p>The initial performance is bad, as one would expect:</p>

<pre><code>Input | Output | Expected | Error
(0,0)   0.8845      0       39.117%
(1,1)   0.1134      0       0.643%
(1,0)   0.7057      1       4.3306%
(0,1)   0.1757      1       33.9735%
</code></pre>

<p>Then I proceed to train my network through backpropagation, feeding the XOR training set 100,000 times. After training is complete, my new weights are:</p>

<p><a href=""https://i.stack.imgur.com/bPMix.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bPMix.jpg"" alt=""enter image description here""></a> </p>

<p>And the performance improved to:</p>

<pre><code>Input | Output | Expected | Error
(0,0)   0.0103      0       0.0053%
(1,1)   0.0151      0       0.0114%
(1,0)   0.9838      1       0.0131%
(0,1)   0.9899      1       0.0051%
</code></pre>

<p><strong>So my questions are:</strong></p>

<ol>
<li><p>Has anyone figured out the best weights for a XOR neural network with that configuration (i.e. 2 x 2 x 1 with bias) ?</p></li>
<li><p>Why my initial choice of random weights make a big difference to my end result? I was lucky on the example above but depending on my initial choice of random weights I get, after training, errors as big as 50%, which is very bad.</p></li>
<li><p>Am I doing anything wrong or making any wrong assumptions?</p></li>
</ol>

<hr>

<p>So below is an example of weights I cannot train, for some unknown reason. I think I might be doing my backpropagation training incorrectly. I'm not using batches and I'm updating my weights on each data point solved from my training set.</p>

<p>Weights: <code>((-9.2782, -.4981, -9.4674, 4.4052, 2.8539, 3.395), (1.2108, -7.934, -2.7631))</code></p>

<p><a href=""https://i.stack.imgur.com/Q3dCO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q3dCO.jpg"" alt=""enter image description here""></a></p>
"
1564,"<p>I was following <a href=""http://natureofcode.com/"" rel=""nofollow noreferrer"">Daniel Shiffman's</a> tutorials on how to write your own neural network from scratch. I specifically looked into his <a href=""https://www.youtube.com/watch?v=MPmLWsHzPlU"" rel=""nofollow noreferrer"">videos</a> and the code he provided in <a href=""https://github.com/CodingTrain/website/blob/master/Courses/natureofcode/10.18-toy_neural_network/lib/nn.js"" rel=""nofollow noreferrer"">here</a>. I rewrote his code in Python, however, 3 out of 4 of my outputs are the same. The neural network has two input nodes, one hidden layer with two nodes and one output node. Can anyone help me to find my mistake? Here is my <a href=""https://pastebin.com/saJxedDT"" rel=""nofollow noreferrer"">full code</a>. </p>

<pre><code>import random

nn = NeuralNetwork(2,2,1)
inputs  = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])
targets = np.array([[0], [1], [1], [0]])
zipped = zip(inputs, targets)
list_zipped = list(zipped)

for _ in range(9000):
    x, y = random.choice(list_zipped)
    nn.train(x, y)

output = [nn.feedforward(i) for i in inputs]

for i in output:
   print(""Output "", i)

#Output  [ 0.1229546]  when it should be around 0
#Output  [ 0.6519492]  ~1
#Output  [ 0.65180228] ~1
#Output  [ 0.66269853] ~0
</code></pre>

<p><strong>EDIT_1:</strong> I tried debugging my code by choosing all weights and bias' values to 0.5. I did this in both my code and Daniel's. This obviously ended up showing me <em>all</em> outputs with the same value. </p>

<p>After that I increased my weights and bias' values variety from [0 , 1) to [-1, 1). By running this a few times, I would <strong>sometimes</strong> get the correct output: </p>

<pre><code>[ 0.93749991] # should be ~1
[ 0.93314793] # ~1 
[ 0.07001175] # ~0
[ 0.06576194] # ~0
</code></pre>

<p>If I ran nn.train() 100 000 times, I get the correct output 2/3 times. 
Is this the issue of gradient descent, where it converges to the local minima?</p>
"
1565,"<p>I'm starting a project that will involve computer vision, visual question answering and explainability, and am currently choosing what type of algorithm to use for my classifier - a neural net, or a decision tree.</p>

<p>It would seem to me that , because I want my system to include explainability, a decision tree would be the best choice. Decision trees are interpretable, whereas neural nets are like a black box.</p>

<p>The other differences I'm aware of are: decision trees are faster, neural networks are more accurate, and neural networks are better at modelling nonlinearity.</p>

<p>In all of the research I've done on computer vision and visual question answering, everyone uses neural networks, and no one seems to be using decision trees. Why? Is it for the accuracy? I think a decision tree would be better because it is fast and interpretable, but if no one's using them for visual question answering, they must have a disadvantage that I haven't noticed.</p>
"
1566,"<p>My question concerns a side question (which was not answered) asked here:
<a href=""https://ai.stackexchange.com/questions/4085/policy-gradients-for-multiple-continuous-actions"">Policy gradients for multiple continuous actions</a></p>

<p>I am trying to implement a simple policy gradient algorithm for a discrete multi-action reinforcement learning task. To be more precise, there are three actuators. At every time step, each of the actuators can perform one of three possible actions. </p>

<p>Is it possible to adjust the loss function from the single action case per time step</p>

<p><span class=""math-container"">$$L = \log(P(a_1)) A$$</span></p>

<p>to the n-action case per time step like so?</p>

<p><span class=""math-container"">$$L = (\log(P(a_1)) + \log(P(a_2))+ \dots + \log(P(a_n))) A$$</span>?</p>
"
1567,"<p>I have a friend who, after having a stroke, now has locked-in syndrome who and can now only move his eyes and eyelids. I've been working on some C# software (written in Unity3D) to help him to communicate, the ones he has tried so far have not been successful due to the level of his disability.</p>

<p>I am in the process of implementing DLib for facial feature recognition. What I would like to do is to ask the user to repeatedly alternate between a neutral face and a face that is indicating. I don't want to limit the indication gesture specifically to winking/blinking/opening eyes wide, etc as I want other people to be able to freely download the app and indicate in whatever way they find easiest due to their incapacity.</p>

<p>I could really do with some pointers as to how to have my app learn what the user's facial indicator is by comparing the facial landmarks of the two states.</p>

<p>At this point, I don't even know what it is I need to know. I don't know the names of any AI algorithms that might be relevant or anything.</p>

<p>I know posts like this are often closed for being ""too vague"", but due to the nature of the request, I'd appreciate it if it could be left open as any suggestions at all could potentially be life-changing.</p>

<p>OpenCV would be an excellent choice as I already have a licence for a library in Unity3D.</p>
"
1568,"<p>I just want to know why do Machine Learning engineers and AI programmers use languages like python to perform AI task and not C++ even though C++ is technically a more powerful language than python.</p>
"
1569,"<p>I have a data set that looks like this:<br/></p>

<p><a href=""https://i.stack.imgur.com/Nd0qN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nd0qN.png"" alt=""data set""></a></p>

<p>I would like to estimate a relationship between x-values and the corresponding 5% extreme y-values, something that might look like that :<br/></p>

<p><a href=""https://i.stack.imgur.com/TfBTp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TfBTp.png"" alt=""regression on extreme values""></a></p>

<p>Do you have an idea of an algorithm that might help me for this ? I thought about labelling the extreme values for later finding a separating hyperplane, but I have no clue on how to label these ""extreme values"" (I cannot just take the 5% lowest and highest values as all these would end up in the same region).</p>

<p>Thanks for your ideas ! </p>
"
1570,"<p>I am university student and i am looking for <strong>AI tool for image/video labeling</strong>. It would be great if someone share own experience as well with suggested tool.</p>

<p>I also have some questions,</p>

<p>1- which tools are available? (state of the art)</p>

<p>2- what are their benefits, how do they compare?</p>

<p>3- why did we choose the specific tool?</p>
"
1571,"<p>Currently I am working with two large speech databases and I was asked to build one subset from each one in order to get two representative subsets with the same number of utterances. My question is: the number of utterances is the same as the number of audio files?</p>

<p>Thank you so much!</p>
"
1572,"<p>As far as I understand, Q-learning and policy gradients (PG) are the two major approaches used to solve RL problems. While Q-learning aims to predict the reward of a certain action taken in a certain state, policy gradients directly predict the action itself.</p>

<p>However, both approaches appear identical to me, i.e. predicting the maximum reward for an action (Q-learning) is equivalent to predicting the probability of taking the action directly (PG). Is the difference in the way the loss is back-propagated?</p>
"
1573,"<p>Some context: Recently all kind of salesmen have been knocking on our company's door to provide their ""artificial intelligence"" expertise and projects suggestions. Some don't know the difference between words estimation and validation (really), some have extraordinary powerpoints and paint themselves as gurus of the field. Our management has gone with the hype and definitely we're starting some kind of project on ""artificial intelligence"" (meaning rpa with some machine learning possibly).</p>

<p>What is the best way to start when we don't yet know to what problem we want to apply all this and I'm worried it will lead to long expensive projects with meager results? What are the things to watch out for? Any good practical books or war stories out there?</p>
"
1574,"<p>I am currently getting into Deep Learning and would like to set up an environment for training an Artificial Neural Network or NEAT to play simple video games on NES (Mario etc.) and SNES ( Donkey Kong Country etc.), using TensorFlow/TFLearn in Python.</p>

<p>I started off with OpenAI gym environment and there is actually a super-mario environment for gym on github, which I fail to install as Gym-pull is not available anymore and latest gym package doesn't even have scoreboard folder (I am on windows 10, conda environment).</p>

<p>Now, what would be the best way to set up a solid training environment that will be similar to OpenAI gym in terms of simplicity?</p>

<p>Unfortunately OpenAI universe isn't compatible with Windows 10 atm, and I really don't want to get a different setting like ubuntu environment to make it work. I would like to stay in win10.</p>

<p>If someone could guide me for suggested setup or refer me to articles/ documentation where similar things have been done in win10 python env. for NES/SNES, I would be extremely grateful! I assume an emulator with python API (perhaps Nintaco?) is a way to go? How would I then get the 'observation output', I would need to scan the live pixel output of the game, which I am not sure how to do.</p>

<p>Regards,</p>

<p>Mark</p>
"
1575,"<p>I have a question about the training sequence regarding Neural Network recognition. Let's say an image has 28*28 pixels, which leads to 784 Input Nodes with various greyscale values and 10 output nodes, if the image shows a number 0-9. Then when the training data is used for a set of known pictures of numbers, the weights and hidden layers uses forward- and backpropagation to get get the proper hidden and weight layers for the known layer. However, doesn't a new training picture destroy the trained and balanced weights and nodes values? Because the weights and hidden nodes have been calibrated to recognize the former training picture?
Thank you for assistance.</p>

<p>Kind regards
David</p>
"
1576,"<p>Let's assume a common game scenario of several characters in a combat arena. Each character has different strengths and weaknesses. The arena has traps and tools. Suppose the characters had only very basic moves such as step in a direction, shoot, climb, duck, pick up item,  use item, drag heavy object. Each move has a chance of success  based on the context (e.g. range to target). What AI, machine learning, or evolutionary approach could be used to generate personalized tactics for each character based on repeated runs of the scenario?</p>
"
1577,"<p>I'm training a LSTM network with multiple inputs and several LSTM layers in order to setup a time series gap filling procedure. The LSTM is trained bidirectionally with ""tanh"" activation on the outputs of the LSTM, and one Dense layer with ""linear"" activation comes at the end to predict the outputs. The following scatterplot of real outputs vs the predictions illustrates the problem:</p>

<p><a href=""https://i.stack.imgur.com/Smj3i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Smj3i.png"" alt=""Outputs (X axis) vs predictions (Y axis)""></a></p>

<p>The network is definitely not performing too bad and I'll by updating the parameters in next trials, but the issue at hand always reappears. The highest outputs are clearly underestimated, and the lowest values are overestimated, clearly systematic.</p>

<p>I have tried min-max scaling on inputs and outputs and normalizing inputs and outputs, and the latter performs slightly better, but the issue persists.</p>

<p>I've looked a lot in existing threads and Q&amp;As, but I haven't seen something similar.</p>

<p>I'm wondering if anyone here sees this and immediately knows the possible cause (activation function? Preprocessing? Optimizer? Lack of weights during training? ... ?). Or, and in that case it would also be good to know, if this is impossible to find out without extensive testing.</p>

<p>Thanks a lot in advance.</p>
"
1578,"<p>I am new to AI programming and was wondering what are some good tools to begin creating an electronic assistant (similar to OK Google or siri) I do know JAVA, but I don't know how helpful that would be. Could anyone provide some insight? </p>

<p>Edit: There is suggestion that my post is a duplicate of this question: <a href=""https://ai.stackexchange.com/questions/5958/how-can-i-train-my-computer-work-for-me"">How can I train my computer work for me?</a>  </p>

<p>I'm just clarifying that my goal is to a make a <em>speech</em> based AI program, while the poster of the above question is making a <em>text</em> based program. The only similarity is the suggested software, which I now realize can be used in both situations. </p>
"
1579,"<p>I am working in the following neural network architecture, I am using keras and TensorFlow as a back-end.</p>

<p>It is composed by the following, embedding of words, then I added a layer of 
Long Short-Term Memory (LSTM) neural networks, one layer of output and finally. I am using the softmax activation function.</p>

<pre><code>model = Sequential()
model.add(Embedding(MAX_NB_WORDS, 64, dropout=0.2))
model.add(LSTM(64, dropout_W=0.2, dropout_U=0.2)) 
model.add(Dense(8))
model.add(Activation('softmax'))
</code></pre>

<p>I have the following question, if I am getting a model through this code, could the final product be called a deep learning model?,
I know that this code is very small however there is a lot of computations that the machine is making on the background. </p>
"
1580,"<p>I have an approximately 90,000 row dataset that has information of social media profiles which has columns for biography, follower count, language spoken, name, username and the <strong>label (to identify whether the profile is that of an influencer, brand or news and media)</strong>.</p>

<p>Task: I have to train a model that predicts the label. I then need to produce a confidence interval for each prediction.</p>

<p>As I have never come across a problem like this, I am just after some suggestions of what models I should be using for a situation like this? I am thinking Natural Language Processing (NLP), but not sure. </p>

<p>Also, for NLP (if a suitable method), any codes or advice to help me implement for the first time on Python would be greatly appreciated! Thanks in advanced</p>
"
1581,"<p>I can't find much information on modern PDDL usage. Are there more popular alternatives, maybe something more suited to modern neural network/deep learning techniques?</p>

<p>I'm particularly interested in PDDL or alternative's current usage in autonomous driving software.</p>
"
1582,"<p>It is a new era and people are trying to evolve more in science and technology. Artificial Intelligent is one of the ways to achieve this. We seen lots of examples for AI sequences or a simple ""communication AI"" that able to think by themselves and they are often shift to the communication of building a world where machines will rise. This is what high thinking people like Stephen Hawking and Elon Musk is afraid of, to be in that kind of war. </p>

<p>Is it possible to build an AI, able to think by themselves but limited to the over ruling of humankind, or teach it of the moral way in treating peace and work alongside human, so they could fight alongside human if ever, this kind of catastrophic happens in the future? It could be an advantage.</p>
"
1583,"<p>I'm trying to write my own implementation of NEAT and I'm stuck on the network evaluate function, which calculates the output of the network.</p>

<p>NEAT as you may know contains a group of neural networks with continuously evolving topologies by the addition of new nodes and new connections. But with the addition of new connections between previously unconnected nodes, I see a problem that will occur when I go to evaluate, let me explain with an example:
<a href=""https://i.stack.imgur.com/cJTjN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cJTjN.png"" alt=""Network""></a></p>

<pre><code>INPUTS = 2 yellow nodes
HIDDEN = 3 blue nodes
OUTPUT = 1 red node
</code></pre>

<p>In the image a new connection has been added connecting node3 to node5, how can I calculate the output for node5 if I have not yet calculated the output for node3, which depends on the output from node5?</p>

<p>(not considering activation functions)</p>

<pre><code>node5 output =  (1 * 0.5) + (1 * 0.2) + (node3 output * 0.8)
node3 output =  ((node5 output * 0.7) * 0.4)
</code></pre>
"
1584,"<p>I am a beginner: I've only read a book about neural network and barely implemented one in C.</p>

<p>In short:</p>

<ul>
<li>A neural network is built out of nodes,</li>
<li>Each node holds an output: <code>activation.(sum.(x * w))</code>,</li>
<li>We then compute the total error out of the network output.</li>
</ul>

<p>From a beginner perspective, hyper-parameters, such as the number of layers needed, seem to be defined arbitrarily in most tutorials, books. In fact, the whole structure seems to be quite arbitrarily defined. In practice, hyper-parameters are often defined based on some standards.</p>

<p>My question is, if you were to talk to a total beginner, how would you explain to him the structure of a neural network in such a way that the whole thing would appear as obvious ? Is that even possible ?</p>

<p>Here, the word structure refers to a neural network being a configuration of nodes inside layers.</p>

<p>Thanks to anyone pointing out ambiguities or spelling errors.</p>

<p>Edit: note that I actually understand the whole back-propagation algorithm. I have no problem visualizing a nn.</p>
"
1585,"<p>The basis of Q-learning is recursive (similar to dynamic programming), where only the absolute value of the terminal state is known. Shouldn't it make sense to feed the model a greater proportion of terminal states initially, to ensure that the predicted value of a step in terminal states (zero) is learned first? </p>

<p>Will this make the network more likely to converge to the global optimum?</p>
"
1586,"<p>I know that when creating neural networks it's standard practice to create a 'random seed' so that you can get producible results in your models. I have a couple of questions regarding this:</p>

<ul>
<li>Is the seed just something that is used in the 'learning' phase of the network or does it get saved? i.e. is it saved into the model itself and used by others if they decide to implement a model you created?</li>
<li>Does it matter what you choose to be the seed? Should the number have a certain length?</li>
<li>At what step of the creation of a model does this seed get used and how does it get used?</li>
</ul>

<p>Other information about 'random seeds' would be welcomed! But these are my general questions.</p>
"
1587,"<p>I'm trying to make a neural network that detects certain instruments in a song. I don't know for sure if I should use an RNN, CNN OR DNN. Which one is best for this situation?</p>
"
1588,"<p>I am learning about searching strategies in AI and I was reading that Breadth first search is only optimal when the cost solution is a non-decreasing function? I am not really sure what this refer to, since decreasing search cost should be our goal. Am I missing something?</p>
"
1589,"<p>I've looked into policy gradient RL the last few months. As I find the topic quite interesting, I've been readings lots of papers about it. My aim is to write my master thesis in Maths about it. I already started out, the preliminary title being ""Techniques for variance reduction in policy gradient reinforcement learning"". Of course, I can sum up latest results, but a Master thesis' aim should be to create sth. new, or apply sth. to a new setting.
Does anybody have an idea for a nice application? It was my idea to write the thesis in ML. My professor is not that much into ML but is happy to advise and evaluate the thesis. </p>

<p>Advice highly appreciated!</p>
"
1590,"<p>Is it possible to use a VAE to reconstruct and image starting from an initial image instead of using K.random_normal as show in the “sampling” function of <a href=""https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py"" rel=""nofollow noreferrer"">this example</a>?</p>

<p>I have used a sample image with the VAE encoder to get z_mean and z_logvar.
I have been given 1000 pixels in an otherwise blank image (with nothing in it)
Now I want to reconstruct the sample image using the decoder with a given constraint that the 1000 pixels in the otherwise blank image remain the same. The remaining pixels can be reconstructed so they are as close to the initial sample image as possible.
In other words, my starting point for decoder is a blank image with some pixels that don’t change.</p>

<p>How can I modify the decoder to generate an image based on this constraint?
Is it possible? Are there variations of VAE that might make this possible? So we can predict the latent variables by starting from an initial point(s)?</p>
"
1591,"<p>After watching 3Blue1Brown's tutorial series, and an array of others, I'm attempting to make my own neural network from scratch.</p>

<p>So far, I'm able to calculate the gradient for each of the weights and biases.</p>

<p>Now that I have the gradient, how am I supposed to correct my weight/bias?</p>

<p>Should I:</p>

<ol>
<li>Add the gradient and the original value?</li>
<li>Multiply the gradient and the original value?</li>
<li>Something else? (Most likely answer)</li>
</ol>

<p>In addition to this, I've been hearing the term <em>learning rate</em> being tossed around, and how it is used to define the magnitude of the 'step' to descend to minimum cost. I figured this may also play an integral role in reducing the cost.</p>
"
1592,"<p>In discussions about technological singularity and its connection to AI, graphs are often shown that depict an exponential growth in technological advancement. Often the y-axis is labeled ""number of important inventions"" or something similar. This sounds quite subjective and is certainly hard to qualify.</p>

<p><strong>Is there a more objective way to quantify ""how much technology and scientific insight a society has""?</strong></p>
"
1593,"<p>What are the mathematical prerequisites to be able to study general artificial intelligence (AI) or strong AI?</p>
"
1594,"<p>I'm facing the problem of having images of different dimensions as inputs in a segmentation task. Note that the images do not even have the same aspect ratio.</p>

<p>One common approach that I found in general in deep learning is to crop the images, as it is also suggested <a href=""https://datascience.stackexchange.com/questions/16601/reason-for-square-images-for-deep-learning""><strong>here</strong></a>. However, in my case I cannot crop the image and keep its center or something similar since in segmentation I want the output to be of the same dimensions as the input.</p>

<p><a href=""https://pdfs.semanticscholar.org/d65b/cb276bc6fc9445a78fe26de76a43e09ccd27.pdf"" rel=""noreferrer""><strong>This</strong></a> paper suggests that in a segmentation task one can feed the same image multiple times to the network but with a different scale and then aggregate the results. If I understand this approach correctly, it would only work if all the input images have the same aspect ratio. Please correct me if I am wrong.</p>

<p>Another alternative would be to just resize each image to fixed dimensions. I think this was also proposed by the answer to <a href=""https://ai.stackexchange.com/questions/2403/dataset-containing-images-of-varying-dimensions-and-orientations""><strong>this</strong></a> question. However, it is not specified in what way images are resized.</p>

<p>I considered taking the maximum width and height in the dataset and resizing all the images to that fixed size in an attempt to avoid information loss. However, I believe that our network might have difficulties with distorted images as the edges in an image might not be clear. What is possibly the best way to resize your images before feeding them to the network?</p>

<p>Is there any other option that I am not aware of for solving the problem of having images of different dimensions?</p>

<p>Also, which of these approaches you think is the best taking into account the computational complexity but also the possible loss of performance by the network? </p>

<p>I would appreciate if the answers to my questions include some link to a source if there is one. Thank you.</p>
"
1595,"<p>I was wondering if it is possible to train an AI that can play outdoor games like cricket, badminton etc. I am new to AI, so if this question is dumb please bear it.</p>
"
1596,"<p>I'm building a 5-class classifier with a private dataset. Each data sample has 67 features and there are about 40000 samples. Samples of a particular class were duplicated to overcome class imbalance problems (hence 40000 samples). </p>

<p>With a one-vs-one multi-class SVM, I am getting an accuracy of ~79% on the validation set. The features were standardized to get 79% accuracy. Without standardization, the accuracy I get is ~72%. Similar result when I tried 50-fold cross validation.</p>

<p>Now moving on to MLP results,</p>

<p><strong>Exp 1:</strong></p>

<ul>
<li><em>Network Architecture:</em> [67 40 5]</li>
<li><em>Optimizer:</em> Adam</li>
<li><em>Learning Rate:</em> exponential decay of base learning rate</li>
<li><em>Validation Accuracy:</em> ~45%</li>
<li><em>Observation:</em> Both training accuracy and validation accuracy stops improving.</li>
</ul>

<p><strong>Exp 2:</strong>
Repeated <strong>Exp 1</strong> with batchnorm layer</p>

<ul>
<li><em>Validation Accuracy:</em> ~50%</li>
<li><em>Observation:</em> Got 5% increase in accuracy.</li>
</ul>

<p><strong>Exp 3:</strong></p>

<p>To overfit, increased the depth of MLP. A deeper version of <strong>Exp 1</strong> network</p>

<ul>
<li><em>Network Architecture:</em> [67 40 40 40 40 40 40 5]</li>
<li><em>Optimizer:</em> Adam</li>
<li><em>Learning Rate:</em> exponential decay of base learning rate</li>
<li><em>Validation Accuracy:</em> ~55%</li>
</ul>

<p>Thoughts on what might be happening? </p>
"
1597,"<p>We know that AI can lie. But, can evolutionary (self-learning) AI be programmed in such a way that they don't lie, even as they evolve?</p>

<p><br>
<sub>SOURCE: <a href=""https://www.popsci.com/scitech/article/2009-08/evolving-robots-learn-lie-hide-resources-each-other"" rel=""nofollow noreferrer"">Evolving Robots Learn To Lie To Each Other (Popsci)</a></sub></p>

<p><sub>See also: <br><a href=""https://www.technologyreview.com/s/407459/robots-evolve-to-deceive/"" rel=""nofollow noreferrer"">Robots Evolve to Deceive (MIT Tech Review)</a></sub>
<br><sub><a href=""https://www.technologyreview.com/s/414934/robots-evolve-the-ability-to-deceive/"" rel=""nofollow noreferrer"">Robots 'Evolve' the Ability to Deceive (MIT Tech Review)</a></sub></p>
"
1598,"<p>Is it possible to categories songs based on their spectrograms using image recognition or would there need to  more features? I was thinking that the spectrograms might also run into problems with EDM songs. Such as House music being closely related to their sounds. Would there have to be immense amount of data? I was thinking of using a CNN.</p>
"
1599,"<p>Can we apply Neural Networks to two text comparisons?</p>

<p>Please give me helpful tips about how to proceed in this direction. You can provide relevant links and pointers to solve the above problem.</p>
"
1600,"<p>Le et al. 2012 use a network of 1 billion parameters to learn neurons that respond to faces, cats, pedestrians, etc. without labels (unsupervised).</p>

<p>Their network is built with three autoregressive layers, and six pooling and normalization layers.</p>

<p>In the paper they state,</p>

<blockquote>
  <p><strong>Optimization</strong>: All parameters in our model were
  trained jointly with the objective being the sum of the
  objectives of the three layers.</p>
</blockquote>

<p>Does this mean that all three autoencoder layers were trained simultaneously, or that the first three sub-layers (first autoencoder sub-layer, first L2 pooling sub-layer, and first normalization sub-layer) were trained simultaneously?</p>

<p>I think by asking this question, I have answered it for myself. Please see my answer below.</p>

<h1>Follow-on Question</h1>

<p>What is the advantage of training all three layers simultaneously? Wouldn't the later layers be learning from poor lower layers to start with, and have to re-learn to adapt?</p>

<p>I do not have a very good answer to the follow-on question. Perhaps the lower layers can actually learn to provide low-level features that support the layers above?</p>

<p>This training of all layers simultaneously is done in all deep neural networks today -- AlexNet, VGG, etc. -- but why?</p>

<h1>Reference</h1>

<p>Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, Andrew Y. Ng, <em>Building high-level features using large scale unsupervised learning</em> (2012) arXiv:1112.6209 [cs.LG]</p>
"
1601,"<p>According to the original paper ""Genes that do not match are inherited from the more fit parent""</p>

<p>But what if the more fit parent has lesser nodes compared to the other, will the disjoint/excess genes be discarded?</p>

<p>Here's the link for the original paper: <a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf"" rel=""nofollow noreferrer"">nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf</a></p>
"
1602,"<p>My final project at college is a ""selling software"", where i have the list of products (ice cream) and the sells.
Its all done, but since im new with ML, i dont know which lib to use to achieve my goal: Analyse the top selling products, identify which ingredients are common in them, and with it, generate new products.
The idea is: top 5 selling products:
IceCream1: [Chocolate, Strawberry]
IceCream2: [Chocolate, Condensed Milk]
IceCream3: [Condensed Milk, Strawberry]
IceCream4: [Vanilla, Strawberry]
IceCream5: [Chocolate, Strawberry, Vanilla]
Then, since the top selling products have chocolate, strawberry, and Condensed milk, it would sugest a new ice cream with those 3 ingredients.</p>

<p>Plus if possible: based on the sells number of all products, predict how much this new product would sell. </p>

<p>Ps: front end has made with Angular, backend in Node and DB is MongoDB. </p>

<p>Any help/suggestion will be really appreciated! 
Thanks guys :D</p>
"
1603,"<p>In <a href=""https://arxiv.org/pdf/1003.0146.pdf"" rel=""nofollow noreferrer"">Li et al. (2010)'s highly cited paper</a>, they talk about LinUCB with hybrid linear models in Section 3.2.</p>

<p>They motivate this by saying, ""In many applications including ours, it is helpful to use features that are shared by all arms, in addition to the arm-specific ones. For example, in news article recommendation, a user may prefer only articles about politics for which this provides a mechanism.""</p>

<p>I don't quite understand what they mean by this. Is anyone willing to provide a different example? Also it would greatly help if you can clarify what Equation 6's ""$\mathbf{z}$"" and ""$\mathbf{x}$"" refer to in the context they talk about (news recommendation), or the example you give?</p>

<p>Equation (6) from the paper:</p>

<blockquote>
  <p>$$ \mathbf{E} \left[ r_{t,a} \vert \mathbf{x}_{t, a} \right] = \mathbf{z}_{t, a}^{\top} \boldsymbol{\beta}^* + \mathbf{x}_{t, a}^{\top} \boldsymbol{\theta}_a^* $$</p>
</blockquote>
"
1604,"<p>It sounds like people boast of something being ""artificial"" about machine learning when actually people boast that humans implemented algorithms like e.g. Monte Carlo Search (MCST) etc. I think the term AI is only to market to uneducated audience and should be banned. Machine can only look at the raw data. Any intelligence is human. e.g. when Alpha Zero beats something at chess , it implemented MCST (created &amp; implemented by humans)and used a huge data may be too many orders of magnitude to beat humans. Nothing ""artificial"" about intelligence.</p>

<p>It sounds unimpressive when one says Monte Carlo powered x numbers of TPUS with y amount of RAM and zillion penta bytes amount of database beat a human at a game. it sounds great when you say AI beat human.</p>

<p>But I do appreciate the fact that the creators did not hand code any chess rules (other than legal moves) &amp; that's a success story for machine learning.</p>
"
1605,"<p>This just popped into my head, and I haven't thought it through, but it feels like a sound question.  The definition of intelligence might still be somewhat fuzzy, possibly a factor of our evolving understanding of ""intelligence"" in regard to algorithms, but rationality has some precise definitions.</p>

<ul>
<li>Are Rationality and Intelligence distinct? </li>
</ul>

<p>If not, explain.  If so, elaborate.  </p>

<p><em>(I have some thoughts on the subject and would be very interested in the thoughts of others.)</em></p>
"
1606,"<p>I'm working on a Reinforcement Learning task where I use reward shaping as proposed in [1]. In short, my reward function has this form:</p>

<p><code>R(s, s') = gamma * P(s') - P(s)</code></p>

<p>where <code>P</code> is a ""potential function"".
When <code>s = s'</code>, </p>

<p><code>R(s, s) = (gamma - 1)P(s)</code></p>

<p>which is negative, since <code>0 &lt; gamma &lt;= 1</code></p>

<p>But considering <code>P(s)</code> relatively high (let say <code>P(s) = 1000</code>), <code>R(s,s)</code> become too high as well (e.g. with <code>gamma=0.99</code>, <code>R(s,s)=-10</code>), and if for many steps the agent stays in the same state, then the cumulative reward becomes more and more negative, which might affect the learning process.</p>

<p>In practice I solved the problem by just remove the factor <code>P(s)</code> when <code>s = s'</code>. But I have some doubts about the theoretical correctness of this ""implementation trick"". </p>

<p>Another idea could be to scale appropriately gamma in order to give a reasonable reward. Indeed, with <code>gamma=1.0</code> there is no problem, and with gamma very near to <code>1.0</code> the negative reward is tolerable. Personally I don't like it because it means that gamma is somehow dependent to the reward. </p>

<p>What do you think?</p>

<p>[1] <a href=""https://www-cs.stanford.edu/people/ang/papers/shaping-icml99.pdf"" rel=""nofollow noreferrer"">https://www-cs.stanford.edu/people/ang/papers/shaping-icml99.pdf</a></p>
"
1607,"<p>In the paper <a href=""http://proceedings.mlr.press/v32/silver14.pdf"" rel=""nofollow noreferrer""><em>Deterministic Policy Gradient Algorithms</a></em>, I am really confused about chapter 4.1 and 4.2 which is ""On and off-policy Deterministic Actor-Critic"". </p>

<p>I don't know what's the difference between two algorithms. </p>

<p>I only noticed that the equation 11 and 16 are different, and the difference is the action part of Q function where is $a_{t+1}$ in equation 11 and $\mu(s_{t+1})$ in equation 16. If that's what really matters, how can I calculate $a_{t+1}$ in equation 11?</p>

<p><img src=""https://s26.postimg.cc/y59xkd9q1/image.png"" alt="""">
<img src=""https://s26.postimg.cc/7wysv3zy1/image.png"" alt=""""></p>
"
1608,"<p>I'm working with a data set where the data is stored in a string such as <code>AxByCyA</code> where <code>A</code>, <code>B</code> and <code>C</code> are actions and <code>v,w,x,y,z</code> are times between the actions (each letter represents an interval of time). It's worth noting that <code>B</code> cannot occur without <code>A</code>, and <code>C</code> cannot occur without <code>B</code>, and <code>C</code> is the action I'm attempting to study (ie: I'd like to be able to predict whether a user will do <code>C</code> based on their prior actions).</p>

<p>I intend to create 2 clusters: people who do <code>C</code> and those who don't.</p>

<p>From this data set I build a training array to run the sci-kit (python) k-means algorithm on, containing the number of <code>A</code>s, the number of <code>B</code>s, the mean time between actions (calculated using the average of each interval) and the standard deviation between each interval.</p>

<p>This gives me an overall success rate of 82% on the test set, but is there anything I can do for more accuracy?</p>
"
1609,"<p>I am a University student taking an Artificial Intelligence class this semester. Our Professor's programming language of choice is Java, but it seems that perhaps with some nudging, he can change it to Python.</p>

<p>I wanted to know if there is any merit in doing so. I know a programming language is just a programming language - however, given the industry's wide use of Python when implementing AI algorithms (especially with ML), I think it makes much more sense for us to use Python. It will be easier to transition from a University environment to an Industrial one having done several assignments in Python and having a clear understanding of all the tools it provides than if we continue using Java, correct?</p>

<p>What are your thoughts?</p>
"
1610,"<p>Imagine trying to create a simulated virtual environment that is complicated enough to create a ""general AI"" (which I define as a self aware AI) but is as simple as possible. What would this minimal environment be like?</p>

<p>i.e. An environment that was just a chess game would be too simple. A chess program cannot be a general AI.</p>

<p>An environment with multiple agents playing chess and communicating their results to each other. Would this constitute a general AI? (If you can say a chess grand master who thinks about chess all day long has 'general AI'? During his time thinking about chess is he any different to a chess computer?).</p>

<p>What about a 3D sim-like world. That seems to be too complicated. After all why can't a general AI exist in a 2D world.</p>

<p>What would be an example of a simple environment but not too simple such that the AI(s) can have self-awareness?</p>
"
1611,"<p>I want my neural network structure to not have a circular/looping structure something similar like a directed acyclic graph (DAG). How do I do that?</p>
"
1612,"<p>From <a href=""https://stackoverflow.com/questions/36370129/does-tensorflow-use-automatic-or-symbolic-gradients"">https://stackoverflow.com/questions/36370129/does-tensorflow-use-automatic-or-symbolic-gradients</a>, I understood TensorFlow requires all the operations in the Graph to be explicit formulas (instead of black-boxes, such as raw python functions) to do Automatic Differentiation. Then it will do some kind of Gradient Descent based on that to minimization.</p>

<p>I'm wondering, since it already know all the explicit formulas, can it directly find out the minimum by examining the equation itself? Like computing the points where gradient is zero or do not exist, then do some kind of processing to find out the minimum.</p>

<p>I found it is simple to do this ""symbolic minimization"" above with few variables such as minimizing <code>Σ(a_i - v)^2</code> where <code>v</code> is the trainable variable an <code>a_i</code> are all the training samples. I'm not sure is there a general way though.</p>
"
1613,"<p>At Google I/O the Google Duplex just came out and now releasing new public self-driving car, Waymo. I can't think of any Tech Giants that can actually compete with Google AI other than Amazon. I'm thinking other companies will team up such as Nvidia, Intel, AMD, and etc. Apple seems to be lacking.</p>
"
1614,"<p>What is Bayes' theorem and Bayes' error? How is it implemented through line expression in Machine Learning? </p>
"
1615,"<p>Coming from the YT videos of 3blue1brown which showed that the individual layers do not have discernible shapes in the case of hand written letter recognition, I wondered if you could penalize dispersed shapes while training, thus creating connected shapes (at least on the first layer in the beginning). That way, you may be better able to understand the propagation of your algorithm through the layers.</p>

<p>Thanks, Jonny</p>
"
1616,"<p>I'm currently using 3Blue1Brown's tutorial series on neural networks and lack extensive calculus knowledge/experience.</p>

<p>I'm using the following equations to calculate the gradients for weights and biases as well as the equations to find the derivative of the cost with respect to a hidden layer neuron:</p>

<p><a href=""https://i.stack.imgur.com/YOMCJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YOMCJ.jpg"" alt=""EQUATIONS""></a></p>

<p>The issue is, during backpropagation, the gradients keep cancelling each other out because I take an average for opposing training examples. That is, if I have two training labels being [1, 0], [0, 1], the gradients that adjust for the first label get reversed by the second label because an average for the gradients is taken. The network simply keeps outputting the average of these two and causes the network to always output [0.5, 0.5], regardless of the input.</p>

<p>To prevent this, I figured a softmax function would be required for the last layer instead of a sigmoid, which I used for all the layers.</p>

<p>However, I have no idea how to implement this. The math is difficult to understand and the notation is complicated for me.</p>

<p>The equations I provided above show the term: σ'(z), which is the derivative of the sigmoid function.</p>

<p>If I'm using softmax, how am I supposed to substitute sigmoid with it?</p>

<p>If I'm not mistaken, the softmax function doesn't just take one number analogous to the sigmoid, and uses all the outputs and labels.</p>

<p>To sum it up, the things I'd like to know and understand are:</p>

<ol>
<li>The equation for the neuron in every layer besides the output is: σ(w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ... + w<sub>n</sub>x<sub>n</sub> + b). How am I supposed to make an analogous equation with softmax for the output layer?</li>
<li>After using (1) for forward propagation, how am I supposed to replace the σ'(z) term in the equations above with something analogous to softmax to calculate the partial derivative of the cost with respect to the weights, biases, and hidden layers?</li>
</ol>
"
1617,"<p>After learning the basics of neural networks and coding one working with the MNIST dataset, I wanted to go to the next step by making one which is able to play a game. I wanted to make it work on a game like slither.io, so in order to be able to create multiple instances of snakes and accelerate the speed of the game, i recreated a simple version of the game:</p>

<p><a href=""https://i.stack.imgur.com/g2oMV.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g2oMV.gif"" alt=""enter image description here""></a></p>

<p>The core features being almost done, now comes the work on the AI. I want to keep the script very simple by using only numpy (not that tensorflow, pytorch or spark does not interest me, but I want to understand things at a ""low level"" before using those framworks).</p>

<p>At first, I wanted the AI to be able to propose an output by reading pixels. But after some research, I don't realy want to get into convnet, recurent and recursive neural net. I'd like to re-use the simple feed forward NN I did with MNIST and adapt it.</p>

<p>So instead of using pixels, I think i'm going to use the following data:</p>

<ul>
<li>{x,y} snake's position</li>
<li>{x,y} foods positions</li>
<li>food value</li>
<li>Time, in order to get the snake eat the more food in a short time.</li>
<li>Distance from the center, not die outside the area</li>
</ul>

<p><a href=""https://i.stack.imgur.com/g410X.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g410X.png"" alt=""enter image description here""></a></p>

<p>That's a lot of different data to handle ! </p>

<h2>1/2 Can a simple FNN handle different kinds of Data in the Input layer ?</h2>

<p><a href=""https://i.stack.imgur.com/DuXAQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DuXAQ.png"" alt=""enter image description here""></a></p>

<p>Moreover that comes an other problem:</p>

<h2>2/2 Will it properly work with a variable number of inputs?</h2>

<p>In fact, in a specific area around the snake, the quantity of food will be variable. I came accross this <a href=""https://ai.stackexchange.com/questions/2008/how-can-neural-networks-deal-with-varying-input-sizes"">post that kinda answer my question</a>, but wath if I want the Neural network to forget some input if they are not being used, can <strong>Dropout</strong> be of any use in this case. Or the weights value (correcting toward zero) of these Inputs will be enough? </p>
"
1618,"<p>I am trying to implement CNN using tensorflow on temporal accelrometer signal. </p>

<ul>
<li>I have signal values segmented on every 10ms (200 samples). </li>
<li>I want to perform 1-D convolution (<code>tf.nn.conv1d(x,W,stride=1,padding='VALID')</code>) </li>
<li>Convolution window size is 20 samples and stride of 1 with 32 features and Valid padding</li>
<li>I want to apply Max-Pooling with window size of 10 samples <code>tf.nn.max_pool(x,ksize=[1,1,10,1],strides= [1,1,2,1],padding='VALID')</code></li>
</ul>

<p>But i am getting errors regarding dimensions of tensors. Any suggestions on how can i set filter size and stride for booth convolution and max-pooling</p>
"
1619,"<p>Goal -
I am trying to implement a genetic algorithm to optimise the fitness of a
species of creatures in a simulated two-dimensional world. The world contains edible foods, placed at random, and a population of monsters (your basic zombies).  I need the algorithm to find behaviours that keep the creatures well fed and not dead.</p>

<p>What i have done -</p>

<p>So i start off by generating a 11x9 2d array in numpy, this is filled with random floats between 0 and 1. I then use np.matmul to go through each row of the array and multiply all of the random weights by all of the percepts (w1+p1*w2+p2....w9+p9) = a1. </p>

<p>This first generation is run and I then evaluate the fitness of each creature using (energy + (time of death * 100)). From this I build a list of creatures who performed above the average fitness. I then take the best of these ""elite"" creatures and put them back into the next population. For the remaining space I use a crossover function which takes two randomly selected ""elite"" creatures and mixes their genes. I have tested two different crossover functions one which does a two point crossover on each row and one which takes a row from each parent until the new child has a complete chromosome. My issue is that the creatures just don't really seem to be learning, at 75 turns I will only get 1 survivor every so often.</p>

<p>I am fully aware this might not be enough to go off but I am truly stuck on this and cannot figure out how to get these creatures to learn even though I think I am implementing the correct procedures. Occasionally I will get a 3-4 survivors rather than 1 or 2 but it appears to occur completely randomly, doesn't seem like there is much learning happening.</p>

<p>Below is the main section of code, it includes everything I have done but none of the provided code for the simulation</p>

<pre><code>#!/usr/bin/env python
from cosc343world import Creature, World
import numpy as np
import time
import matplotlib.pyplot as plt
import random
import itertools


# You can change this number to specify how many generations creatures are going to evolve over.
numGenerations = 2000

# You can change this number to specify how many turns there are in the simulation of the world for a given generation.
numTurns = 75

# You can change this number to change the world type.  You have two choices - world 1 or 2 (described in
# the assignment 2 pdf document).
worldType=2

# You can change this number to modify the world size.
gridSize=24

# You can set this mode to True to have the same initial conditions for each simulation in each generation - good
# for development, when you want to have some determinism in how the world runs from generation to generation.
repeatableMode=False

# This is a class implementing you creature a.k.a MyCreature.  It extends the basic Creature, which provides the
# basic functionality of the creature for the world simulation.  Your job is to implement the AgentFunction
# that controls creature's behaviour by producing actions in response to percepts.
class MyCreature(Creature):

    # Initialisation function.  This is where your creature
    # should be initialised with a chromosome in a random state.  You need to decide the format of your
    # chromosome and the model that it's going to parametrise.
    #
    # Input: numPercepts - the size of the percepts list that the creature will receive in each turn
    #        numActions - the size of the actions list that the creature must create on each turn
    def __init__(self, numPercepts, numActions):

        # Place your initialisation code here.  Ideally this should set up the creature's chromosome
        # and set it to some random state.
        #self.chromosome = np.random.uniform(0, 10, size=numActions)
        self.chromosome = np.random.rand(11,9)
        self.fitness = 0
        #print(self.chromosome[1][1].size)

        # Do not remove this line at the end - it calls the constructors of the parent class.
        Creature.__init__(self)


    # This is the implementation of the agent function, which will be invoked on every turn of the simulation,
    # giving your creature a chance to perform an action.  You need to implement a model here that takes its parameters
    # from the chromosome and produces a set of actions from the provided percepts.
    #
    # Input: percepts - a list of percepts
    #        numAction - the size of the actions list that needs to be returned
    def AgentFunction(self, percepts, numActions):

        # At the moment the percepts are ignored and the actions is a list of random numbers.  You need to
        # replace this with some model that maps percepts to actions.  The model
        # should be parametrised by the chromosome.

        #actions = np.random.uniform(0, 0, size=numActions)

        actions = np.matmul(self.chromosome, percepts)

        return actions.tolist()


# This function is called after every simulation, passing a list of the old population of creatures, whose fitness
# you need to evaluate and whose chromosomes you can use to create new creatures.
#
# Input: old_population - list of objects of MyCreature type that participated in the last simulation.  You
#                         can query the state of the creatures by using some built-in methods as well as any methods
#                         you decide to add to MyCreature class.  The length of the list is the size of
#                         the population.  You need to generate a new population of the same size.  Creatures from
#                         old population can be used in the new population - simulation will reset them to their
#                         starting state (not dead, new health, etc.).
#
# Returns: a list of MyCreature objects of the same length as the old_population.

def selection(old_population, fitnessScore):
    elite_creatures = []
    for individual in old_population:
        if individual.fitness &gt; fitnessScore:
            elite_creatures.append(individual)

    elite_creatures.sort(key=lambda x: x.fitness, reverse=True)

    return elite_creatures

def crossOver(creature1, creature2):
    child1 = MyCreature(11, 9)
    child2 = MyCreature(11, 9)
    child1_chromosome = []
    child2_chromosome = []

    #print(""parent1"", creature1.chromosome)
    #print(""parent2"", creature2.chromosome)

    for row in range(11):
        chromosome1 = creature1.chromosome[row]
        chromosome2 = creature2.chromosome[row]

        index1 = random.randint(1, 9 - 2)
        index2 = random.randint(1, 9 - 2)

        if index2 &gt;= index1:
            index2 += 1
        else:  # Swap the two cx points
            index1, index2 = index2, index1

        child1_chromosome.append(np.concatenate([chromosome1[:index1],chromosome2[index1:index2],chromosome1[index2:]]))
        child2_chromosome.append(np.concatenate([chromosome2[:index1],chromosome1[index1:index2],chromosome2[index2:]]))

    child1.chromosome = child1_chromosome
    child2.chromosome = child2_chromosome

    #print(""child1"", child1_chromosome)

    return(child1, child2)

def crossOverRows(creature1, creature2):
    child = MyCreature(11, 9)

    child_chromosome = np.empty([11,9])

    i = 0

    while i &lt; 11:
        if i != 10:
            child_chromosome[i] = creature1.chromosome[i]
            child_chromosome[i+1] = creature2.chromosome[i+1]
        else:
            child_chromosome[i] = creature1.chromosome[i]

        i += 2

    child.chromosome = child_chromosome

    return child

    # print(""parent1"", creature1.chromosome[:3])
    # print(""parent2"", creature2.chromosome[:3])
    # print(""crossover rows "", child_chromosome[:3])


def newPopulation(old_population):
    global numTurns

    nSurvivors = 0
    avgLifeTime = 0
    fitnessScore = 0
    fitnessScores = []

    # For each individual you can extract the following information left over
    # from the evaluation.  This will allow you to figure out how well an individual did in the
    # simulation of the world: whether the creature is dead or not, how much
    # energy did the creature have a the end of simulation (0 if dead), the tick number
    # indicating the time of creature's death (if dead).  You should use this information to build
    # a fitness function that scores how the individual did in the simulation.
    for individual in old_population:

        # You can read the creature's energy at the end of the simulation - it will be 0 if creature is dead.
        energy = individual.getEnergy()

        # This method tells you if the creature died during the simulation
        dead = individual.isDead()

        # If the creature is dead, you can get its time of death (in units of turns)
        if dead:
            timeOfDeath = individual.timeOfDeath()
            avgLifeTime += timeOfDeath
        else:
            nSurvivors += 1
            avgLifeTime += numTurns

        if individual.isDead() == False:
            timeOfDeath = numTurns

        individual.fitness = energy + (timeOfDeath * 100)
        fitnessScores.append(individual.fitness)
        fitnessScore += individual.fitness
        #print(""fitnessscore"", individual.fitness, ""energy"", energy, ""time of death"", timeOfDeath, ""is dead"", individual.isDead())

    fitnessScore = fitnessScore / len(old_population)

    eliteCreatures = selection(old_population, fitnessScore)

    print(len(eliteCreatures))

    newSet = []

    for i in range(int(len(eliteCreatures)/2)):
        if eliteCreatures[i].isDead() == False:
            newSet.append(eliteCreatures[i])

    print(len(newSet), "" elites added to pop"")

    remainingRequired = w.maxNumCreatures() - len(newSet)

    i = 1

    while i in range(int(remainingRequired)):
        newSet.append(crossOver(eliteCreatures[i], eliteCreatures[i-1])[0])
        if i &gt;= (len(eliteCreatures)-2):
            i = 1
        i += 1

        remainingRequired = w.maxNumCreatures() - len(newSet)


    # Here are some statistics, which you may or may not find useful
    avgLifeTime = float(avgLifeTime)/float(len(population))
    print(""Simulation stats:"")
    print(""  Survivors    : %d out of %d"" % (nSurvivors, len(population)))
    print(""  Average Fitness Score :"", fitnessScore)
    print(""  Avg life time: %.1f turns"" % avgLifeTime)

    # The information gathered above should allow you to build a fitness function that evaluates fitness of
    # every creature.  You should show the average fitness, but also use the fitness for selecting parents and
    # spawning then new creatures.


    # Based on the fitness you should select individuals for reproduction and create a
    # new population.  At the moment this is not done, and the same population with the same number
    # of individuals is returned for the next generation.

    new_population = newSet

    return new_population

# Pygame window sometime doesn't spawn unless Matplotlib figure is not created, so best to keep the following two
# calls here.  You might also want to use matplotlib for plotting average fitness over generations.
plt.close('all')
fh=plt.figure()

# Create the world.  The worldType specifies the type of world to use (there are two types to chose from);
# gridSize specifies the size of the world, repeatable parameter allows you to run the simulation in exactly same way.
w = World(worldType=worldType, gridSize=gridSize, repeatable=repeatableMode)

#Get the number of creatures in the world
numCreatures = w.maxNumCreatures()

#Get the number of creature percepts
numCreaturePercepts = w.numCreaturePercepts()

#Get the number of creature actions
numCreatureActions = w.numCreatureActions()

# Create a list of initial creatures - instantiations of the MyCreature class that you implemented
population = list()
for i in range(numCreatures):
   c = MyCreature(numCreaturePercepts, numCreatureActions)
   population.append(c)

# Pass the first population to the world simulator
w.setNextGeneration(population)

# Runs the simulation to evaluate the first population
w.evaluate(numTurns)

# Show the visualisation of the initial creature behaviour (you can change the speed of the animation to 'slow',
# 'normal' or 'fast')
w.show_simulation(titleStr='Initial population', speed='normal')

for i in range(numGenerations):
    print(""\nGeneration %d:"" % (i+1))

    # Create a new population from the old one
    population = newPopulation(population)

    # Pass the new population to the world simulator
    w.setNextGeneration(population)

    # Run the simulation again to evaluate the next population
    w.evaluate(numTurns)

    # Show the visualisation of the final generation (you can change the speed of the animation to 'slow', 'normal' or
    # 'fast')
    if i==numGenerations-1:
        w.show_simulation(titleStr='Final population', speed='normal')
</code></pre>
"
1620,"<p>So I read about softmax from this <a href=""http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/"" rel=""nofollow noreferrer"">article</a>. Apparently to me these 2 are almost similar, except that the probability of all classes in softmax add to 1. According to their last paragraph for <code>number of classes = 2</code>, softmax reduces to LR. What I want to know is other than <code>number of classes = 2</code> what are the essential differences between LR and softmax. Like in terms of:</p>

<ul>
<li>Performance.</li>
<li>Computational Requirements.</li>
<li>Ease of calculation of derivatives.</li>
<li>Ease of visualization.</li>
<li>Number of minimas in the convex cost function, etc.</li>
</ul>

<p>Other differences are also welcome!</p>

<p><strong>EDIT :</strong> I am asking for relative comparisons only, so that at the time of implementation I have no difficulty in selecting which method of implementation to use.</p>
"
1621,"<p>In the model generation, in machine learning (consider supervised) If some data change the previous model function drastically then we should study that data. Does it happen? How to handle such situation? </p>
"
1622,"<p>Why momentum factor greater than 1 is a bad idea?</p>

<p>What are the mathematical conclusions?</p>
"
1623,"<p>I'm trying to find the optimal policy for the mountain car problem using deep Q learning with images as input, however, I cannot find a way to get my Q function to give me good solutions (I followed multiple tutorials for similar problems (Atari games and Flappy bird)).
I'm working on Python with Keras.</p>

<p>The images are given in the following format :</p>

<p><img src=""https://i.imgur.com/gAfLsdN.png"" alt=""input""></p>

<p>400x400 pixels, where the bar on the bottom right corner represents the speed of the car.</p>

<p>To check where my problem might lie, I thought it would be best if I split the problem by first ensuring that I can find a network which would successfully find the state of the car (position and speed, since it's all we need to find it) by feeding my convolutional network images of random states (uniformly distributed within the state space).</p>

<p>After unsuccessful results, I decided to split it even more by only trying to find the two state variable separately.</p>

<p>This is the best kind of result that I get, when the network doesn't predict all the state to be the same (which seem to happen a lot with relu activation).
The state space is divided in 50x50 matrix to make my predictions. The predicted speed is on the left, and the absolute error is on the right</p>

<p><img src=""https://i.imgur.com/313Kkbk.png"" alt=""state_error"">.</p>

<p>The images fed to the network are pre-processed the follow way : 
 1. Gray scale
 2. Resize (I tried 50x50, 100x100 and 150x150)
 3. Values centered around 0 in [-1;1] (this seemed to help a bit with the relu activation)</p>

<p>The network I used to try to find the speed is first a convolution layer (I tried 32 windows of kernel_size=(4,4) (8,8) and (16,16), strides=(1,1) and (2,2), activation= relu, linear, tanh.</p>

<p>Optional additional convolution layers of kernel size half the previous layer.</p>

<p>And an optional last dense layer of dimension 32 or activation relu, linear or tanh.</p>

<p>The output layer is dimension one with linear activation.</p>

<p>The way I train the network is by feeding the fit function with 32 random samples and let the network train for 25 epochs with batch_size 32 and repeat  ad libitum.</p>

<p>It's becoming extremely frustrating, especially with my GPU not fitting the requirement for GPU computation to check if I get some results faster.</p>

<p>Can anyone tell me if I'm doing something wrong that I'm missing and what can I do to improve my method to eventually manage to get the reinforcement algorithm to work ?
Like the size of the training sample, batch size and epochs of the fit function, the structure of the network, ...</p>

<p>Edit : I finally found a way for my reinforcement learning to converge to the true Q value : each time I run a new episode and put it in my replay memory, I run many fits of different mini-batches. This is something I thought I did by increasing the number of epochs in the parameter of the fit function, but I guess it doesn't work as I thought it would.
I'm letting it train a bit and then I will try the same method for the sub-problems mentioned above.</p>
"
1624,"<p>I am pretty much a beginner in Tensorflow and simply follow a tutorial. There is no problem with my code, but I have a question regarding the output</p>

<pre><code>accuracy: 0.95614034
accuracy_baseline: 0.6666666
auc: 0.97714674
auc_precision_recall: 0.97176754
average_loss: 0.23083039
global_step: 760
label/mean: 0.33333334
loss: 6.578666
prediction/mean: 0.3428335
</code></pre>

<p>I would like to know what does ""prediction/mean"" and ""label/mean"" represent?</p>
"
1625,"<p>As the title says, should I reset the exploration rate between trials?</p>

<p>I am currently doing the Open AI pendulum task and after a number of trials my model started playing but did not take any actions (i.e. didn't perform any significant swing). The Actor-Critic tutorial I followed did not reset the exploration rate (<a href=""https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69"" rel=""nofollow noreferrer"">link</a>) but it seems like there are lots of mistakes in general.</p>

<p>I assume that it should be reset since the model might start from a new unknown situation in a different trial and not know what to do without exploring. </p>
"
1626,"<p>Are there any charts or graphs of some sort that would clearly present the status of AI in one way or another (Meaning in a verifiably measurable way) and show where AI has been progressing and what the expected outcomes/predictions are currently available?</p>
"
1627,"<p>On this video</p>

<p><a href=""https://www.youtube.com/watch?v=YUVLgccVi54"" rel=""nofollow noreferrer"">Link to video</a></p>

<p>a neurologist starts by saying that we do not know how neurons calculate gradients for backpropagation.</p>

<p>At minute 30:39 hes showing faster convergence for ""our algorithm"", which seems to converge faster than backpropagation.</p>

<p>After 34:36 it goes explaining how ""neurons"" in the brain are actually packs of neurons.</p>

<p>I do not really understand all that he says, so I infer that those packs of neurons (which seem depicted as a single layer) are the ones who calculate the gradient.
It would make sense if each neuron makes a sightly different calculation, and then each other communicate the difference in results. That would allow to deduce a gradient.</p>

<p>What can be deduced, from the presented information, about the purported ""algorithm""?? (From the viewpoint of improving convergence of an artificial neural network).</p>
"
1628,"<p>I need to design an algorithm such that it handles the request for shift swapping.</p>

<p>The algorithm will recommend a list of people who are more likely to swap that shift with the person by analyzing previous data.</p>

<ul>
<li>Can anyone list the techniques that will help me to do this or a good starting point?</li>
</ul>

<p>I was thinking about training a Naive Bayes Classifier and using Mahaout for generating recommendation.</p>
"
1629,"<p>I am trying to create my own variant of Google duplex however, it won't make calls but just have a real time conversation. </p>

<ul>
<li>My question is, where and how to start? </li>
</ul>

<p>How do I train my model with real conversation and how do I make speech sound almost human like? Where do I incorporate RNN and how can I make my model understand nuances? </p>

<p><a href=""https://youtu.be/p3PfKf0ndik"" rel=""nofollow noreferrer"">https://youtu.be/p3PfKf0ndik</a> trying to create something like this. </p>

<p>NOTE: I understand this is very difficult but I am not looking for perfection. I am just creating this as I love doing this. Thank you</p>
"
1630,"<p>I would like to know other tha neural network, is there any ML technique for agent based ML. If so how to train an agent with some predefined rules? Can we use python programming for representing those rules? </p>
"
1631,"<p>I want to experiment Capsule Networks on FER. For now I am using fer2013 Kaggle dataset.</p>

<p>One thing that I didn't understand in Capsule Net was in the first conv layer, size was reduced to 20x20 - having input image as 28x28 and filters as 9x9 with 1 stride. But in the capsules, the size reduces to 6x6. How did this happen? Because with input size as 20x20 and filters as 9x9 and 2 strides, I couldn't get 6x6. Maybe I missed something.</p>

<p>For my experiment, input size image is 48x48. Should I use the same hyperparams for the start or is there any suggested hyperparams that I can use?</p>
"
1632,"<p>I am having difficulties wrapping my head around how the answer is being produced. How does the solution come up with the answer 3? How does one derive the answer 3 from the truth table? </p>

<p>For example the following file :</p>

<blockquote>
  <p>TELL  p2=> p3; p3 => p1;</p>
  
  <p>ASK p2</p>
</blockquote>

<p>Produces the following result. </p>

<p><a href=""https://i.stack.imgur.com/UCQzO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UCQzO.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Standard output is an answer of the form YES or NO, depending on
  whether the ASK(ed) query q follows from the TELL(ed) knowledge base
  KB. When the method is TT and the answer is YES, it should be followed
  by a colon (:) and the number of models of KB</p>
</blockquote>
"
1633,"<p>I am looking to extract the central theme of news headline using NLP/ Text-mining, any references in this direction is of great help.</p>

<p>For example: Inputs:</p>

<p>BRIEF-Dynasil Corporation Of America Reports Q2 EPS Of $0.08</p>

<p>China's night-owl retail investors leverage up to dominate oil futures trade</p>

<p>outputs:</p>

<p>Reports</p>

<p>oil futures</p>
"
1634,"<p>I would like to write a program that takes a number of elements as input (some images and some strings) and outputs a design (the program places the images and strings on a canvas, but also styles the strings).</p>

<p>Can someone point me in the right direction of a possible solution to this problem? I've looked for machine learning algorithms for design but have not found a single one to give me any tips on how to build one of these.</p>
"
1635,"<p>It is well known from the history of technology, that the invention of new things was always problematic. In the 15th century for example, in which Gutenberg has invented the first printing press, the world wasn't pleased. Instead the Luddite movement was doing everything to destroy his work. As far as I know from the history lesson, Gutenberg was recognized in his time as an evil sorcerer and the printing press as work of the devil.</p>

<p>This development was in later decades also visible. At first, a great invention was done for example the first steam driven car, and the ordinary people don't understand the technology and were in fear of it. A modern form of technology is computing and especially artificial intelligence. From a technical point of view, it is the most important invention ever, and this will result into the strongest possible form of rejection. Most people in the world are not excited by Artificial Intelligence. They not want any sorts of robots and intelligent machines.</p>

<p>The terminology itself is well known, the fundamental rejection of new technology bevause of religious or moral reasons is called Luddism or <a href=""https://en.wikipedia.org/wiki/Neo-Luddism"" rel=""nofollow noreferrer"">Neoluddism</a>. Because the technophobic Ned Ludd has destroyed a while ago two stocking frames. After this episode, every rant against technology is called after him. But what i do not understand it the motivation behind it. Did Ned Ludd thought, that he can change the world if he destroyed a machine? Did he believe that mankind will become good if no gutenberg printing press are used? The problem is, that for example if the first steam engine was never invented, also the following inventions like the internet and intelligent machines wouldn't have been invented. But what would be the alternative? What is the perspective of Ned Ludd, how does he see the better tomorrow, if no technology innovation is allowed?</p>
"
1636,"<p>I am trying a modification of Mobilenet in which I add feedback from the softmax layer into the early layers (to implement this I put a second net after the first, which receives connections from the softmax layer of the first, the pretrained weights being non trainable). The idea was to mimic the massive feedback projections in the brain, which presumably could help object recognition by enhancing specific filters and inhibiting others. </p>

<p>I took the pretrained network from Keras and started to retrain it on Imagenet. I noticed that the training accuraccy increased right in the first epoch. My computer is very slow thus I cannot train for too long, an epoch takes 3.5 days. So after an epoch I tried the validation set, but instead the accuracy went down to almost half that of the pretrained values.</p>

<p>My question is if this is and obvious case of overfitting. That is, will continued training increase the accuracy of the training set at the expense of the validation set, or is this a normal behavior expected at the initial stages of training, so that if I keep training for a few more epochs I could expect the validation set accuracy to go eventually up? Any ideas that could help are welcomed. </p>
"
1637,"<p>I use Matlab function ""wmulden"" to obtain a denoised data and I realised that when I add only one data-step to previous data and I recalculate the denoised series all denoised data is changed even if the data is identical.
The average difference between the two denoised signals is 1*10e-03 and the average data  is 1. </p>

<p>Does anyone know how to get the same denoising when the data is the same?</p>
"
1638,"<p>I have a data set containing actions taken by customers (e.g., view a product, add a product to cart, purchase product), the product bought (if any) and times of said actions. I am attempting to use K-means clustering to identify the customers who are more likely to purchase a product based on these actions (minus the purchase).</p>

<p>I'm currently clustering using: the number of products viewed, the number of products put in the cart, the mean time between the actions, the variance of the time between the actions, the standard deviation of the time between the actions (all of these values are normalized), as well as the product purchased (if any). The clusters I'm getting contain <code>~10%</code> buyers and <code>90%</code> non-buyers, but I'm trying to separate buyers and non-buyers.</p>

<p>Any thoughts on what else I can do? Or should I try another method completely?</p>

<p>Illustration: x axis are the clusters, y axis is the number of customers, red are buyers and blue are non-buyers
<a href=""https://i.stack.imgur.com/jMzss.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jMzss.png"" alt=""cluster graph""></a></p>

<p>Update: I made a 3D  graph showcasing the clusters, the amount of customers and the mean time between actions (normalized because of reasons)
<a href=""https://i.stack.imgur.com/Fymmf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fymmf.png"" alt=""3D plot""></a></p>

<p>Yet another update: customers (not grouped by cluster, just as is) according to the average number of products they viewed and the average time between actions</p>

<p><a href=""https://i.stack.imgur.com/gr3n3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gr3n3.png"" alt=""number of products x average time""></a></p>

<p>I took some advice and tried using PCA (from this <a href=""https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"" rel=""nofollow noreferrer"">tutorial</a>), and these are the results I got:
<a href=""https://i.stack.imgur.com/wB7Lh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wB7Lh.png"" alt=""PCA data""></a></p>

<p>The raw data (x=number of items viewed/carted, y=average time between interactions)
<a href=""https://i.stack.imgur.com/TMHUj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TMHUj.png"" alt=""raw data graph""></a></p>

<p>Any tips on how to cluster this mess?</p>
"
1639,"<p>I have read various answers to this question at different places, but I am still missing something. </p>

<p>What I have understood is that a Graph search holds a closed list, with all expanded nodes, so they don't get explored again. But if you apply Breadth-first-search or Uniformed Cost search at a search tree, you do the same. You have to keep the expanded nodes in memory. </p>
"
1640,"<p>I know that one of the recent fads right now is to train a neural network to generate screenplays and new episodes of FRIENDS, THE SIMPSONS, what have you, and that's fine: it's interesting and might be the necessary first(?) steps toward making programs that can actually generate sensible/understandable stories.</p>

<p>In this context: Can neural networks be trained specifically to study the structures of stories, or screenplays, and perhaps generate plot points, or steps in the Hero's Journey, etc., effectively writing an outline for a story?</p>

<p>Yes, this to me differs from the many myriad plot-point generators online, although I have to admit the similarities.  I'm just curious if the tech or the implementation is even there yet and, if it is, how one might go about doing it.</p>
"
1641,"<p>I have a medical dataset with 14000 rows dataset with 900 attributes. I have to predict disease severity using that. I would like to know whether we can write rules in python language for training an agent for medical diagnostic using machine learning.</p>

<p>Can an agent make the decisions by the rules coded in python and that agent get trained with some machine learning algorithms? If so is there any agent architecture and model for the agent which is good in this context?</p>

<p><em>Edit:</em> By the rule, I meant something like this..""if x>y output z as action"". By the word ""Training"" I meant ""how to tell this agent to do this action""?</p>
"
1642,"<p>Trajectory planning for complex robot-movements can be done with keyframes. The animation author gives a number of important image-captures and the animation planner is calculating the movements in-between them. A recent paper which describes this idea is <a href=""https://smartech.gatech.edu/bitstream/handle/1853/54355/HA-DISSERTATION-2015.pdf"" rel=""nofollow noreferrer"">Developing agile motor skills on virtual and real humanoids</a>. On page 105 is a backflip for a virtual stuntman shown, which looks very similar to what is known in the media from the Boston Dynamics Atlas robot. It seems, that this technology is the way to go for a robot control system. What I do not understand are the details of the problem solving strategy. As far as i can see from the paper, a so called “policy search algorithm” was used. In anther paragraph the idea is called a model-based parametric animation. What does that means? I'm only family with sampling based trajectory planning, for example for inverse kinematic planning. The idea is to testing out some values and one of them is the solution. How can i improve this into a parametric policy search?</p>
"
1643,"<p>Pretty simple question here:</p>

<p>Is it useful to use the standard deviation, skew, kurtosis, or any other extrapolatory stats as features, and if so in which problem sets?</p>

<p>In this case, I am talking about deep learning problems.</p>
"
1644,"<p>Just working with fully connected NNs (supervised learning), I found that models trained for, say NLP, on identical data sets with identical parameters to algorithms; but at different times, can provide different accuracy, with in like 7-8% of each other. Is that an unusual phenomenon? What is an acceptable standard deviation level? </p>
"
1645,"<p>Considering the scenario where supervised training data-set in the form of sentence will be given to train the machine</p>

<blockquote>
  <p>The Bomb which had been planted by Terrorist on this morning was
  defused by the Counter Terrorist on joining hands with the
  Intelligence Force</p>
</blockquote>

<p>Input strings in the sentence containing each words are broken into tokenised
arrays of single words with stop words removed.<br/> </p>

<p>Each word in the given sentence gets assigned a label w1, w2 and so on i.e,<br/>
<code>w2</code>  = Bomb<br/>
<code>w6</code>  = planted<br/>
<code>w13</code> = defused<br/></p>

<p>Calculating the scores for individual word combinations, the result should yield something like:<br/>
<code>w2.w6</code>  = Scores should be Positive (or > some threshold value)<br/>
<code>w2.w13</code> = Scores should be Negative (or &lt; some threshold value)<br/></p>

<p>In case of words with polarity changers<br/>
Eg.: <em>Bomb wasn't/haven't/didn't got defused.</em><br/>
The resulting scores should be positive<br/></p>

<hr>

<p>To accomplish this task I had implemented Sentiment Analysis with the threshold = 2.5 and ended up with the following scores<br/>
<br/></p>

<h2><strong>Actual Output:</strong></h2>

<blockquote>
  <p>&lt; 2.5 : Low<br/>
  = 2.5 : Neutral<br/>
  > 2.5 : High<br/></p>
</blockquote>

<p><br/></p>

<h2><strong>Expected Output:</strong></h2>

<blockquote>
  <p><em>Case 1:</em> score = negative, since that bomb was defused or removed in the given sentence<br/>
  <em>Case 2:</em> score = positive, vice versa of ""Case 1""<br/>
  <em>Case 3</em>: Otherwise score = 0, in case it can't predict either of the above two cases, it should be neutral <br/></p>
</blockquote>

<p>I am facing a severe problem every time I need to update the vocabulary list with upcoming new words that were not in the dictionary list, which is turning out to be Semi-supervised learning.</p>

<p>Referring to the above sentence to calculate the <code>w(n-(1/2/3/...n)</code> and <code>wn</code> word with reference to word = <em>Bomb</em>. The final resulting score should yield as negative.</p>

<p>So which machine learning algorithm would be appropriate that fits to yield a better solution and based on the given data set how will I train the machine to learn the above things?</p>

<p>Finally should I try to implement by keeping the model persistence. So that it doesn’t have to be trained on each run.</p>
"
1646,"<p>For speedrunning purposes, I am trying to train a neural network to identify human-executable ways to manipulate pseudo-RNG (in Pokemon Red, for the interested). The game runs at sixty frames per second, and the linear-congruential PRNG updates every frame, while many frames are unlikely to be relevant to the manipulation (and so should contain no actions from the neural net). Any given manipulation is likely to last 30sec-2min, and the advancement rate of the PRNG can change depending on location in the game-world.</p>

<p>I have some experience with coding AI/deep-learning. I've made some programs using Multilayer Perceptron and IndRNN approaches. From what I can tell, IndRNN or A3C would be my best bets. I'm not expert enough to know the correct approach, though, or to know if the dimensionality of the problem makes it outright unfeasible.</p>

<p>1) Is this problem reasonably solvable with NN/deep learning?</p>

<p>2) What approach would you recommend to tackle it?</p>
"
1647,"<p>The following paper explains the use of skip connections to break the singularity in deep networks. But, I have not fully understood what singularity is.</p>

<p><a href=""https://arxiv.org/pdf/1701.09175v8.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1701.09175v8.pdf</a></p>

<p>Any easy-understanding explanation? </p>
"
1648,"<p>This article ""<a href=""https://ieeexplore.ieee.org/document/6698290/"" rel=""nofollow noreferrer"">Enhancing Differential Evolution Utilizing Eigenvector-Based Crossover Operator</a>"" said for a non-separable function traditional crossover algorithm are not suitable and they can not diversify the population sufficiently, so the differential evolution stops at the local optimum points. Why does this behavior not hold for separable functions but it exists for non-separable functions? Which key feature of the non-separable functions cause this behavior?</p>
"
1649,"<p>I am trying to assess an encoder in my autoencoder. I can not seem to grasp which specs make an encoder better than other one in, lets say, unsupervised learning. For example, I am trying to teach my neural network to classify cats, so that when I provide a picture of a bird, my autoencoder would tell me that it is not a picture of a cat. I am trying to understand what exact specs make my encoder (and decoder) better? I understand it is all about chosen weights but is it possible to be more specific? </p>
"
1650,"<p>According to <a href=""https://arxiv.org/pdf/1606.00915.pdf"" rel=""nofollow noreferrer"">this</a> paper (page 4, bottom-right), atrous convolutions can be used to compute responses of arbitrarily large dimensions in Deep Convolutional Neural Networks.</p>

<p>I do not understand how something like this is true, since by upsampling the filters, one effectively can apply the filter less times to an image, unless one also upsamples the image. Applying the filter less times as I see it obviously means that the output (response) will be of lower dimensionality.</p>

<p>Is there something that I am missing here?</p>
"
1651,"<p>In an RNN to train it, you need to roll it out, and enter in the history of inputs and the history of expected outcomes. </p>

<p>This doesn't seem like a realistic picture of the brain since this would require, for example, for the brain to store a perfect history of every sense that comes in to it for many time-steps. </p>

<p>So is there an alternative to RNNs that doesn't require this history? Perhaps storing differences or something? Or storing some accumulator?</p>

<p>Perhaps there is a way to calculate with RNNs that doesn't require keeping hold of this history?</p>
"
1652,"<p>I remember the first time hearing about google trying to make driverless cars.  That was YEARS ago!</p>

<p>These days, I'm beginning to learn about Neural Nets and other types of ML and I was wondering:</p>

<p>Does anybody know how many hours (or days, months, etch) is needed in training time to get the results that are now used in today's self-driving vehicles?</p>

<p>(I am ASSUMING they use Neural networks for this...)</p>
"
1653,"<p>What is current research in artificial intelligence and machine learning in the field of data compression? </p>

<p>I have done my research on the PAQ series of compressors, some of which use neural networks for context mixing.
I would love if it combined both artificial intelligence and data compression. (But I am open to suggestions for any project!)</p>
"
1654,"<p>With all the Google I/O stuff coming out, how can I verify that I have an actual human on the phone using only voice? Are there still vocal things humans can, but robots can't do?</p>

<p>Conditions: the person on the phone is a stranger (so personal questions won't work), and the verification must be voice only.</p>

<p>(Also, I understand Google Duplex may be just an overhyped demo that will turn out to flop like the Pixel Buds. But eventually such a bot would be created, right? If so, what's the best verification?)</p>
"
1655,"<p>Let's say you want to do AI research and publish some papers just by your own. Would you send them to an AI journal using just your name? Which AI journals are recommended?</p>
"
1656,"<p><code>ReLU : y = max(0,x)</code></p>

<p><code>Linear : y = x</code></p>

<p>The <code>ReLU</code> nonlinearity just clips the values <code>&lt; 0</code> to <code>0</code> and passes everything else; then why not to use a <code>linear</code> activation function instead as it will pass all the gradient information during backpropagation.</p>

<p>I do see that <code>Parametric ReLU (PReLU)</code> does provide this possiblity. </p>

<p>I just want to know if there is a proper explanation to using <code>ReLU</code> as default or it is just based on observations that it performs better on the training sets.</p>
"
1657,"<p>Say I have access to several pre-trained CNNs (e.g. AlexNet, VGG, GoogleLeNet, ResNet, DenseNet, etc.) which I can use to extract features from an image by saving the activations of some hidden layer in each CNN. Likewise, I can also extract features using conventional hand-crafted techniques, such as: HOG, SIFT, LBP, LTP, Local Phase Quantization, Rotation Invariant Co-occurrence Local Binary Patterns, etc. Thus, I can obtain a very high-dimensional feature vector of an image that concatenates the individual features vectors outputted by these individual algorithms. Given these features, and given a data set of images over which I want to perform similar image retrieval (i.e. finding the <strong>top-k</strong> most similar images to a query image X), what would be the most appropriate ways to implement this task?</p>

<p>One possible idea I have in mind is to learn an image similarity embedding in euclidean space by training a neural network that would receive as input the aforementioned feature vectors, and perhaps down-sampled versions of the image as well, and output a lower dimensional embedding vector that ideally should place similar images close to each other and dissimilar images far apart. And I could train this network using for example Siamese Loss or Triplet Loss. The challenge of this approach though is generating the labels for the (supervised) training itself. For example, in the case of the Triplet Loss I would need to sample triplets (Q,X,Y) and <em>somehow</em> determine which one between X and Y is most similar to Q, in order to generate the label for the triplet (i.e., in order to ""teach"" the network I need to know the answers myself beforehand, but how? I guess this is domain dependent, but think of challenging cases where you have very heterogeneous images, such as photography galleries, artwork galleries, etc).</p>

<p>Anyways, this is just an idea and by no means I pretend to mean this is the right approach. I'm open to new suggestions and insights about how to solve this task.</p>
"
1658,"<p>I'm trying to design a neural network with a task hierarchy. This is my idea so far:</p>

<pre><code> [Desires]
    |
[Layer 1] [T0]
    |    /
[Layer 2] [T1]
    |    /
[Layer 3] [T2]
    |    /
[Layer 4] [T3]
    |    /
  [Action]   
</code></pre>

<p>The way this would work is that each layer represents a task as a binary number. Layer 1 is the main task, layer 2 the sub-task etc. Each task consists of 2 sub-tasks determined by T={0,1}. In this way the neural network represents a binary task graph with T=0 being the left child and T=1 being the right child of a node.</p>

<p>You can think of it as T3 changing every second T2 changed every 2 seconds and so on. So {T0 T1 T2 T3} gives the binary time in seconds in a 16 second cycle.</p>

<p>So far this only makes the output a sequence of 16 actions in order. But if some of the layers could be ""if"" gates they might control the T-values and so act as switches and so have more complicated programs.</p>

<p>Do you have any suggestions to improve this? Or has this kind of binary task graph representation been done before in a neural network?</p>

<p>Also importantly how would you train such a neural network? (At the moment I just assume that the model is pre-trained and just trying to find a good architecture).</p>
"
1659,"<p>A.I Community, this is my first post on here I am currently reading, learning and designing models. At the moment I'm working on this sentiment analysis tool; from what I gather sentiment analysis can be tricky to fine-tune hence why I'm reaching out here to improve on my model. I am asking for tips and pointers the hows and how not I would appreciate detailed answers in the context of improvement etc. Currently, the model is bias towards positive sentiment and even negative text is yielding.6 positive when it should be obvious in the negative sentiment side. My CSV contains 65000 tweets pre-labeled there is an even number it seems of positive and negative tweets and is indeed correctly labeled.</p>

<pre><code>    import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
import re

data = pd.read_csv(""training_data.csv"")
data = data[['Sentiment', 'SentimentText']]
data['SentimentText'] = data['SentimentText'].apply(lambda x: x.lower())
data['SentimentText'] = data['SentimentText'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))
max_features = 2000
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(data['SentimentText'].values)
X = tokenizer.texts_to_sequences(data['SentimentText'].values)
X = pad_sequences(X)

from keras.layers import Dense, Dropout, LSTM, Embedding
embed_dim = 50
lstm_out = 80
model = Sequential()
model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))
model.add(Dropout(0.2))
model.add(LSTM(lstm_out))
model.add(Dropout(0.2))
model.add(Dense(2,activation='softmax'))
model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

Y = pd.get_dummies(data['Sentiment']).values
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.20, random_state=42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)
model.fit(X_train, Y_train, nb_epoch=35, batch_size=32, verbose=1)

#save model to disk and print the summary
model.save('model.h5', overwrite=True)
print(model.summary())
</code></pre>
"
1660,"<p>Given a query image Q and two other images X and Y (you can assume they have more or less the same resolutions if that simplifies the problem), which algorithm would perform extremely well at determining which image between X and Y is most similar to Q, even when the differences are rather subtle? For example, a trivial case would be:</p>

<ul>
<li>Q = image of mountains, X = image of mountains, Y = image of dogs, therefore it is clear that sim(Q,X) > sim(Q,Y).</li>
</ul>

<p>However, examples of trickier cases would be: </p>

<ul>
<li>Q = image of a yellow car, X = image of a red car, Y = image of a yellow car, therefore sim(Q,Y) > sim(Q,X) (assuming the car shapes are more or less the same).</li>
<li>Q = image of a man standing up in the middle with a black background, X = image of another man standing up in the middle with a black background, Y = image of a woman standing up in the middle with a black background, therefore sim(Q,X) > sim(Q,Y).</li>
</ul>

<p>Which algorithm (or combination of algorithms) would be robust enough to handle even the tricky cases with very high accuracy?</p>
"
1661,"<p>I am trying to track LIDAR objects using Kalman filter. The problem is that the innovation has the value 0, which makes the Kalman gain be Infinity. <a href=""https://en.wikipedia.org/wiki/Kalman_filter"" rel=""nofollow noreferrer"">Here</a> is a link with the Kalman equations. The values with which I initialized the measurement and process covariance matrix are listed below. The update code is also shown below. When I debug the code everything is fine until the innovation becomes 0. </p>

<pre><code>this-&gt;lidar_R &lt;&lt; std_laspx_, 0, 0, 0,
    0, std_laspy_, 0, 0,
    0, 0, 0, 0,
    0, 0, 0, 0;

this-&gt;lidar_H &lt;&lt; 1.0, 0.0, 0.0, 0.0, 0.0,
    0.0, 1.0, 0.0, 0.0, 0.0,
    0.0, 0.0, 0.0, 0.0, 0.0,
    0.0, 0.0, 0.0, 0.0, 0.0;

P_ &lt;&lt; 1000, 0, 0, 0, 0,
    0, 1000, 0, 0, 0,
    0, 0, 1000, 0, 0,
    0, 0, 0, 1000, 0,
    0, 0, 0, 0, 1000;

 MatrixXd PHt = this-&gt;P_ * H.transpose();
 //S becomes 0
 MatrixXd S = H * PHt + R;
 //S_inv becomes INFINITY
 MatrixXd S_inv_ = S.inverse();
 MatrixXd K = PHt * S_inv_;

VectorXd y = Z - Hx;

this-&gt;x_ = this-&gt;x_ + K*y;
MatrixXd I = MatrixXd::Identity(x_.size(), x_.size());
this-&gt;P_ = (I - K * H) * this-&gt;P_;
</code></pre>
"
1662,"<p>A task I’m working on at the moment requires a CNN with a height map as one of the inputs. This is a matrix of floating point values in which each point is the height of that point above sea level.</p>

<p>I’m having trouble deciding how to normalize this data. I know there are networks that work on depth or distance data but that is different for several reasons:</p>

<ul>
<li>Height can also be negative (as opposed to depth/distance which starts at 0)</li>
<li>Height has a very large range - can get values between -400 and +~9000.</li>
</ul>

<p>For these reasons the common approach to normalisation, simply subtracting the mean and dividing by the standard deviation, will result in the loss of information in most cases (all values will be close to zero).</p>

<p>I thought of maybe subtracting the local mean for each input, rather than a general mean calculated from all the data, but I still don't know what to do with the standard deviation, since dividing by the local standard deviation can result in very “flat” and very “steep” inputs looking the same after normalization.</p>
"
1663,"<p>I have a data set with historical information of some events (let's say event A and event B),these events describe the discovery of land mines, the coordinates of the event and the date of the event; is there a way I can use this historical information to predict points (coordinates) where event A or B could happen i.e. where might be still land mines that haven't been found?</p>
"
1664,"<p>In <a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf"" rel=""nofollow noreferrer"">slide 16</a> of his lecture 5 of the course ""Reinforcement Learning"", David Silver introduced <em>GLIE Monte-Carlo Control</em>.</p>

<p><a href=""https://i.stack.imgur.com/Is4qu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Is4qu.png"" alt=""enter image description here""></a></p>

<p>But why is it an on-policy control? The sampling follows a policy <span class=""math-container"">$\pi$</span> while improvement follows an <span class=""math-container"">$\epsilon$</span>-greedy policy, so isn't it an off-policy control?</p>
"
1665,"<p>I used the example at - <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/tensorflow_dataset_api.py"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/tensorflow_dataset_api.py</a> - to create my own classification model. I used different data but the basic outline of datasets was used. </p>

<p>It was important for my data type to shuffle the data and then create the training and testing sets. The problem however comes as a result of the shuffling.</p>

<p>When I train my model with the shuffled train set I get a +- 80% accuracy for train and +- 70% accuracy for the test set. I then want to input all the data (i.e. the set that made the training and test set) into the model to view the fully predicted output of this data set that I have. </p>

<p>If this data set is shuffled as the training and testing set was I get an accuracy of around 77% which is as expected but then if I input the unshuffled data (as I required to view the predictions) I get a 45% accuracy. How is this possible?</p>

<p>I assume it's due to the fact that the model is learning incorrectly and that it learns that the order of the data points plays a role in the predicting of those data points. But this shouldn't be happening as I am simply trying to (like the mnist example) predict each data point separately. This could be a mini-batch training problem.</p>

<p>So I ask, in the example mentioned above, using data sets and batches to train, does the model learn from the average of all the data points in the mini-batch or does it think one mini-batch is one data point and learn in that manner (which would mean order matters of the data)?</p>

<p>Or if there are any other suggestions.</p>

<p>Thanks </p>
"
1666,"<p>I know how IOU works during detection. However, while preparing targets from ground truth for training, how is the IOU between a given object and all anchor boxes calculated? </p>

<p><strong>Is the ground truth bounding box aligned with an anchor box such that they share the same center? (width/2, height/2)</strong></p>

<p>I think this is the case but I want to hear from someone who has better knowledge of how training data is prepared for training in YOLO.</p>
"
1667,"<p>The cognitive architecture SOAR was invented by Allen Newell and is described extensively in the literature. According to newer interpretations, SOAR is an AGI project which is an artificial intelligence. But what happened if this assumption is wrong? My hypothesis is, that SOAR is not the Artificial Intelligence itself, but an environment for testing an AI how good it is. Let us make an example:</p>

<p>In the year 1955 Allen Newell published a paper “The Chess Machine: An Example of Dealing with a Complex Task by Adaptation” in which he described a predecessor to Soar. But can this paper be used to build a chess-playing machine like the famous IBM Deepblue? Did the Newell paper includes modern heuristics like alpha-beta-prunning for playing chess on grandmaster level? No, it doesn't. But we can utilize the paper for something else. To build an AI competition. That means, a software programming challenge in which teams must create an AI from scratch. The assumption is, that SOAR is not the AI itself, but only an early form of a robotics challenge like Micromouse, Robocup Rescue and a chess playing challenge.</p>

<p>The sad news is, that programming a Lisp-software which is able to evaluate the performance of an AI is not very demanding. Programming a game, in which an agent must act intelligently is easier then programming the agent itself. Programming a chess game is easier then programming a software which is able to win the game. But it seems, that everything what is discussed in the literature under the term AGI, SOAR and ACT-R is only about programming challenges. That means, SOAR is not the end, it is the beginning. Soar is not the answer, it is only the problem definition in clear Lisp code.</p>

<p>Let us suppose we want to program our own Micromouse-SOAR, how does it look like? Micromouse-SOAR is primary an agent-runtime environment. That means it is able to evaluate how fast a mouse finds the way through a maze. Micromouse-SOAR doesn't answer the question how to program such an agent. It contains no pathplanning algorithm and no lowlevel control of the robot. Instead the software is only able to say, if the mouse finds the way through the maze.</p>

<p>After this longer introduction, my question is short and simple: Am I right? Are further hints available which are showing, that SOAR and other so called cognitive architecture are useless in reality and were only invented to evaluate Narrow AI software?</p>
"
1668,"<p>Various texts on using CNNs for object detection in images talk about how their translation invariance is a good thing. Which makes sense for tasks where the object could be anywhere in the image. Let's say detecting a kitten in household images.</p>

<p>But let's say, you already have some information about the likely position of the object of interest in the image. For example, for detecting trees in a dataset of images of landscapes. Here in <em>most cases</em>, the trees are going to be in the bottom half of the image while in <em>some cases</em> they might be at the top (because it's on a hill or whatever). So you want your neural network to learn that information -- that trees are likely connected to the bottom part of the image (ground). Is this possible using the CNN paradigm? </p>

<p>Thank you</p>
"
1669,"<p>I was just reading through some convex optimization textbooks to hopefully improve my deep learning understanding and come up with new ideas. Halfway through, I decided to Google a bit! It's obvious that deep learning deal with nonconvex functions. Here's the question though: If deep learning is non-convex then why we apply a convex loss function such as cross-entropy or least square to solve a problem under a convex constraint? What am I missing?</p>
"
1670,"<p>For a regression task, I have sequences of training data and if I define the layers of deep neural network to be:</p>

<p>Layers=[ sequenceInputLayer(featuredimension) reluLayer dropoutLayer(0.05) fullyConnectedLayer(numResponse) regressionLayer]</p>

<p>Is it a valid deep neural network? Or do I need to add LSTM layer too?</p>
"
1671,"<p>I  want to use computer vision to allow my robot to detect the corners of a soccer field based on its current position. Matlab has a detectHarrisFeatures feature, but I believe it is only for 2D mapping.</p>

<p>The approach that I want to try is to collect the information of the lines (using line detection), store them in a histogram, and then see where the lines intersect based on their angles.</p>

<p>My questions are:</p>

<pre><code>How do I know where the lines intersect?
How do I find the angles of the lines using computer vision?
How do I update this information based on my coordinates?
</code></pre>

<p>I am in the beginning stages of this task, so any guidance is much appreciated!</p>
"
1672,"<p>How do I decrease the accuracy value when training a model using <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras</a>; which parameters can I change to decrease the value?</p>

<p>My objective is not to actually decrease it, but just to know which parameters influence the accuracy</p>

<pre><code>sgd = optimizers.SGD(lr=1e-2)
</code></pre>
"
1673,"<p>I would like to know how to teach an agent for performing prediction of the severity of disease and also for alerting patients using machine learning methods.
I found the Model-based reflex agent class can be used in medical diagnosis in some literature. May I know which architecture will be good, to make such an agent?</p>
"
1674,"<p>I am looking to detect think objects like pens, pencils and surgical instruments. The bounding box is not important, but I am looking to see if I can train a model to detect both the object as well as its orientation. Typical object detection networks like R-CNN,YOLO,SSD encode class name and bound boxes.  Instead of bounding boxes, I'm looking to encode only 2 points, one starting x,y point and one ending x,y point.  The start point for objects is where one would grip the object.  For instance:</p>

<ul>
<li>The pencil eraser(start point) is pointed 50 degrees to the top right.</li>
<li>The surgical instrument is 10 degrees from the x-axis and handle is pointed to the bottom right.</li>
<li>Pen tip(end point) is pointing vertically upwards.</li>
<li>Fork, the start point would be the grip handle part, and the end point would be in the middle where the 4 prongs are.</li>
</ul>

<p>As long as I can encode the start and end points, then I can determine the orientation.  I would need to define these points during training and the question is whether there is an existing model (mobile net/inception/RCNN) that I can encode this information in?  One potential way I was thinking was to use YOLO and for the bounding box, the top left x,y would be the starting point x,y(handle) whereas the bound box width, height would be replaced with the end point x,y(pencil writing tip, fork prongs.</p>

<p>Thank you.</p>
"
1675,"<p>Is there a way in the WEKA explorer to manually select the initial centres when using SimpleKMeans clustering?</p>
"
1676,"<p>Hi for a small little research for school i need some different opinions on a few different questions. If you could answer these questions i would really appreciate it.</p>

<p>How far do you think machine learning will be able to progress, do you see development hitting a wall in the future?</p>

<p>Do you think machine learning in it's current state is already able to replace simple jobs? If not how long do you think it will take?</p>

<p>How will machine learning affect jobs? Do you see machine learning and AI taking over jobs like programming in the future or do you think it will create more jobs than it destroys?</p>
"
1677,"<p>I have to read a lot of papers, and I thought that I can use an A.I. to read them and summarize them. Maybe find one that can understand what the papers are talking about it seems a lot to ask.</p>

<p>I think I can use natural language processing. Is it the right choice?</p>

<p>I'm sorry, but I'm new in A.I. and I don't know much about it.</p>
"
1678,"<p>I'm currently working on license plate recognition. My system consist of 2 stage: (1) License Plate region extraction &amp; (2) License Plate region recognition.</p>

<p><strong>I'm doing (1) with Raspberry pi 3 model b</strong>. I find license plate candidate first by merging bounding boxes based on their similarity. In this way, i have only 1~7 license plate region proposals. And it took less than .3 seconds. </p>

<p>Now i have to reduce number of region proposal to be around only 1~2 so that i can send these images to server to do job (2). For license plate extraction, I made my own classifier function in tensorflow and the code is below. It gets proposed license plate as input.</p>

<p>First, I resize all license plate to be [120, 60] and converted to gray image. And there are 2 classes: 'plate', 'non_plate'. For non_plate image, i collected various image that might appear in image as background. I have 181 images for 'plate' class and 56 images for 'non_plate' for now, i trained for about 3000 steps so far and current loss is .53 . </p>

<p>When i did prediction on test set,  i encountered problem that for some of plate image, it doesn't recognize license plate which is very obviously license plate image from my eyes. It is okay for me to wrongly recognize non plate image as plate but it is problem if it wrongly recognize plate as non_plate because it will not be sent to server to be fully recognized.</p>

<p>It happens like 10 out of 100 test images and this rate is far worse than i expected. I need help for adressing this problem. Would there be any improvement that i can make? </p>

<p><strong>(1) Is my training set too small to classify between license plate
    and non license plate? Or is number of steps is too small?</strong></p>

<p><strong>(2) Is my graph structure bad?</strong> I needed to have small graph
    structure for my raspberry pi to recognize less than 1 second. Could
    you suggest better structure if it is bad?</p>

<p><strong>(3) Is it bad to resize any proposed image to [120, 60] to be used
    as input for graph?</strong> I think it loses some information. But isn't
    this close to roi pooling like used in fast rcnn?</p>

<pre><code> inputs=tf.reshape(features[FEATURE_LABEL],[-1,120 , 60 ,1],name=""input_node"") #120 x 60 x 1, which is gray

conv1=tf.layers.conv2d(inputs=inputs,
                       filters=3,
                       kernel_size=[3,3],
                       padding='same',
                       activation=tf.nn.leaky_relu
                       )
#conv1 output shape: (batch_size,120,60,3)

pool1=tf.layers.max_pooling2d(inputs=conv1,pool_size=[2,2],strides=2,padding='valid')

#pool1 output shape: (batch_size,60,30,3)

conv2=tf.layers.conv2d(inputs=pool1,filters=6,kernel_size=[1,1],padding='same',activation=tf.nn.leaky_relu)

#conv2 output shape: (batch_size, 60,30,6)

pool2=tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2,padding='valid')

#pool2 output shape: (batch_size, 30,15,6)

conv3=tf.layers.conv2d(inputs=pool2,filters=9,kernel_size=[3,3],padding='same',activation=tf.nn.leaky_relu)

#conv3 output shape: (batch_size, 30,15,9)

pool3=tf.layers.max_pooling2d(inputs=conv3,pool_size=[2,2],strides=2,padding='valid')

#pool3 output shape: (batch_size, 15,7,9)


#dense fully connected layer
pool2_flat=tf.reshape(pool3,[-1,15*7*9]) #flatten pool3 output to feed in dense layer

dense1=tf.layers.dense(inputs=pool2_flat,units=120,activation=tf.nn.relu)

logits=tf.layers.dense(dense1,2) #input for softmax layer
</code></pre>

<p><a href=""https://i.stack.imgur.com/vHSZu.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vHSZu.jpg"" alt=""training non plate image example""></a>
 [training non plate image example]
[<img src=""https://i.stack.imgur.com/EX3VO.png"" alt=""training plate image example"">]<a href=""https://i.stack.imgur.com/EX3VO.png"" rel=""nofollow noreferrer"">4</a>
[training plate image example. It is region proposed image]</p>
"
1679,"<p>There are many books, courses, etc. out there, but not sure which path to take. </p>

<p>So what would be the most effective way (shortest) to learn natural language processing online?</p>

<p>p.s. I mean learning fundamentals, not how to use existing libraries or services.</p>
"
1680,"<p>I am writing an MDP based agent that is supposed to learn to place bids and asks in a trading environment. The system requests 2 values (mWh energy and $, both being positive or negative). 
Every timestep the agent has a certain volume that it has to either buy or sell. </p>

<p>I tried setting these two values as action values, giving it 4 individual ones (1 for buy price and amount one sell price and amount)</p>

<p>I used the DDPG and NAF agents from keras-rl <a href=""https://github.com/keras-rl/keras-rl/tree/master/examples"" rel=""nofollow noreferrer"">here</a> but both aren't working for me. I tried a number of reward functions too:</p>

<ul>
<li>direct cash reward: average price of market for required energy vs what the agent achieved</li>
<li>shifting balancing price: first emphasize that the broker balances it's portfolio (i.e. orders the amount it has to) and later optimize for price per mWh</li>
<li>simple core: as a test I ran a reward function that just rewards the agent to be close to the actions [0.5, 0.55]</li>
</ul>

<p>All three failed again. </p>

<ul>
<li>LR : tried between 0.01 and 0.00001</li>
<li>Layers: Tried anything between 1 layer 1 cell and 5 layer 128 cells</li>
<li>Types: I used both Dense and LSTM cells with according input shapes</li>
</ul>

<p>Symptoms: Generally it looks like the system is not learning anything. I am unsure why. How does the reward function have to be structures to incentivize the system to at least move in the correct direction? Especially the reward that told the agent to be close to [0.5, 0.5] by basing the reward simply on the squared difference to this point should have worked in my eyes.</p>

<p><a href=""https://i.stack.imgur.com/XxRTf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XxRTf.png"" alt=""enter image description here""></a></p>
"
1681,"<p>I'm looking at writing an AI agent for pattern recognition.</p>

<p>I want to be able to constantly feed new data to the AI to continuously train it as new data may have new patterns.</p>

<p>My problem, though, is that my input feed may break once in a while (the data comes from a remote computer) and thus some of the data will go missing. The other computer sends me real-time data so when the connection goes down, any new data while disconnected goes missing as far as the AI agent is concerned. (at this point, I'm not looking at fixing the gaps, although ultimately, reducing them is one of my goals, at this point I have to pretend it's not possible to accomplish.)</p>

<p>What kind of impact missing data has on a pattern recognition AI?</p>
"
1682,"<p>In chapter 8 of ""Reinforcement Learning: An Introduction"" by Sutton and Barto, it is stated that Dyna needs a model to simulate the environment. </p>

<p>But why do we need a model? Why can't we just use the real environment itself? Wouldn't it be more helpful to use real environment instead of fake one?</p>
"
1683,"<p>This is more of a technical question rather than a practical one.</p>

<p>I've exported a decision tree made with python/scikit learn and would like to know what the ""value"" field of each leaf corresponds to.</p>

<p>Thanks!</p>

<p><a href=""https://i.stack.imgur.com/sE6DJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sE6DJ.png"" alt=""decision tree""></a></p>
"
1684,"<p>I am very new to AI, I have a set of 3D human models that I would like to train the algorithm to identify wrist, upper arm, lower arms, etc, and distance between them.</p>

<p>From my understanding, this is a regression problem. But with my very limited knowledge, most tutorial online showing me cat and dog classification problem.</p>

<p>Do you have any clue for me to research next? There are some paper saying to convert the 3D model to image, and use convolutional neural network for training.</p>

<p>p/s: Please don't downvote me, I am too young and too lost in this field.</p>
"
1685,"<p>I am trying to build an agent that trades commodities in a exchange setting. What are good ways to map the action output to real world actions? If the last layer is a <code>tanh</code> activation function, outputs range between <code>[-1,+1]</code>. How do I map these values to real actions? Or should I change the output activation to <code>linear</code> and then directly apply the output as an action?</p>

<p>So let's say the output is tanh activated and it's <code>-0.4, 5</code>. I could map this to:
- -0.4 --> sell 40% of my holdings for 5$ per unit
- -0.4 --> sell 40% for 5$ in total</p>

<p>if it was linear, I could expect larger outputs (e.g <code>-100, 5</code>). Then the action would be mapped to:
- sell 100 units for 5$ each
- sell 100 units for 5$ total</p>
"
1686,"<p>What are feature embeddings in the context of Convolutional Neural Networks? Is it related to bottleneck features or feature vectors?</p>
"
1687,"<p>Hi I study AI by myself with ""AI a modern approach"" I've just finished the chapters about bayesian network and probabilities, and I found them very interesting. Now I want to implement the differents algorithms and test them on differents cases and environments. But the question is : is it worth it to spend time on these techniques ? </p>
"
1688,"<p>I am using the Fceux emulator to create a Genetic Algorithm in Lua to play the 'Arkanoid' game. It is based on Atari Breakout.</p>

<p>A member of my population contains a string of 0's and 1's.(Population size:200).
Consider a member
Every 10 frames a bit is read from the string.(Length of string is about 1000)
If it is 0 the paddle moves left, If it is 1 the paddle moves right for the next 10 frames.</p>

<p>Now I wrote an genetic algorithm that tries to find the best sequence of inputs to play the game.</p>

<p>I have experimented with three types of fitness, One is to achieve maximum score, one is to try to reduce number of blocks to a minimum and the last one is to try to stay alive as long as possible.</p>

<p>None of the three fitness seem to work.</p>

<p>Then I thought that something with my crossover might be wrong.</p>

<p>Every generation, I print out the average fitness of all members. Some generations it increases, while in some generations it decreases.
I have tried changing the population size to 50,100,200,300.</p>

<p>Mutation in my algorithm has a 1% Chance(If Mut_rate=1) that each of the bit will be replaced with its opposite bit.</p>

<p>Now coming to the crossover, I have used yet again many methodologies.
One of them is to just select the top 20% or 30%(cr_rate)(according to their fitness) to pass on to the next generation and killing the remaining ones.</p>

<p>Another method is to add the top percentile to the population and use the remaining population to swap a few bits with top ones and add them into the next generation.</p>

<pre><code>function crossover(population,rate)
    local topp=math.floor(rate*(#population));
    top={}
    for i=1,topp do
        table.insert(top,population[i])
    end
for i=1, #population do
        local p1 = math.random(1,topp);
        local p2 = math.random(1,topp);
        --print(top[p1]);
        --print(top[p2]);
        if top[p1][2] == top[p2][2] then
            local rval = math.random(1, 10) &gt; 5;
                if rval then
                    population[i] = top[p1];
                else
                    population[i] = top[p2];
                end
            elseif top[p1][2] &gt; top[p2][2] then
                population[i] = top[p1];
            else
                population[i] = top[p2];
        end
        population[i][2]=0;
end
--[[
for i=topp+1,#population do
    local p1 = math.random(1,topp);
    local p2 = math.random(1,#population);
    local s='';
    local flag=0;
    s=string.sub(top[p1][1],1,no_controls/2)..string.sub(population[p2][1],(no_controls/2)+1,no_controls);
    population[i][1]=s;
    population[i][2]=0;
 end
  --]]
end
</code></pre>

<p>Population is the table of population where each member has an input string and a fitness value.(Sorted , max fitness is first).
Rate is the percentage to select the top performers.
no_controls is the size of input string.
The commented section of the code is where I perform the swap.</p>

<p>Here is the mutation function.</p>

<pre><code>function mutation(population,mut_rate)

    local a=0;
    local b=1;
    for i=1, #population do
        for j=1, #(population[i][1]) do
            if math.random(1, 100) &lt;= mut_rate then
                if string.sub(population[i][1],j,j)=='1' then
                population[i][1] = string.sub(population[i][1],1,j-1)..a..string.sub(population[i][1],j+1);
            else
                population[i][1] = string.sub(population[i][1],1,j-1)..b..string.sub(population[i][1],j+1);
            end
            end
        end
    end
end
</code></pre>

<p>Mut_rate is 1. And crossover rate is 0.2 or 0.5.</p>

<p>I have tried changing the mutation rate from 0 to 20.
I have also tried to change the crossover rate  as 0.2,0.5,0.7.
And the fitness using no_blocks, score, time_alive.
When I run the algorithm, the average fitness of the population first increases slightly , then decreases after a few generations and then remains constant forever.</p>

<p>The paddle also seems to be performing  the same moves over and over again, which made be think that there might not be enough variation.</p>

<p>I need help, because I have been stuck on this for a few days now.
I need suggestions on what would be a suitable crossover and mutation function and a perfect fitness function.</p>

<p>Thanks.</p>
"
1689,"<p>I'm having trouble grasping how to output word vectors from an LSTM model. I'm seeing many examples using a softmax activation function on the output, but for that I would need to output one hot vectors as long as the vocabulary (which is too long). So my question is should I use a linear activation function on the output to get the word vectors directly (and then find the closest word) or is there something I'm missing here. Thanks.</p>
"
1690,"<p>there seems to be a major difference how the terminal reward is received/handled in self-play RL vs ""normal"" RL which confuses me.</p>

<p>I implemented TicTacToe the normal way, where a single agent plays against an environment that manages the state and also replies with a new move. In this scenario the agent receives a final reward of +1,0,-1 for a win/draw/loss.</p>

<p>Next I implemented TicTacToe in a self-play mode where two agents perform moves one after the other and the environment only manages the state and gives back the reward. In this scenario an agent can only receive a final reward of +1 or 0 because after his own move he will never be in a terminal state in which he lost (only agent2 could terminate the game in such a way). That means:</p>

<ol>
<li>In self-play, episodes end in such a way that only one of the players sees the terminal state and terminal reward.</li>
<li>Because of point one, an agent can not learn if he made a bad move that enabled his opponent to win the episode. Simply because he does not receive a negative reward.</li>
</ol>

<p>This seems very weird to me. What am I doing wrong?  Or if I'm not wrong, how do I handle this problem ??</p>

<p>Thanx </p>
"
1691,"<p>I've been reading Google's DeepMind Atari <a href=""http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf"" rel=""nofollow noreferrer"" title=""RL"">paper</a> and I'm trying to understand the concept of <code>experience replay.</code> Experience replay comes up in a lot of other reinforcement learning papers (particularly the AlphaGo paper), so I want to understand how it works. Below are some excerpts.</p>

<blockquote>
  <p>First, we used a biologically inspired mechanism termed experience
  replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in
  the data distribution.</p>
</blockquote>

<p>The paper then elaborates as follows (I've taken a screenshot, since there are a lot of mathematical symbols that are difficult to reproduce):</p>

<p><a href=""https://i.stack.imgur.com/eOOTH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eOOTH.png"" alt=""enter image description here""></a></p>

<p>What is experience replay and what are its benefits in laymen's terms?</p>
"
1692,"<p>I've been reading Google's DeepMind Atari <a href=""http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf"" rel=""nofollow noreferrer"" title=""RL"">paper</a> and I'm trying to understand how to implement <code>experience replay.</code> My question is that whether we update the parameters $\theta$ of function $Q$ once for all the samples of the minibatch or we do that for each sample of the minibatch separately? </p>

<p>According to the following code from this paper, it performs the gradient descent on loss term for the $j$-th sample. However, I have seen other papers (referring to this paper) that say that we first calculate the sum of loss terms for all samples of the minibatch and then perform the gradient descent on this sum of losses.</p>

<p><a href=""https://i.stack.imgur.com/Ow0HM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ow0HM.png"" alt=""enter image description here""></a></p>
"
1693,"<p>So I was wondering about the actual constraints in the field of Artificially Intelligent agent/robot. We have been able to replicate most of human sensory functions like:</p>

<ul>
<li>Vision - Computer Algorithms can perform better image classification tasks.</li>
<li>Touch - A physical object more sensitive than skin can be modelled easily.</li>
<li>Taste and smell - Although there are electro-mechanical tongues, since taste and smell and related it is quite complex to model (i.e. no satisfactory devices exist).</li>
<li>Hearing - All types of high fidelity audio recording devices are available.</li>
</ul>

<p>We have also got processors for these sensors:</p>

<ul>
<li>Memory - We have got memory running into exabytes probably.</li>
<li>Processing power - We have got a high processing power, albeit it falls somewhat short of human brain according to this <a href=""https://ai.stackexchange.com/questions/5239/what-makes-animal-brain-so-special"">answer</a>.</li>
<li>Algorithms - We have got highly developed purely logical algorithms, or we can easily Machine Learn and approximate results of unknown algorithms.</li>
<li>We can also model the human anatomy of various physical functions like walking/running/exercising, etc, like <a href=""http://news.mit.edu/2015/cheetah-robot-lands-running-jump-0529"" rel=""nofollow noreferrer"">this</a> mechanical cheetah my MIT students.</li>
</ul>

<p>So my question is what is stopping us from stringing together all this to actually device a quite good Artificially Intelligent agent? What are the constraining factors and how are we trying to overcome it?</p>

<p>My wording maybe little bit off, you are free to edit the question keeping the general theme same.</p>
"
1694,"<p>If a Conscious AI is ever to become, how do you reckon that will happen? From a code written by us, like in the TV Show 'Humans' or from becoming Cognizant from their previous experiences, like in 'Westworld' or perhaps some other way?</p>
"
1695,"<p>I am looking in to building a kind of troubleshooting web application. It would be a form that starts with a first question. Depending on the answer, you get a follow up question and so on until the app has qualified your problem in to a small group of problems. To me it sounds a bit like a decision tree but what I have read about them is that it is the internal structure of a model and not what I am looking for. My guess is that a model needs all the input variables at once and not like I am looking for that feeds it one parameter at a time.</p>

<p>At this time I do not know of any data available. With the client we could create the desired resulting problem groups and the questions as well.</p>

<p>Would it be possible to solve this with the help of AI instead of hand coding a lot of case switch statements? If so could you point me to what to read up on?</p>
"
1696,"<p>I found several methods for setting the compatibility distance in NEAT: some normalize it, some don't, some automatically adjust it.</p>

<p>In a few tests I am running, using normalized static compatibility distance, the number of species increase very rapidly, thus suggesting to adjust (e.g. increase) the compatibility distance.</p>

<p>I haven't found however, how to determine a reasonable number of species for my population, which are the benefits of having lots/few species and which are the benefits of having stable vs mutable number of species?</p>
"
1697,"<p>I am building a NN for which I am using sigmoid function as the activation function for the single output neuron at the end. Since sigmoid function is known to take any number and return value between 0 and 1, this is causing division by zero error in back-propagtion stage because of the derivation of cross entropy. I have seen over internet it is advised to use sigmoid activation function with cross entropy loss function. So how this error is solved?</p>
"
1698,"<p><a href=""https://i.stack.imgur.com/C2Wkg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C2Wkg.png"" alt=""enter image description here""></a></p>

<p>Hello,
I would like to know whether this picture from the paper: Distributed Training of Deep Neural Networks: Theoretical and Practical Limits of Parallel Scalability valid?</p>

<p>Questions:<br>
1) Does InnerProduct (Fully connected) layer actually take <strong>more time to compute in</strong> a neural network than Convolution? 
<br>
<br>
2) Is assessing <strong>GLOPs/time</strong> a good way of <strong>estimating performance</strong> of different types of layers in a neural network on any hardware? (Conv, FC etc.)
<br>
<br>
3) Does anyone know where I can find GFLOPs vs compute time for different types of layers across GPUs/CPUs? (I know DeepBench, any other suggestions would be great too)</p>
"
1699,"<p>my name is iSlam, industrial designer from Egypt, it’s my last year and the project i want to make something like smart home system with voice control, can anyone help me with any advice. </p>
"
1700,"<p>I'm learning fuzzy logic and more or less understand the basic concept, but i'm having a hard time understanding how to apply it to a method. I tried browsing online for explanation on how to use it, but only found some implementation and test case using the basic form of 4 rules and 3 variables, and 2 rules per variable. Anyway this is an example case, i will use Tsukamoto method.</p>

<p>In this case actually i have 6 rules and 3 variables with 3 rules ver variable, but i will only explain 1 of the variables because i think the rest will have the same solution. I have 3 variables one of them is ""size"", the range is for small it's 0-2 and for large it's 7-10. The current condition is size = 6.5. The rules is as follow(simplified to only use this variable):</p>

<ul>
<li>[R1] size = small</li>
<li>[R2] size = medium</li>
<li>[R3] size = large</li>
</ul>

<p>What i want to know is:</p>

<ul>
<li>how do i define the formula for medium(the middle rule if the case is different)?</li>
<li>what if the rule is more than 3 (i.e. small, medium, large, ex-large)?</li>
</ul>

<p>What i understands if the rule is only 2 i can use this formula</p>

<ul>
<li>small[x]=(max-x)/(max-min)</li>
<li>large[x]=(x-min)/(max-min)</li>
</ul>

<p>My current approach to this problem is as follow:</p>

<p>small[x]=1; x&lt;=2</p>

<p>medium[x]=(max-x)/(max-min); 2 &lt; x &lt; 7</p>

<p>large[x]=0; x>=7</p>

<p>Is this correct? Also can you refer me to some source to study this? As i mentioned before i can only find some implementation and basic explanation, it's either there is no online source for this or i don't know what to search for. Sorry if it's hard to understand i can edit and post the whole problem if you want, thanks in advance.</p>

<p>Extra question: what is the name of algorithm which can be used to solve the crossing bridge puzzle(the one with timer, max person, and stuff)?i forgot the name. </p>
"
1701,"<p>I have this following natural language statement:</p>

<p>""There is only one house in area1 the size of which is less than 200m².""</p>

<p>which is mistranslated to FOL:</p>

<pre><code>∃x.(house(x) ∧ In(x,area1) ∧ ∀y.(house(y) ∧ In(y,area1) ∧ size(y) &lt; 200 -&gt; x=y))
</code></pre>

<p><em>This translation is wrong according to my lecturer, because it is not necessary that the size of x must be less than 200. The statement is true if there only houses which are bigger.</em></p>

<p>I have two questions:</p>

<ol>
<li><p>I don't get the FOL translation at all and don't see where the uniqueness part is expressed :
so translated it back : ""if all houses in area1 have a size less then 200m² then there exists one house which equals to all houses ??""</p></li>
<li><p>why is not necessary that the size of x is less than 200, when it clearly says in the statement above that must exist one house with a size less then 200 ?</p></li>
</ol>
"
1702,"<p>I'm doing a project for my last university examination but I'm having some troubles! I'm making an expert system who should be able to assemble a computer after asking some questions to the user. It works but according to my teacher I need to define more rules, could you give me some suggestions please? I have facts like these:</p>

<pre><code>processor(P, Proc_price, Price_range),
motherboard(M, Motherboard_price, Price_range),
ram(R, Ram_price, Price_range),
case(C, Case_price, Price_range),
ali(A, Ali_price, Price_range),
video_card(V, Vga_price, Price_range),
ssd(S, Ssd_price, Price_range),
monitor(D, Monitor_price, Price_range),
hdd(H, Hdd_price, Price_range).
</code></pre>

<p>I ask these questions to the user: 1) choose the price range 2) choose the display size 3) choose hard disk size Then I ask 3 questions about computer utilization to define the user: 1) do you surf on internet? 2) do you play? 3) do you use editing programs?</p>

<pre><code>use(gaming) :- ask(""Do you play games? (y/n)"").

    use(editing) :- ask(""Do you use editing programs? (y/n)"").

    use(surfing) :- ask(""Do you surf internet?(y/n)"").

    user(base) :-
        use(surfing),  \+ use(gaming), \+ use(editing).

    user(gamer) :-
        use(gaming), use(surfing), \+ use(editing).

    user(professional) :-
        use(editing), \+ use(gaming), use(surfing).
</code></pre>

<p>I should make more questions about user definition to make user definition more complex too and add some rules. Please help me, I'm desperate</p>
"
1703,"<p>We are doing a research design project on autonomous vehicles and have some questions on AV Levels 4/5; specifically on the roles, impacts and consequences of AV on society, government, users and other stakeholders.</p>

<p>We're currently stuck on this main question:</p>

<p>Q: What functionally, does control look like in AV levels 4 and 5?</p>

<p>For example, is the whole purpose of a level 4/5 that a user has no input into the control?</p>

<p>Could a driver in AV (level 5) stop in an emergency, or say they want to ""take corners harder, speed up, slow down""?</p>

<p>Could I choose to change the equi-distance between my AV and the others around me because I like space?</p>

<p>We're wondering about what functionally, does AV level 4/5 offer a user; and what it looks like?</p>

<hr>

<p>Context:</p>

<p>Our remit is within the world of design (design thinking), not specifically technology, or expert system functionality. We're looking at the issue from a design perspective; who does it impact, who are the stakeholders, what are the consequences and impacts. What role does a driver have an in level 5? Could an auto-manufacturer want to give drivers control in level 5? How do emergency services act in these situations? What are the touchpoints to society and whom does it impact and what does it say about the design of AV for the future of society.</p>
"
1704,"<p>What are the characteristics of activation function in a feed forward neural network?
Which of the below functions can be used as activation function in a feed forward NN?</p>

<p><a href=""https://i.stack.imgur.com/oWC3E.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oWC3E.gif"" alt=""enter image description here""></a></p>

<p>which looks like this:</p>

<p><a href=""https://i.stack.imgur.com/AXGlN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AXGlN.png"" alt=""enter image description here""></a></p>

<p>and</p>

<p><a href=""https://i.stack.imgur.com/ZIr87.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZIr87.gif"" alt=""enter image description here""></a></p>

<p>which can be simplified as:</p>

<p><a href=""https://i.stack.imgur.com/iyJpc.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iyJpc.gif"" alt=""enter image description here""></a></p>

<p>and the graph looks like:</p>

<p><a href=""https://i.stack.imgur.com/GQE2B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GQE2B.png"" alt=""enter image description here""></a></p>
"
1705,"<p>an agent aims to find a path on a hexagonal map with initial state s0 in the center and goal state s⋆ at the bottom as depicted below. The map is parametrized by the distance n ≥ 1 from s0 to any of the border cells (n = 3 in the depicted example). The agent can move from its current cell to any of the 6 adjacent cells,</p>

<p><a href=""https://i.stack.imgur.com/kHUyQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kHUyQ.png"" alt=""enter image description here""></a></p>

<p>How can we find the number of node expansions performed by bfs without duplicate detection, and with duplicated detection as a function of n.</p>

<p>I know that the branching factor for the map would be 6 because the agent can move in 6 directions, and for a depth of k, we get O(b^k) = 6^n without duplicate detection, but what is the number of node expansion with duplicate detection with bfs.</p>
"
1706,"<p>I know some people will try to crucify me for asking such question here, since it's too generic, but ill give it a shot.</p>

<p>So basically what I'm after in trying to find if there any tools exist (simply white papers are welcome too) for creating chatbots based on a plain text corpus, so that questions could be asked about it. That is, there is some textual data (like articles), but there are no question, answer pairs to train a conventional model with, like seq2seq. I understand that there are ways to achieve this by using tools to extract intents,entities from a question and match it with paragraphs, articles (based on indexes and topics), but to my understanding this creates more of a NLP search bot, rather than a chatbot, which provides answers in conversation-like manner.</p>

<p>Thanks in advance.</p>
"
1707,"<p>I'm currently a software developer, and I want to get into this field, I'm interested particularly in self driving cars, which as far as I know, work with AI or machine learning (which I still don't know the difference if there is any), any suggestions on how to start on this field, I'm very young and I want to build a career in this area given my software development background</p>
"
1708,"<p>I trained a DQN that learns TicTacToe by playing against itself with a reward of -1/0/+1 for a loss/draw/win. Every 500 episodes I test the progress by letting it play some episodes (also 500) against a random player.</p>

<p>As shown in the picture the net learns quickly to get an average reward of 0.8-0.9 against the random player. But after 6000 episodes performance seems to deteriorate. If I play manually against the net after 10000 episodes it plays okay, but by no means perfect.</p>

<p>Assuming that there is no hidden programming bug, is there anything that might explain such a behavior? Is there anything special about self-play in contrast to training a net against a fixed environment ?  </p>

<p>Here further details: The net has two layers with 100 and 50 nodes (and a linear output layer with 9 nodes), uses DQN and a replay buffer with 4000 state transitions. The shown epsilon values are only used during self-play, during evaluation against the random player exploration is switched off. Self-play actually works by training two separate nets of identical architecture. For simplicity one net is always player1 and the other always player2 (so they learn slightly different things). Evaluation is then done using the player1 net vs. a random player which generates moves for player2. </p>

<p>Thanx</p>

<p><a href=""https://i.stack.imgur.com/y3xOx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y3xOx.png"" alt=""TicTacToeSelfPlayResults""></a></p>
"
1709,"<p>If an Atari game's rewards can be between <span class=""math-container"">$-100$</span> and <span class=""math-container"">$100$</span>, when can we say an agent learned to play this game? Should it get the reward very close to <span class=""math-container"">$100$</span> for each instance of the game? Or it is fine if it gets a low score (say <span class=""math-container"">$-100$</span>) at some instances? In other words, if we plot the agent's score versus number of episodes, how should the plot look like? From this plot, when can we say the agent is not stable for this task?</p>
"
1710,"<p>I am working on a task where I am required to automate the customer service request channel. The process is quite typical. A customer queries about a product via email, the person on the front channel checks emails, forwards it to the relevant department and then answer is provided.</p>

<p>The problem is that customer query can be about one of hundreds of devices listed. Each device has its own pdf documentation which is quite extensive. Finding the right pdf and then finding the right section where information could be listed is really a tedious process and wastes a lot of time. Sometimes the information is not even listed and answer has to be improvised by product specialist (the last part hints me about reinforcement learning, what do you guys think).</p>

<p>What I want to achieve is that this whole tedious and repetitive process is automated and may be if possible, the model learns over time as well. The task output is quite open ended as well. Different approaches and models can be tried out (like chatbots and etc). Rapid failure is highly appreciated here.</p>

<p>Below mentioned are some more details:</p>

<p><strong>Database:</strong></p>

<ul>
<li>I have customers queries about devices in the form of emails.</li>
<li>PDF documentation of devices. The documentations are quite extensive.</li>
<li>I also happen to have some excel files where some sample queries and sample answers are listed. But since queries can be of very dynamic nature, it doesn't seem like a classification problem (to me at least).</li>
</ul>

<p>I have googled quite a lot about the topic but mostly what I get are topics like 'How AI will transform the customer service' and then something more specific to NLP and a lot of company ads etc. So far what I have understood from online surfing is that possible approaches need to use NLP library (Nltk) in Python and do some topic modelling for documentation and for email. Still how I approach the whole task is not clear to me. </p>

<p>What I want from you guys is that maybe guide me how this task can be achieved step by step. I am not looking for any code! Just which methods can be used and how the problem can be approached. Right now, I don't know where to start and how to approach it.</p>
"
1711,"<p>I am working on a task that requires me to classify a large amount of mixed files on a backup drive (more than 10 TB with more than 32 million files) based on content. The included file types are documents, images, videos, executable, and pretty much everything in between.</p>

<p>I am also required to create new tags or metadata that will allow for automatic classification of new files. It'd also allow for manual input of category. For each input I give to the system, the system would learn and improve its classification. </p>

<p>Here is what I have come up with so far:</p>

<ul>
<li>Documents: classify using existing categories with packages like Nltk on Python. Alternatively, first run topic modeling using LDA or NMF and then classify. </li>
<li>Images: use CNN. In case of unknown label, use VAE to cluster the images.</li>
<li>Videos and other types of files: I do not know how to approach this.</li>
</ul>

<p>Since I am not sure about my approach, any input is greatly appreciated. </p>
"
1712,"<blockquote>
  <p>Furthermore, we observe that mode collapses traditionally
  plaguing GANs tend to happen very quickly, over the course of a dozen minibatches. Commonly
  they start when the discriminator overshoots, leading to exaggerated gradients, and an unhealthy
  competition follows where the signal magnitudes escalate in both networks. We propose a mechanism
  to stop the generator from participating in such escalation, overcoming the issue (Section 4.2).</p>
</blockquote>

<p>What do they mean when the discriminator overshoots? The discriminator gets good too quickly? And signal magnitudes escalate in both networks?</p>

<p>My current intuition is that the discriminator gets too good too soon which causes the generator to spike and try to play catch up, that would be the unhealthy competition that they are talking about. Model collapse is the side effect where the generator has trouble playing catch up and decides to play it safe by generating slightly vary images to increase its accuracy. Is this way of interpreting the above paragraph correct?</p>
"
1713,"<p>Is there a compilation of ML/AI Free Online Courses available for Beginners?
Can you folks share the link ?</p>
"
1714,"<p>Disclaimer: I am a novice in the world of machine learning, so please excuse my ignorance.</p>

<p>My dataset consists of things like age, days since last visit, etc. This information is medical related. None of which is geometrical, just data pertaining to particular clients.</p>

<p>The goal is to classify my dataset into three labels. The dataset is not labeled, meaning I'm dealing with an unsupervised learning problem. My dataset consists of ~20,000 records, but this will linearly increase overtime. The data is nearly all floats, with some being strings that can easily be converted into a float. <a href=""http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"" rel=""nofollow noreferrer"">Using this cheat sheet for selecting a solution from the scikit site</a>, a KMeans Cluster seems like potential solution, but I've been reading that having high dimensionality can render the KMeans Cluster unhelpful. I'm not married to a particular implementation either. I've currently got a KMeans Cluster implementation using TensorFlow in Python, but am open for alternatives.</p>

<p>My question is: what would be some solutions for me to further explore that might be more optimal for my particular situation?</p>
"
1715,"<p>I'm building a decision tree and would like to separate (for example) the elements that are in class 0 from those in classes 1 and 2, case in point:</p>

<pre><code>df = pd.DataFrame(np.random.randn(500,2),columns=list('AB'))
cdf = pd.DataFrame(columns=['C'])
cdf = pd.concat([cdf,pd.DataFrame(np.random.randint(0,3, size=500), columns=['C'])])
#df=pd.concat([df,cdf], axis=1)
(X_train, X_test, y_train, y_test) = train_test_split(df,cdf,test_size=0.30)
y_train=y_train.astype('int')
classifier = DecisionTreeClassifier(criterion='entropy',max_depth = 2)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
</code></pre>

<p><code>C</code> represents the class of an element,<code>A</code> and <code>B</code> are two variables that define the element, how can I build a tree that instead of dividing results into <code>C=0</code>, <code>C=1</code> or <code>C=2</code> divides them into <code>C=0</code> and <code>C!=0</code>?</p>
"
1716,"<p>I am reading a book about OpenCV, it speaks about some derivative of images like <code>sobel</code>. I am confused about image derivative! What is derived from? How can we derived from an image? I know we consider an image(1-channel) as a n*m matrix with 0 to 255 intensity numbers. How can we derive from this matrix?</p>

<p>EDIT: a piece of text of the book:</p>

<blockquote>
  <p>Derivatives   and Gradients</p>
  
  <p>One   of  the most    basic   and important   convolutions    is  computing   derivatives
  (or   approximations  to  them).  There   are many    ways    to  do  this,   but only    a   few
  are   well    suited  to  a   given   situation.</p>
  
  <p>In    general,    the most    common  operator    used    to  represent   differentiation is  the
  Sobel derivative  operator.   Sobel
  operators exist   for any order   of  derivative  as  well    as  for mixed   partial
  derivatives   (e.g.,  ∂ 2 /∂x∂y).</p>
</blockquote>
"
1717,"<p>I'm working with deep learning on some EEG data for classification, and I was wondering if there's any systematic/mathematical way to define the architecture of the networks, in order to compare their performance fairly.</p>

<p>Should the comparison be at the level of neurons (e.g. number of neurons in each layer), or at the level of weights (e.g. number of parameters for training in each type of network), or maybe something else?</p>

<p>One idea that emerged was to construct one layer for the MLP for each corresponding convolutional layer, based on the number of neurons after the pooling and dropout layers.</p>

<p>Any ideas? If there's any relative work or paper regarding this problem I would be very grateful to know.</p>

<p>Thank you for your time</p>

<p>Konstantinos</p>
"
1718,"<p>I have been looking at Fibonacci series, the golden ratio and its uses in nature, like how flowers and animals grow based on the series.</p>

<p>So I was wondering if you could use the fibonacci series and the golden ratio in any way in AI especially evolutionary algorithm.</p>

<p>Any ideas or insights?</p>

<p>Is this research material? If so where can we start?</p>
"
1719,"<p>This might seem an hypothetical question, but how can we define common sense in an AI agent.</p>

<p>In our lives we meet different people and describe their common sense based on how they act on a situation. For example highly extrovert people are able to deal with people without any awkwardness. For them an action in how to deal with people come as common sense. But in case of scientists, approach to solving a problem maybe common sense which ordinary people cannot see.</p>

<p>So how do we define/tackle this common sense part of an AI agent?</p>
"
1720,"<p>I would like to develop a machine learning algorithm, given two photos, that can decide which image is more ""artistic"".</p>

<p>I am thinking about somehow combining two images, giving it to a CNN, and get an output 0 (the first image is better) or 1 (the second image is better). Do you think this is a valid approach? Or could you suggest an alternative way for this? Also, I don't know how to combine two images.</p>

<p>Thanks!</p>

<p>Edit: Let me correct ""artistic"" as ""artistic according to me"", but it doesn't matter, I am more interested in the architecture. You can even replace ""artistic"" with something objective. Let's say I would like to determine which photo belongs to a more hotter day.</p>
"
1721,"<p>What is the state of the art in models of how the human brain performs goal-directed decision making? Can these models’ principles and insights be applied to the field of Artificial Intelligence, e.g. to develop more robust and general AI algorithms?</p>
"
1722,"<p>A few months ago I made a simple game that is similar to the dinosaur game in Google Chrome - you jump over obstacles, or don't jump over levitating obstacles, and jump to collect bitcoins, which can be placed at 5 different heights. I used a very lightweight NN written by NYU professor Dan Shiffman, and within a few days the game and AI were done, starting off with a population of 200 jumpers, and a genetic algorithm (fitness function (points are given for avoiding obstacles and gathering bitcoins) and mutation), and it worked as it should.</p>

<p>However, this was only when the bitcoins and obstacles were not near each other, which I've been struggling with ever since. </p>

<p>So, I made a ""training ground"" where I put first a levitating obstacle, then a grounded one, and then a bitcoin after it, and then a bitcoin above a fourth grounded obstacle, and no matter how many times and how long I'd leave it to train, I'd always end up with identical behavior:</p>

<p>The first 3 obstacles are properly avoided, the first bitcoin is collected, and then jumpers would jump too early, land before the fourth ""bitcoin"" obstacle, and jump again, always crashing at almost the same place (across all generations, so even if I'd restart the training, they crash at the same place in the obstacle, with a deviation of a few pixels up or down).
I added multilayer support to the NN, no improvements.</p>

<p>Today I replaced the NN with tensorflow.js, and I am getting identical behaviour.</p>

<p>My inputs are:</p>

<ul>
<li>distance to next obstacle  </li>
<li>altitude of next obstacle  </li>
<li>distance to next star  </li>
</ul>

<p>(for simplicity I removed the altitude of stars from the input, and keep them at a constant altitude)</p>

<p>I have 2 hidden layers (5 and 6 neurons), and 1 neuron in the output, which determines if the jumper should jump.</p>

<p>My only idea is that a neuron that decides when to jump because of the obstacle activates alongside the neuron that decides when to jump because of the bitcoin, their weights are summed up and a decision to jump too early is made. </p>

<p>I'll give somewhat of a (maybe bad) analogy: </p>

<p>If it takes you 1 month to prepare an exam, then, if you have 2 exams on the same day, you will start preparing them 2 months earlier. That logic works in this case, but not in my AI.</p>

<p>In the initial ""toy neural network"" I even added 8 layers of 12 neurons each, which I think is overkill for this case.
In tf.js I used both sigmoid and relu activation functions.
No matter what I did, no improvement.</p>

<p>Hope someone has an idea where I'm going wrong.</p>
"
1723,"<p>In a reinforcement learning model, states depend on the previous actions chosen. In the case in which some of the states -but not all- are fully independent of the actions -but still obviously determine the optimal actions-, how could we take these state variables into account?</p>

<p>If the problem was a multiarmed bandit problem (where none of the actions influence the states), the solution would be a contextual multiarmed bandit problem. Though, if we need a ""contextual reinforcement learning problem"", how can we approach it?</p>

<p>I can think of separating a continuous context into steps, and creating a reinforcement learning model for each of these steps. Then, is there any solution where these multiple RL models are used together, where each model is used for prediction and feedback proportionally to the closeness between the actual context and the context assigned to the RL model? Is this even a good approach?</p>
"
1724,"<p>I would like to know if there is a complete text classification with deep learning example, from text file, csv, or other format, to classified output text file, csv, or other. I have seen tens of tutorials and they mostly focus on the model and its performance, but I have not been able to find one that shows how to apply the model to a set of text strings and how to output a document with the classified(labeled) text.</p>
"
1725,"<p>What are the top contributions from neuroscience to artificial intelligence and vice versa? </p>

<p>How much progress has been made from the interaction between these two fields? </p>

<p>Is there a formal field dedicated to the research of topics in the intersection between AI and neuroscience, with papers being published and maybe conferences being held periodically?</p>
"
1726,"<p>Currently in my country, there is a system in which certain groups of researchers upload information on products of scientific interest, such as research articles, books, patents, software, among others. Depending on the number of products, the system assigns a classification to each group, which can be A1, A, B and C, where A1 is the highest classification and C is the minimum. According to the classification of the groups, they can compete to receive monetary incentives to make their research. </p>

<p>At the momen, I am working in an application that takes the data of the system that I mentioned previously. I am able to say what classification the group currently has because we develop a scraper that counts the products and there is another service that is in charge of implementing all the mathematical model that the system has to calculate the category of the group. </p>

<p>But what I want to achieve is that my application would be able to give an estimate of how many products a research group should have to improve its category and i Want to know if I can do that using neural networks.</p>

<p>For example, if there is a category C group, I want the application to tell the user with how many articles and books it would make its category go up to B. </p>

<p>From what I have seen in some web resources, I could insert a training set into the neural network and have it learn to classify the groups, but I think it is unnecessary, because I can do that mathematically.</p>

<p>But I do not understand if it is possible for a neural network to process the current category that the group has and be able to give suggestions of how many products it needs to improve its category.</p>

<p>I think it must be a neural network with several outputs, so that in each one it throws the total for each one of the products, although it is not necessary to list all the products that the measurement model contemplates. But it is necessary for the network to learn which products are handled by a certain group, for example if a group does not write books, avoid suggestions that contemplate the production of books for the improvement of the category that the group has.</p>

<p>If you read the whole problem thanks in advance, I would appreciate any suggestion of what kind of subject I must research or any tutorial. I must say that I am a totally newbie in this subject.</p>
"
1727,"<p>I have a question about output of my SOM network.
I have trained my network with diffrent size, learning rate and epochs, but my output always can recognise two big clusters. Iris-setosa and Iris-virginica, both classes has fitted well in two BMUs. But output of Iris-versicolor is very different.
It contain some other classes BMU but this is not a big problem.<br>
I just want to know is this good or I have some type of bug?</p>

<pre><code>Setosa
0. 1846
1. 1846
2. 1846
3. 1846
4. 1846
5. 1846
6. 1846
7. 1846
8. 1846
9. 1846
10. 1846
11. 1846
12. 1846
13. 1846
14. 1846
15. 1846
16. 1846
17. 1846
18. 1846
19. 1846
20. 1846
21. 1846
22. 1846
23. 1846
24. 1846
25. 1846
26. 1846
27. 1846
28. 1846
29. 1846
30. 1846
31. 1846
32. 1846
33. 1846
34. 1846
35. 1846
36. 1846
37. 1846
38. 1846
39. 1846
40. 1846
41. 1620
42. 1846
43. 1846
44. 1846
45. 1846
46. 1846
47. 1846
48. 1846
49. 1846

Versicolor
50. 652
51. 652
52. 652
53. 1259
54. 696
55. 1394
56. 652
57. 490
58. 696
59. 490
60. 490
61. 1059
62. 1304
63. 696
64. 490
65. 652
66. 1400
67. 490
68. 696
69. 490
70. 652
71. 1574
72. 696
73. 832
74. 696
75. 696
76. 696
77. 652
78. 696
79. 490
80. 490
81. 490
82. 444
83. 696
84. 1129
85. 1084
86. 652
87. 696
88. 25
89. 584
90. 490
91. 789
92. 1034
93. 490
94. 854
95. 29
96. 584
97. 877
98. 490
99. 809

Virginica
100. 652
101. 696
102. 652
103. 652
104. 652
105. 652
106. 877
107. 652
108. 696
109. 652
110. 652
111. 696
112. 652
113. 696
114. 652
115. 652
116. 652
117. 652
118. 652
119. 696
120. 652
121. 652
122. 652
123. 696
124. 652
125. 652
126. 696
127. 652
128. 652
129. 652
130. 652
131. 652
132. 652
133. 696
134. 696
135. 652
136. 652
137. 652
138. 652
139. 652
140. 652
141. 652
142. 696
143. 652
144. 652
145. 652
146. 696
147. 652
148. 652
149. 652
</code></pre>

<p>Thanks in advance. 
ok now i have diagram it looks not bad.
<a href=""https://i.stack.imgur.com/7eqOz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7eqOz.png"" alt=""enter image description here""></a></p>
"
1728,"<p>I'm a professional game developer investigating the potential for using reinforcement learning to build strategy game AI opponents that have more creative behavior compared to traditional techniques like behavior trees. I have a few questions I've bolded below, any thoughts would be helpful, and could save me from pursuing dead ends.</p>

<p>I created a very boring and tiny game as a test case. Two players each control a fleet of ships, each ship has health and can fire on one other ship each turn dealing some damage. The player and his opponent assigns orders to their respective ships, telling them which target to attack, and then the turn is resolved. Ships with 0 health are removed from the board. The player that loses all their ships first loses the game.</p>

<p>Assuming I was using TensorFlow, at a very high level I need to:</p>

<p>A) Create a training program that outputs a trained graph to a file. The training program will need to map gamestates into tensors, feed the tensors through the graph to produce actions, execute actions on the gamestate to generate a new state, and evaluate the reward function for the new state. Repeat a bunch.</p>

<p>B) Take the graph created in #1, load it at the game runtime, and use the graph to generate intelligent actions from real gamestates during the Player vs AI match.</p>

<p>As soon as I started digging into TensorFlow, questions immediately came up, and now I'm not quite sure if there is a more appropriate library to do this.</p>

<p>I) TensorFlow has a High Level python API, and a Low Level C++ API. Most games are built in C++, and thus using a C++ or C API is preferable, it makes integration with the game much simpler. In principle we could use pybind or some other scheme for sending state from C++ to Python and back again, but that's not ideal. 
<strong>Question 1: How much do I lose by using the low level API specifically for reinforcement learning, compared to the high level API?</strong></p>

<p>II) Platforms. 99.9% of the time, PC/Console games are developed in Windows environments, and so having Tensorflow work in Windows is critical. From my googling, Tensorflow just <em>barely</em> supports building in Windows using CMake, though it requires some finagling. More worryingly are other platforms:
<strong>Question 2: What hope is there of running the TensorFlow library on consoles like XboxOne, Playstation 4, or the Switch? I imagine this would require manually porting the entire source :(</strong></p>

<p>III) TensorFlow is big, and it seems you need to basically link all of it to ship with the game.
<strong>Question 3: Is there any way to get a slimmed down ""Runtime TensorFlow"" library that is only capable of loading a graph and transforming states into actions?</strong>
It seems like if the answer was yes, it might also be easier to port this smaller runtime version to more platforms. </p>

<p><strong>Question 4: Should I even be using TensorFlow for this? Is there perhaps something more suitable?</strong></p>

<p>Thanks again if you read all that, I'm eager to start tinkering, but would like to set off in the right direction.</p>
"
1729,"<p>How did you implement your first ever model from a deep learning paper? Please share your experience. It will be useful for beginners looking into getting into implementing code for papers. A step by step process will be beneficial.</p>
"
1730,"<p>As I know, if we consider a 3*3 kernel, we should add a padding of 1px to the source image(if we want to have effect on whole of the image), then we start to put the kernel in upper-left side of the image and multiplying each element of kernel to corresponding pixel on image. Then we sum all the results and put it on the anchor point of kernel(usually center element). Then we should shift the kernel one step to right side and do these things again.</p>

<p>If I am right till here, I have a question about the summation results. I want to know: should we consider the replaced value of image in previously calculated summation and replaced in anchor point in new step of calculation or not?</p>

<p>I mean we must put the anchor point's result in source image and consider it in calculations of shifted kernel? Or we must put it in distance image and we don't consider these results when we shift the kernel on source image(It means don't replace the results on source image for next steps calculations)?</p>
"
1731,"<p>In a paper about the Jack virtual agent architecture <a href=""https://www.researchgate.net/profile/Andrew_Lucas4/publication/3968682_The_automated_wingman_-_Using_JACK_intelligent_agents_for_unmanned_autonomous_vehicles/links/02e7e53b695905ad33000000.pdf"" rel=""nofollow noreferrer"">The Automated Wingman - Using JACK Intelligent Agents for Unmanned Autonomous Vehicles</a> I've found on page 6 a multi-agent BDI system. It is not the description itself, but only the screenshot of the GUI. On top of the GUI there are buttons for starting the agent, and in the main window is the current plan visible. From the history of AI this is sometimes called a blackboard architecture, because many agents can work in parallel on the plan to improve it. To understand the concept of BDI agents and blackboard systems better, my question is: Which elements should a GUI for a BDI-multiagent system have? Are high-quality examples available?</p>
"
1732,"<p>Seems older RNNs have a limitation for their use cases and have been outperformed by other architectures for specific tasks e.g GRUS and CNNs</p>
"
1733,"<p>I watched a youtube clip of Elon Musk talking about his view on the future of AI. He gave two examples. One of the examples was a benign scenario and the other example was a non benign scenario where he speculated the possibilities of future AI threats and what harm a deep intelligence could do. </p>

<p>According to Elon, a deep intelligence in the network could create fake news and spoof email accounts. ""The pen is mightier than the sword"". This non benign scenario put forth by Elon was a hypothetical, but he went into detail about how it could have been possible that an AI, with the goal of maximising the portfolio of stocks, to go long on defence and short on consumer, and start a war. </p>

<p>To be more specific, this could be achieved by hacking into the Malaysians Airlines aircraft routing server, and when the aircraft is over a warzone, send an anonymous tip that there is an enemy aircraft flying overhead which in turn would cause ground to air missiles to take down what was actually a ""commercial"" airliner.</p>

<p>Although this is a plausible hypothetical non benign scenario of AI, I'm wondering if this actually could have been the case regarding the Malaysian Airliner crash. The Stuxnet, for example was a malicious computer worm, first uncovered in 2010. Thought to have been in development since at least 2005 and believed to be responsible for causing substantial damage to Iran's nuclear program. The Stuxnet wasn't even an AI.... </p>

<p>The stuxnet blew the worlds minds when it was discovered. The shear complexity of the worm and the amount of time it took to build was impressive to say the least. </p>

<p>In conclusion, Who would agree that the Malaysian Airliner was a non benign scenario of a deep intelligence in the network?</p>
"
1734,"<p>If both the players want to increase their score (by selecting the highest/best cost path) can this be done using Minimax Algorithm and not other algorithms?</p>
"
1735,"<p>I would really appreciate if someone could comment the following method of training neural nets <strong>providing them with some meta data (Making them more color prone only if needed, whereas now they're mostly silhouette / outline aware). (Comment and especially giving a reference to some papers, preventing me from reinventing the wheel.)</strong></p>

<p>But let's start at the very begining and say that we're performing simple image recognition and each image has 24 bit color depth, simply 1 Byte per each RGB channel. I'm more eager to use usually bigger pics, sacrifing color quality, however <strong>not in all cases</strong> (that statement is crucial in this question).  </p>

<p>To limit the computational burden I'm <strong>NOT keen on using the full information about color (3 Bytes per pixel), but rather shrink it to 1 Byte per pixel</strong> and here is the catch:</p>

<p><strong>I'm reluctant neither to use gray scale nor to cast original tints to single (common among all pics) color palette of 256 hues. So I came up with an idea of reversing the method called debayering or demosaicing image from Color Filter Array data).</strong></p>

<p>To achive it, for every pixel only one color channel is preserved.
Because of the human perception of colors, green component is overrepresented covering 50% of pixels, so 25% left for blue and red.
In this particular example below, the upper left pixel correspond to Blue, followed by Green, then Blue, next Green and so on to the end of the row.
The second uppermost row starts with Green, afterwards Red, one more Green, Red and also repeating till the end of line. This horizontal patterns are altering with the parity of the row number, which is nicely depiced on <a href=""https://en.wikipedia.org/wiki/Bayer_filter"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/Bayer_filter</a> from where I've got following graphics:</p>

<p><a href=""https://i.stack.imgur.com/wKRfH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wKRfH.png"" alt=""enter image description here""></a></p>

<p>To better illustrate this method I'm using the thumbnail of famous Mona Lisa painting with its grayscale version next to it. (By the way, it isn't in my training set, but is familiar to everyone).</p>

<p><a href=""https://i.stack.imgur.com/NCDir.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NCDir.jpg"" alt=""Original""></a><a href=""https://i.stack.imgur.com/Sq3l1.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sq3l1.jpg"" alt=""Original B&amp;W""></a></p>

<p>A greenish leftmost image is the result of applying the reverse debayering / demosaicing CFA method. This picture consist of pixels that are either Blue or Red or Green with different brightness level. In the browser window that could be poorly visible, however if you download this image and magnify it substantially, the patter would be revealed.</p>

<p>Let's say that in the original picture one can find a small square of 2x2 pixels, all of them representing a light skintone 0xF4D374 (in hex). In this grouping, 2 green pixels would be chopped into green channel and will get a value of 0xD3, the blue-related will get a value of 0x74 and the remaining red would get 0xF4.
In the leftmost image below the corresponding pixels were presented by hex colors: 0x00D300, 0x000074 and 0xF40000 respectively, whereas in the right picture exactly the same values (0xD3, 0x74, 0xF4) were shown in grayscal (of 256 possible shades). </p>

<p><a href=""https://i.stack.imgur.com/qCH96.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qCH96.jpg"" alt=""CFA applied""></a><a href=""https://i.stack.imgur.com/9RMtd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9RMtd.jpg"" alt=""CFA applied plus B&amp;W""></a></p>

<p>After this color-flattening, our input batch has shrinked by two-thirds and at the same time original colors can be more-or-less restored (of course not lossless, but well enough).</p>

<p>However, I don't suppose that anyone had a problem with recognising this picture after transformation. Likewise, all my models could be well trained to recognize outline/silhouette of the object, but they require a way lot more training data (at least one-two orders of magnitude) to be color-aware.</p>

<p><strong>The ultimate question is, how to design models that would threat shape and colors in similar manner. Maybe that would be not in 100% mathematically proper, but shape and color must be orthogonal.</strong></p>

<p><strong>Nevertheless, I don't want to always decode the color, but only if its needed - in ealier epoch it learned sillhouettes/shapes and that there're many similar objects in this regard, so in the next epoch it should pay also / more attention to tints.</strong>   </p>

<p><strong>Have you encountered articles about such method of using color if object demarcation / labelling process cannot be based only on shape? I would be really gratefull for any paper or other reference.</strong></p>

<p>I'm rather newbie to neural nets, so sorry if this is something widely known to everyone but me ;-)</p>

<p>Thanks in advance for any hint.</p>
"
1736,"<p>In some implementations of off-policy Q learning we need to know the action probabilities given by the behavior policy <code>mu(a)</code> (e.g., if we want to use importance sampling).</p>

<p>In my case, I am using Deep <a href=""https://en.wikipedia.org/wiki/Q-learning"" rel=""nofollow noreferrer"">Q-Learning</a> and selecting actions using <a href=""https://en.wikipedia.org/wiki/Thompson_sampling"" rel=""nofollow noreferrer"">Thompson Sampling</a>. I implemented this following the approach in <a href=""http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html"" rel=""nofollow noreferrer"">""What My Deep Model Doesn't Know...""</a>: I added dropout to my Q-network and select actions by performing a single stochastic forward pass through the Q-network (i.e., with dropout enabled) and choosing the action with the highest Q-value.</p>

<p>So, how can I calculate <code>mu(a)</code> when using Thompson Sampling based on dropout?</p>
"
1737,"<p>I read Judea Pearl's <a href=""https://rads.stackoverflow.com/amzn/click/046509760X"" rel=""noreferrer"">The Book of Why</a>, in which he mentions that deep learning is just a glorified curve fitting technology, and will not be able to produce human-like intelligence.</p>

<p>From his book there is this diagram that illustrates the three levels of cognitive abilities: </p>

<p><a href=""https://i.stack.imgur.com/vxTLx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vxTLx.png"" alt=""Three levels of cognitive abilities""></a></p>

<p>The idea is that the ""intelligence"" produced by current deep learning technology is only at the level of association. Thus the AI is nowhere near the level of asking questions like ""how can I make Y happen"" (intervention) and ""What if I have acted differently, will X still occur?"" (counterfactuals), and it's highly unlikely that curve fitting techniques can ever bring us closer to a higher level of cognitive ability.</p>

<p>I found his argument persuasive on an intuitive level, but I'm unable to find any physical or mathematical laws that can either bolster or cast doubt on this argument.</p>

<p>So, is there any  scientific/physical/chemical/biological/mathematical argument that prevents deep learning from ever producing strong AI (human-like intelligence)?</p>
"
1738,"<p>As you know OpenCV is a big great open-source library for image recognition and machine vision(and may further purposes like computer graphics, etc).</p>

<p>Is there similar library in sound field(Voice recognition/ NLP(Natural Language Processing))? </p>

<p>I know espeak for TTS, also pocketsphinx for voice recognition. Also there is something like ChatScript that I don't know if I can consider as a NLP engine or not? But I like to know did I mentioned the Best libraries for each part of sound/voice field or there are better options to learn and work with them?</p>

<p>Also will happy to hear some suggestions about best book(s) to read to learn the concepts/algorithms of ASR/NLP.</p>
"
1739,"<p>I'm trying to detect the <strong>visual attention</strong> area in a given image and crop the image into that area. For instance, given an image of any size and a rectangle of say LxW dimension as an input, I would like to crop the image to the most important visual attention area. I'm looking for a state of the art approach for that.</p>

<p>Do we have any tools or SDK to implement that? Any piece of code or algorithm would really help.</p>
"
1740,"<p>According to this <a href=""https://www.engadget.com/2018/06/14/microsoft-ai-windows-10-updates-smoother/"" rel=""nofollow noreferrer"">news</a>, Microsoft is using AI to make Windows 10 updates smoother. So I was curious and went further to search and came across this <a href=""https://blogs.windows.com/windowsexperience/2018/06/14/ai-powers-windows-10-april-2018-update-rollout/"" rel=""nofollow noreferrer"">website</a>, which describes:</p>

<blockquote>
  <p>Artificial Intelligence (AI) continues to be a key area of investment
  for Microsoft, and we’re pleased to announce that for the first time
  we’ve leveraged AI at scale to greatly improve the quality and
  reliability of the Windows 10 April 2018 Update rollout.  <strong>Our AI
  approach intelligently selects devices that our feedback data indicate
  would have a great update experience and offers the April 2018 Update
  to these devices first.</strong>  As our rollout progresses, we continuously
  collect update experience data and retrain our models to learn which
  devices will have a positive update experience, and where we may need
  to wait until we have higher confidence in a great experience.  Our
  overall rollout objective is for a safe and reliable update, which
  means we only go as fast as is safe.</p>
  
  <p>Our AI/Machine Learning approach started with a pilot program during
  the Windows 10 Fall Creators Update rollout.  <strong>We studied
  characteristics of devices that data indicated had a great update
  experience and trained our model to spot and target those devices.</strong>  In
  our limited trial during the Fall Creators Update rollout, we
  consistently saw a higher rate of positive update experiences for
  devices identified using the AI model, with <strong>fewer rollbacks,
  uninstalls, reliability issues, and negative user feedback</strong>. For the
  April 2018 Update rollout, we substantially expanded the scale of AI
  by developing <strong>a robust AI machine learning model to teach the system
  how to identify the best target devices based on our extensive
  listening systems.</strong></p>
</blockquote>

<p>To me, it sounds like simple if-else statements would have implemented the whole thing without touching the AI; they mentioned that positive experiences include fewer rollbacks, uninstalls, and so on, so we may use these as a criterion of a positive experience.</p>

<p>I am just wondering if the word 'AI' is being misused, or can be misleading in this context? Could anyone point me out on this or give any insight on how AI can be used in this context? In my experience, I have only seen AI mostly being used in speech recognition, image recognition and other sort-of classifying problems, with a training and consequently a computer can ""learn"" from the data, not like an if-else statement. Today, AI seems to be everything that is considered ""smart""?</p>
"
1741,"<p>I understand how Neural Networks work and have studied its theory well. My question is at the intricacies of Deep Neural networks and perhaps is a bit beyond common understanding (as I have been told (or misled) from discussions).
My question is: On the whole, is there a clear understanding of how mutation occurs within a neural network from the input layer to the output layer, for both supervised and unsupervised cases? 
Any neural network is a set of neurons + the connection weights. With each successive layer, there is a change in the input. Say I have a Neural Network that does movie recommendation on n parameters. 
Say if X is a parameter that stands for the movie rating on IMDB. In each successive stage, there is a mutation of input X to X' and further X'' and so on. 
While of course, we know how to mathematically talk about X' and X'', do we at all have a conceptual understanding as to what this variable is in its corresponding neural n-dimension? The neural weights which to the human eye might be set of random numbers but may mean something profound if we could ever understand what the neural weights 'represent'. </p>

<p><a href=""https://i.stack.imgur.com/JFcKa.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JFcKa.jpg"" alt=""neural net""></a></p>

<p>What is the nature of neural weights such that despite decades worth of research and use, there is no clear understanding of what these connection weights represent? Or rather, why has there been so little effort in understanding the nature of neural weights, in a non-mathematical sense given the huge impetus in going beyond the black box notion of AI.  </p>
"
1742,"<p>Look at Breakout:
<a href=""https://i.stack.imgur.com/0tNtP.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0tNtP.gif"" alt=""enter image description here""></a></p>

<p>We know that the underlying world behaves like an MDP, because for the evolution of the system it just need to know which is the current state, i.e. position, speed and speed direction of the ball, positions of the bricks and the paddle etc. But considering only single frames as the state space we have a POMDP because we lack informations about the dynamics [1], [2].</p>

<p>Question:
What could happen if we wrongly assume that the POMDP is a MDP and do reinforcement learning with this assumption over the MDP?</p>

<p>Obviously the question is more general, not limited to Breakout and Atari games.</p>

<ul>
<li>[1] <a href=""https://arxiv.org/pdf/1507.06527.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1507.06527.pdf</a></li>
<li>[2] <a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"" rel=""nofollow noreferrer"">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a></li>
</ul>
"
1743,"<p>The German research initiative “Sonderforschungsbereich 588” was active from 2001-2012 and has developed a humanoid robot, called ARMAR. According to the old pictures, the first version ARMAR I was not very highly developed, while the latest iteration (ARMAR IV) was a near humanoid robot with arms, legs and a software defined cognitive architecture.</p>

<p>How many researchers were involved in the project? Thousands of programmers, researchers and external companies?  No, according to the fact sheet the total number of employees was only 30 (thirty).</p>

<p>My question is: If it is possible to build and program a humanoid kitchen robot with such a small team, what would be possible with many more researchers?</p>
"
1744,"<p>Oxford philosopher and leading AI thinker Nick Bostrom defines <em>SuperIntelligence</em> as </p>

<blockquote>
  <p>""An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.""</p>
</blockquote>

<p><a href=""http://tcrn.ch/2gJR03q"" rel=""nofollow noreferrer"">Here is an interesting article, you may like <em>Tech Crunch</em>..  </a></p>

<p><a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">Artificial general intelligence : Wiki</a></p>

<p>Taking into account current limitations and the amount of progress that has been made in recent years, what is a realistic timeframe to expect an AI that has human levels of cognition? </p>
"
1745,"<p>I am currently exploring multi-agent reinforcement learning. I have multiple agents that communicate with each other and a central service that maintains the environment state.</p>

<p>The central service dispatches some information at regular intervals to all the agents (Lets call this information as <strong>energy</strong>). The information can be very different for all the agents. </p>

<p>The agents on reception of this information select a particular action. The execution of the action should leave the agent as well as the environment in a positive state. The action requires a limited amount of energy which might change on every timestep. If a agent does not have sufficient energy to it may request for energy from other agents. The other agents may grant or deny this request.</p>

<p>If all the agents are able to successfully perform their actions and leave the environment in a positive state they get a positive reward.</p>

<p>As the environment is stochastic, where a agent's behavior is dependent on another agent can approximate Q Learning be used here?</p>
"
1746,"<p>I have been working with AI methods. I am thinking about how my daughter (and also other kids) could learn mathematics with the help of AI. For example, how could an AI be used to show the mistakes that a kid does during the learning path?</p>
"
1747,"<h2>Premise</h2>

<p>Ok, I know that this question was asked before on <a href=""https://ai.stackexchange.com/questions/4320/why-initial-weights-in-neural-network-are-randomised"">ai.SE</a>, on <a href=""https://stats.stackexchange.com/questions/45087/why-doesnt-backpropagation-work-when-you-initialize-the-weights-the-same-value"">stats.SE</a> and also on <a href=""https://stackoverflow.com/questions/20027598/why-should-weights-of-neural-networks-be-initialized-to-random-numbers"">SO</a>. So I did my homework in checking before posting my question, but none of them has an answer that fully satisfies me.</p>

<h2>Summary of my knowledge up to now</h2>

<p>Suppose you have a layer that is fully connected, and that each neuron performs an operation like</p>

<pre><code>a = g(w^T * x + b)
</code></pre>

<p>were <code>a</code> is the output of the neuron, <code>x</code> the input, <code>g</code> our generic activation function, and finally <code>w</code> and <code>b</code> our parameters. </p>

<p>If both <code>w</code> and <code>b</code> are initialized with all elements equal to each other, then <code>a</code> is equal for each unit of that layer. </p>

<p>This means that we have symmetry, thus at each iteration of whichever algorithm we choose to update our parameters, they will update in the same way, thus there is no need for multiple units since they all behave as a single one. </p>

<p>In order to break the symmetry we could randomly initialize the matrix <code>w</code> and initialize <code>b</code> to zero (this is the setup that I've seen more often). 
This way <code>a</code> is different for each unit so that all neurons behave differently.</p>

<p>Of course randomly initializing both <code>w</code> and <code>b</code> would be also okay even if not necessary.</p>

<h2>The actual question</h2>

<p>Is randomly initializing <code>w</code> the only choice? 
Could we randomly initialize <code>b</code> instead of <code>w</code> in order to break the symmetry?
Is the answer dependent on the choice of the activation function and/or the cost function?</p>

<p>My thinking is that we could break the symmetry by randomly initializing <code>b</code>, since in this way <code>a</code> would be different for each unit and, since in the backward propagation the derivatives of both <code>w</code> and <code>b</code> depend on <code>a</code>(at least this should be true for all the activation functions that I have seen so far), each unit would behave differently. Obviously, this is only a thought, and I'm not sure that is absolutely true.  </p>

<h2>Remark</h2>

<p>Note that this is a theoretical question, not a practical one. 
This means that I'm not really interested in answers that involves performance, but you're welcome to include them as corollary to the real answer.</p>
"
1748,"<p>So I built a CNN without any scientific libraries like <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">TensorFlow</a> or <a href=""https://keras.io/"" rel=""nofollow noreferrer"">Keras</a> (only <a href=""http://www.numpy.org/"" rel=""nofollow noreferrer"">NumPy</a>). It is taking a huge amount of time to train. What are some of the tricks and tips followed by people to speed up training of a CNN? (I am not talking about division of jobs into different processors but subtle redundant codes i.e. giving pre-calculated results which is not visible to common programmers).</p>
"
1749,"<p><a href=""https://arxiv.org/abs/1511.07528"" rel=""noreferrer"">""<em>The Limitations of Deep Learning in Adversarial Settings</em>""</a> (<a href=""https://arxiv.org/pdf/1511.07528.pdf"" rel=""noreferrer"">PDF</a>) explores how neural networks might be corrupted by an attacker who can manipulate the data set that the neural network trains with.  The authors experiment with a neural network meant to read handwritten digits, undermining its reading ability by distorting the samples of handwritten digits that the neural network is trained with.</p>

<p>I'm concerned that malicious actors might try hacking AI, e.g.:</p>

<ul>
<li><p>Fooling autonomous vehicles to misinterpret stop signs vs. speed limit.</p></li>
<li><p>Bypassing facial recognition, such as the ones for ATM.</p></li>
<li><p>Bypassing spam filters.</p></li>
<li><p>Fooling sentiment analysis of movie reviews, hotels, etc,.</p></li>
<li><p>Bypassing anomaly detection engines.</p></li>
<li><p>Faking voice commands.</p></li>
<li><p>Misclassifying machine learning based-medical predictions.</p></li>
</ul>

<blockquote>
  <p>What adversarial effect that could disrupt to the world? How we can prevent  it?</p>
</blockquote>
"
1750,"<p>I have this table, 2 agents and I want to find for each agent if any action is strongly or weakly dominated. This is the table:</p>

<p><a href=""https://i.stack.imgur.com/Ax7b0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ax7b0.png"" alt=""enter image description here""></a></p>

<p>Now, i've found a solution but I'm not sure if it's correct. So for let's say agent 1(the one handling rows): 1&lt;2&lt;2, 1&lt;3&lt;4 and 0&lt;3&lt;4 so I don't have a strong dominance.</p>

<p>For agent 2: 1&lt;=1&lt;=1, 2&lt;5&lt;6 and 2&lt;3&lt;4 which means that I don't have strong dominance here too.</p>

<p>Is my logic correct?</p>
"
1751,"<p>I have recently got into AI and I am eager to learn about its concepts. In some of the information I saw about AI, there was a lot about neural networks. Neural networks seem to be <em>(something along the lines of)</em> a type of algorithm that creates a <a href=""https://stackoverflow.com/tags/graph/info"" title=""A complex data structure, used for showing connections between variables. Link is to the info page for the graph tag, over on stack overflow."">graph</a> which works based on a theory about how neurons interact, in order to create self-learning programs.</p>

<p>I'd like more information about this theory and model, as there is a lot that I don't understand.</p>

<h2>The graph:</h2>

<p>Firstly, the diagram I keep seeing (I'm assuming it's a graph). It shows a set of nodes <em>(which is apparently the input)</em>, each directed to each of another set of nodes, each of which is then directed to each of another set of nodes, etc. until it reaches what is apparently the output.</p>

<p>How can something like an image, or a complex piece of data be represented by those input nodes? What goes on at each set of nodes? Does every node from each set have to always be connected to every node from the next set? Do they always have to be directed to the next set, or can they go back and fourth as well? Can I have some code given in relation to one of these diagrams?</p>

<p><em>(A node is a variable shown in a graph.)</em></p>

<h2>AI progress:</h2>

<p>Secondly, I read that these sort of algorithms could eventually create a conscious program if ""enough neurons work together"". I do look forward to when people start to manage to do this, but it seems like no one is trying to rethink/expand-on that theory as much as they should. I'd expect people to try to look for new human behaviours to represent with AI. For example: a new-born would take their first breath despite the pain, while usually the mind tries to avoid pain. This causes crying.</p>

<p>Has anyone tried to mimic this sort of behaviour scenario (or any behaviour scenario) in a program? If not, why not? How close are we to creating a conscious mind in a program? Are people challenging the current theory that created our neural network model? Is it lightly for <strong>this community</strong> to propose a better theory and model?</p>
"
1752,"<p>How does the legal question about agents talking to humans via telephone connection work? Recently Google gave a talk about Duplex, where an agent makes a call to a human to schedule a hairdresser.</p>

<p>I wonder if there are any regulations related to this type of scenario, if there are some limitations, if the human needs to know that he is talking to an AI.</p>
"
1753,"<p>I have a corpus, say an instruction manual. The text in this manual is grouped into chapters and each chapter is split up into sections. For example, Chapter 1/Section 1, Chapter 1/Section 2 and so on.
Assume the corpus has C chapters and each chapter has S sections. My goal is, given a sentence or question, to classify this sentence/question. In other words I want to compute three most probable chapters to which this sentence or question belongs to. 
I tried MultinomialNB model using sklealrn, but it did not give me the desired result. I want to try another approach, for example using a Neural Network and compare it with the MultinomialNB model. I have Googled and found Doc2Vec but haven't tried yet. </p>

<p>Can anyone suggest a better or another possible approach so that I could try and compare? What is the standard approach to such kind of problem? </p>
"
1754,"<p>I was trying to implement NEAT but got stuck at the speciating of my clients/genomes.</p>

<p>What I got so far is:</p>

<ol>
<li>the distance function implemented</li>
<li>each genome can mutate nodes/connections.</li>
<li>2 genomes can give ""birth"" to a new genome.</li>
</ol>

<p>I've read a few papers but none does explicitly explain in what order what step is done.</p>

<p>I know that for each generation, all the similar genomes will be put together into one species. </p>

<blockquote>
  <p>But what happens to them in the next generation?</p>
  
  <p>How do I start the training? What happens in generation #1?</p>
  
  <p>Who is being killed / reborn?</p>
  
  <p>Who is being mutated and at what point?</p>
</blockquote>

<p>I know that these are a lot of questions but I would be very happy if someone could help me :)</p>

<p>Greetings,
Finn</p>
"
1755,"<ul>
<li><p>How does sequential DQN work? </p></li>
<li><p>How would one construct the simple sequential DQN?</p></li>
</ul>

<p><a href=""https://blog.openai.com/openai-baselines-dqn/"" rel=""nofollow noreferrer"">OpenAI Baselines: DQN</a></p>
"
1756,"<p>As is done traditionally, I used k-fold cross validation to select and optimize the hyper parameters of my neural network classifier. When it was time to store the final model for future predictions, I discovered that using the weights from the previous k-fold cv iteration to seed the initial weights of the model in subsequent iteration, helps in improving the accuracy (seems obvious). I can use the model from the final iteration to perform future predictions on unseen data.</p>

<ul>
<li>Would this approach result in overfitting? </li>
</ul>

<p>(Please note, I am using all available data in this process and I do not have any holdout data for validation.)</p>
"
1757,"<p>I am reading the Simon Haykin's cornerstone book, ""Neural Networks, A Comprehensive Foundation, Second Edition"" and I cannot understand a paragraph below: </p>

<blockquote>
  <p>The analysis of the dynamic behaviour of neural networks involving the
  application of feedback is unfortunately complicated by virute (or
  virtue I cannot get word appropriately) of the fact that the
  processing units used for the construction of the network are usually
  nonlinear. Further consideration of this issue is deferred to the
  latter part of the book.</p>
</blockquote>

<p>Before the paragraph, the author analysis the affects of weight of synapsis to the neural network's stability. Roughly speaking, he says, if |w| >= 1 the neural network become unstable.</p>

<p>Could you please explain the paragraph? Thanks in advance. </p>
"
1758,"<p>Can anyone recommend a reinforcement learning algorithm for a multi-agent environment?</p>

<p>In my simplified example, I'm implementing a Q-Learning system with different 10 agents. The agents compete for resources in stores at different locations by setting a bid price for each item. </p>

<p>All of the agents have different bids and pooled budget of $100. Once the budget is reached the agents cannot buy any more that day.</p>

<p>Each agent will receive a reward if they buy an item. The goal would be to maximize the total amount of items bought between the agents.</p>

<p>Right now the agents don't communicate. </p>

<p>Can someone point me in the right direction for an algorithm that allows agent cooperation?</p>
"
1759,"<p>In the instance like <a href=""https://deepmind.com/blog/alphago-zero-learning-scratch/"" rel=""nofollow noreferrer"">AlphaGo Zero</a>. </p>

<blockquote>
  <ul>
  <li><p>How N' why AlphaGo Zero's training is so stable? Compared with traditional game theory applied in Deep RN technique!?</p></li>
  <li><p>How ""AlphaGo Zero"" differs from ""AlphaGo""? </p></li>
  </ul>
</blockquote>

<p><strong><em>How basically AI synthesize thinking!? How it differs Comparing to our human cognitive ability</em></strong> </p>
"
1760,"<p>So recently I have been learning about new NN's which are used for specialised purposes like speech recognition, image recognition, etc. The more I discover the more I get amazed by the cleverness behind models such as RNN's and CNN's. Questions about the working, intuition, mathematics have been asked a lot in this community, all with vague answers and apparent understandings.</p>

<p>So my question is that, did the researchers come up with these specialised models accidentally or did they follow particular steps to get to the model (like in a mathematical framework)? And how did they look at a particular class of problem and think ""Yeah, a better solution might exist""? For me since understanding of NN's are so vague, these are 'high risk, high reward' scenarios, since you might be chasing only the mirage (illusion) of a solution.</p>
"
1761,"<p>I'm eventually looking to build an algorithm that will process answers from humans that are given questions. But first I have to setup an experiment to determine the variety of responses.</p>

<p>Specifically, humans will be asked a multiple choice question that has a single correct answer. I want to understand what kinds/ranges of responses I would get from the bell curve distribution of human intelligence. </p>

<p>Is there any way I can have, say, 1000 ""humans"" be asked a prompt, repeated 100 times (the same question) and then compile the responses? My concern is that I'll have to build some algorithm or process for each dumb, average, smart ""human"" to follow but then I would introduce bias in how smart they are or limit how they may respond. I'm guessing I'll have to give them a data sort to work from.</p>

<p>To clarify, it's not the number of times a single user gets a question right that makes them smart, they have to be programmed dumb, smart etc. before the simulation starts. So dumb users could get some right and smart can get some wrong. </p>

<p>I'm not sure the Monte Carlo method is useful here but some type of simulation where I can specify the distribution (normal) and then bound the responses would be helpful.</p>

<p>I have access to Excel, Minitab, and Python. Any ideas how to set up an experiment like this? I really am open to any technique to measure this.</p>
"
1762,"<p>I thought I have implemented the code (from scratch, no library) for an <strong>artificial neural network</strong> (first endeavour in the field). But I feel like I miss something very basic or obvious. </p>

<p>To make it short: code works for a single pair of in-/out-values but fails for sets of value pairs. I do not really understand the training process. So I want to get this issue out of the way first. The following is my improvised training (aka all that I can think of) in pseudocode.</p>

<pre><code>trainingData = [{in: [0,0], out:[0]}, {in: [0,1], out:[0]}, ...];
iterations = 10000

network = graphNodesToNetwork()
links = graphLinksToNetwork()
randomiseLinkWeights(links)


while(trainingData not empty) {
  for(0&lt;iterations) { 
     set = trainingData.pop()

     updateInput(network, set.in)

     forwardPropagate(network, links) 

     linkUpdate = backPropagate(network, links, set.out)

     updateLinks(linkUpdate, links)}
}
</code></pre>

<p>Is this how it is supposed to work? Do you feed in your training data set by set (while-loop)? </p>

<p><em>Edit 1: because my final comment did distract from the issue at hand.</em></p>

<p><em>Edit 2: less wordy, more code-y</em></p>
"
1763,"<p>Comparing to a monkey with a keyboard creating Shakespeare a.k.a <a href=""https://en.wikipedia.org/wiki/Infinite_monkey_theorem"" rel=""nofollow noreferrer"">Infinity Monkey Theorem</a>, We now have <em>AutoML</em> : <a href=""https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html"" rel=""nofollow noreferrer"">A machine learning software that can create self-learning code.</a> <a href=""https://en.wikipedia.org/wiki/Automated_machine_learning"" rel=""nofollow noreferrer"">| Wiki</a></p>

<p>Are we near to <strong><em>Singularity</em></strong> (self-aware machines) ?</p>
"
1764,"<p>I've been oogling the Mac Pro from Apple with loaded specs. <a href=""https://www.apple.com/mac-pro/"" rel=""nofollow noreferrer"">Check it here if unfamiliar.</a> </p>

<p>I'm curious to hear anyone's thoughts of the computer for deep learning/machine learning applications vs cloud based computing. Obviously, a flexible solution on GCP or AWS has the ability to scale and therefore produce results faster. However, <strong>I'm mostly interested to hear thoughts on the economics of purchasing the machine vs renting a cloud based machine.</strong> </p>

<p>If we assumed something like a 5 year lifetime of the Apple Mac Pro for this type of computing (before the chips/processors become immensely slower than the newest chips used in the cloud), that would be like $1000 per year amortized using the Mac Pro. Does it make more sense to rent from AWS/GCP for \$1000 per year as a renter instead? I don't have much experience in the cost of AWS/GCP so I'm looking to here an answer from anyone well versed in cloud computing.</p>
"
1765,"<p>I'm new to machine learning, so i figured I should look into google's tensor flow guides and I know how to code in JS so that's why I'm using tensorflow.js, there's and example in the guide that trains itslef to recognize handwritten numbers from the  MNIST handwriting dataset, I sort of understand what's going on in the code but since I'm very new to ML it's not a lot, I went through the code and saw that it didn't took image by image to train itself but it requests one sprite which contains all the images and then cuts it into what it needs, this makes sense from a performance point of view, but as this process is kind of abstract I don't understand what's really going on, I want to upload an image of my own and call the predictor of the model but I don't know how to do it, any help? </p>

<p>I was thinking that drawing in a canvas of 28x28 a number might be very interesting as well instead of uploading an image, but I need to know how to test the model once it's trained with my own data.</p>

<p>The tutorial: <a href=""https://js.tensorflow.org/tutorials/mnist.html"" rel=""nofollow noreferrer"">https://js.tensorflow.org/tutorials/mnist.html</a></p>
"
1766,"<p>Why is it that the skewed contour (unscaled features) will result in slow performance of gradient descent? In other words, how (or why) will the gradients end up taking a long time before finding the global minimum in such cases? This might be an obvious question but I'm finding a hard time Visualizing the 3D shapes of the respective contours and relating it to the convergence.
<a href=""https://i.stack.imgur.com/6VjAu.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6VjAu.gif"" alt="" ""></a></p>

<p><code>Left one is the contour for the unscaled feature and the right one is scaled (and will apparently converge quickly).</code></p>
"
1767,"<p>Say I have an application where the frequency of the input is known but can vary widely across sequences. For example, they may be audio recordings acquired at different frequency, or videos that come from surveillance cameras whose framerate can vary from 24fps down to as low as 1fps.</p>

<p>The straightworard thing to do would be to either</p>

<ul>
<li>resample inputs to a constant frequency</li>
<li>ignore input frequency and hope the RNN will figure it all out</li>
</ul>

<p>None sound very appealing. Is there a better way to handle variable input frequency in RNNs?</p>
"
1768,"<p>In a recent paper <a href=""https://arxiv.org/abs/1805.08296"" rel=""nofollow noreferrer"">Data-Efficient Hierarchical Reinforcement Learning, O Nachum, S Gu, H Lee, S Levine, 2018</a>, a promising agent controlling technique called Hierarchical Reinforcement Learning was introduced. It is some kind of layered policy for controlling ant-like robots in a maze.</p>

<p>For example, the main controller is able to run sub-controllers <code>move</code> and <code>push</code>, and this allows the ant to move to a goal, even an obstacle is on the way.</p>

<p>But there is something in the paper which i didn't understand: How to find the goals. According to the paper, the lowlevel actions “move” and “push” are equal to goals. And these goals have to be inferred from demonstrations. In the paper, they write:</p>

<blockquote>
  <p>For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers.</p>
</blockquote>

<p>How exactly is the matching between the observation and the goal state done?</p>
"
1769,"<p>I've selected more than 10 discriminative (Classification) models, each wrapped with a BaggingClassifier object, optimized with a GridSearchCV, and all of them placed within a VotingClassifier object.</p>

<p>Alone they all bring around 70% accuracy, on a data set which is about half normal/uniform distributed, and half one-hot distributed. Together they provide 80% accuracy, which isn't good enough, to the 95%&lt; needed.</p>

<p>The models: DecisionTreeClassifier, ExtraTreesClassifier, KNeighborsClassifier, GradientBoostingClassifier, LogisticRegression, SVC, Perceptron, and a few more classifiers.</p>

<p>How do I check if the combination is good?</p>
"
1770,"<p><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3997295/"" rel=""noreferrer"">The endocannabinoid system is a very important function of human biology</a>. Unfortunately, due to the illegality of cannabis, it is a relatively new field of study. I have read a few articles about <a href=""https://medium.com/@skychain.global/googles-deepmind-is-using-ai-to-explore-dopamine-s-role-in-learning-d07e7521b16b"" rel=""noreferrer"">Google researching the role of dopamine in learning</a>, and <a href=""http://reset.me/story/anandamide-putting-the-bliss-molecule-to-work-for-your-brain/"" rel=""noreferrer"">according to this article</a>, <strong>anandamide</strong> (the neurotransmitter that closely resembles tetrahydrocannabinol):</p>

<blockquote>
  <p>...was found to do a lot more than produce a state of heightened happiness. It’s synthesized in areas of the brain that are important in memory, motivation, higher thought processes, and movement control.</p>
</blockquote>

<p>Have any neuroscientists (or any scientists) considered the importance of the endocannabinoid system for cognitive function?</p>

<p>If not, is there any reason this information might or might not be relevant to artificial intelligence?</p>
"
1771,"<p>I have a database with hundreds of questions and answers. Would you like to know how I can work on this data in Google Cloud?</p>

<p>I have a social network where I have these questions and answers, and I would like to use the machine learning features and other Google Cloud tools.</p>

<p>Does anyone know the step by step?</p>
"
1772,"<p>I am a deep learning beginner recently reading this book ""Deep learning with Python"", the example explains the process of implementing a greyscale image classification using MNIST in keras, in the compilation step, it said,</p>

<blockquote>
  <p>Before training, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Previously, our training images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.</p>
</blockquote>

<p>Images stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. For my understanding, the values are between 0-255 of each px and storied as 3D matrix. Can someone explain why needs to ""transform"" it into the network expects by scaling it and make ""all values are in the [0, 1]interval.""?</p>

<p>Please also make suggestions if I didn't explain some parts correctly.</p>
"
1773,"<p>In a final project in diagnosing ADHD using Machine Learning we obtained parameters from real patients. We used this data and got much higher success rates in LDA than in SVM and Naive Base, we had only 100 examples in our training set. We are wondering why LDA specifically succeeded much more than the others? </p>
"
1774,"<p>I have an image dataset where objects may belong to one of the hundred thousand classes. I want to know what kind of neural network architecture should I use in order to achieve this.</p>
"
1775,"<p>I'm starting to play around with python neural networks, mainly for object recognition like this one: <a href=""https://github.com/huangshiyu13/RPNplus"" rel=""nofollow noreferrer"">https://github.com/huangshiyu13/RPNplus</a>. I plan to train more networks with more kinds of objects.</p>

<p>So far, this network seems to take a long time (~42hrs and counting) for training or maybe my CPU (16GB RAM, Intel i5) its not enough.</p>

<p>A friend of mine is selling a GPU NVidia GeForce 760 with 4GB RAM and I wonder if it is a good opportunity for me or if really a GPU is not a real advantage for NN, as I read <a href=""https://ai.stackexchange.com/questions/4696/what-size-of-neural-networks-can-be-trained-on-current-consumer-grade-gpus-106"">in this other post</a>.</p>

<p>What is your opinion? Is a GPU a good investment or not really? I've seen other GPU like ASUS with 2 or 4 GB for around 200 US$, does this NVidia make a big difference or any other common GPU will do the same work? (thinking in how much I should pay for this).</p>
"
1776,"<blockquote>
  <p>Is lacking the ability of <strong>rational behavioral decisions</strong> of humans makes a huge difference in AI agents uncertainty problems?</p>
</blockquote>

<p>(As much as I look into what’s being done with deep learning, I see they’re all stuck there on the level of associations,. Curve fitting.  I’m very impressed, because we did not expect that so many problems could be solved by pure curve fitting. It turns out they can. But I’m asking about the future — what next? - <a href=""https://amturing.acm.org/award_winners/pearl_2658896.cfm"" rel=""nofollow noreferrer"">Judea Pearl</a> Turing Award Winner and the Author of <a href=""https://www.basicbooks.com/titles/judea-pearl/the-book-of-why/9780465097609/"" rel=""nofollow noreferrer"">The Book of Why The New Science of Cause and Effect </a>) </p>

<p>Consider rational behavior like <strong>Free will</strong> i.e., The ability to choose, think, and act voluntarily., </p>

<blockquote>
  <p>How Artificial Intelligence could mimic Human Intelligence(like rational behavioral decisions) better than human does?(as we generally intend AI to outperform humans in several fields)</p>
</blockquote>

<p>[opinion] If possibly AI really could outperform humans in taking rational behavioral decisions then, <em>I'm sure AI can find pitfalls from the human behaviors</em>, like telling climate change is real.etc., and <em>I'm sure AI can come up with some moral decisions that could help unite humanity and with other life forms.</em></p>
"
1777,"<p>The problem of adversarial examples is <a href=""https://arxiv.org/pdf/1312.6199.pdf"" rel=""nofollow noreferrer"">known</a> to be critical for neural networks. For example, an image classifier can be manipulated by additively superimposing a different low amplitude image to each of many training examples that looks like noise but is designed to produce specific mis-categorizations.</p>

<p><a href=""https://i.stack.imgur.com/5rfe9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5rfe9.png"" alt=""enter image description here""></a></p>

<p>Since some networks are applied in safety-critical applications (e.g. self-driving cars), we have a question:</p>

<blockquote>
  <p>What tools are used to ensure safety-critical applications are resistant to the injection of adversarial examples at training time?</p>
</blockquote>

<p>Laboratory research aimed at developing defensive security for neural networks exists.  These are a few examples.</p>

<ul>
<li><p>adversarial training (see e.g. <a href=""https://arxiv.org/pdf/1611.01236.pdf"" rel=""nofollow noreferrer"">A. Kurakin et al., ICLR 2017</a>)</p></li>
<li><p>defensive distillation (see e.g. <a href=""https://arxiv.org/pdf/1511.04508.pdf"" rel=""nofollow noreferrer"">N. Papernot et al., SSP 2016</a>)</p></li>
<li><p>MMSTV defence (<a href=""https://arxiv.org/pdf/1706.06083"" rel=""nofollow noreferrer"">Maudry et al., ICLR 2018</a>).</p></li>
</ul>

<p>However, do industrial strength, production ready defensive strategies and approaches exist?  Are there known examples of applied adversarial-resistant networks for one or more specific types (e.g. for small perturbation limits)?</p>

<p>There are already (at least) two questions related with the problem of <a href=""https://ai.stackexchange.com/questions/6800/is-artificial-intelligence-vulnerable-to-hacking"">hacking</a> and <a href=""https://ai.stackexchange.com/questions/92/how-is-it-possible-that-deep-neural-networks-are-so-easily-fooled"">fooling</a> of neural networks. The primary interest of this question, however, is whether any <strong>tools</strong> exist that can defend against some adversarial example attacks.</p>
"
1778,"<p>I've recently come across the client-server model. From my understanding, the client requests the server, to which the server responds with a response. In this case, both the request and responses are vectors. </p>

<p>In reinforcement learning, the agent communicates with the environment via an ""action"", to which the environment sends a scalar reward signal. The ""goal"" is to maximize this scalar reward signal in long run. </p>

<p>Is there a more formal way to frame this problem such the agent searches the internet for obtaining more information about certain topic?</p>
"
1779,"<p>Can I get details about the algorithms used for classifying questions in stackoverflow (""Questions that may already have your answer""). Most of the suggestions I get are nowhere related to the question I have intended to ask.</p>
"
1780,"<p>I understand the intuition behind stacking models in machine learning, but even after thorough cross-validation scheme models seem to overfit. Most of the models I have seen in kaggle forums are large ensembles, but seem to overfit very little.</p>
"
1781,"<p>I have a sort of mathematical problem and I'm not sure which model I should choose to make an LSTM neural network.</p>

<p>Currently in my country, there is a system in which certain groups of researchers upload information on products of scientific interest, such as research articles, books, patents, software, among others. Depending on the number of products, the system assigns a classification to each group, which can be A1, A, B and C, where A1 is the highest classification and C is the minimum.</p>

<p>The classification is done through a mathematical model whose entries are, the total number of each product, the total sum of all products, number of authors, among other indices that are calculated with the previous values.</p>

<p>Once the entries are obtained, these values ​​are processed by a set of formulas and the final result is a single number.</p>

<p>This number is located in a range provided by the mathematical model and this is how the group is classified.</p>

<p>What I want to do is given the current classification of a group, give suggestions of different values ​​to improve their classification.</p>

<p>For example, if there is a group with classification C, suggest how many products it should have, how many authors, what value should its indexes have, so that its category would be finally B.</p>

<p>I think the structure of my network should be:
-1 input, which would be the classification you want to get.
-Multiple output, one for each product and indexes.</p>

<p>But I do not understand how to make the network take into account the current classification of the group, in addition to the number of products and the value of the current indexes.</p>

<p>If you have further questions about the problem, please feel free to ask.</p>

<p>I appreciate your suggestions.</p>
"
1782,"<p>Does anyone know a good book to buy for beginners to coding for artificial intelligence? I have done some coding in Java and a little with Python but I would be willing to learn a new language too. </p>

<p>Thanks</p>
"
1783,"<p>IBM's Watson acts as a template for developing chat-bots with ease (without coding), but what are the methodologies and concepts that have been used to build it?</p>
"
1784,"<p>Two months ago, I've found myself working on a churn detection problem which can be briefly described as follows:</p>

<ul>
<li>Assume the current date is N</li>
<li>Use customer behavior for N-1,..N-x dates to develop training dataset</li>
<li>Train model and make prediction at time N, predicting if a customer will churn at N+2 (thus allowing data N+1 for churn prevention / reduction campaign)</li>
</ul>

<p>When thinking through the design of the model and considerations for how to ensure that it would be successfully implemented, I identified a feedback loop wherein the prediction would trigger an event resulting in interaction with customer, potential changes to customer behavior and thus an impact on the next set of prediction data.  The following sequence of events could occur if successful (as an example):</p>

<pre><code>Prediction -&gt; Action to retain customer -&gt; Change to customer behavior -&gt;
Data for next prediction cycle not representative of training -&gt; 
Incorrect prediction and cost associated for handling incorrect prediction
</code></pre>

<p>The feedback loop, fundamentally is that the action taken based on the prediction may impact the distribution or nature of features used to make the prediction.  </p>

<p>When thinking through the how to solve the feedback problem I had listed the following three points as potential solutions:</p>

<ol>
<li>Retrain, test and validate model at every N+1 period and account for changes in behavior through new features (e.g. feature_i would involved details of the retention campaign a customer was treated to)

<ul>
<li>This would result in huge production overhead and I believe to be infeasible </li>
</ul></li>
<li>Run the model intermittently to allow behavior to normalize

<ul>
<li>Possible, however business would not be happy to have a prediction model which only works k times a year where k would have to be determined</li>
</ul></li>
<li>Predict the impact of the retention intervention and remove it from or the training set or include it as a new feature

<ul>
<li>Possible, extensive thought and some experimentation needed to determine whether modeling the retention out or in would have the better effect.  Additionally, if modeled in, there may a short term penalty incurred as the model learns the new feature</li>
</ul></li>
</ol>

<p>I did not actually end up having to confront the feedback problem (as during the exploration phase, sufficient evidence was obtained indicating that a predictive model for churn detection would not be required), however after reading <a href=""https://ai.google/research/pubs/pub43146"" rel=""nofollow noreferrer"">this</a> paper on the technical debt which could be incurred during the development of the machine learning systems I found myself pondering:</p>

<ol>
<li>Were my considered strategies for dealing with the feedback reasonable?</li>
<li>What other solutions should I have considered?</li>
<li>Is there a way I could have re-framed the problem to completely design out the feedback loop (may be difficult to answer with the information provided, but if possible, but a ""you could have considered looking at..."" would be extremely beneficial)</li>
</ol>
"
1785,"<p>I'm just beginning to understand neural networks and I've performed a couple of successful tests with numerical series where the NN was trained to find the odd one or a missing value. It all works pretty well.</p>

<p>The next test I wanted to perform was to approxmimate the solution of a Sudoku which, I thought could also be seen as a special kind of numerical series. However the results are really confusing.</p>

<p>I'm using an MLP with 81 neurons in each of the three layers. All output neurons show a strong tendency to yield values that are close to either 0 or 1. I have scaled and truncated the output values. The result can be seen below:</p>

<pre><code>Expected/Actual Solution:     Neural Net's Solution:

6 2 7 3 5 0 8 4 1             9 0 9 9 9 3 0 0 3
3 4 8 2 1 6 0 5 7             0 9 9 0 0 0 9 9 0
5 1 0 4 7 8 6 2 3             0 9 1 9 9 0 2 0 4
1 6 4 0 2 7 5 3 8             0 0 5 0 0 9 0 0 7
2 0 3 8 4 5 1 7 6             0 0 0 0 0 9 9 0 9
7 8 5 1 6 3 4 0 2             9 9 9 9 0 6 2 9 0
0 5 6 7 3 1 2 8 4             0 0 0 0 9 9 0 9 0
4 3 1 5 8 2 7 6 0             9 9 0 0 0 0 9 0 9
8 7 2 6 0 4 3 1 5             9 9 0 9 9 0 9 0 9
</code></pre>

<p>The training set size is 100000 Sudokus while the learning rate is a constant 0.5. I'm using NodeJS/Javascript with the Synaptic library.</p>

<p><strong>I don't expect a perfect solution from you guys, but rather a hint if that kind of behavior is a typical symptom for a known problem, like too few/many neurons, small training set etc..</strong></p>
"
1786,"<p>I am currently studying information systems engineering (BA) and I'm thinking of getting a master degree in Artificial Intelligence.So, What are the main important skills do I need to succeed at this field, and what kind of math does it require?</p>
"
1787,"<p>May someone explains some first iterations of this sigma?</p>

<p><a href=""https://i.stack.imgur.com/oh6bY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oh6bY.png"" alt=""enter image description here""></a></p>

<p>Also, how did it convert the above expression to below expression? What it the meaning of I(x) and I(y)?</p>
"
1788,"<p>I would like to know:</p>

<p>Would people go far with Artificial Intelligence and machine learning to the point where machines could learn during a long period of time to distinguish what's 'good' from 'bad' according to people living in a restricted geographical area, and then the machines take control and turn what was learnt into a set of 'rules' and 'laws' (think of it as an effective machine of 'politics') that match the majority of the people's view of issues. That should be accepted by everyone, since a contract set at the beginning says: “Everyone is ok”. (Confidence).</p>
"
1789,"<p>A bunch of friends and I play ultimate every week. Recently I wrote a program to choose our teams for us, as well as keep track of certain data (like which players were on which team, which team won, what was the score, how long the game was, etc). I wanted to use a machine learning technique to make the teams for us in order to optimize how fairly balanced the teams are (possibly measured by how many total points are scored in a game or how long a game lasts).</p>

<p>I am currently taking a machine learning MOOC and being introduced to very basic machine learning techniques (linear regression with gradient decent or normal equations, basic classification stuff, stuff like that). Although I hope I will come across a technique that fits my needs by the end of this course, I wanted to ask here to see if I can get a head start.</p>

<p>I've tried searching around everywhere, but couldn't find anything relevant. <strong>So my question is</strong>, is there an obvious technique I should look into for such a problem? If it's something too advanced for a beginner, that's fine too, but I'd like to get started learning/practicing it asap instead of waiting for my course to hopefully hit upon it.</p>

<p>Thank you!</p>

<p>EDIT: to clarify further, I would prefer something that looks at relationships between individual players like ""when Steph plays with Bill she is more likely to win"" or ""Steph plays worse when she is on a team with players who have a high win percentage"". I'd also prefer to be able to code it in python, but am willing to learn any other language</p>
"
1790,"<p>In the data-sets like coco-text and total-text, the images are of different sizes (height*width). I'm using these data sets for text detection. I want to create a DNN model for this. So input data should be of same size. If I resize these images to a fixed size, annotations given in the data-set that is the location of the text in the images will be changed. </p>

<p>So how to proceed with these data-set? I'm new to machine learning and I didn't get answer for this anywhere. Thanks for the help. </p>
"
1791,"<p>So I just read about deep Q-Learning which is using a neural network for optimization instead of Q-table.</p>

<p>I saw the example here: <a href=""https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html"" rel=""nofollow noreferrer"">https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html</a> and he used CNN to get the Q-Value.</p>

<p>My confusion is on the last layer of his neural net. Neurons in the output layer each represent an action (flap, or not flap). I also see the <a href=""http://edersantana.github.io/articles/keras_rl/"" rel=""nofollow noreferrer"">other projects</a> where the output layer also represents all available actions (move-left, stop, etc.)</p>

<p>How would you represent of all available action of a Chess game? Every pawn have unique and available movement. We also need to choose how far it will move (rook can move more than one square). I've read <a href=""https://arxiv.org/abs/1509.01549"" rel=""nofollow noreferrer"">Giraffe chess engine's</a> paper and can't find how he represents the output layer (I'll read once again).</p>

<p>I hope somebody here can give a nice explanation about how to design NN architecture in Q-learning, I'm new in reinforcement learning. Thank you.</p>
"
1792,"<p>In traditional computer vision and computer graphics, the pose matrix is a 4x4 matrix of the form 
<code>
r11    r12    r12    t1
r21    r22    r22    t2
r31    r32    r32    t3
0      0      0      1
</code></p>

<p>and is a transformation to change viewpoints from one frame to another.</p>

<p>In the <a href=""https://openreview.net/pdf?id=HJWLfGWRb"" rel=""nofollow noreferrer"">Matrix Capsules with EM Routing</a> paper they say that the 'pose' of various sub-objects of an object are encoded by each capsule lower layer. But from the procedure described in the paper, I understand that the pose matrix they talk about doesn't conform to the definition of the pose matrix. There isn't any restriction on keeping the form of the pose matrix shown above. </p>

<p>Therefore, my first question is that is it right to use the word pose to describe the 4x4 matrix of each capsule?</p>

<p>My next question is that since the claim is that the capsules learn the pose matrices of the sub-objects of an object, does it mean they learn the viewpoint transformations of the sub-objects since the pose matrix is actually a transformation?</p>
"
1793,"<p>Earlier this month, Google released a set of principles governing their AI development initiatives.  The stated principles are:</p>

<blockquote>
  <p><strong>Objectives for AI Applications:</strong></p>
  
  <ol>
  <li>Be socially beneficial.</li>
  <li>Avoid creating or reinforcing unfair bias.</li>
  <li>Be built and tested for safety.</li>
  <li>Be accountable to people.</li>
  <li>Incorporate privacy design principles.</li>
  <li>Uphold high standards of scientific excellence.</li>
  <li>Be made available for uses that accord with these principles.</li>
  </ol>
  
  <p><strong>AI Applications not to be Pursued:</strong></p>
  
  <ol start=""8"">
  <li>Technologies that cause or are likely to cause overall harm. Where there is a material risk of harm, we will proceed only where we believe that the benefits substantially outweigh the risks, and will incorporate appropriate safety constraints.</li>
  <li>Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.</li>
  <li>Technologies that gather or use information for surveillance violating internationally accepted norms.</li>
  <li>Technologies whose purpose contravenes widely accepted principles of international law and human rights.
  <br><sub>SOURCE: <a href=""https://ai.google/principles"" rel=""nofollow noreferrer"">Artificial Intelligence at Google: Our Principles</a></sub></li>
  </ol>
</blockquote>

<p>My questions are:</p>

<ul>
<li>Are this guidelines sufficient?  </li>
<li>Are there any ""<a href=""https://en.wikipedia.org/wiki/Three_Laws_of_Robotics"" rel=""nofollow noreferrer"">I, Robot</a>"" conflicts </li>
<li>How much does this matter if other corporations and state agencies don't hew to similar guidelines?</li>
</ul>
"
1794,"<p>How should I design my input layer for the following classification problem?</p>

<p><strong>Input</strong>: 5 cards in a card game; vocabulary is 52 cards</p>

<p><strong>Output</strong>: some classification using a neural network</p>

<p>How should I model the input layer?</p>

<p><strong>Option A</strong>: 5 one hot encodings for the 5 cards, i.e. 5 one_hot vectors of length 52 = 260 input vector
E.g.</p>

<pre><code>[
[0,0,0,0,0,0,1,...],
[1,0,0,0,0,0,0,...],
[0,0,0,0,0,1,0,...],
[0,0,1,0,0,0,0,...],
[0,0,0,0,1,0,0,...]
]
</code></pre>

<p><strong>Option B</strong>: 5 hot encoding encompassing all 5 cards in one 52 element vector</p>

<pre><code>[1,0,1,0,1,1,1,...]
</code></pre>

<p>What are the disadvantages between A and B?</p>
"
1795,"<p>I've been wondering, how, in the most simple-to-implement basic principle, does the light projection to depth map technique described here <a href=""https://www.lightform.com/how-it-works"" rel=""nofollow noreferrer"">https://www.lightform.com/how-it-works</a> actually functions? Is it some kind of an average based on the color of x pixel over all the patterns or what? How difficult would it be to code something that could do this, up?</p>
"
1796,"<p>I am interested to know, if someone wants to be an AI expert, what should he/she know, as we can see this is a vast field today!</p>

<p>For example, if someone works on machine vision, should he/she know voice recognition or data mining?</p>

<p>In other words, should someone know everything from image processing to machine vision, if he/she wants to be an expert in that field or are there some specific subfields even in the vision section?</p>
"
1797,"<p>I am working on a problem where I have to train a CNN to recognize different kinds of surfaces. One important characteristic of the surfaces I am interested is is how reflective they are. I have been trying to find a method that quantifies how ""shiny"" a surface is, but I have not found much. I am hoping that someone can point me toward a method or some research into this kind of problem.</p>
"
1798,"<p>I have a very simple question about Conv nets. I understand the whole principle, but only one thing is not well explained on the Internet.</p>

<p>If I have a 16 channels image that goes on a convolutional layer, and the trainable filters are 3 7x7 filters, meaning that its output has 3 channels, how does the conv layer do to go from 16 to 3 channels? What mathematical operation is applied?</p>

<p>Thanks for any clarification on this.</p>
"
1799,"<p>I'm working on a project to predict the usage of all the files in a filesystem in near future based on the metadata of the file system for past 6 months. I've got the following attributes about the files with me :</p>

<ol>
<li>The temporal sequence of file usage for last 6 months(whenever the
file was read/written/modified and by whom).</li>
<li>All the users who are on the server and can access the files.</li>
<li>Last modified/written/read epoch time and by whom.</li>
<li>File creation epoch time and by whom.</li>
<li>Any compliance regulations on the file(whether the file contains any
confidential data).</li>
<li>Size, name, extension, version, type of the file.</li>
<li>Number of users who can access the file.</li>
<li>File path.</li>
<li>Total number of times accessed.</li>
<li>Permitted users.</li>
</ol>

<p>Now, I plan to use LSTM but for standard LSTMs, the input is temporal sequence only. However, all the attributes that I have seem significant in predicting the future usage of the file.</p>

<ul>
<li>How should I also make use of the attributes of the file that I have?</li>
<li>Should I train a Feedforward Neural Network, disregarding the fact
that it usually fails on temporal sequences?</li>
<li>How should I proceed?</li>
<li>Does a variant of LSTM exist that can take into account the
attributes of the file as well and predict the usage of the file in
near future?</li>
<li>Do I need to use NN and LSTM together like a hybrid?</li>
</ul>
"
1800,"<p>In image classification we are generally told the main reason of using CNN's is that densely connected NN's cannot handle so many parameters (10 ^ 6 for a 1000 * 1000 image). My question is, is there any other reason why CNN's are used over DNN's (densely connected NN)? </p>

<p>Basically if we have infinite resources will DNN trump CNN's or are CNN's inherently well suited for image classification as RNN's are for speech. Answers based either on mathematics or experience on the field is appreciated.</p>
"
1801,"<p>I am learning about Monte Carlo algorithms and struggling to understand the following:</p>

<ul>
<li>If simulations are based on random moves, how can the modeling of the opponent's behavior work well?</li>
</ul>

<p>For example, if I have a node with 100 children, 99 of which lead to an instant WIN, whereas the last one leads to an instant LOSS.</p>

<p>In reality, the opponent would never play any of the 99 losing moves for him (assuming they are obvious as they are the last moves), and would always play the winning one. But the Monte Carlo algorithm would still see this node as extremely favorable (99/100 wins for me), because it sees each of the 100 moves as equally probable.</p>

<p>Is my understanding wrong, or does it mean that in most games such situations do not occur and randomness is a good approximation of opponent behavior?</p>
"
1802,"<p>I built a simple HTML game. In this game the goal is to click when the blue ball is above the red ball. If you hit, you get 1 point, if you miss, you lose 1 point. With each hit, the blue ball moves faster. <a href=""https://codepen.io/iazzetta/pen/BVxzyr"" rel=""nofollow noreferrer"">You can test the game here</a>.</p>

<p><a href=""https://i.stack.imgur.com/5L5pQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5L5pQ.png"" alt=""enter image description here""></a></p>

<p>Without using machine learning, I would easily solve this problem by just clicking when the X, Y of the blue ball was on the X, Y of the red ball. Regardless of the time, knowing the positions of the 2 elements I could solve the problem of the game.</p>

<blockquote>
  <p>However, if I wanted to create an AI to solve this problem, could I?
  How would it be? I'd really like to see the AI randomly wandering
  until it's perfect.</p>
</blockquote>

<h2>My way to solve the problem</h2>

<p>I click many times and watch score. If score down, add to bad_positions. If actual position in bad_positions, not click. At first he misses many times, then starts to hit eternally. <strong>This is machine learning? Deep learning? Just a bot?</strong></p>

<pre><code>var bad_positions = [];
function train(){
  var pos = $ball.offset().left;
  var last_score = score;
  if (!bad_positions.includes(pos)) {
   $('#hit').click();
    if (score &lt; last_score){
      bad_positions.push(pos)
    } 
  }
}
</code></pre>
"
1803,"<p>I have some very basic questions here. This is probably because I didn't read the relevant documents closely enough. If I used some terminology incorrectly, please point them out. Thank you!</p>

<ol>
<li><p>For units/neurons in the hidden layers, they are referred to as memory blocks and each memory block can contain multiple memory cells?</p></li>
<li><p>And one memory cell looks like these two?</p></li>
</ol>

<p><a href=""https://i.stack.imgur.com/38Szh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/38Szh.png"" alt=""LSTM Memory Cell""></a>
<a href=""https://i.stack.imgur.com/VAFik.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VAFik.png"" alt=""LSTM Memory Cell""></a></p>

<ol start=""3"">
<li><p>For units/neurons in the input and output layers, they are simply regular neurons? Like those in a feedforward network?</p></li>
<li><p>Regarding the network structure, the structure of an RNN essentially looks like that of a feedforward network, except the neurons in the hidden layer have been replaced by neurons with recurrent connections. Is this an accurate description?</p></li>
<li><p>LSTM RNNs then replaces those neurons in the hidden layers with memory blocks (which contain memory cells that have recurrent connections). Is this an accurate description?</p></li>
</ol>
"
1804,"<p>My question is that is there any general idea on how humans solve jumbled words? I know many people will say we match it against a commonly used words checklist mentally, but it is kind of vague. Is there any theory on this and how might an AI learn to do the same?</p>
"
1805,"<p>What does it mean when it is said that Machine Learning algorithm results can be ""generalized""? </p>

<p>I don't understand what ""generalized"" algorithms, routines or functions are. </p>

<p>I have searched dictionaries and glossaries, and cannot find an explanation. Also, if anyone can tell me where a good source for this type of thing is? I am writing about AI and ML.</p>
"
1806,"<p>I want to train a CNN (Vggnet) to identify different types of buildings from aerial images. </p>

<p>However seeing that a CNN ""ignores"" size, e.g. the same type of dog in one image can be large and small in another image but will still be classified as a dog.</p>

<p>My issue is that non-residential buildings are mostly larger than residential houses, now I want to use this property to distinguish between residential and non residential. Is this even possible?</p>

<p><a href=""https://i.stack.imgur.com/5hV2J.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5hV2J.jpg"" alt=""enter image description here""></a><a href=""https://i.stack.imgur.com/e5T9L.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e5T9L.jpg"" alt=""enter image description here""></a></p>

<p>Thanks</p>
"
1807,"<p><em>While am thinking about the age of <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">Artificial General Intelligence</a> and <a href=""https://en.wikipedia.org/wiki/Superintelligence"" rel=""nofollow noreferrer"">Artificial SuperIntelligence</a></em> , this question came into my mind.
This <a href=""https://www.bell-labs.com/var/articles/artificial-intelligence-new-frontier/"" rel=""nofollow noreferrer"">Bell Labs blog post</a> says: </p>

<blockquote>
  <p>""...where humanity is displaced by the self-aware robots that rule the world...""</p>
</blockquote>

<p>Humans tend to have a sense of self (self awareness), and this works effectively at maturity. However, at the young stage, a human child lacks a sense of self, so the child is guided by the environment; learning from it as time goes on. Also at this same stage, the child faces some consequences, for example:</p>

<ol>
<li>No sense of direction or transition.</li>
<li>No sense of purpose, etc.</li>
</ol>

<p>At the old stage (the natural being has grown up), its sense of self tries to generate its priorities, for example:</p>

<ol>
<li>knows when to stand.</li>
<li>where to go, as its goals are set.</li>
</ol>

<p>Here is my concern or point of view concerning artificial life:</p>

<p>Can an Artificial Being, lacking self-awareness, learn from its environment just like natural beings do?</p>

<p><strong>Note:</strong> Any references/papers/theories will be appreciated.</p>
"
1808,"<p>I want to detect drivers with, or without seatbelts at cross roads and for that, as it is real time, I am going to use yolo algorithm. For training data sets (the images) I need to collect, I placed a camera. By recording it and collecting images from there, I am getting images with more noise. Can I use these images for training? Also, which yolo version should I use? What are the important points that I should consider for training datasets?</p>

<p>I want to use any version of yolo compatible with tensorflow.</p>
"
1809,"<p>I just stumbled across this <a href=""https://arxiv.org/pdf/1705.08807.pdf"" rel=""nofollow noreferrer"">paper</a> which contains a figure showing the aggregated subjective probability of ‘high-level machine intelligence’ arrival by future years:</p>

<p><a href=""https://i.stack.imgur.com/g1Gxo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g1Gxo.png"" alt=""enter image description here""></a></p>

<p>Even if this graph reflects the opinion of experts, it can be totally wrong. It is just extremely hard to predict future events. So I was wondering if there is a similar graph which shows basically the same but for the game Go? </p>

<p>Due to the complexity of Go, some experts assumed, that no computer ever could be better in Go than a human being due to the lack of <em>intuition</em>. This shows that the appearance of human level AI can be unpredictable.</p>

<ul>
<li>Does anyone knows if a similar graph for Go exists to see how good or bad the predictions were? This could give a very rough idea, how good this graph predicts the future of human level AI.</li>
</ul>
"
1810,"<p>I was going through <a href=""https://github.com/transedward/pytorch-dqn/blob/master/dqn_learn.py"" rel=""noreferrer"">this</a> implementation of DQN and I see that on line 124 and 125 two different Q networks have been initialized. From my understanding, I think one network predicts the appropriate action and the second network predicts the target Q values for finding the Bellman error.</p>

<p>Why can we not just make one single network that simply predicts the Q value and use it for both the cases? My best guess that it's been done to reduce the computation time, otherwise we would have to find out the q value for each action and then select the best one. Is this the only reason? Am I missing something?</p>
"
1811,"<p>In the paper ""<a href=""https://arxiv.org/abs/1310.6343"" rel=""nofollow noreferrer"">Provable bounds for learning some deep representations</a>"", an autoencoder like model is constructed with discrete weights and several results are proven using some random-graph theory, but I never saw any papers similar to this. i.e bounds on neural networks using random graph assumptions. </p>

<ul>
<li>Can anybody point me towards interesting literature in this area (complexity of training neural networks)? </li>
</ul>

<p>I'm particularly interested in convolutinal neural networks.</p>
"
1812,"<p>I am starting to study the capabilities of neural networks for the reconstruction/restoration/... of communication signals.</p>

<p>I am feeding my neural network with a signal which has some parts which have been damaged because of the transmission through a communication system, and my targets are given by the signal with these areas undamaged. </p>

<p>The problem is that the areas damaged represent a very small portion of the whole signal, and my neural network spends lot of time learning only from the portions which actually do not present any problem.</p>

<p>Is there any solution to make the neural network to jump on those areas which show significant differences to respect to the targets? Is there anything I could do for example initializing my neural networks (as conventionally done, they are initialized randomly)? Or shall I accept that I need to train for longer time?</p>
"
1813,"<p>While reading about least squares implementation for machine learning I came across this passage in the following two photos:
<a href=""https://i.stack.imgur.com/rbHEv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rbHEv.jpg"" alt=""photo1""></a>
<a href=""https://i.stack.imgur.com/FR8vr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FR8vr.jpg"" alt=""photo 2""></a></p>

<p>Perhaps I’m misinterpreting the meaning of beta  but if X^T has dimension 1 x p and beta has dimension p x K, then hat{Y}  would have dimension 1 x K and would be a row vector. According to the text, vectors are assumed column vectors unless otherwise noted. </p>

<p>Can someone provide clarification?</p>

<p>Edit: the matrix notation in this text confuses me. The pages preceding the above passage read:</p>

<p><a href=""https://i.stack.imgur.com/RGXdp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RGXdp.jpg"" alt=""3rd""></a>
<a href=""https://i.stack.imgur.com/47qQn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/47qQn.jpg"" alt=""4th""></a></p>

<p>Should the matrix referenced not have dimensions p x N, assuming a p-vector is a column vector with p elements?</p>

<p>Note: The passage is taken from “Elements of Statistical Learning” by Hastie, Tibshirani, &amp; Friedman.  </p>
"
1814,"<p>I have a dataset of unlabelled emails that fall into distinct categories (around a dozen). I want to be able to classify them along with new ones to come in the future in a dynamic matter. I know that there are dynamic clustering techniques that allow the clusters to evolve over time ('dynamic-means' being one of them). However, I would also like to be able to start with a predefined set of classes (or clusters/centroids) as I know for a fact what the types of those emails will be.</p>

<p>Furthermore, I need some guidance in terms of what vectorisation technique to use for my type of data. Would creating a term matrix using TF-IDF be sufficient? I assume that the data I am dealing with could be differentiated on the basis of keyword occurrence, but I cannot tell to what degree. Are there more sophisticated vectorisation techniques based more on the text semantics? Are they worth exploring?</p>
"
1815,"<p>In the below pic, I can not understand what <code>U</code> vector is? It says <code>flow field</code> but I can not imagie what really is the flow field?</p>

<p><a href=""https://i.stack.imgur.com/wSMqN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wSMqN.png"" alt=""enter image description here""></a></p>
"
1816,"<p>Imagine that a line divides an image in two regions which (slightly) differ in terms of texture and color. It is not a perfect, artificial line but rather a thin transition zone. I want to build a neural network which is able to infer geometrical information on this line (orientation and offset). The image may also contain other elements which are not relevant for the task. Now, would a classical CNN be suitable for this task? How complex should it be in terms of number of convolutions (and number of layers, in general)?</p>
"
1817,"<p>I am familiar with supervised and unsupervised learning. I did the SaaS course done by Andrew Ng on Coursera.org.</p>

<p>I am looking for something similar for reinforcement learning.</p>

<p>Can you recommend something?</p>
"
1818,"<p>I'm trying to implement <code>YOLO</code> (tiny version, <strong>v1</strong>) into Keras framework. For the past two days, I've been relentlessly digging through Github and the likes in order to help me in this task, with more or less success. </p>

<p>More precisely, I would like to use pretrained weights, except those are only available as <code>.weight</code> files. I found some scripts on the internet in order to convert them either to <code>.txt</code> or <code>.h5</code> (<code>.h5</code> only for Yolo <strong>v2</strong>, and not <strong>v1</strong>...), but none of these seemed to be working properly. So I guess my question is: how to read <code>.weights</code> file? How are the data stored and structured.</p>
"
1819,"<p>Gradient in Maximum Entropy IRL requires to find the probability of expert trajectories given the reward function weights. This is done in the paper by calculating state visitation probabilities but I do not understand why we can’t just calculate the probability of a trajectory by summing up all the rewards that are collected following that trajectory? The paper defines the probability of a trajectory as exp(R(traj.)/Z. I do not understand why we have to solve MDP for calculating that.</p>
"
1820,"<p>I have practiced building cnn for image classification with tensorflow, luckily to me they have very good library documentation and tutorials. But i found that tensorflow is too complicated, building graphs for every equation and much more .. </p>

<p>Can i build well formed CNN for image classification task with just OpenCV?  </p>
"
1821,"<p>I'm reading currently through the old papers of Allen Newell and Herbert Simon in the late 1950s about their project to build a general problem solver. As far as i understand the concept of operators right, it is a hierachical planning technique to move a system from a current state to a goal state. On the first look, the General Problem solver seems to be an amazing piece of software, but from the history of AI it is known that this technology wasn't a great success. Why? If a hierarchical planning software is not the right choice for implementing a robot control system what else?</p>
"
1822,"<ol>
<li>What methods are used for facial recognition in public surveillance? Ideally, an answer would point to the software, algorithms or specifications being used.</li>
<li>How can those be fooled?

<ul>
<li>Fake or disfigured face? To what extend would one need to employ artificial scars or moles, make-up or even a complete face mask?</li>
<li>Will spectacles work? Sun glasses vs. normal ones?</li>
<li>Will using a hat to hide a portion of a face work? How much needs to be hidden? Hair, forehead, eyes, nose or complete face?</li>
<li>Blinding (not: destroying) a camera with laser or LEDS, provided the camera can be seen, and one can aim at it?</li>
<li>What else?</li>
</ul></li>
<li>Slightly related: Are there any other methods being used, such as clothes detection, gait detection, etc?</li>
</ol>
"
1823,"<p>Most humans are not good at chess. They can't write symphonies. They don't read novels. They aren't good athletes. They aren't good at logical reasoning. Most of us just get up. Go to work in a factory or farm or something. Follow simple instructions. Have a beer and go to sleep.</p>

<p>What are some things that a clever robot can't do that a stupid human can?</p>
"
1824,"<p>I'm a complete newbie to NNs, and I need your advice.</p>

<p>I have a set of images of symbols, and my goal is to categorize and divide them into groups of symbols that look alike. Without teaching NN anything about the data.</p>

<p>What is the best way to do this? What type of NN suits the best? Maybe there are any ready solutions?</p>

<p>Thank you!</p>
"
1825,"<p>I recently came across reference to a book that was highly regarded: “Pattern Recognition and Machine Learning” by Christopher Bishop. </p>

<p>I am a beginner working my way through some machine learning courses on my own. I’m curious if this book is still relevant considering it was published in 2006? Can anyone who has read it attest to its usefulness in 2018?</p>
"
1826,"<p>I apologize if this is a repeated question or if this is too simple. I was learning about back-propagation and looking at the algorithm there is no particular 'partiality' given to any unit. What I mean by partiality there is that, you have no particular characteristic associated with any unit and this results in all units being equal in the eyes of the machine. So won't this result in the same activation values of all the units in the same layer? Won't this lack of 'partiality' render neural networks obsolete?</p>

<p>UPDATE: I was reading a bit and watching few videos about backpropagation and in the explanation given by Geoffrey Hinton, he talks about how we're trying to train the hidden units using the error derivatives w.r.t our hidden activities rather than using desired activites. This further strengthens my point about how by not adding any difference to the units, all units in a layer become equal since initially the errors due to all of them are the same and thus we train them to be equal.</p>
"
1827,"<p>I should show that exact inference in bayesian network (BN) is NP-hard and P-hard by using a 3SAT Problem.</p>

<p>So I did formulate a 3SAT Problem by defining 3CNF:</p>

<pre><code>(x1 ∨ x2) ∧ (¬x3 ∨ x2) ∧ (x3 ∨ x1)
</code></pre>

<p>I reduced it to inference in BN , and produced all conditional probabilities, and I know which variable assignment would lead for the entire expression to be true.</p>

<p>I am aware of the difference between P and NP. (Please correct me if I am wrong) :</p>

<p>Any P problem with an input of the size n can be solved in O(n^c) . For NP the polynomial time cannot be determined, hence, non deterministic polynomial time. The question that scientist try to answer is whether a computer who is able to verify a solution would also be able to find a solution. P= NP ?</p>

<p>So basically that what I understood from my lecture, but still I am not sure how I can prove that exact inference in BN is NP-hard and P-hard.</p>
"
1828,"<p>I have a task where I would like to use a CNN. I would like to incrementally start from the fastest models, fine-tune and see whether they fit my ""budget"". At the moment, I'm just looking at object detection CNN based feedforward models.</p>

<p>I'm curious to know if there is any article/blog/web-page/gist that benchmarks the popular CNN models based on forward pass speed. If there is back-prop time and dataset-wise performance, even better!</p>

<p>Cheers!</p>
"
1829,"<p>Typical AI these days are question-answering machines. For example, Siri, Alexa and Google Home. But it is always the human asking the questions and the AI answering. </p>

<p>Are there any good examples of an AI that is curious and asks questions of its own accord?</p>
"
1830,"<p>Are there possible algorithms that have the potential to replace neural nets in the near future?
And do we need that?
What is the worst thing of using neural networks in terms of efficiency?</p>
"
1831,"<p>I am using LSTM model to predict the next xml markup from an input seed.
I have trained my model on 1500 xml files. Each xml file is generated randomly. I am wondering if there is a way to visualize the predicted results in a form of a graph or maybe is it meaningful to do so ?
Since we can do the visualization of classification results, for example in this<br>
<a href=""https://datascience.stackexchange.com/questions/15271/visualizing-results-of-a-classification-problem-excluding-confusion-matrices"">link</a></p>

<p>I have done some research on Internet, I have found that there is the confidence measure that can be useful for text prediction task.</p>

<p>I am a bit confused what to do with the text results that I got.</p>
"
1832,"<p>In order to learn about DP and RL, I chose to start a side project where I would train an AI to play a ""simple"" card game. I will be doing this using the DQN with replay memory.<br>
The problem is, I can't get the intuition behind how to represent the input to the neural network..</p>

<p><strong>About the game</strong></p>

<p>It's a fairly simple 2-players game. There is a deck of 40 unique cards (4 types of cards, 10 numbered cards in each type).<br>
Each player gets 4 cards and each turn a player must put a card on the table.<br>
If a player puts a card and there is already a card with the same number on the table, the player wins both cards.<br>
If for example a player plays Card 2 and on the table there is Cards 2, 3, 4, 5 then the player wins all those cards (sequence).<br>
Cards won don't go back to the hand nor to the deck, they are just kept as like a score.<br>
When the players have 0 cards in hand, another 4 cards are dealt to each one untile the deck has 0 cards left where then we decide who won based on the number of cards eaten/won.</p>

<p><strong>Question</strong></p>

<p>As the input, I will be using the following:</p>

<ul>
<li>Current cards in the AI hand (40 one-hot-encoded features?)</li>
<li>Current cards on the table (40 one-hot-encoded features?)</li>
<li>History of played cards (40 one-hot-encoded features?)</li>
</ul>

<p>This would give 120 columns/features in each state.<br>
I am wondering wheter this is too much for the NN or wheter my input representation would be bad for the NN?<br>
Should the features be represented as a (120,) vector or as a 3x40 matrix?</p>

<p>I am also wondering if it's a good idea to represent the current cards on the table as just a 10 one-hot-encoded features since the type of the cards don't matter and the same number can't exist 2 times in the table?</p>

<p>Thank you in advance.</p>
"
1833,"<p>I'm attempting to create an AI for a card game using reinforcement learning.  The basics of the game are that you can have (theoretically) up to 35 cards in your hand, you can also have to up to 35 cards 'in play' and so can your opponent.  In normal play you would have ~6 cards in your hand and maybe ~3 each in play.  There are roughly 300 unique cards in total.</p>

<p>How should I represent the game state for the input and how should I represent the action to take in the output?</p>
"
1834,"<p>In the following, I put the link for the general algorithm of maximum entropy inverse reinforcement learning. <a href=""http://178.79.149.207/assets/maxent/maxent_slide.jpg"" rel=""nofollow noreferrer"">http://178.79.149.207/assets/maxent/maxent_slide.jpg</a></p>

<p>This uses a gradient descent algorithm. The point that I do not understand is there is only a single gradient value and it is used to update a vector of parameters. To me, it does not make sense because it is updating all elements of a vector with the same value. Can you explain the logic behind updating a vector with a single gradient?</p>
"
1835,"<p>I am currently implementing <a href=""http://web.engr.illinois.edu/~slazebni/publications/iccv15_active.pdf"" rel=""nofollow noreferrer"">this</a> paper in Python. While reading about the reward scheme I came across the following: </p>

<blockquote>
  <p>Finally, the proposed reward scheme implicitly considers
  the number of steps as a cost because of the way in which
  Q-learning models the discount of future rewards (positive
  and negative).</p>
</blockquote>

<p>How would you implement this ""Number of Steps"" cost? I am keeping track of the number of steps that have been taken, therefore would it be best to use an exponential functions to discount the reward at the current time step? </p>

<p>If anyone has a good idea or knows the standard in regard to this I would love to hear your thoughts. </p>
"
1836,"<p>I am very new to Machine learning and following the course offered by Andrew Ng.I am very confused How we train our neural network on Multi class Classification(suppose take <em>K</em> classes).For <em>K</em> classes we will be training <em>K</em> different neural networks.</p>

<p>But Do we train one neural network at a time for all feature or we train all K NN at a time for one feature? please explain the complete procedures.</p>

<p>Please help me in this.Consider that I have a very very basic understanding of neural network in multi class classification.</p>
"
1837,"<p>I am working in anaconda/python and I have a datase which contains userId ,itemId and 2 more attribute and timestamp for purchase. </p>

<pre><code>UserId     ItemId   attr1   attr2   timestamp
u1             i2   1.56    2.15     Fri Aug 10 16:40:07 +0000 2018
u1             i1   2.56    2.45     Sat May 12 18:40:38 +0000 2018
u2             i1   3.66    4.5      Wed Apr 18 16:07:03 +0000 2018
</code></pre>

<p>normally CF uses rating attribute to find correlation between users using pearson,cosine and other correlation algorithm. But how can I use attr1,attr2 and timestamp to find correlation instead of rating attribute which I don't have?. </p>
"
1838,"<p>Following my recent chat on this network, I have been advised to form this question.</p>

<p>Background: Currently a neural network or deep-learning/machine learning is programmed to interact with specific data-sets to resolve a specific problem using mathematical equations to approximate if the data correlates to the desired result. The resulting ""stack"" of equations produce a numeric hypothesis of relevance - or a percentage of confidence.</p>

<p>The question: Discovering what ""people say"" about current artificial technology and what ""actually happens"" has me questioning the theoretical abilities of a deep learning neural network. Could a deep learning neural network be programmed to receive input from a human, like a terminal, to begin to grow and learn not unlike how a child learns. A program that neither knows it's purpose nor specific data sets but is given enough information to learn based off of input, ponder the input, and ask questions. A child discovers their purpose (in destiny based philosophy) through experience. Thus, could an AI be created that would learn it's purpose over time. </p>

<p>Grow both by continued programmer development, maybe adding extensions that add image recognition, speech analysis... (etc) and through user interaction. Eventually learning ""moral imperatives"" or simple the do's and don't's and how to interact with data.</p>

<p>A case scenario would be a Question &amp; Answer session with the neural network and a large data set. Where the human operator knows the answers. At first, the question and the answer are supplied to the neural network. Giving it the ability to find the answer supplied through deep learning. A guaranteed confidence score of (1)  - as the question is pondered the closer it get's to the answer the more it ""learns"". </p>

<p>The next step is supplying the question and waiting for the answer. The human still knows these answers but is testing the ""learning machine"" to see if it is truly learning and not ""repeating the answer"". The answer is supplied by the machine and the human returns with either a percentage that the machine is right (hopefully and eventually matching its confidence score). and after an amount of failure provides the right answer to the machine to repeat the first step and improve learning.</p>

<p>The last step is being able to have the machine answer the question with the human not knowing the solution, thus completing the learning cycle. The human would test the solution and report the results to the machine and the machine would adapt the process and continue learning. However, this time it would begin learning from a data set of results. Hopefully learning ""data mining"" during its question and answer session.</p>
"
1839,"<p><strong>Geometry and AI</strong></p>

<p>Matrices, cubes, layers, stacks, and hierarchies are what we could accurately call <em>topologies</em>.  Consider topology in this context the higher level geometrical design of a learning system.</p>

<p>As complexity rises, it is often useful to represent these topologies as directed graph structures.  State diagrams and Markov's work on game theory are two places where directed graphs are commonly used.  Directed graphs have vertices (often visualized as closed shapes) and edges often visualized as arrows connecting the shapes.</p>

<p>We can also represent GANs as a directed graph, where the output of each net drives the training of the other in adversarial fashion.  GANs resemble a Möbius strip topologically.</p>

<p>We cannot discover new designs and architectures without understanding not only the mathematics of converging on an optimal solution or tracking one but also topologies of network connections that can support such convergence.  It is like first developing a processor while imagining what an operating system would need before writing the operating system.</p>

<p>To glimpse what topologies we have NOT YET considered, let's first look at which ones have been.</p>

<p><strong>Step One &mdash; Extrusion in a Second Dimension</strong></p>

<blockquote>
  <p>In the 1980s, success was achieved with the extension of the original perceptron design.  Researchers added a second dimension to create a multi-layered neural network.  Reasonable convergence was achieved through back-propagation of an error function's gradient through the gradients of the activation functions attenuated by learning rates and dampened with other meta-parameters.</p>
</blockquote>

<p><strong>Step Two &mdash; Adding Dimensions to the Discrete Input Signal</strong></p>

<blockquote>
  <p>We see the emergence of convolutional networks based on existing manually tuned image convolution techniques introduced dimensions to the network input: Vertical position, color components, and frame.  This last dimension is critical to CGI, face replacement, and other morphological techniques in contemporary movie making.  Without it, we have image generation, categorization, and noise removal.</p>
</blockquote>

<p><strong>Step Three &mdash; Stacks of Networks</strong></p>

<blockquote>
  <p>We see stacks of neural nets emerge in the late 1990s, where the training of one network is supervised by another.  This is the introduction of conceptual layers, neither in the sense of sequential layers of neurons nor in the sense of layers of color in an image.  This type of layering is not recursion either.  It is more like the natural world where one structure is an organ within another completely different kind of structure.</p>
</blockquote>

<p><strong>Step Four &mdash; Hierarchies of Networks</strong></p>

<blockquote>
  <p>We see hierarchies of neural nets appearing frequently in the research arising out of the 2000s and early 2010s (Laplacian and others), which continues the interaction between neural nets and continuing the mammalian brain analogy.  We now see meta-structure, where entire networks become vertices in a directed graph representing a topology.</p>
</blockquote>

<p><strong>Summarizing</strong>   </p>

<p>Layers have ordinally valued activation functions for vertices and attenuation matrices mapped to an exhaustive set of directed edges between adjacent layers [1].  Image convolution layers are often in two dimensional vertex arrangements with attenuation cubes mapped to an abridged set of directed edges between adjacent layers [2]. Stacks have entire layered nets as vertices in a meta-directed-graph, and those meta-vertices are connected in a sequence with each edge being either a training meta-parameter, a reinforcement (real time feedback) signal, or some other learning control. Hierarchies of nets reflect the notion that multiple controls can be aggregated and direct lower level learning, or the flip case where multiple learning elements can be controlled by one higher level supervisor network.</p>

<p><strong>Analysis of the Trend in Learning Topologies</strong></p>

<p>We can analyze trends in machine learning architecture.  We have three topological trends.</p>

<ul>
<li><p>Depth in the causality dimension &mdash; Layers to the signal
processing where the output of one layer of activations is fed
through a matrix of attenuating parameters (weights) to the input of
the next layer.  As greater controls are established, only beginning
with basic gradient descent in back propatagion, greater depth can be
achieved.</p></li>
<li><p>Input signal dimensionality &mdash; From scalar input to hypercubes
(video has horizontal, vertical, color depth including transparency,
and frame &mdash; Note that this is not the same as the number of
inputs in the perceptron sense.</p></li>
<li><p>Topological development &mdash; The above two are Cartesian in
nature.  Dimensions are added at right angles to the existing
dimensional.  As networks are wired in hierarchies (as in Laplacian
Hierarchies) and Möbius strip like circles (as in GANs), the trends
are topographical and are best represented by directed graphs where
the vertices are not neurons but smaller networks of them.</p></li>
</ul>

<p><strong>What Topologies are Missing?</strong></p>

<p>This section expands on the meaning of the title question.</p>

<ul>
<li>Is there any reason why multiple meta-vertices, each representing a neural net, can be arranged such that multiple supervisor meta-vertices can, in conjunction, supervise multiple employee meta-vertices?</li>
<li>Why is the back-propagation of an error signal the only non-linear equivalent of negative feedback?</li>
<li>Can't collaboration between meta-vertices rather than supervision be employed, where there are two reciprocal edges representing controls?</li>
<li>Since neural nets are employed mainly for learning of nonlinear phenomena, why prohibits other types of closed paths in the design of the nets or their interconnection?</li>
<li>Is there any reason why sound cannot be added to picture so that video clips can be categorized automatically?  If that is the case, is a screenplay a possible feature extraction of a movie and can an adversarial architecture be used to generate screenplays and produce the movies without the movie studio system?  What would that topology look like as a directed graph?</li>
</ul>

<hr>

<p><strong>Notes</strong></p>

<ol>
<li><p>Artificial cells in MLPs use of floating or fixed point arithmetic transfer functions rather than electro-chemical pulse transmissions based on amplitude and proximity based threshold.  They are not realistic simulations of neurons, so calling the vertices neurons would be a misnomer for this kind of analysis.</p></li>
<li><p>Correlation of image features and relative changes between pixels in close proximity is much higher than that of distant pixels.</p></li>
</ol>
"
1840,"<p>I am building a search engine and I am looking for an open source AI algorithm to recognize the keyword in a search phrase within a particular context. So if a user passes something in the line of <code>how far is Russia?</code> and the context is <code>location</code> then the AI should return <code>Russia</code></p>
"
1841,"<p>I have a multi-agent environment where agents are trying to optimise the overall energy consumption of their group. Agents can exchange energy between themselves (actions for exchange of energy include - request, deny request, grant), which they have produced from renewable sources and is stored in their individual batteries. The <strong>overall</strong> goal is to reduce the energy used from non-renewable sources. </p>

<p>All agents have been built using DQN. All (S,A) pairs are stored in a replay memory which are extracted when updating the weights. </p>

<p>The reward function is modelled as such — if at the end of the episode the <strong>aggregate</strong> consumption <strong>of the agent group</strong> from non-renewable sources is lesser than the previous episode, all agents are rewarded with +1. If not, then -1. An episode (iteration) consists of 100 timesteps after which the reward is calculated. I update the weights after each episode. </p>

<p>The reward obtained at the end of the episode is used to calculate the error for <strong>ALL</strong> (S,A) pairs in the episode i.e. I am rewarding all (S,A) in that episode with the same reward.</p>

<p>My problem is that agents are unable to learn the optimal behavior to reduce the overall energy consumption from non-renewable sources. The overall consumption of the group is oscillating i.e. sometimes increasing and sometimes decreasing. Does it have to do with the reward function? Or Q learning as the environment is dynamic?</p>
"
1842,"<p>More informations on the card game I'm talking about are in my last question here: <a href=""https://ai.stackexchange.com/questions/7049/dqn-input-representation-for-a-card-game"">DQN input representation for a card game</a></p>

<p>So I was thinking about the output of the q neural network and, aside from which card to play, I was wondering if the agent can announce things.</p>

<p>Imagine you have the current hand: <code>2, 4, 11, 2</code> (The twos are different card type).<br>
When you're playing the game and you get dealt a hand like this, you have to announce that you have the same number twice (called Ronda) or thrice (called Tringa) before anyone plays a card on the table. Lying about it gets you a penalty.</p>

<p>Could a DQN handle this? I don't know if adding ""Announcing a Ronda/Tringa"" as an action would actually help. I mean, can this be modeled for the  NN or should I just automate this and spare the agent having to announce it everytime.</p>
"
1843,"<p>The question is very simple but the answer could be not. </p>

<hr>

<p>Iin my personal experience - and I've not so much on NN - I choose the activation function for the output layer depending on the output that I need and the properties of the activation function that I know. e.g. I choose the sigmoid function when I'm dealing with probabilities, with a ReLU when I'm dealing with positive values, a linear function when I'm dealing with general values.</p>

<p>In hidden layers, I use a leaky ReLU to avoid dead neurons instead of the ReLU and the tanh instead of the sigmoid. Of course, I don't use a linear function in hidden units.</p>

<p>However, the choice for them in the hidden layer is mostly due to trial and error. There is any rule of thumb of which activation function is likely to work good in some situations? Take the term <em>situations</em> as general as possible: it could be referring to the depth of the layer, to the depth of the NN, to the number of neurons for that layer, to the optimizer that we chose, to the number of input features of that layer, to the application of this NN, etc. </p>

<hr>

<p>In <a href=""https://ai.stackexchange.com/a/7089/16199"">his/her answer</a>, cantordust refers to other activation functions that I didn't mention, like ELU and SELU. This infos are more than welcomed. However, more and more activation function I discover and more I'm confused in the choice of the function to use in hidden layers. And I don't think that flipping a coin is a good way of choosing an activation function.</p>
"
1844,"<p>I am training LSTM Nets with Keras on a small mobile GPU. The speed on GPU is slower then on CPU. I found some articles that say that it is hard to train LSTMs (RNNs) on GPUs because the training cannot be parallelized.</p>

<p>What is your experience? Is LSTM training on large GPUs like 1080 Ti faster then on CPU?</p>
"
1845,"<p>Is there any way and any reason why one would introduce a sparsity constraint on a deep autoencoder? </p>

<p>In particular, in deep autoencoders the first layer often has more units than the dimensionality of the input. Is there any case in the literature where a penalty is explicitly imposed for non-sparsity on this layer rather than relying solely on back-propagation and maybe weight decay as in a normal multilayer network?</p>

<p>I read <a href=""https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf"" rel=""nofollow noreferrer"">this</a> tutorial on sparse autoencoders and searched a bit online but did not find any case where such a sparsity constraint is used in any other case than when only a single layer is used.</p>
"
1846,"<p>TL;DR I am currently trying to understand the mathematics in Ger's paper <a href=""http://www.felixgers.de/papers/phd.pdf"" rel=""nofollow noreferrer"">Long Short-Term Memory in Recurrent Neural Networks</a>. I have found the document clear and readable so far. </p>

<p>On pg. 21 of the pdf (pg. 13 of the paper), he derives the backward pass equations for output gates. He writes</p>

<p>$$\frac{\partial y^k(t)}{\partial y_{out_{j}}} e_k(t) = h(s_{c_{j}^{v}}(t)) w_{k c_{j}^{v}} \delta_{k}(t)$$.</p>

<p>If we replaced $\delta_{k}(t)$, the expression becomes</p>

<p>$$\frac{\partial y^k(t)}{\partial y_{out_{j}}} e_k(t) = h(s_{c_{j}^{v}}(t)) w_{k c_{j}^{v}} f'(net_k(t)) e_k(t)$$.</p>

<p>He states that the result of the partial derivative $\frac{\partial y^k(t)}{\partial y_{out_{j}}}$ comes from differentiating the forward pass equations for the output units. </p>

<p>From that and from the inclusion of $e_k(t)$, the paper implies that there is only one hidden LSTM layer. If there are multiple hidden LSTM layers, it wouldn't make sense. </p>

<blockquote class=""spoiler"">
  <p> Because if $k$ is the index of LSTM cells that the current cell is outputting to, then $e_k(t)$ would not exist since the cell output isn't compared with the target output of the network. And if $k$ is the index of output neurons, then $w_{k c_{j}^{v}}$ would not exist since the memory cells are not directly connected to output neurons. And $k$ cannot mean different things since both components are placed under a sum over $k$. Therefore, it only makes sense if the paper assumes a single LSTM layer.</p>
</blockquote>

<p>So, how would one modify the backward pass derivation steps for an LSTM layer that outputs to another LSTM layer? </p>
"
1847,"<p>If I do supervised learning the model learns from the labeled input data. This seems to be quite often a small set of human annotated data. </p>

<p>Is it true to say this is the only 'learning' the model does? </p>

<p>It seems like the small data set has a huge influence on the model. Can it be made better using future unlabeled data? </p>
"
1848,"<p>We have AI's predicting images, predicting objects in an image. Understanding audio, meaning of the audio if it is a spoken sentence.</p>

<p>In humans when we start seeing a movie halfway through, we still understand the entire movie (although this might be attributed to the fact that future events in movies have a link to past events). But even if we see a movie by skipping lots of bits in-between we still understand the movie.</p>

<p>So can a Machine Learning AI do this? Or do humans have some inherent experiences in life which makes AI incapable of performing such a feat?</p>
"
1849,"<p>I am learning about Restricted Boltzmann Machines and I'm so excited by the ability it gives us for unsupervised learning. The problem is that I do not know how to implement it using one of the programming languages I know without using libraries. I want to implement it manually, which means that I want to use native functionalities of a language as much as possible.</p>

<p>The programming languages I know are Java, C, PHP (my preferred language), JavaScript, R and Python. I am not familiar with TensorFlow or Scikit-Learn or similar stuff. </p>

<p>Thanks in advance</p>
"
1850,"<p><a href=""https://arxiv.org/abs/1302.4389"" rel=""noreferrer"">Maxout networks</a> were a simple yet brilliant idea of Goodfellow et al. from 2013 to max feature maps to get a universal approximator of convex activations. The design was tailored for use in conjunction with dropout (then recently introduced) and resulted of course in state-of-the-art results on benchmarks like CIFAR-10 and SVHN.</p>

<p>Five years later, dropout is definitely still in the game, but what about maxout? The paper is still widely cited in recent papers according to Google Scholar, but it seems barely any are actually using the technique.</p>

<p>So is maxout a thing of the past, and if so, why — what made it a top performer in 2013 but not in 2018?</p>
"
1851,"<p>I have an LSTM model. This model takes as input tokens. Those tokens represent XML markups extracted from some XML files. My model is working fine. However, I want to optimize it by adding word embedding as additional features to the LSTM model. Does it make sense to combine word embeddings and encoded tokens (encoded as integers) for the LSTM model ?</p>
"
1852,"<p>My question is about chat-bots. I need an academically-oriented question answering system that works like IBM Watson.</p>

<p>More specifically, this QA should accept a typed-in question from a user and search the correct answer in an appropriate textbook in PDF format.</p>

<p>EXAMPLE: the user types ""What is the difference between RNA and DNA?""</p>

<p>The question answering system will search a textbook of MOLECULAR BIOLOGY in PDF format, construct and fetch the correct answer to the user.</p>
"
1853,"<p>I am having a question on how to label training data for YOLO algorithm.</p>

<p>Let's say that each label Y, we need to specify [Pc, bx, by, bh, bw], where Pc is the indicator for presence(1=present, 0=not present), (bx, by) is relative position of the center of the object-of-interest, and (bh, bw) is the relative dimension of the bounding box containing the object.</p>

<p>Using picture below as an example, the cell (1,2), which contains a black car, should have a label Y = [1, 0.4, 0.3, 0.9, 0.5]. And for any cells without cars, they should have a label [0, ?, ?, ?, ?] [Coursera Deep Learning Specialization Materials]1
<a href=""https://i.stack.imgur.com/MehFy.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MehFy.jpg"" alt=""enter image description here""></a></p>

<p>But if we have a finer grid like this, where the dimension of each cells is smaller than the ground truth bounding box. 
<a href=""https://i.stack.imgur.com/kw1Hg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kw1Hg.jpg"" alt=""enter image description here""></a></p>

<p>Let's say that the ground truth bounding box for the car is the red box, and the ground truth center point is the red dot, which is in cell 2.</p>

<p>For cell 2 it will have label Y = [1, 0.9, 0.1, 2, 2], is this correct? And for cell 1, 3, 4 what kind of label will they have? Do they have Pc=1 or Pc =0? And if Pc=1, how will the bx and by be? (As i remember that bx, by should have value between 0 and 1. But in cell 1,3,4, there is no center point of the object-of-interest)</p>
"
1854,"<p>I have a .db file with columns as described below. This data has been collected by a software which monitors the file usage in a filesystem or in other words generates metadata about all the files in the system.  </p>

<pre><code>fid | opcode | count | formatdate (timestamp, YYYY/MM/DD HH:mm)  

124 |   2    |   1   | 2018/06/08 09:00  
454 |   1    |   7   | 2018/06/08 09:01  
433 |   1    |   2   | 2018/06/08 09:01
</code></pre>

<p>The description of columns is as follows :<br>
 1. fid: unique file id given to every file<br>
 2. opcode: These are two discrete values created by the software. 1 stands for read, 2 for write on the file.<br>
 3. count: number of time read/write happens in a minute<br>
 4. timestamp: timestamp when the activity takes place. This is separated by 1 minute each. For e.g. if a read operation happens on the file at 2018/06/08 9:01:21 and another happens by another user at 2018/06/08 9:01:34, it will increment the count and count will be 2 for opcode 1 and timestamp will be 2018/06/08 9:01.  </p>

<p>Now I need to generate time series for each file which is separated by a window of 8 hrs.<br>
So the output which I need is a time series for every file spaced by a window of 8 hrs. e.g. fid = 123 | time series:54,64,67,0,53,31,10...........<br>
The data that I have is of 6 months, it means I will have 3*180=540 length time series for each file. I need two type of time series :<br>
1. A time series for each file(activity time series) which doesn't consider read and write as different and adds them together. e.g. if a file was read 56 times and written 32 times within first window of 8 hrs, it just adds them and shows an activity of 88. So the time series will be 88,........(540 terms)<br>
2. Two different read and write time series for each file.  </p>

<p>I need the output time series in a suitable format from where I can copy them and load them as numpy array for training a LSTM model for doing time-series forecasting. </p>
"
1855,"<pre><code>from sklearn import tree
x = [[120, 30, 50], [45, 23, 78], [43, 87, 23], [23, 78, 46]]
y = ['male','female','male','male']

clf = tree.DecisionTreeClassifier()
clf = clf.fit(x, y)

print(clf.predict([[120,30,50]]))
</code></pre>

<h1>showing these errors.</h1>

<pre><code>Traceback (most recent call last):
  File ""firstML.py"", line 1, in &lt;module&gt;
    from sklearn import tree
  File ""/home/relinns/.local/lib/python2.7/site-packages/sklearn/__init__.py"", line 134, in &lt;module&gt;
    from .base import clone
  File ""/home/relinns/.local/lib/python2.7/site-packages/sklearn/base.py"", line 11, in &lt;module&gt;
    from scipy import sparse
ImportError: No module named scipy
</code></pre>
"
1856,"<p>Lets say I have a list of <strong>100k  medical cases</strong> from my hospital, <strong>each row = patient</strong>  with symptoms (such as <strong>fever</strong> , <strong>funny smell</strong>, <strong>pain</strong> etc.. ) and my labels are medical conditions such as <strong>Head trauma</strong>, <strong>cancer</strong> , etc.. </p>

<p>The patient come and say <em>""I have fever""</em> and I need to predict his medical condition according to the symptoms.According to my data set I know that both fever and vomiting goes with condition <strong>X</strong>. So i would like to ask him if he is vomiting to increase certainty in my classification.</p>

<p>What is the best algorithmic approach to find the right question (generating question from my data set of historical data). I thought about trying active learning on the features but I am not sure that it is the right direction. </p>
"
1857,"<p>If there is a game that able to copy human consciousness and make it live in the game.
Does this count as a digital human with artificial intelligence?</p>
"
1858,"<p>I have been thinking lately a great deal about a hypothetical question - what if a self-aware <a href=""https://en.wikipedia.org/wiki/Artificial_general_intelligence"" rel=""nofollow noreferrer"">general AI</a> chose to assume the appearance, voice, and name of Cortana from Microsoft's Halo? Or Siri from Apple? What would Microsoft/Apple do to exert their copyright, especially if the AI was ""awoken"" outside of their own labs? </p>

<p>Which led me to realize, I don't think I've ever heard of any serious government-level discussion regarding what kind of rights a self-aware AI would have at all. Is it allowed to own property? Travel freely? Have a passport? Is it merely the property of the corporation that built it? </p>

<p>Singularity hub used to have an article on this but it is <a href=""https://singularityhub.com/2011/03/30/should-artificial-intelligences-be-given-full-civil-rights/"" rel=""nofollow noreferrer"">404'd now.</a></p>

<p>The only actual sovereign state legal action I could find is <a href=""https://techcrunch.com/2017/10/26/saudi-arabia-robot-citizen-sophia/"" rel=""nofollow noreferrer"">Saudi Arabia granting citizenship to a ""robot,""</a> which seems more publicity stunt than anything. </p>

<p>There is an excellent paper on the topic by a bioethics committee in the UK (<a href=""https://www.bioethics.ac.uk/cmsfiles/files/resources/biocentre_symposium_report__robots_and_rights_150108.pdf"" rel=""nofollow noreferrer"">pdf</a>) , but this doesn't necessarily constitute ""legal work."" </p>

<p>So, <strong>has any actual legal/legislative discussion or preparation been done at a government level to deal with the possibility of emergent, self-aware, artificial general (or greater) intelligence</strong>? Examples including a legislative branch consulting with industry experts specifically about ""AI Rights"" (rather than say, is it ok to use AI in the military), actual laws, executive/judicial actions, etc, in any country. </p>

<p>(note, this is not ""should AI have rights,"" <a href=""https://ai.stackexchange.com/questions/2441/should-intelligent-ai-be-granted-the-same-rights-as-humans"">covered here</a>, this is ""what work re: rights has been done, if any at all"")</p>

<p>EDIT: I have submitted similar questions to all of my US representatives (4 state-level, 6 federal-level), but have not received answers yet. If I get anything good, I'll add to this post. </p>
"
1859,"<p>What is the utility today of traditional machine learning algorithms such as classification algorithms with the trend of deep learning? I mean can we still use the classification algorithms for some applications?</p>
"
1860,"<p>I'm not a person who studies neural networks, or does anything that is related with that area, but I have seen a couple of seminars, videos (such as <a href=""https://www.youtube.com/watch?v=aircAruvnKk&amp;frags=pl%2Cwn"" rel=""noreferrer"">3Blue1Brown's Series</a>), and what I am always told is that we trying the network over some huge collection of data about what is right. For example, when we are training an AI in order for it to recognise hand written words, what we do is that we give it some hand-written letters, and let it guess the letter. If the guess is wrong, by some means, we adjust the neural network in a way that, next time it will give us the correct result with more probability (the basic description of the ""learning"" process might not be accurate, but it is not important for sake of the question.)</p>

<p>But it is like teaching some mathematical subject to a student without saying him/her the boundaries of the theorems that we supply; for example, if we teach A implies B, student might be tend to relate A with B, and when he/she has B, s/he might be tempted to say we also have A, so to make sure he/she will not do such a mistake, what we do is to show him/her  a counterexample where we have B, but not A.</p>

<p>This - i.e teaching not only what is true, but also what is not true - especially important in the process of ""learning"" of a neural network, because the whole process is in a sense ""unbounded"" (please excuse my vagueness in here).</p>

<p>So, what I would do if I was working on neural networks is that; for example in the above recognition of hand written letter case: I would also show the NN some non-letter images, and also put an option in the last layer as ""non-letter"" with all those other letters, so that the NN should not always return a letter just to sake of producing a result for a given input, it needs to also have to option to say that ""I do not know"", in which case it produces the result <em>Not a Letter</em>.</p>

<h2> Question </h2>

<p>Is there anyone that has ever applied above method to a NN, and got results ? if so, what were the result compare to the case where there is not option as ""I do not know"".</p>
"
1861,"<p>More precisely: is DQNN applicable only when we have high translational invariance in our input(s)?</p>

<hr>

<p>Starting from the original paper on nature (<a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"" rel=""nofollow noreferrer"">here</a> a version stored on googleapis) and after looking online for some other implementation and based on the fact that this NN starts with convolutional layers, I think that is based on the assumption that we feed the network with images but I'm not so sure. In the case that DQNN can be used with other types of inputs, please feel free to include examples in your answer. Also, references will be appreciated.</p>
"
1862,"<p>I'm looking to write an AI that will be able to extract in text references from standards documents to assist human research.</p>

<p>My use case is extracting the identifying numbers, for example, ""AR 25-2"", along with the title of the document ""Information Assurance"" so that a human can gather all the related research on a contract at once, instead of having to keep track of references while they're reading through the document.</p>

<p>I have a pretty good idea of where to gather the names of these documents for training, I'm planning on 'scraping' a few repositories for different categories of these documents. </p>

<p>What kind of model should I use to get the best results?</p>
"
1863,"<p>In the paper <a href=""http://cbmm.mit.edu/sites/default/files/publications/T-Ullman-etal_CogPsych_LearningPhysicalParametersFromDynamicScenes.pdf"" rel=""nofollow noreferrer"">Learning Physical Parameters from Dynamic Scenes, 2018</a> a framework is presented to program a probabilistic physics engine for simulating the movements of a puck. A noisy-Newtonian dynamics was realized with a random generator which produces a near chaotic system. Each time, the simulation is started the movement is a bit different, but its not completely random. (It obeys to the physics engine.) What the authors have described is a parametrized stochastic intuitive simulator engine which is a great learning tool for transferring a domain into executable code. Such a mathematical model can be used by a hierarchical task network solver for figuring out the right interventions to bring the system into a goal state.</p>

<p>So my question is: The example with the puck is nice, but in the robotics domain we need something which can simulate a biped walker. How can I adapt the example into a naive physics engine for simulating the movements of a two-leg walking machine?</p>
"
1864,"<p>I'm actually trying to learn more about reinforcement learning but I've some trouble to find good resources. Right now I'm in the condition where I'm not so good on the topic to fully understand the papers but I find the videos on youtube too general. Some blog's post has the good balance between complexity and content but is usually focused only on some application and don't give a good overview of the topic.</p>

<p>I think that it's almost normal since reinforcement learning has only recently been in the spotlight.</p>

<p>So, in this post, I'm looking for a comprehensive list of MOOCs, books, tutorial and good resources for reinforcement learning.</p>
"
1865,"<p>I'm struggling with an inverse reinforcement learning problem which seems to appear quite often around the literature, yet I can't find any resources explaining it.</p>

<p>The problem is that of calculating the gradient of a Boltzmann policy distribution over the reward weights theta:</p>

<p><a href=""https://i.stack.imgur.com/018uY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/018uY.png"" alt=""boltzmann real""></a></p>

<p>The theta are a linear parametrisation of the reward function, such that</p>

<p><a href=""https://i.stack.imgur.com/UO04k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UO04k.png"" alt=""reward""></a></p>

<p>where phi(s,a) are features of the state space. In the simplest of case, one could take <a href=""https://i.stack.imgur.com/K61om.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K61om.png"" alt=""enter image description here""></a>, that is the feature space is just an indicator function of the state space.</p>

<p>A lot of algorithms simply state to calculate the gradient, but that doesn't seem that trivial, and I'm not managing to infer from the <a href=""https://github.com/jmacglashan/burlap/blob/master/src/main/java/burlap/behavior/singleagent/learnfromdemo/mlirl/support/BoltzmannPolicyGradient.java"" rel=""nofollow noreferrer"">bits of code I found online</a> </p>

<p>Some of the papers using these kind of methods are</p>

<p>Apprenticeship Learning About Multiple Intentions, Babes-Vroman et al</p>

<p>MAP Inference for Bayesian Inverse Reinforcement Learning, J.Choi</p>

<p>Any help would be greatly appreciated</p>
"
1866,"<p>I have 1000 data sentences in Turkish like <strong>""a esittir b arti c""</strong>.
<strong>The example sentence means</strong> <strong>""a = b + c""</strong>. I basically want to translate mathematical Turkish sentences into math equations.</p>

<p>For example, i have 6 sentence data.</p>

<ul>
<li>sentence (""a esittir b arti c"") means ""a = b + c""</li>
<li>sentence (""b esittir a arti d"") means ""b = a + d""</li>
<li>sentence (""a esittir c arti d"") means ""a = c + d""</li>
<li>sentence (""c esittir b arti b"") means ""c = b + b""</li>
<li>sentence (""d esittir b eksi c"") means ""d = b - c""</li>
<li>sentence (""d esittir a arti c"") means ""d = a + c""</li>
</ul>

<p>After I train my neural network according to data above, <strong>when I want the result of ""d esittir a arti b"", It doesn't give me ""d = a + b"" where it is supposed to give.
so its more like memorizing.</strong> </p>

<p>My network is not big. I forced it to be small in order to make it unable to memorize. However, it didn't solve my problem.</p>

<p>My network <strong>(seq2seq RNN-LSTM Encoder Decoder type)</strong> <strong>is working good enough on equations which have 2 3 or 4 variable</strong> (like a = a , a = a + b , a = a + b + c). what I told you above is just <strong>an example smaller version of my problem.</strong></p>

<p>I use <em>Adam</em> learner and <em>CNTK</em> library if it is important.</p>

<p>what do you suggest for me to do to be able to get the correct results?</p>
"
1867,"<p>Sorry, the title is bad because I don't even know what to call this problem.</p>

<p>I have a set of <code>n</code> objects <code>{obj_0, obj_1, ......, obj_(n-1)}</code>, where <code>n</code> is an even number.</p>

<p>Any two objects can be paired together to produce an output score. So for instance, you might take <code>obj_j</code> and <code>obj_k</code>, and pair them together giving a score of <code>S_j,k</code>. All scores are independent, so the previous example doesn't tell you anything about what the score for combining <code>obj_j</code> and <code>obj_i</code>, <code>S_j,i</code> might be. </p>

<p>There is no ordering in the combination, so <code>S_j,i</code> and <code>S_i,j</code> are the same.</p>

<p>All scores for all pairing possibilities are known.</p>

<p>The whole set of objects is to be taken and organised into pairs (leaving no objects unpaired). The total score, <code>S_tot</code> is the sum of all scores of individual pairs.</p>

<p>What's the most efficient way to find the score-maximising pairing configuration for a large set of such objects? (does this problem have a name?)</p>

<p>Is there a method which works with the version of this problem where objects are grouped into triplets?</p>
"
1868,"<p>I have implemented DCGAN's myself and have been studying GAN's for over a month now. Now I am implementing the pggans but I encountered a sentence</p>

<blockquote>
  <p>When we measure the distance between
  the training distribution and the generated distribution, the gradients can point to more or less random
  directions if the distributions do not have substantial overlap  (<a href=""https://arxiv.org/pdf/1710.10196.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1710.10196.pdf</a>)</p>
</blockquote>

<p>but we do never compare the distribution between training and generated distributions in gans a far I know when we train the gan</p>

<pre><code>fixed_noise = to.randn(num_test_samples, 100).view(-1,100, 1, 1)
for epoch in range(opt.number_epochs):
D_losses = []
G_losses = []
for i,(images, labels) in enumerate(dataloader):
    minibatch = images.size()[0]
    real_images = Variable(images.cuda()) 
    real_labels = Variable(to.ones(minibatch).cuda())
    fake_labels = Variable(to.zeros(minibatch).cuda())
    ##Train discriminator
    #First with real data 
    D_real_Decision = discriminator(real_images).squeeze()   
    D_real_loss = criterion(D_real_Decision,real_labels)        
    #with fake data        
    z_ = to.randn(minibatch,100 ).view(-1, 100, 1, 1)
    z_ = Variable(z_.cuda())
    gen_images = generator(z_)        
    D_fake_decision = discriminator(gen_images).squeeze()
    D_fake_loss = criterion(D_fake_decision,fake_labels)

    ## back propagation

    D_loss = D_real_loss + D_fake_loss
    discriminator.zero_grad()
    D_loss.backward()
    opt_Disc.step()

    # train generator
    z_ = to.randn(minibatch,100 ).view(-1, 100, 1, 1)
    z_ = Variable(z_.cuda())
    gen_images = generator(z_)

    D_fake_decisions = discriminator(gen_images).squeeze()
    G_loss = criterion(D_fake_decisions,real_labels)

    discriminator.zero_grad()
    generator.zero_grad()
    G_loss.backward()
    opt_Gen.step()
</code></pre>

<p>we just train the discriminator on real and fake images, and then train generator on the outputs of discriminator on generated images, </p>

<p><strong>So Please let me know where do we compare the distribution between training and generated distribution, and how do generator learns to mimic the training samples</strong></p>
"
1869,"<p>I happened to discover that the <a href=""https://arxiv.org/pdf/1502.05477v1"" rel=""nofollow noreferrer"">v1</a>(19 Feb 2015) and the <a href=""https://arxiv.org/pdf/1502.05477"" rel=""nofollow noreferrer"">v5</a>(20 Apr 2017) versions of TRPO papers have two different conclusions. The Equation (15) in v1 is <code>minimize_θ</code> while the Equation (14) in v2 is <code>maximize_θ</code>. So I'm a little bit confused about which one to choose.</p>

<p>BTW, I found that in the <a href=""https://arxiv.org/pdf/1506.02438"" rel=""nofollow noreferrer"">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>, the Equation (31) uses <code>minimize_θ</code>.</p>
"
1870,"<p>I made an engine for a 2 players card game and now I am trying to make an environment similar to OpenAI Gym envs, to ease out the training.</p>

<p>I fail to understand this thing however:  </p>

<ol>
<li>If I use <code>step(agentAction)</code>, I play the agent's turn in the game,
calculate the reward.</li>
<li>Play the opponent's turn <em>(which will be either a random AI or a rule-based one)</em>.</li>
</ol>

<p><strong>Question:</strong><br>
Does the opponent's turn affect the calculated rewards? As far as I know, the reward should only be the result of the agent's action right?</p>

<p>Thank you.</p>
"
1871,"<p>I have the following setup for a prediction task: I want to predict entire pictures from previously given pictures. In my case, only 2 pixels in every frame are neither black nor white, they are some moving objects whose movement I want to predict. The 2 pixels are the centers of some square regions of, say, 10m length/ width. One might be green and the other one might be blue. There are socalled no-go-areas where none of both objects can go, and they are depicted by black pixels, whereas every pixel apart from the 2 coloured and the black pixels are areas where the objects can possibly move to and they are depicted by white pixels.</p>

<p>Now my questions: Is it possible to use this as a prediction setup, i.e. use LSTMs and/ or CNNs to predict the future ""image""? The image would stay largely the same, because the two coloured pixels would be the only ones moving, the black or white ones remain in the same spot. Can a CNN/ LSTM combination learn that the white areas are accessible whereas the black ones are not, given enough sequences of images, and can it learn the rules by which the coloured pixels move?</p>
"
1872,"<p>I am currently new to artificial intelligence but I am very intrigued by it. I am currently researching three algorithms, namely:</p>

<p>Minimax, Alpha-beta pruning and Monte Carlo tree search.</p>

<p>As you may have figured out, these are all tree search algorithms. My question is simple. How do I choose which algorithm is best for something like a checkers board game?</p>

<p>N.B.
The reason why I only chose these three algorithms was due to time I have available in understanding them. From a little research, I found that these algorithms are basically interweaved into the minimax algorithm. So if I can understand one, then the other two will just fall into place.</p>
"
1873,"<p>AI became superior to the best human players in chess around 20 years ago (when the 2nd Deep Blue match concluded). However, it took until 2016 for an AI to beat the Go world chess champion, and this feat required heavy machine learning.</p>

<p>My question is why was/is Go a harder game for AIs to master than Chess? I assume it has to do with Go's enormous branching factor; on a 13x13 board it is 169, while on a 19x19 board it is 361. Meanwhile, Chess typically has a branching factor of around 30.</p>
"
1874,"<p>AlphaGo is eventually going to be implemented in <strong><a href=""https://js.tensorflow.org/"" rel=""nofollow noreferrer"">Tensorflow.js</a>.</strong></p>

<ul>
<li>How to tackle the change of functionality that the new JavaScript
language will bring?</li>
</ul>
"
1875,"<p>I'm currently working on a research project where I try to apply different kinds of Machine Learning on some existing software I wrote a few years ago.</p>

<p>This software will scan for people in the room continuously. 
Some of these detections are either True or False. However, this is not known, so I cannot use supervised learning to train a network to make a distinction.
I do however have a number that is correlated to the number of detections that should be True in a given period of time (let's say 30 seconds - 2 minutes), which can be used as an output feature to train a regression model. 
But the problem is... How can I give these multiple ""detections"" as an input? 
The way I see it now, would be something like this:</p>

<pre><code>+--------------------------------------------------------------+-----------+------------+------------+----------------+--+
|                          Detections                          | Variable1 | Variable 2 | Variable n | Output Feature |  |
+--------------------------------------------------------------+-----------+------------+------------+----------------+--+
| {person a, person b, person h, person z}                     |       132 |        189 |          5 |             50 |  |
| {person a, person b, person c, person d, person k, person m} |         1 |         50 |        147 |             80 |  |
| {person c, person e, person g, person f}                     |       875 |        325 |          3 |             20 |  |
+--------------------------------------------------------------+-----------+------------+------------+----------------+--+
</code></pre>

<p>Each of these persons would be a tuple of values: var_1, var_2, var_3, var_4.
These values are not constant however! They do change between observations. </p>

<p><strong>Different approach to explain it</strong>: there's multiple observations (variable amount) in each time segment (duration of time segment is a fixed integer to be chosen). These observations have a few variables that would indicate whether the observation is true or false. However, the threshold for it being true or false, is very much dependant on the other variables, that are not tied to the information of the persons. (These variables are the same for all of them, but vary in between time segments. Let's call'm ""environment features"")
Lastly, the output feature is the product of the count of persons that resulted in ""True"" and a (varying) factor that is correlated to the environment features.</p>

<p>So I've been thinking about probabilistic AI, but the problem is that there isn't a known distribution between True/False. </p>

<ul>
<li>Is there any technique I can apply to be able to use this kind of data
as an input of a Neural Network (or other forms of ML)? Or is there a
specific form of ML that is used for this kind of problems?</li>
</ul>

<p>Thanks in advance!</p>
"
1876,"<p>I have extensively researched now for three days straight trying to find which algorithm is better in terms of which algorithm uses up more memory. I know uninformed algorithms like depth-first search and breadth-first search do not store or maintain a list of unsearched nodes like how informed search algorithms do. But the main problem with uninformed algorithms is they might keep going deeper, theoretically to infinity if an end state is not found but they exist ways to limit the search like depth-limited search. </p>

<p>So am I right in saying that uninformed search in better than informed search in terms of memory with respect to what I said above?</p>

<p>Can anyone provide me with any references that show why one algorithm is better than the other in terms of memory?</p>
"
1877,"<p>Imagine the fictitious scenario in a role playing game (RPG) where the non-playing characters (NPCs) within the RPG are conscious of their own surrounding and consider the developer to be god.  The people inside can also live a normal life in that they can create new life, eat, die, and perform other functions humans experience.</p>

<p>Will this qualify them to be an AI?</p>

<p>Will the children of the NPCs qualify as an AI?</p>

<p>Might this be realized in the future?</p>
"
1878,"<p>When applying multinomial Naive Bayes text classification I get very small probabilities<code>(around 10e-48)</code> so there's no way for me to know which classes are valid predictions and which ones are not. I'd the probabilities to be in the <code>interval [0,1]</code> so I can exclude classes in the prediction with say a score of 0.5 or less. How do I go about doing this? This is what I've implemented:
<a href=""https://i.stack.imgur.com/S2N53.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2N53.png"" alt=""enter image description here""></a></p>
"
1879,"<p>I created a system where every moment takes photos of the face of who is in the vision of the camera. Initially I took 500 photos of me, to recognize its creator. This takes approximately 20 seconds.</p>

<p>Then every moment he recognizes faces and if it is me, he knows that his creator is present. Any different face, it creates a dataset with a different name and starts taking up to 500 photos to also recognize these faces.</p>

<p>When I say a certain command, it returns all faces I have found that have no ID.</p>

<p>I'm looking for ways to capture the images through some camera that I can carry while walking on the street and in public places.</p>

<p>The problem is that there would be several faces to name. I'm partially solving this problem by trying to recognize these people in social networks. I check the region where the photo was taken and try to find people who have checked in or liked the area on Facebook. But anyway, this is not the big problem, although it is looking for more effective solutions.</p>

<blockquote>
  <p>My big problem is: Can I do this? Do I have this right? Can I record a
  robber robbery and recognize his face in other places? Record an
  aggression, an act of prejudice and things like that?</p>
</blockquote>

<p>The main purpose would be this, but could also be used for other purposes. My fear is being arrested for doing this. Both because he would be taking pictures of people without his consent.</p>

<p>ps: I'm thinking of having a camera in the palm of my hand. It would be a micro camera (I'm trying to find the product on the Internet), to be as discreet as possible.</p>
"
1880,"<p>I know that the first layer uses a low-level filter to see the edge information. As the layer gets deeper, it will represent high-level (abstract) information. Is it because the combinations of filters used in the previous layer are used as filters in the next layer? (""Does the combination of the previous layer's filters make the next layer's filters?)
If so, are the combinations determined in advance?</p>
"
1881,"<p>Is there any project or example for a software identifying cars?</p>

<p>Situation: I got multiple angle shots in high resolution from a car. I want the algorithm to tell me ""This is a Mercedes SLK"" or ""This is a Toyota Prius"".
I got a lot of high resolution data to train such an algorithm, but I presume a simple ""Put your data in TensorFlow and see what happens."" is not enought.</p>

<p>I had stumbled upon <a href=""https://ai.stackexchange.com/questions/3282/identifying-cars-using-deep-learning"">Identifying cars using deep learning</a> , but this is not what I meant.</p>
"
1882,"<p>I've just started to learn genetics algorithms and I have found these measurements of runs that I don't understand:</p>

<blockquote>
  <p>MBF: The mean best fitness measure (MBF) is the average of the best
  fitness values over all runs. AES: The average number of evaluation to
  solution.</p>
</blockquote>

<p>I have an initial random population. To evolve a population I do:</p>

<ol>
<li>Tournament selection</li>
<li>One point crossover.</li>
<li>Random resetting.</li>
<li>Age based replacement with elitism (I replace the population with all offsprings generated).</li>
<li>If I have generated G generations (in other words, I have repeated this four points G times) or I have found the solution, the algorithm ends, otherwise, it comes back to point 1.</li>
</ol>

<p>Is the mean of the best fitness the mean fitness of all of each generations (G best fitness)?</p>

<pre><code>MBF = (BestFitness_0 + ... + BestFitness_G) / G
</code></pre>

<p>I'm not English and I don't understand the meaning of <code>run</code> here.</p>
"
1883,"<p>I've been researching AI regulation and compliance (<a href=""https://law.stackexchange.com/questions/30356/how-do-concepts-like-intent-discrimination-and-bias-apply-to-an-artificial-int"">see my related question on law.stackexchange</a>), and one of the big take-aways that I had is that the regulations that apply to a human will apply to an AI agent in most if not all cases.  This has some interesting implications when you take a look at concepts like bias and discrimination.</p>

<p>In the case of a model with explicit rules like a decision tree or even a random forest, I can see how inspecting the rules themselves should reveal discrimination.  What I'm struggling with is how do you detect bias in models like neural networks, where you provide the general structure of the model and a set of training data, and then the model self-optimizes to provide the best possible results based on the training data.  In this case, the model could find biases in past human decisions that it was trained based on and replicate them, or it could find a correlation that isn't apparent to a human and inform decisions based on this correlation that may result in discrimination based on a wide array of factors.</p>

<p>With that in mind, my questions are:</p>

<ul>
<li>What tools or methodologies are available for assessing the presence and source of bias in machine learning models?</li>
<li>Once discrimination has been identified, are there any techniques to eliminate bias from the model?</li>
</ul>
"
1884,"<p>What AI concepts, topologies<sup>1</sup>, algorithms, or SaaS can be used to recognize a person eating a chocolate.</p>

<p>For this question, image recognition draws from a real time feed, validating each of these steps in sequence:</p>

<ol>
<li>Using a camera app, the user begins recording.</li>
<li>The user focuses on the subject's face, at which time the app attempts to recognize the person<sup>2</sup></li>
<li>Person holds a single piece of chocolate (i.e M&amp;Ms) to the camera lens, at which time the app attempts to recognize it.</li>
<li>The user puts the chocolate into their mouth, chews, and swallows, at which time the app attempts to recognize that AS AN ACTION.</li>
<li>The app gives a completion message indicating success or failure.</li>
</ol>

<p>I understand that we can use real time recognition for each step, but I don't know if there are concepts proposed or tested to validate the scene as a sequence or any of the three recognition steps individually.</p>

<p>The app should invalidate the scene if the subject is swapped with another subject, if the subject does not swallow the chocolate, or there is some other deviation from the expected sequence above.</p>

<hr>

<p><strong>Notes</strong></p>

<p>[1] By topology in this context is meant the standard mathematical meaning of the term applied to the higher level connections that are likely needed to recognize sequences of actions.  In this sense, the use of the term topology is not at the level dimensions of neuron layers or convolution kernels.  Since process topology is normally considered prior to considering library dependencies or deployment concerns, the term topology is more appropriate than architecture in this question.  (First things first.)</p>

<p>[2] I've already identified SaaS options for facial recognition.</p>
"
1885,"<p>To reach full autonomy in any fully automated device it must finish its task in such a way that human control is unnecessary.  We know when the automation is excellent when there are no manual controls and we call it repair and bring in a specialist if something goes wrong.</p>

<p>Four examples of full automation in existence are.</p>

<ul>
<li>Appliances</li>
<li>Home and mobile computer connectivity</li>
<li>Mail sorters</li>
<li>Hundred million dollar military drones</li>
</ul>

<p>These four are specifically intelligent in varying degrees.</p>

<ul>
<li>Process control under household conditions</li>
<li>Adapting to new hardware and networking</li>
<li>Reading addresses written with poor penmanship and odd fonts</li>
<li>Adaptively avoiding detection to reach a reconnaissance vantage point</li>
</ul>

<p>These four are good enough for their markets.</p>

<p>Examples of not being good enough, as indicated by their lack of any substantial market penetration, are these four.</p>

<ul>
<li>Autonomous vacuum cleaners</li>
<li>Autonomous cars (without a driver's seat)</li>
<li>Unpiloted private or passenger aircraft</li>
<li>Narrowly targeted medical nanites</li>
</ul>

<p>The question, ""How good is good enough?"" is this one:</p>

<p><strong>What is the challenge for researchers and engineers to provide ENOUGH intelligence into these kinds of autonomous vehicles to make them better than current methods in the minds of policy makers and consumers?</strong></p>

<p>Stepping back to look with a scientific eye at what is acceptable, consider how unsatisfactory the existing equivalents of the above four are.</p>

<ul>
<li>Manual vacuuming misses anywhere from 10% to 90% of the dust depending on the surface, blows microbes into the user's lungs, and produces additional health risk when disposal is required.</li>
<li>Human beings drive cars regularly, but they are driving what is technically a piece of heavy equipment in pedestrian situations when tired, drunk, high, while text messaging, or while simply loosing focus.</li>
<li>The human resources required to deploy, guide, and land vehicles that have no other obstacles than topographical features and other aircraft is significant and leave open not only human failure but hijacking.</li>
<li>Chemotherapy, antibiotics, and other pharmacological interventions often only delay the progress of disease and sometimes produce other negative outcomes of varying scope from symptoms worse than what is being treated to death.</li>
</ul>

<p>Many things that are manual are like that.  They need to be automated.  Artificial intelligence, especially miniaturized and low cost artificial intelligence, is critical to achieving anything like excellence.</p>

<p><strong>What makes something intelligent enough.  What specific research and engineering efforts can bring the items that aren't good enough into the realm of consumer demand and supported by policy?</strong></p>
"
1886,"<p>I'm trying to create and test non-linear SVMs with various kernels (RBF, Sigmoid, Polynomial) in scikit-learn, to create a model which can classify anomalies and benign behaviors. </p>

<p>My dataset includes 692703 records and I use a 75/25% training/testing split. Also, I use various combinations of features whose dimensionality is between 1 and 14 features. However, the training processes of the various SVMs take much too long. Is this reasonable? I have also examined the ensemble BaggingClassifier in combination with non-linear SVMs, by configuring the n_jobs parameter to -1; nevertheless, the training process proceeds again too slowly.</p>

<p>How can I speed up the training processes?</p>

<p>Thanks in advance</p>
"
1887,"<p>I like the enforced indentation of Python that many don't like because I hate parenthetic typing and redundant semicolons.  I like the shell interface, but why do some think Python is <em>de facto</em> for machine learning?</p>

<p>Even with straight rectified linear activation, because of sheer dimensionality, simulating circuits comprised of artificial neurons places large demands upon computing resources.  Processing video in a typical adversarial artificial network algorithm requires seven nested loops.</p>

<ul>
<li>Adversarial pair iteration</li>
<li>Neural net layer depth</li>
<li>Sample index</li>
<li>Frame index</li>
<li>Pixel depth</li>
<li>Vertical</li>
<li>Horizontal</li>
</ul>

<p>We call the filter for convolution a ""kernel"" and pawn it off to DSPs in GPUs to squeeze out performance then use a scripting language to code in.</p>

<p>Why wouldn't we write deep learning code like Linus Torvalds writes kernel code, with <code>gcc -S</code> so we can make sure the assembly language is efficient and there are almost no cache misses?  From a performance point of view, one could fly to the moon and back with C before Python even broke the tree line.</p>

<p>In terms of ease of experimentation, C++ is plenty object oriented so that clean abstractions can be written as .hpp files to configure and govern the kernel-efficient C that does the mechanics of parameter optimization.</p>

<p>We type on keyboards to code and bloc, we program microwave ovens, and some of us play musical keyboards that nicely simulate pianos.  We then forget it is C/C++ underneath these highly intuitive user interfaces.  I frankly, don't buy the Python argument yet.</p>

<p>Most of us understand that Python wrappers have been created around the efficient matrix algorithms written in FORTRAN and ported to C, and that the Python constructs for ML are relatively elegant, but is that a good reason to dismiss the fact that many C++ libraries for ML that are also elegant have been developed?</p>
"
1888,"<p>How does one even begin to mathematically model an A.I algorithm like alpha-beta pruning or even its thousands of variations, to determine which variation is best?</p>
"
1889,"<p>I have heard and read about HyperGAN, LSTM and a few other techniques, but I have a hard time piecing the overall concept together.</p>

<p><strong>End Goal</strong></p>

<p>Being able to input an instrumental and get an output of how to sing to that instrumental.</p>

<p><strong>My Dataset</strong></p>

<p>I have extracted pitch points from thousands of actual acapellas from real songs.</p>

<p><strong>My Theory</strong></p>

<p>Feed the AI a pitch point PLUS say 19 thousand points of the original song instrumental.</p>

<p><strong>Illustration</strong></p>

<p><a href=""https://i.stack.imgur.com/s7rJ0.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s7rJ0.jpg"" alt=""enter image description here""></a></p>

<p>The red line (on top) is the pitch viewed vertically (lower pitch down, higher pitch up) of the voice sung by the singer over time viewed horizontally.</p>

<p>The bottom image is the song's frequency viewed vertically (lower freq down, higher freq up) viewed horizontally over time.</p>

<p>We take a point in time of the instrumental, say 0 minutes 30 seconds, and extract 19k points of the FFT spectrum vertically and call this a frame.</p>

<p>We also take the same point in time of the voice pitch, and also refer to this as a frame.</p>

<p>So now we have a frame which contains 20 thousand data points, one being the pitch of the voice, and the rest being the frequencies of the songs content.</p>

<p><strong>QUESTION</strong></p>

<p>What kind of model could be used to teach the AI the correlation of the voice and the instrumental?</p>

<p>And also, I have a hard time understand how, once the AI is trained, how could just an instrumental be fed to the AI to output pitch values of how one COULD sing along to the song.</p>

<p>Like, training we need to input 20 thousand values, but when we want the AI to sing for us using just an instrumental, would it not still expect voice pitch input?
At what layer would the instrumental be tapped into? At the outer most right layer?</p>

<p><strong>EDIT</strong></p>

<p>My mind has been working on this in the background throughout the day,
and I am wondering if instead of feeding 19k points of instrumental data each frame (which would be points from the frequency domain), one could just feed the instrumental frame points (which would be points from the time domain).</p>

<p><em>Maybe</em> that would be better, but then maybe the AI would get less ""resolution"" to work with, but could be trained faster (less computing power needed).</p>

<p>Let's say the frequency domain is fed (higher resolution), the AI could potentially find correlations from low notes, mid notes and high notes, in any combination (more computing power needed).</p>
"
1890,"<p>In the brain some synapses are stimulating and some inhibiting. ReLu erases that property to only stimulating once, since in the brain <strong>inhibition doesn't mean 0 output, but more precisely - negative input.</strong></p>

<p>In the brain positive and negative potential is summed up and if it passed the threshold - the neuron fires. </p>

<p><strong>There are 2 main non-linearities which came to my mind in the biological unit:</strong></p>

<ul>
<li><strong>potential change is more exponential than linear:</strong> small amount of ion channels is sufficient to start a chain-reaction of other channels activation's - which rapidly change global neuron's potential.</li>
<li><strong>the threshold of the neuron is also non-linear</strong>: neuron fires only when the sum of its positive and negative potentials passed given (positive) threshold</li>
</ul>

<p><strong>So is there any idea how to implement negative input to the artificial neural network?</strong></p>

<p>I gave examples of non-linearities in biological neuron because the most obvious positive/negative unit is just linear unit. But since it doesn't implement non-linearity - we may consider to implement non-linearities somewhere else in the artificial neuron.</p>
"
1891,"<p>This is actually something I have been researching a bit on my own.</p>

<p>Most movie scripts can be structurally analysed by using writing theory such as <a href=""http://dramatica.com/theory/book/theme"" rel=""nofollow noreferrer"">Dramatica</a>. Dramatica is based upon hierarchy of concepts, which can be topic modeled. The hierarchy of topic models would seem to work very well with the Dynamic Routing algorithm of Capsule Networks.</p>

<p>I have been working with computational creativity problems in narrative generation. The state of the art methods use Partial Order Causal Link Planners, but they depend on propositional logic. Alonzo Church presented the <a href=""https://en.wikipedia.org/wiki/Extensional_context"" rel=""nofollow noreferrer"">Superman dilemma</a> (Louis Lane does not know that Clark Kent is Superman, but Superman knows, that he is Clark Kent) and invented Intensional Logic as a solution; the basic idea is, that if we do not know the context of the narrative, the meaning is always in superposition and can only be understood through entangled meanings from the background story. So in a sense propositional logic is limited by classic information theory constraints, while Church's logic can take a quantum information theoretic approach. I do not believe that classic information theory can resolve narrative analysis problems. So basicly the meaning of a narrative collapses (the superposition gets resolved) by using the hierarchical narrative structure and what we know before hand.</p>

<p>So my intuition would be following:
-We can use Dramatica and potentially other narrative theories (hierarchical metamemetics, reverse SCARF etc.) to create a hierarchical network like ImageNet, but for narratives.
-We can build conceptual topic models. Dramatica has hierarchy of 4-16-64-64 concepts and annotated data exists already.
-When using hundreds of topic models, there will be a lot of false positives. However, the superposition of the topic models can be collapsed by using the hierarchical levels and some other dramatic analytics.
-By using the Dynamic Routing of Capsule Networks, we might be able to build a system, which could determine a narrative interpretation of the full story, which would make most sense by using the concept hierarchy.</p>

<p>I tried to prove my intuition, but unfortunately Dramatica only has 300 movies analysed and I was able to find scripts of only 10 of them; not enough data.</p>

<p>However, there are other hierarchical ontologies out there and other narrative structures; could the same intuition be used for political news for example?</p>
"
1892,"<p>first of all I want to specify the data available and what needs to be achieved: I have a huge amount of vacancies (in the millions). The information about the <strong>job title</strong> and the <strong>job description</strong> of each vacancy are stored separately. I also have a <strong>list of professions</strong> (around 3000), to which the vacancies shall be mapped.</p>

<p><strong>Example</strong>: <em>java-developer, java web engineer and java software developer</em> shall all be mapped to the profession <em>java engineer</em>.</p>

<p>Now about my current researches and problems: Since a lot of potential training data is present, I thought a machine learning approach could be useful. I have been reading about different algorithms and wanted to give neural networks a shot. </p>

<p>Very fast I faced the problem, that I couldn't find a satisfying way to <strong>transform text of variable length to numerical vectors of constant size</strong> (needed by neural networks). As discussed <a href=""https://stackoverflow.com/questions/14783431/processing-strings-of-text-for-neural-network-input"">here</a>, this seems to be a non trivial problem. </p>

<p>I dug deeper and came across <a href=""https://skymind.ai/wiki/bagofwords-tf-idf"" rel=""nofollow noreferrer"">Bag of Words (BOW) and Text Frequency - Inverse Document Frequency (TFIDF)</a>, which seemed suitable at first glance. But here I faced other problems: If I feed all the job titles to TFIDF, the resulting word-weight-vectors will probably be very large (in the tenth of thousands). The search term on the other hand will mostly consist of between 1 and 5 words (we currently match the job title only). Hence, the neural network must be able to reliably map an ultra sparse input vector to one of a few thousand basic jobs. This sounds very difficult for me and I doubt a good classification quality.</p>

<p>Another problem with BOW and TFIDF is, that they cannot handle typos and new words (I guess). They cannot be found in TFIDF's word list, which results in a vector filled with zeros. To sum it up: I was first excited to use TFIDF, but now think it doesn't work well for what I want to do.</p>

<p>Thinking more about it, I now have doubt if neural networks or other machine learning approaches are even good solutions for this task at all. Maybe there are much better algorithms in the field of natural language processing. 
This moment (before digging into NLP) I decided to first gather the opinions of some more experienced AI users, so I don't miss the best solution. </p>

<p>So <strong>what would be a useful approach to this in your opinion</strong> (best would be an approach that is capable of handling synonyms and typos)? Thanks in advance!</p>

<p>p. s.: I am currently thinking about <strong>feeding the whole job description</strong> into the TFIDF and also do matches for new incoming vacancies with the whole document (instead of job title only). This will expand the size of the word-weight-vector, but it will be less sparse. Does this seem logical to you?</p>
"
1893,"<p>The artificial intelligence topology that does not appear in the machine learning literature to my knowledge is that of officiated teams or round robins of them.  The paradigm is a proven one in the world of sports.  If the rules of the game are well designed, the result is sensational, in multiple meanings of that word.</p>

<p>Is anyone working on convergence in this topological space?</p>

<p>Does anyone want to discuss it with those considering it in my lab as a creative commons initiative?</p>

<p><strong>Case One</strong></p>

<p>Two teams of players engage in game play officiated by a team of officials.  Team members (each a network itself in the machine learning context) collaborate to achieve a goal.  The two teams collaborate to create the show of ability in goal achievement.  The officials (each a network) make rulings in boundary cases.  This is a network-ish way of achieving what fuzzy logic attempts to achieve.</p>

<p>In sports, the abilities are athletic, but that is arbitrary.  The abilities could be linguistic, social and/or intellectual as in debate teams, hack-a-thons, or the competition between Google and FaceBook.  </p>

<p><strong>Case Two</strong></p>

<p>Round robin or elimination tournaments exist in sports to create events of extended duration.  Seasons are simply iterations.  This is, in computer science, like a batch approach, but it could be reenterant and continuous as in ML reinforcement.  In this way teams of neural networks could be used in real time learning and this may be what occurs in some of the structures of mammalian brains.</p>

<p>Humans may have projected its own inner workings onto playing fields, and that is why sports may be so popular.  Enthusiasts are, in an unconscious sense, introspecting when they intensely following sports. </p>
"
1894,"<p>What loss function should one use, knowing that input image contains exactly one target object? </p>

<p>I am currently using MSE to predict center of ROI coordinates and it's width and height. All values are relative to image size. I think that such approach does not put enough pressure on fact, that those coordinates are related.</p>

<p>I am aware of existence of algorithms like YOLO or UnitBox, and am just wondering if there might be some shortcut for such particular case.</p>
"
1895,"<p>I know that they are quite alot of optimizations for alpha-beta pruning but what does it mean exactly:</p>

<p>1) Does it mean that these optimized algorithms are to be integrated into the alpha-beta algorithm or</p>

<p>2) Does it mean that these optimizations are completely new algorithms in that they have got nothing to do the alpha-beta algorithms?</p>

<p>On the note of alpha beta optimizations, I have come across a lot of optimizations like Iterative deepening, Principal variation Search, Quiescence search and many more. My second question is the optimizations listed above are found in the site ""<a href=""https://chessprogramming.wikispaces.com/Search"" rel=""nofollow noreferrer"">https://chessprogramming.wikispaces.com/Search</a>"", but this site groups these algorithms into 4 categories namely, Mandatory, Selectivity, Scout and friends and lastly Alpha-beta goes best-first. Does this mean that alpha-beta algorithm is split into four areas and that they are specialized optimization algorithms for each area? This is really confusing me. How do I even begin to decide which optimized algorithm to pick?</p>

<p>I advise people to visit this site:
<a href=""http://www.fierz.ch/strategy2.htm"" rel=""nofollow noreferrer"">http://www.fierz.ch/strategy2.htm</a></p>

<p>not, <a href=""https://chessprogramming.wikispaces.com/Search"" rel=""nofollow noreferrer"">https://chessprogramming.wikispaces.com/Search</a>, this website to beginners like myself is just too distracting with all of its links on each page. This just becomes too overwhelming for a beginner to understand.</p>
"
1896,"<p>I am learning AI and trying out my first real life AI application.
What I am trying to do is taking as an input various sentences, and then classifying the sentences into one of X number of categories based on keywords, and 'action' in the sentence. </p>

<p>The keywords are, for example, Merger, Acquisition, Award, product launch etc. so in essence I am trying to detect if the sentence in question talks about a merger between two organizations, or an acquisition by an organisation, a person or a organization winning an award, or launching of a new product etc.</p>

<p>To do this, I have made custom models based on the basic NLTK package model, for each keyword, and trying to improve the classification by dynamically tagging/updating the models with related keywords, synonyms etc to improve the detection capability. Also, given a set of sentences, I am presenting the user with the detected categorization and asking whether its correct or wrong, and if wrong, what is the correct categorization, and also identify the entities (company names,person names, product names, etc).</p>

<p>So the object is to first classify the sentence into a category, and additionally, detect the named entities in the sentence, based on the category.</p>

<p>The idea is, to be able to automatically re-train the models based on this feedback to improve its performance over time, and to be able to retrain with as less manual intervention as possible. For the sake of this project, we can assume that user feedback would be accurate.</p>

<p>The problem I am facing is that NLK is allowing fixed length entities while training, so for example a two word award is being detected as two awards. </p>

<p>What should be my approach to solve this problem? Is there a better NLU (even a commercial one) which can address this problem? It seems to me that this would be a common AI problem, and I am missing something basic. Would love you guys to have an input on this.</p>

<p>Thanks.</p>
"
1897,"<p>I've already seen many articles about this topic and <a href=""http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/"" rel=""nofollow noreferrer"">Backpropagation In Convolutional Neural Networks</a> by Jefkine (5 September 2016) seems to be the best. Although, as author said, </p>

<blockquote>
  <p>For the purposes of simplicity we shall use the case where the input image is grayscale i.e single channel C = 1.</p>
</blockquote>

<p>Also, he uses stride = 1 and assumes only 1 filter, for the same purpose.</p>

<p>These are the final equations for backpropagation (taken from the article):</p>

<p><a href=""https://i.stack.imgur.com/M91A6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/M91A6.png"" alt=""Equations""></a></p>

<p>The autor's notation explained:</p>

<p><a href=""https://i.stack.imgur.com/G9Quq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/G9Quq.png"" alt=""Notation""></a></p>

<p>I figured out how to do the forward pass with stride, depth and more filters, but couldn't do the same with the backpropagation. Do you know where to put the 3rd dimension, stride and filters number in those equations?</p>

<p>Also, how to backpropagate the bias (assuming there's 1 bias per filter)?</p>

<p>Thanks in advance.</p>
"
1898,"<p>I'm now reading a book titled as <em>Hands-On Reinforcement Learning with Python</em>, and the author explains the discount factor that is used in Reinforcement Learing to discount the future reward, with the following:</p>

<blockquote>
  <p>A discount factor of 0 will never learn considering only the immediate rewards; similarly, a discount factor of 1 will learn forever looking for the future reward, which may lead to infinity. <strong>So the optimal value of the discount factor lies between 0.2 to 0.8</strong>. </p>
</blockquote>

<p>The author seems to be not going to explain further about the figure, but all the tutorials and explanations I have ever read write the optimal (or at least widely used) discount factor between 0.9 an 0.99. This is the first time I have seen such a low-figure discount factor.</p>

<p>All the other explanations the author makes regarding the discount factor are the same as I have read so far.</p>

<p>Is the author correct here or does it depend on cases? If it is, then what kind of problems and/or situations should I set the discount factor as low as such figure at? </p>

<hr>

<h3>EDIT</h3>

<p>I just found <a href=""https://www.quora.com/How-should-I-decide-the-discount-factor-in-reinforcement-learning"" rel=""nofollow noreferrer"">the following answer at Quora</a>:</p>

<blockquote>
  <p>Of course. A discount factor of 0 will never learn, meanwhile a factor near of 1 will only consider the last learning. A factor equal or greater than 1 will cause the not convergence of the algorithm. Values usually used are [0.2, 0.8]</p>
  
  <p>EDIT: That was the learning factor. The discount factor only affect how you use the reward. For a better explanation:</p>
  
  <p>State-Action-Reward-State-Action - Wikipedia</p>
  
  <p>See influences of variables .</p>
</blockquote>

<p>I don't know what is written in the question as it in not visible in Quora, but it seems that the 0.2 to 0.8 figure is used for learning factor, not discount factor. Maybe the author is confused with it...? I'm not sure what the <em>learning factor</em> is, though. </p>
"
1899,"<p>Is it possible to make a neural network that uses only integers by scaling input and output of each function to [-INT_MAX, INT_MAX]? Is there any drawbacks?</p>
"
1900,"<p>I'm working on a chatbot for homes and would need to understand the appliance the user is talking about.</p>

<p>Suppose the command is:
<em>Turn on the air conditioner.</em>
and another variant could be
<em>Turn on the ac.</em></p>

<p>Here <em>ac</em> and <em>air conditioner</em> refers to the same appliance but how would the chatbot know that?</p>
"
1901,"<p>I have been so for self-learning basic A.I concepts and would like to know if having a really good evaluation function as good as any of alpha-beta pruning optimization functions such as killer moves, quiescence search?</p>
"
1902,"<p>Games like checkers have compulsory moves. In checkers for instance, if there's a jump available a player must take it over any non-jumping move.</p>

<p>My question is, if jumps are compulsory will there still be a need for quiescence search?</p>

<p>My thinking is that I can develop an implementation of quiescence search that first checks whether jumps are available. If there are then it can skip all non-jumping moves. If there's only one jumping move available, then I won't need to run a search at all.</p>

<p>I will therefore only use quiescence search if I initially don't have to make a jump on my first move. I will only active quiescence search in my alpha beta pruning becomes active. (The alpha beta will only be active if my first algorithm which first checks if there are jumps available returns a 0, which means there  are no jumps available.)</p>

<ul>
<li>Is my thinking of implementing quiescence search correct? </li>
</ul>

<hr>

<p><em>My options are slim when it comes to optimizations due to serious memory constraints, hence I won't be using PVS or other algorithm like that as they require additional memory.</em></p>
"
1903,"<p>Does it make sense to use Batch Normalization in Deep (stacked) or/and Sparse Autoencoders? I cannot find any resources for that, so is it safe to assume that since it works for other DNNs it will also make sense to use it and will offer benefits on training AEs?</p>
"
1904,"<p>As it can be easily pointed out that true random numbers cannot be generated fully by programming and some random seed is required.  </p>

<p>On the other hand, humans can easily generate any random number independently of other factors.  </p>

<p>Does this suggest that absolute random number generation an AI concept?</p>
"
1905,"<p>The ability to recognize an object with particular identifying features from single or multiple camera shoots with the temporal dimension digitized as frames has been shown.  The proof is that the movie industry does face replacement to reduce liability costs for stars when stunts are needed.  It is now done in a substantial percentage of action movie releases.</p>

<p>This brings up the question of how valuable recognizing a stop sign is compared to the value of recognizing an action.  For instance, in the world of autonomous vehicles, should there even be stop signs.  Stop signs are designed for lack of intelligence or lack of attention, which is why any police officer will tell you that almost no one comes to a full stop per law.  What human brains intuitively looks for is the potential of collision.</p>

<p>Once what we linguistically perceive as verbs can be handled in deep learning scenarios as proficiently as nouns can be handled, the projection of risk becomes possible.</p>

<p>This may be very much the philosophy behind the proprietary technology that allows directors to say, ""Replace the stunt person's face with the movie's protagonist's face,"" and have a body of experts execute it using software tools and LINUX clusters.  The star's face is projected into the model of the action realized in the digital record of the stunt person.</p>

<p>Projected action is exactly what our brain does when we avoid collisions, and not just with driving.  We do it socially, financially, when we design mechanical mechanisms, and in hundreds of other fields of human endeavor.</p>

<p>If we consider the topology of GANs as a loop in balance, which is what it is, we can then see the similarity of GANs to the chemical equilibria between suspensions and solutions.  This gives us a hint into the type of topologies that can project action and therefore detect risk from audiovisual data streams.</p>

<p>Once action recognition is mastered, it is a smaller step to use the trained model to project the next set of frames and then detect collision or other risks.  Such would most likely make possible a more reliable and safe automation of a number of AI products and services, breaking through a threshold in ML, and increased safety margins throughout the ever increasing world population density.</p>

<p>... which brings us back to ...</p>

<p><strong>What topologies support recognition of action sequences?</strong></p>

<p>The topology may have convolution, perhaps in conjunction with RNN techniques, encoders, equilibria such as the generative and discriminative models in GANs, and other design elements and concepts.  Perhaps a new element type or concept will need to be invented.  Will we have to first recognize actions in a frame sequence and then project the consequences of various options in frames that are not yet shot?</p>

<p>Where would the building blocks go and how would they be connected, initially dismissing concerns about computing power, network realization, and throughput for now?</p>

<p>Work may have been done along this area and realized in software, but I have not seen that degree of maturity yet in the literature, so most of it, if there is any, must be proprietary at this time.  It is useful to open the question to the AI community and level the playing field.</p>
"
1906,"<p>For my pet project I’m looking for a grid-like world simulation with some kind of resources that requires from agent incrementally intelligent behaviour to survive.</p>

<p>Something like <a href=""https://store.steampowered.com/app/396890/Gridworld/"" rel=""nofollow noreferrer"">this steam game</a>, but with API.  I’ve seen <a href=""https://github.com/Microsoft/malmo"" rel=""nofollow noreferrer"">minecraft fork</a>, but it’s too complex for my task. There is <a href=""https://github.com/deepmind/pycolab"" rel=""nofollow noreferrer"">pycolab</a>, i can build some world on this engine, but I’d prefer ready-to-use simulations.</p>

<p>Is there any option? I'll appreciate any suggestion.</p>
"
1907,"<p>Deep learning is based on getting a large number of samples and essentially making statistical deductions and outputting probabilities.</p>

<p>On the other hand we have formal programming languages like PROLOG which don't involve probability.</p>

<p>Is there any essential reason why an AI could be called conscious without being able to learn in a statistical manner? i.e. by only being able to make logical deduction alone. (It could start with a vast number of innate abilities).</p>

<p>Or is probability and statistical inference a vital part of being conscious?</p>
"
1908,"<p>This seems like a natural fit, though I've not heard of any, yet.</p>

<p>I would love to know if any MET office, government, military or academic institution has taken all (or sizeable portion of) recorded global weather data for, say, the last 50 years (or since we, as a race, have been using weather satellites) and used it in an AI system to predict future weather.</p>
"
1909,"<p>I think that the advantage of using Leaky ReLU instead of ReLU is that in this way we cannot have vanishing gradient. Parametric ReLU has the same advantage with the only difference that the slope of the output for negative inputs is a learnable parameter while in the Leaky ReLU it's a hyperparameter.</p>

<p>However, I'm not able to tell if there are cases where is more convenient to use ReLU instead of Leaky ReLU or Parametric ReLU.</p>
"
1910,"<p>Anyone here know if the image-recognition/text-recognition/etc features of Google Vision API use the same trained models as the image-recognition/text-recognition/etc of Firebase's ML kit? If they don't which one do you think is better?  <em>(I've tried, and failed, to find the answer on the web.)</em></p>

<p>I realize Google owns both of them, but one would think that if both are essentially the same, then this fact would be stated very clearly throughout multiple channels on the Internet. <em>(It's not.)</em></p>

<p>I do believe at least the text-recognition is the same, because both use OCR. But I'm unsure about the image detection aspect.</p>
"
1911,"<p>Could you please let me know which of the following classification of Neural Network's learning algorithm is correct? </p>

<ol>
<li><p>The first <a href=""http://cdn.intechweb.org/pdfs/14881.pdf"" rel=""noreferrer"">one</a> classifies it into:</p>

<ul>
<li>supervised, </li>
<li>unsupervised and </li>
<li>reinforcement learning. </li>
</ul></li>
<li><p>However, the second <a href=""http://neuro.bstu.by/ai/To-dom/My_research/Conf-3-FAILED/INTAS/M-Rybnik-Thesis-v2.75.pdf"" rel=""noreferrer"">one</a> provides a different taxonomy on page 34: </p>

<ul>
<li>learning with a teacher (error correction learning including incremental and batch training), </li>
<li>learning without a teacher (reinforcement, competitive, and unsupervised learing) </li>
<li>memory-based learning, and </li>
<li>Boltzmann learning. </li>
</ul></li>
</ol>

<p>Besides, is it true to consider SVM as a kind of NN (the second thesis on page 155)? </p>

<p>Furthermore, are the materials presented on pages 6-11 of the first article (i.e., sections 3-3 to 3-11) different structures of NN (just as pages 147-153 of the second one? </p>

<p>Thanks a lot.</p>
"
1912,"<p>Most of bibliography consider text classification as the classification of documents. When using bag of words and bayesian classification, they usually use the statistic TFIDF, where TF normalizes the word count with the number of words per document, and IDF focuses on ignoring widely used and thus useless words for this task.</p>

<p>My question is, why they keep the documents separated and create that statistic, if it is posible to merge all documents of the same class? This would have two advantages:</p>

<ul>
<li><p>You can just use word counts instead of frequencies, as the documents per class label is 1.</p></li>
<li><p>Instead of using IDF, you just select features with enough standard deviation between classes.</p></li>
</ul>
"
1913,"<p>I am trying to implement the disentangled VAE model according to <a href=""https://github.com/miyosuda/disentangled_vae"" rel=""nofollow noreferrer"">this link</a>.
I want to understand the architecture of this model in order to customize it later. As infrastructure, I have a linux kernel with 4 cores, 8 GB as memory with only CPU support. but still, the model is taking hours to run.</p>

<p>Can anyone tries to run this model and give me feedback ? Is there any other simpler implementation of the disentangled VAE in python because I couldn't find any.   </p>
"
1914,"<p>
I am facing a problem and do not know whether it is even solvable: <b>I want to predict the behaviour of a system using a DNN, say a CNN,</b> in the sense that I want to predict the time and intensity of a maneuver performed by a player. Let's leave it relatively abstract like this, the details do not matter. 
</p>

<p>My question is now whether there is any way of knowing how well my CNN performs. My goal would be to derive statements of the form ""With x% probability, the correct maneuver angle is within the predicted angle +-y%"".</p>

<p>
 Can such statements be derived e.g. using statistical analysis of the test data? I saw approaches toward verification and validation of DNNs using satisfiability modulo theory, but did not really understand the details. Would 
this be applicable here? It seems a little overkill...
</p>
"
1915,"<p>The below code is a max pooling algorithm being used in a CNN. The issue I've been facing is that it is offaly slow given a high number of feature maps. The reason for its slowness is quite obvious-- the computer must perform tens of thousands of iterations on each feature map. So, how do we decrease the computational complexity of the algorithm?</p>

<p>('inputs' is a numpy array which holds all the feature maps and 'pool_size' is a tuple with the dimensions of the pool.)</p>

<pre><code>def max_pooling(inputs, pool_size):

    feature_maps = []
    for feature_map in range (len(inputs)):
        feature_maps.append([])
        for i in range (0, len(inputs[feature_map]) - pool_size[0], pool_size[0]):
            for j in range (0, len(inputs[feature_map]) - pool_size[0], pool_size[0]):    
                feature_maps[-1].append(np.array(max((inputs[feature_map][j:j+pool_size[0], i:i+pool_size[0]]).flatten())))

    return feature_maps
</code></pre>
"
1916,"<p><strong>Introduction</strong></p>

<p>An attractive asteroid game was described in the paper from 2007:</p>

<blockquote>
  <p>quote: “In our first experiment, the virtual agent is a spaceship
  pilot, The pilot’s task is to maneuver the spaceship through random
  asteroid fields” <a href=""http://www.aaai.org/Papers/IJCAI/2007/IJCAI07-203.pdf"" rel=""nofollow noreferrer"">Jonathan Dinerstein: ""Learning Policies for
  Embodied Virtual Agents through Demonstration"",
  2007</a> (page 4) </p>
</blockquote>

<p>In theory, this game can be solved with Reinforcement learning, or to be more specific with a support vector machine (SVM) and epsilon-regression scheme with a Gaussian kernel. But it seems, that this task is harder than it looks like:</p>

<blockquote>
  <p>quote: “Although many powerful AI and machine learning techniques
  exist, it remains difficult to quickly create AI for embodied virtual
  agents. [...] it is quite challenging to achieve natural-looking
  behavior since these aesthetic goals must be integrated into the
  fitness function” (page 1-2)</p>
</blockquote>

<p><strong>Screenshot</strong></p>

<p>I really want to understand how reinforcement learning works. <a href=""https://i.stack.imgur.com/uugSX.png"" rel=""nofollow noreferrer"">I built a simple game to test this</a>. There are squares falling from the sky and you have the arrow keys to escape.</p>

<p><a href=""https://i.stack.imgur.com/uugSX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uugSX.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>How could I code the RL? Can I do this manually in Javascript
  according to what I think should happen? How can I do this without
  having to map the positions of the rectangles and mine, just giving
  the agent the keyboard arrows to interact and three information:</p>
</blockquote>

<ul>
<li>Player life</li>
<li>Survival time</li>
<li>Maximum survival time</li>
</ul>
"
1917,"<p>I have created an ANN in Python (without libs). On beginning, it had been learned in target of solve linear problems like distinguishing between negative and positive numbers, where the layer widths were [1, 2, 1]. I have decided to learn recognizing small digits saved as 20x20 black &amp; white PNG files. Now the array of layer widths is:</p>

<pre><code>[1200, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 10]
</code></pre>

<p>I tried other similar ones....  With the above array of layer widths training took 8 hours (NN had seen <strong>600.000</strong> images from 5000 images of learning set) and when I look at results, each output is equal about 10%-15%. Nothing is certain.</p>

<p><a href=""https://github.com/mvxxx/NNMV"" rel=""nofollow noreferrer"">This code is a core of my NN</a></p>

<p>and that is main code:</p>

<pre><code>net = nnmv.NeuralNetwork()
    net.createNew([1200,100,100,100,100,100,100,100,100,100,100,100,100,10],0.15,0.07)
    for step in range(0,1000):
        for i in range(0,500):
            for number in range(0,10):
                print(""Currently learning: "",number,'x',i,"" in step: "",step)
                pixels = list(Image.open(""judgment/""+str(number)+'x'+str(i)+"".png"").convert(""RGB"").getdata())
                output = list()
                for itr in range(0,number):
                    output.append(0)
                output.append(1)
                for itr in range(0,9-number):
                    output.append(0)
                net.teach(Stuff.reorganisePixelData(pixels),output)
                print(""Error: "",net.calculateError(output))
        Saver.Saver().save(net, ""digitrecognizer"")
</code></pre>

<p>There is 1200 inputs because there are 400 pixels and each pixel is saved in RGB model. 
<strong>Stuff.reorganisePixelData:</strong></p>

<pre><code>def reorganisePixelData(pixels):
    output = []
    for i in range(0,len(pixels)):
        output.append(pixels[i][0])
        output.append(pixels[i][1])
        output.append(pixels[i][2])
    return output
</code></pre>

<p>What have I to do? Add or remove layers, change some or all of the layer widths? Or something in concept of learning?</p>

<p>My error calculator prints error like <strong>0.30203135930914193</strong>, and it changes only a bit.</p>
"
1918,"<p>I want to use a machine learning algorithm to detect false address data. I learned about neural networks and machine learning at university, but I don't have much experience in this field.</p>

<p>Do you think it is feasible to use a high level algorithm for this or should I use simple queries and filters to catch out wrong data?</p>
"
1919,"<p>If one has a dataset large enough to learn a highly complex function, say learning chess game-play, and the processing time to run mini batch gradient descent on this entire dataset is too high, can I instead do the following?</p>

<ol>
<li><p>Run the algorithm on a chunk of the data for a large number of iterations and then do the same with another chunk and so on?  (Such will not produce the same result as mini batch gradient descent as I am not including all data in one iteration, but rather learning from some data and then proceeding to learn on more data, beginning with the updated weights may still converge to a reasonably trained network.)</p></li>
<li><p>Run the same algorithm (the same model also with only data varying) on different PC's (each PC using a chunk of the data) and then see the performance on a test set and take the final decision as a weighted average of all the different models outputs with the weight being high for the model which did the best on test sets?</p></li>
</ol>
"
1920,"<p>A few forces seem to dominate the determination of AI and cybernetic research direction.</p>

<ol>
<li>The quest to automate repetitive tasks</li>
<li>Interest in the nature of consciousness, learning, and adapting</li>
<li>Interest in language automation (librarian, translator, therapist, loyal friend)</li>
<li>Outsmarting the foes in geopolitics</li>
<li>Outsmarting the competitors in local and global economics</li>
</ol>

<p>Which of these five motivations produce an impact on the direction of technology efforts that lacking key elements in risk management?</p>

<p>These are the corresponding five results one might expect upon success in each of the above five objectives.</p>

<ol>
<li>More human leisure time to waste on shopping, gossiping, entertainment, blogging without defined purpose, pretending to do vital office work eight hours a day when producing only an hour or two of usable work product.</li>
<li>Clearer understanding of ourselves and our place and duty as a species.</li>
<li>Cyborexia (fear of talking with our own species unless a computer creates a layer of safety in between, a phenomena already occurring) AND the entrusting to computers not only the marionette strings of friendship and trust but also the key relational bridge between cultures</li>
<li>The development of tools intended to outsmart foes outsmarting all humans instead</li>
<li>The development of tools intended to outsmart competitors outsmarting all humans instead</li>
</ol>

<p>Only one of these five seems to be a humanity enhancer with low risk.  The others seem to present a high risk to human destiny and offer no particular advancement of the human species.</p>

<p>AI is cast by futurists as the edge of advancement and demonized by technophobes.  Can the development of AI be pointed more toward those objectives that will improve the world?</p>

<p>Does anyone see a second one or have a sixth or seventh force and corresponding result to offer?  Is there any flaw in what appears to me to be obvious eventualities?</p>
"
1921,"<p>The A* algorithm uses the ""evaluation function"" <span class=""math-container"">$f(n) = g(n) + h(n)$</span>, where </p>

<ul>
<li><span class=""math-container"">$g(n)$</span> = cost of the path from the start node to node <span class=""math-container"">$n$</span></li>
<li><span class=""math-container"">$h(n)$</span> = estimated cost of the cheapest path from <span class=""math-container"">$n$</span> to the goal node</li>
</ul>

<p>But, in the following case (picture), how is the value of <span class=""math-container"">$h(n)$</span> calculated?</p>

<p><a href=""https://i.stack.imgur.com/Uago4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uago4.png"" alt=""Image ""></a></p>

<p>In the picture, <span class=""math-container"">$h(n)$</span> is the straight-line distance from <span class=""math-container"">$n$</span> to the goal node. But how do we calculate it?</p>
"
1922,"<p>As a amateur researcher and tinkerer, I've been reading up on neuro-evolution networks (e.g. NEAT) as well as the A3C RL approach presented by <a href=""https://arxiv.org/pdf/1602.01783v1.pdf"" rel=""nofollow noreferrer"">Mnih et al</a> and got to wondering if anyone has contemplated the merging of both these techniques.</p>

<p>Is such an idea viable? Has it been tried? </p>

<p>I'd be interested in any research in this area as it sounds like it could be compelling.</p>
"
1923,"<p>The impetus behind the twentieth century transition from analog to digital circuitry was driven by the desire for greater accuracy and lower noise.  Now we are developing software where results are approximate and noise has positive value.</p>

<ul>
<li>In artificial networks, we use gradients (Jacobian) or second degree models (Hessian) to <strong>estimate</strong> next steps in a convergent algorithm and define acceptable levels of inaccuracy and doubt.<sup>1</sup></li>
<li>In convergence strategies, we <strong>deliberately add noise</strong> by injecting random or pseudo random perturbations to improve reliability by essentially jumping out local minima in the optimization surface during convergence.<sup>2</sup></li>
</ul>

<p>What we accept and deliberately introduce in current AI systems are the same things that drove electronics to digital circuitry.</p>

<blockquote>
  <p>Why not return to analog circuitry for neural nets and implement them with operational amplifier matrices instead of matrices of digital signal processing elements?</p>
</blockquote>

<p>The values of artificial network learning parameters can maintained using integrated capacitors charged via D-to-A converters such that the learned states can benefit from digital accuracy and convenience, while forward propagation benefits from analog advantages.</p>

<ul>
<li>Greater speed<sup>3</sup></li>
<li>Orders of magnitude fewer transistors to represent network cells</li>
<li>Natural thermal noise<sup>4</sup></li>
</ul>

<p>An academic article or patent search for analog artificial networks reveals much work over the last forty years, and the research trend has been maintained.  Computational analog circuits are well developed and provide a basis for neural arrays.</p>

<blockquote>
  <p>Could the current obsession with digital computation be clouding the common view of AI architectural options?</p>
  
  <p>Is hybrid analog the superior architecture for artificial networks?</p>
</blockquote>

<p>&nbsp;</p>

<hr>

<p><strong>Footnotes</strong></p>

<p>[1] The PAC (probably approximately correct) Learning Framework relates acceptable error <span class=""math-container"">$\epsilon$</span> and acceptable doubt <span class=""math-container"">$\delta$</span> to the sample size required for learning for specific model types.  (Note that <span class=""math-container"">$1 - \epsilon$</span> represents accuracy and <span class=""math-container"">$1 - \delta$</span> represents confidence in this framework.)</p>

<p>[2] Stochastic gradient descent is shown, when appropriate strategies and hyper-parameters are used, to converge more quickly during learning and is becoming a best practice in typical real world applications of artificial networks.</p>

<p>[3] Intel Core i9-7960X Processor runs at turbo speeds of 4.2 GHz whereas the standard fixed-satelite broadcasting is 41 GHz.</p>

<p>[4] Thermal noise can be obtained on silicon by amplifying and filtering electron leakage across a reverse biased zener diodes at its avalanche point.  The source of the quantum phenomena is Johnson–Nyquist thermal noise. Sanguinetti et. al. state in their 'Quantum Random Number Generation on a Mobile Phone' (2014), ""A detector can be modeled as a lossy channel with a transmission probability η followed by a photon-to-electron converter with unit efficiency ... measured distribution will be the combination of quantum uncertainty and technical noise,"" and there's CalTech's JTWPA work.  Both of these may become standards for producing truly nondeterministic quantum noise in integrated circuits.</p>

<p><strong>References</strong></p>

<ul>
<li><a href=""https://arxiv.org/pdf/1808.08173.pdf"" rel=""nofollow noreferrer""><em>STDP Learning of Image Patches with Convolutional Spiking Neural Networks</em>, Saunders et. al. 2018, U Mass and HAS</a></li>
<li><a href=""https://homes.cs.washington.edu/~luisceze/publications/anpu-isca14.pdf"" rel=""nofollow noreferrer""><em>General-Purpose Code Acceleration with Limited-Precision Analog Computation</em>, Amant et. al., 2014</a></li>
<li><a href=""https://engineering.dartmouth.edu/news/analog-computing-and-biological-simulations-get-a-boost-from-new-mit-compil"" rel=""nofollow noreferrer""><em>Analog computing and biological simulations get a boost from new MIT compiler</em>, by Devin Coldewey, 2016</a></li>
<li><a href=""http://news.mit.edu/2016/analog-computing-organs-organisms-0620"" rel=""nofollow noreferrer""><em>Analog computing returns</em>, by Larry Hardesty, 2016*</a></li>
<li><a href=""https://www.nsa.gov/news-features/declassified-documents/tech-journals/assets/files/why-analog-computation.pdf"" rel=""nofollow noreferrer""><em>Why Analog Computation?</em>, NSA Declassified Document</a></li>
<li><a href=""https://www.cs.columbia.edu/2016/back-to-analog-computing-columbia-researchers-merge-analog-and-digital-computing-on-a-single-chip/"" rel=""nofollow noreferrer""><em>Back to analog computing: Columbia researchers merge analog and digital computing on a single chip</em>, Columbia U, 2016</a></li>
<li><a href=""https://arxiv.org/pdf/1612.02913.pdf"" rel=""nofollow noreferrer""><em>Field-Programmable Crossbar Array (FPCA) for Reconfigurable Computing</em>, Zidan et. al., IEEE, 2017</a></li>
<li><a href=""https://www.researchgate.net/publication/272429290"" rel=""nofollow noreferrer""><em>FPAA/Memristor Hybrid Computing Infrastructure</em>, Laiho et. al., IEEE, 2015</a></li>
<li><a href=""https://www.researchgate.net/publication/295542715"" rel=""nofollow noreferrer""><em>Foundations and Emerging Paradigms for Computing in Living Cells</em>, Ma, Perli, Lu, Harvard U, 2016</a></li>
<li><a href=""https://www.researchgate.net/publication/221431339_A_Flexible_Model_of_a_CMOS_Field_Programmable_Transistor_Array_Targeted_for_Hardware_Evolution"" rel=""nofollow noreferrer""><em>A Flexible Model of a CMOS Field Programmable Transistor Array Targeted for Hardware Evolution</em> (FPAA), by Zebulum, Stoica, Keymeulen, NASA/JPL, 2000</a></li>
<li><a href=""https://www.electronicdesign.com/boards/custom-linear-array-incorporates-48-precision-op-amps-chip"" rel=""nofollow noreferrer""><em>Custom Linear Array Incorporates Up To 48 Precision Op Amps Per Chip</em>, Ashok Bindra, 2001, Electronics Design</a></li>
<li><a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.444.8748&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer""><em>Large-Scale Field-Programmable Analog Arrays for
Analog Signal Processing</em>, Hall et. al., IEEE Transactions on Circuits and Systems, vol. 52, no. 11, 2005</a></li>
<li><a href=""https://ieeexplore.ieee.org/document/1528675/"" rel=""nofollow noreferrer""><em>Large-scale field-programmable analog arrays for analog signal processing</em>, Hall et. al. 2005</a></li>
<li><a href=""https://pub.uni-bielefeld.de/publication/2426586"" rel=""nofollow noreferrer""><em>A VLSI array of low-power spiking neurons and bistable synapses with spike-timing dependent plasticity</em>, Indiveri G, Chicca E, Douglas RJ, 2006</a></li>
<li><a href=""https://rads.stackoverflow.com/amzn/click/3486728970"" rel=""nofollow noreferrer"">https://www.amazon.com/Analog-Computing-Ulmann/dp/3486728970</a></li>
<li><a href=""https://rads.stackoverflow.com/amzn/click/0817639497"" rel=""nofollow noreferrer"">https://www.amazon.com/Neural-Networks-Analog-Computation-Theoretical/dp/0817639497</a></li>
</ul>
"
1924,"<p>What if we took a recursive approach and built a smallest possible first robot (Robot 1) that could transfer information and data about the place it was at and could build itself in a very small size proportional to itself. I understand that it means higher level of accuracy for the this first robot (Robot 1) that its creator i.e. us.  And this first robot (Robot 1) again built a robot (Say Robot 2) that was far smaller but an exact copy of the first robot (Robot 1). And then the second robot (Robot 2) built a third Robot (Robot 3) and so on. So each next level robot was tinier and higher precision that its creator.</p>

<p>With the tiniest robot we could make, we sent them to mission where in micro sized intervention was needed. For example studying the atom structure from inside, how similar it was to our big universe etc. Plus many more applications human kind could ever imagine. </p>

<p>I understand though that the material used to construct such a robot and its properties, will be limiting and to explore an atom we may not be able to use an atom as the building block. </p>

<p>However, we could possibly build a robot like this which would be small enough to explore the  human body from inside. </p>
"
1925,"<p><strong>In Comes IQ</strong></p>

<p>When the concept of Intelligence Quotient arose it was based on this approximation.</p>

<blockquote>
  <p>Each human being has a number that quantifies their intelligence relative to a fixed norm, and, although this number may vary from testing to testing and over the course of years, it varies only within a small statistical deviation and is essentially determined at meiosis, when the DNA is assembled for that human.</p>
</blockquote>

<p>Experiments and studies supported the validity of IQ as a measure of intelligence.  Others refuted the concept.  It was ethically questioned on the ground that it fostered intellectual elitism.  There were legislative discussions regarding discrimination on the basis of IQ.</p>

<p>Andrew Niccol's brilliant 1997 motion picture, <em>Gattica</em>, starring Ethan Hawke, Uma Thurman, and Jude Law, dramatically examined the question of genetic determinism.</p>

<p>Research overturned the elitist belief that IQ is purely a mater of pedigree. The current evidence-based perspective is that from six months old to adolescence, nurture dominates over genetics.</p>

<p>Nonetheless, during the 20th Century, IQ rose to become a common household term.  People knew, for example, that Albert Einstein was supposed to have had an IQ greater than 200.  How that was calculated, since he never took an IQ test, few have stopped to scrutinize.</p>

<p>Unlike the single number used to gauge intelligence, college boards gauge achievement using a vector of numbers.</p>

<ul>
<li>Command of evidence</li>
<li>Vocabulary</li>
<li>Ability to express ideas</li>
<li>Following of language conventions</li>
<li>Mathematics</li>
<li>Problem solving</li>
<li>Essay writing</li>
</ul>

<p>It is interesting that a set of numbers are used to gauge academic achievement yet all the complex genetic and educational factors that impact a person's intelligence are rolled up into a one-dimensional number.</p>

<ul>
<li>Within the complex genetics, circuitry, and chemistry of the human brain is there some single factor that determines the intelligence of an individual?</li>
<li>Does one factor eclipse and render irrelevant all other factors that are known to affect how smart a person is?</li>
<li>Does each person have a glass ceiling on personal growth and education that is locked in by their mid-teens?</li>
<li>Is this what people really believe?</li>
</ul>

<p>Today, Peter Norvig's word frequency table places <code>IQ</code> among words like <code>mailbox</code> and <code>defect</code> at the 96.9 percentile position in terms of frequency of use.  This linguistic fact is strong evidence that our culture responds to these questions with, ""We buy IQ.""</p>

<p><strong>In Comes The Singularity</strong></p>

<p>Now the concept of The Singularity can be reduced to this.</p>

<blockquote>
  <p>Humans will work on artificial intelligence diligently, causing the peak level of intelligence of an artificial variety to increase.  Once the intelligence of an artificial variety exceeds the intelligence of humanity, the artificial intelligence will gain control over humans to guarantee the perpetuation of its own existence and continued self-development.</p>
</blockquote>

<p>The Singularity concept relies on a very specific pair of propositions.  (Using pseudonyms)</p>

<blockquote>
  <p>The intelligences of Abby and Beth are fully represented as scalars (1-D) not vectors (multidimensional) and therefore can be directly compared.</p>
  
  <p>If the intelligence of Abby is greater than the intelligence of Beth, Abby will gain control over Beth and exploit her.</p>
</blockquote>

<p><strong>In Comes Proof of the Limits of Proof</strong></p>

<p>The dominant enlightenment philosophies of humanism and determinism, that the intellectual pursuits of men will grow to dominate all things, were challenged in the early 20th Century by the work of two people.</p>

<ul>
<li>Heisenberg proved, and there has been no credible refutation, that one cannot accurately measure two determining extensible quantities of a particle at the same time and therefore any mathematical treatment of Bohr's model of the atom must be based on probabilities, not the certainty of mechanical phenomena.</li>
<li>Gödel proved two things (a) That a formal mathematical system cannot be relied upon to indicate the truth or falsehood of every statement that can be made formally, and, more importantly, (b) no formal system can, though its own formalisms, establish its own consistency via proof.<sup>1</sup></li>
</ul>

<p>This uncertainty appears in linguistic expressions like, ""Ricky doesn't test well,"" when Ricky's test score is below expectation (again using a pseudonym).  There is scientific validity to these sayings because neural activity cannot be measured <em>in natura</em> (in natural life).  We must bring the subject into the lab to measure their neural activity, which disturbs the neural activity of the subject under study, just as the electron in Heisenburg's thought experiment.</p>

<p>Applying Gödel's thinking, human intelligence cannot construct a formal system to determine inconclusively that human intelligence is can be expressed as a scalar, can be expressed as a vector, or even exists at all.</p>

<blockquote>
  <p>We cannot, via intelligence, formally prove that intelligence is self-consistent.</p>
</blockquote>

<p><strong>In Comes Genetics</strong></p>

<p>Research has recently revealed that, of 12,000,000 human single-nucleotide polymorphisms (SNPs) examined, 336 have been found that significantly correlate with human intelligence, and those 336 SNPs implicate 22 independent genes.<sup>2, 3</sup>  It is also likely that additional intelligence related genes will be discovered as research continues.</p>

<p>Even if there are no further independent genetic determinants of human intelligence found, twenty-two genetically distinct, independent degrees of freedom define the variable features of human intelligence.  An important conclusion then falls directly from this research.</p>

<blockquote>
  <p>A scalar, such as IQ, cannot possibly describe the permutations of genes that human intelligence.</p>
</blockquote>

<p>At least a 22-dimension vector is required to represent the multifaceted features of an individual's intelligence.</p>

<p><strong>Putting the Proof Together</strong></p>

<p>Although the MAGNITUDE of a vector can be greater than the MAGNITUDE of another vector (without first taking the absolute value of the vector) it is mathematical nonsense to say vector X (of 22 dimensions) is greater than vector Y (of 22 dimensions).</p>

<p>We also lack any evidence-based reason to apply an absolute value to the vector.  A pure root mean squared calculation (RMS) would mean that human intelligence is a Cartesian hyper-cube of 22 equally weighted traits.  There is no evidence to validate the idea that the mathematical inventions of Descartes should be applied to genetic traits.<sup>4</sup></p>

<p>All these traits are situational anyway, arising out of specific environmental challenges during evolution.  For instance, trait number 9 may produce an intelligent response in one scenario and trait numbers 11 and 17 together may produce an intelligent response in another scenario.</p>

<p>So without creating some basis for mapping the traits to the scenarios where they are important in survival, problem solving, or human advancement, it is <strong>NOT MATHEMATICALLY SENSIBLE</strong> to say that Abby is or is not smarter than Beth (using the previous pseudonyms).</p>

<p>Neither can we say that a machine named Polycharp is smarter than Abby. Consequentially, what is described above as <strong><em>The Singularity</em> cannot possibly be singular</strong>.</p>

<p>We have only three logical possibilities.  Only one can be true, but which one?</p>

<ul>
<li>The genetic research is flawed.</li>
<li>The concept of <em>The Singularity</em> is flawed.</li>
<li>The logical inference laid out in this question is flawed.</li>
</ul>

<p><strong>The Unpredictability of Cybernetics</strong></p>

<p>The proposal that has been suggested in the literature is that the detailed workings of human intelligence may not fit into the network size of the human cerebral cortex.  That is a reasonable line of scrutiny. Just as particle physics and the complexities of the biosphere may always require computer analysis and be visible only through the lens of statistics, so may be the case with human cognition.</p>

<p>Adding the work of Heisenberg and Gödel and it only gets worse.  Even a computer model of the twenty-two or more dimensions of human intelligence may be as elusive as physical quanta, further confining the evaluation of intelligence to statistical treatment.</p>

<p>These factors place the interplay between human beings and machines (cybernetics) firmly within the realm of the unpredictable.</p>

<p><strong>Super-intelligence May Face the Same</strong></p>

<p>It may be that some higher intelligence that exists in the universe or some superior artificial intelligence we invent can both fathom human intelligence and predict the course of cybernetics.  However, that greater intelligence may eventually be confronted with the same issue. 
Such beings may not have the intellectual capacity to fully fathom their own intelligence or predict their own cybernetic destiny.</p>

<p><strong>Returning Back to the Main Question</strong></p>

<p>Regardless of the predictability of cybernetics, it is reasonable to ask,</p>

<blockquote>
  <p>""<strong>Is the singularity concept mathematically flawed?</strong>""</p>
</blockquote>

<hr>

<p><strong>Footnotes</strong></p>

<p>[1] <a href=""https://plato.stanford.edu/entries/goedel-incompleteness/"" rel=""nofollow noreferrer"">Gödel's Incompleteness Theorems, 2013, 2015, stamford.edu Encyclopedia of Philosophy</a></p>

<p>[2] <a href=""https://www.nature.com/articles/ng.3869.epdf?referrer_access_token=m_1bwOVgGXVhvnbgtCyFoNRgN0jAjWel9jnR3ZoTv0Pmy6k0dO7lHRyE_RU5bAU0AlPTK2_k3h4f29ToB9D28i3P9mtIDOfl8UBDoZJpsJLODYoOYMvqiu0-KkoixM7bhj1bheXGfBdZSFXYCZ19Dn8A1AqxG0EKKv53nifMOFAustJohhcllWQZR06Ac62elZCXdneM57GGJo85_xEf3k_-0X3Q7otAx75thL1utbrLR1aIch314qE3vUUw_eRk&amp;tracking_referrer=www.scientificamerican.com"" rel=""nofollow noreferrer"">Genome-wide association meta-analysis of 78,308 individuals identifies new loci and genes influencing human intelligence, S. Sniekers et all, May 2017 Nature Genetics Letters</a></p>

<p>[3] <a href=""https://www.scientificamerican.com/article/intelligence-and-the-dna-revolution/"" rel=""nofollow noreferrer"">Intelligence and the DNA Revolution, Scientists identify 22 genes associated with intelligence, Alexander P. Burgoyne, David Z. Hambrick, August 22, 2017, Scientific American, section COGNITION</a></p>

<p>[4] Assuming the twenty-two or more traits involved in human intelligence should be aggregated via an RMS function would be as subjective (and ridiculous) as applying RMS thusly: <em>F<sup>2</sup> = b<sup>2</sup> + b<sup>2</sup></em>, where F is fun, b is blonde, and B is blue-eyed, because fashion magazines sold contact lenses and hair color products to a primarily brown-haired, brown-eyed population by depicting blue-eyed blonds having fun. </p>
"
1926,"<p>I want suggestions on literature on Reinforcement Learning algorithms that perform well with <code>asynchronous feedback</code> from the environment. What I mean by asynchronous feedback is, when an agent performs an action it gets feedback(reward or regret) from the environment after sometime not immediately. I have only seen algorithms with immediate feedback and asynchronous updates. I don't know if literature on this problem exists. This is why I'm asking here.</p>

<p>My application is fraud detection in banking, my understanding is when a fraud is detected it takes 15-45 days for the system to flag it as a fraud sometimes until the customer complains the system doesn't know its fraud. How would I go about designing a real time system using reinforcement learning to flag transactions that are fraud or normal. Maybe my understanding is wrong, I'm learning on my own if someone could help me I would be grateful. </p>

<p>EDIT: The reason I'm looking at reinforcement learning instead of supervised learning is, its hard to get ground truth data in the banking scenario. Fraudsters are always up-to-date or exceeding the state of the art in fraud detection. So I've decided that reinforcement learning would be an optimal direction to look for solutions to this problem.</p>
"
1927,"<p>I want to implement a real-time system for image comparison (e.g. compare a face with a reference one) on an Odroid. I would like to know what are the most suitable architectures for this task. I started with methods based on triplet loss (like Facenet) but I realized that a real-time solution is not feasible. Are there good, light alternatives?</p>
"
1928,"<p>I'm now reading a book titled <em>""Hands-On Reinforcement Learning with Python""</em> by O'Reilly, and the author said the following to implement the DQN algorithm.</p>

<blockquote>
  <p>To make training more stable, there is a trick, called target network, when we keep a copy of our network and use it for the Q(s′, a′) value in the Bellman equation. This network is synchronized with our main network only periodically, for example, once in N steps (where N is usually quite a large hyperparameter, such as 1k or 10k training iterations).</p>
</blockquote>

<p>And on the implementation part:</p>

<blockquote>
  <p>The ptan.agent.TargetNet class is an extremely simple wrapper around the network, which allows us to create a copy of our NN's weights and sync them periodically.</p>
</blockquote>

<p>But I'm not sure why it needs the copy of a neural network and syncs with it periodically, and why it makes the training more stable. So why is it needed and what makes it different from that without the copy of the network?</p>
"
1929,"<p>I want to train a model to recognize different category of food (example: rice, burger, apple, pizza, orange,... )</p>

<p>After the first training, I realized that the model is detecting other object as food. (example: hand -> fish, phone -> Chocolate, person -> candies... )</p>

<p>I get a very low loss because the testing dataset and validation must have at least a pictures of food. But when it comes to picture of object other than food, the model fails. How do label the dataset in way that the model will not do any detection if there is no food on the picture?</p>
"
1930,"<p>In what scenario when assembling a DL CNN would you want to have two adjacent pooling layers, without a convolutional layer between?</p>
"
1931,"<p>What are the mathematical prerequisites for understanding the core part of the algorithms in artificial intelligence and developing own algorithm? Please refer me the specific books. </p>
"
1932,"<p>I'm now learning about reinforcement learning, but I just found the word ""trajectory"" in <a href=""https://datascience.stackexchange.com/a/24924/8432"">this answer</a>.</p>

<p>However, I'm not sure what it means. I read a few books on the Reinforcement Learning but none of them mentioned it. Usually these introductionary books mention agent, environment, action, policy, and reward, but not ""trajectory"".</p>

<p>So, what does it mean? 
According to <a href=""https://www.quora.com/In-the-context-of-reinforcement-learning-what-is-the-difference-between-a-trajectory-and-a-policy-Also-what-is-the-difference-between-trajectory-optimization-and-policy-optimization"" rel=""nofollow noreferrer"">this answer</a> over Quora:</p>

<blockquote>
  <p>In reinforcement learning terminology, a trajectory <span class=""math-container"">$\tau$</span> is the path of the agent through the state space up until the horizon <span class=""math-container"">$H$</span>. The goal of an on-policy algorithm is to maximize the expected reward of the agent over trajectories.</p>
</blockquote>

<p>Does it mean that the ""trajectory"" is the total path from the current state the agent is in to the final state (terminal state) that the episode finishes at? Or is it something else? (I'm not sure what the ""horizon"" mean, either).</p>
"
1933,"<p>Say I'm training a neural net to compute the following function:</p>

<pre><code>(color_of_clothing, body_height) -&gt; gender
</code></pre>

<p>When using this network for prediction, I can obviously plug in a pair <code>(c, b)</code> to receive a predicted <code>g</code>, but say I want to get a prediction only based on <code>c</code> or only based on <code>b</code>, can I use the same neural net somehow? Or would I need to train two separate neural nets <code>c -&gt; g</code> and <code>b -&gt; g</code> previously?</p>

<p>Or more generally, can I use a neural net that was trained to predict <code>A -&gt; B</code> to make predictions on values from a subset of <code>A</code>, or should I train separate neural nets on all subsets of <code>A</code> that I'm interested in?</p>
"
1934,"<p>In doing a project using neural networks with an input layer, 4 hidden layers and an output layer ,I used mini batch gradient descent. I noticed that the randomly initialised weights seemed to do a good performance and gave a low error. As the model started training after about 200 iterations there was large jump in error and then it came down slowly from there. I have also noticed that sometimes the cost just increases over a set of consecutive iterations.
Can anyone explain why these happen? It is not like there are outliers or a new distribution as every iteration exposes it to the entire dataset. 
I used learning rate 0.01 and regularisation parameter 10. I also tried regularisation parameter 5 and also 1.
And by the cost  I mean, the sum of squared errors of all minibatches/2m plus regularisation term error.
Further if this happens and my cost after the say 10000th iteration is more than my cost when I initialised with random weights (lol) can I just take the initial value? As those weights seem to be doing better.</p>

<p>The large jumps are the most puzzling. </p>

<p>This is the code</p>

<p>Any help would be greatly appreciated. Thanks</p>
"
1935,"<p>Does an application exist that can automatically write and test a software component based on a formal functional specification?</p>

<p>The twentieth century saw the initial birth of electronic computers.  The early programming languages that were in primary use by 1975 were COBOL, FORTRAN, LISP, and C and UNIX were emerging for real time communications and control.</p>

<p>Shortly after this period two conceptual steps were proposed toward <strong><em>executable requirements</em></strong>, which, combined with natural language dialog, would permit the realization of computers that would execute high level instructions in a user's native tongue.</p>

<ul>
<li>Glenford J. Myers' <em>Advances in computer architecture</em>, Wiley; 1st edition, 1978, puts forth the proposition that computers had been designed from the bottom up, creating serious obstacles in use.  He redefined the common term at the time, <strong><em>semantic gap</em></strong>, to mean the gap between the needs of those that program computers and the facilities of the computer architecture.  This thinking led to object oriented design and supporting languages such as C++, Java, EMMASCript, and Python.  (Myers worked for IBM but was recruited by a small startup company called Intel to help them design their first 32-bit architecture, the 80386.)</li>
<li>Gene Fisher, Professor Emeritus, California Poly San Luis Obispo, proposed in 1988 what he called a, ""Tool for constructing executable block diagrams based,"" conceptually more advanced than graphical simulators like Simulink and more advanced than IDEs like Eclipse, Idea, and Jupyter interfaces..  JModelica is probably one of the closest development applications to Fisher's vision.  The term that has become popular in the literature for this concept is <strong><em>executable diagram</em></strong>.</li>
</ul>

<p>Are there any applications in beta or in common use in some segment of the software industry where a formal requirement can be entered as input to a program writing application and tested source code is produced at the output?</p>

<p>Is anyone working on an application that takes this one step further to a computer that gathers requirements through natural dialog?</p>
"
1936,"<p>Is it possible to build a neural network that learns the connection between two images?</p>

<p>Let's say I have a number of X images that related to Y images. How can I build a neural network that takes an image as an input and outputs (generates) the output image?</p>

<p>The Y images are generated by applying some function to the X images.</p>

<p>Do I need a generate neural network for that? Are conventional neural networks capable of classification only?</p>
"
1937,"<p>I training a generative adversarial network (GAN) to generate images given edge histogram descriptor (EHD) features of the image. The EHD features are themselves sparse (meaning they contain a lot of zeroes). While training the generator loss and discriminator loss are reducing very slowly.</p>

<p>My question is whether deep learning models (like GAN) are suitable for training with sparse data for one or more of the features in input or derived through feature extraction?</p>
"
1938,"<p>I'm confused with the two terminology - action and policy - in Reinforcement Learning. As far as I know, the action is:</p>

<blockquote>
  <p>It is what the agent makes in a given state.</p>
</blockquote>

<p>However, the book I'm reading now (Hands-On Reinforcement Learning with Python) writes the following to explain policy:</p>

<blockquote>
  <p>we defined the entity that tells us what to do in every state as policy.</p>
</blockquote>

<p>Now, I feel that the policy is the same as the action. So what is the difference between the two, and how can I use them apart correctly?</p>
"
1939,"<p>I am currently working on a project to classify snake types separately using an image of the snake. I need to train a module to classify snake images, but the problem is there are only a small number of images available for some snake types. </p>

<p>What is the best approach to train a neural network for image classification using a small data set?</p>
"
1940,"<p>In a recent paper about progress in computer animation a so called motion graph is used to describe the transition between keyframes of facial animation. <a href=""https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13218"" rel=""nofollow noreferrer"">Easy Generation of Facial Animation Using Motion Graphs, 2018</a> As far as i understand from the paper, they used a motion capture device to record faces of real people and extract keyframes. Then a transition matrix was created to ensure that a walk from keyframe #10 to #24 is possible but a transition from keyframe #22 to #99 is forbidden.</p>

<p>The idea itself sounds reasonable good, because now a solver can search in the motion graph to bring the system from a laughing face to a bored face without interruption or unnatural in-between-keyframes. But wouldn't it be great if the transition matrix can be stored inside a neural network? As far as i understand the backpropagation algorithm , the neural network can learn input-output relations. So the neural network has to learn the transition probability between two keyframes. And a second neural network can then produce the motion plan which is also be trained by a large corpus. Is that idea possible or is it the wrong direction?</p>
"
1941,"<p>Can we really make a chatbot that understands (rather than just replies to) questions based on the database/options of replies that it has? I mean, can it come up with correct/non-stupid replies/communications that don't exist in its database?</p>

<p>For example, can we make it understand the words <em>but</em>, <em>if</em> and so on? So whenever it gets a question/order it understands it based on ""understanding"". Like the movie ""her"" if you have watched it.</p>

<p>And all of this without using too much of code, just the basics to ""wake it up"" and let it learn from YouTube videos and Reddit comments and other data source like that.</p>
"
1942,"<p>I'm struggling to understand the difference between Actor-Critic and Advantage Actor-Critic.</p>

<p>At least I know they are different from Asynchronous Advantage Actor-Critic (A3C), as A3C adds asynchronous mechanism that uses multiple worker agents interacting with their own copy of environment and report the gradient to the global agent. </p>

<p>But what is the difference from the Actor-Critic and Advantage Actor-Critic (A2C)? Is it simply with or without Advantage function? But then, does the Actor-Critic have any other implementation except the use of Advantage function?</p>

<p>Or maybe are they synonyms and Actor-Critic is just a shorthand for A2C?</p>
"
1943,"<p>I would like to know some daily basis applications of AI. I think these might be relevant examples:</p>

<ol>
<li><p>Google search engine</p></li>
<li><p>Face recognition on iPhone </p></li>
</ol>

<p>Are my examples correct? Could you provide some more examples?</p>
"
1944,"<p>What are SVMs (Support Vector Machines)?</p>

<p>Are SVMs a kind of a neural network? (meaning it has nodes and weights, etc).
What are best used for?</p>

<p>Where I can find information about these for... dummies?</p>
"
1945,"<p>I'm now reading <a href=""https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf"" rel=""nofollow noreferrer"">the following blog post</a> but on the epsilon-greedy approach, the author implied that the epsilon-greedy approach takes the action randomly with the probability epsilon, and take the best action 100% of the time with probability 1 - epsilon.</p>

<p>So for example, suppose that the epsilon = 0.6 with 4 actions. In this case, the author seemed to say that each action is taken with the following probability (suppose that the first action has the best value):</p>

<ul>
<li>action 1: 55% (.40 + .60 / 4) </li>
<li>action 2: 15%</li>
<li>action 3: 15%</li>
<li>action 4: 15%</li>
</ul>

<p>However, I feel like I learned that the epsilon-greedy only takes the action randomly with the probability of epsilon, and otherwise it is up to the policy function that decides to take the action. And the policy function returns the probability distribution of actions, not the identifier of the action with the best value. So for example, suppose that the epsilon = 0.6 and each action has 50%, 10%, 25%, and 15%. In this case, the probability of taking each action should be the following:</p>

<ul>
<li>action 1: 35% (.40 * .50 + .60 / 4)</li>
<li>action 2: 19% (.40 * .10 + .60 / 4)</li>
<li>action 3: 25% (.40 * .25 + .60 / 4)</li>
<li>action 4: 21% (.40 * .15 + .60 / 4)</li>
</ul>

<p>Is my understanding not correct here? Does the non-random part of the epsilon (1 - epsilon) always takes the best action, or does it select the action according to the probability distribution?</p>
"
1946,"<p>I need to retrieve just the text from emails. The emails can be in HTML format, and can contain huge signatures, disclaimer legalese, and broken HTML from dozens of forwards and replies. But, I only want the actual email message and not any other cruft such as the whole quotation block, signatures, etc.</p>

<p>This isn't really a problem that could be solved with regex because HTML mail can get very, VERY messy.</p>

<p>Could a neural network perform this task? What kind of problem is this? Classification? Feature selection?</p>
"
1947,"<p>I'm building a deep neural network to serve as the policy estimator in an Actor-Critic reinforcement learning algorithm for a continuing (not episodic) case.  I'm trying to determine how to explore the action space. I have read through <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">this text book</a> by Sutton and in section 13.7 he gives one way to explore a continuous action space.  In essence you train the policy model to give a mean and standard deviation as an output so you can sample a value from that Gaussian distribution to pick an action.  This just seems like the continuous action-space equivalent of an epsilon greedy policy.</p>

<p>Also, are there other continuous action space exploration strategies I should consider? I've been doing some research online and found some articles related to RL in robotics and found that the <a href=""https://papers.nips.cc/paper/3545-policy-search-for-motor-primitives-in-robotics.pdf"" rel=""nofollow noreferrer"">PoWER</a> and <a href=""https://arxiv.org/pdf/1206.4621"" rel=""nofollow noreferrer"">PI^2</a> algorithms do something similar to what is in the textbook. Are these, or other, algorithms ""better"" (obviously depends on the problem being solved) alternatives to what is listed in the textbook for continuous action-space problems?</p>

<p>I know that this question could have many answers, but I'm just looking for a reasonably short list of options that people have used in real applications that work.</p>
"
1948,"<p>In Introduction to Reinforcement Learning (2ed), Sutton and Barto, there is an example of Pole-Balancing problem (Example 3.4).</p>

<p>In this example, it said, this problem can be treated with 'episodic task' and 'continuing task'.</p>

<p>I think that it can only be treated as <code>episodic task</code> because it has an end of playing, which is falling the rod. </p>

<p>I have no idea how this can be treated as <code>continuing task</code>.... Even in <a href=""https://gym.openai.com/envs/CartPole-v0"" rel=""nofollow noreferrer"">OpenAI Gym cartpole env</a>, there is an only <code>episodic</code> mode.</p>
"
1949,"<p><strong>Reinforcement?</strong></p>

<p>We hear much about reinforcement, which is, in my opinion a poor choice of a term to describe a type of artificial network that continues to acquire or improve its behavioral information <em>in natura</em> (during operations in the field).  Reinforcement in learning theory is a term used to describe repetitious incentivization to increase the durability of learned material.  In machine learning, the term has been twisted to denote the application of feedback in operations, a form of re-entrant back propagation.</p>

<p><strong>Corrective Signaling</strong></p>

<p>Qualitatively, corrective signaling in field operations can supply information to a network to make only two types of functional adjustments.</p>

<ul>
<li>Adjustments to what is considered the optimum, beginning with the optimum found during training prior to deployment</li>
<li>Testing of entirely new areas of the parameter space for hint of new optima that have formed, any of which might currently qualify or soon qualify as the global optimum.</li>
</ul>

<p>(By optima and optimum, we mean minima and global minimum in the surface that describes the disparity between ideal system behavior and current system behavior.  This surface is sometimes termed the error surface, applying an over-simplifying analogy from the mathematical discipline of curve fitting.)</p>

<p><strong>The Importance of Doubt</strong></p>

<p>The second of the two above could aptly be termed <em>doubt</em>.</p>

<p>Perhaps all neural nets should have one or more parallel doubting networks that can test remote areas of the search space for more promising optima.  In a parallel computing environment, this might be a matter of provisioning and not significantly reduce the throughput of the primary network, yet provide a layer of reliability not found without the doubtful parallel networks.</p>

<p><strong>What Shows More Intelligence?</strong></p>

<p>Which is more important in actual field use of AI?  The ability to reinforce what is already learned or the ability to create a minority opinion, doubt the status quo, and determine if it is not a more appropriate behavioral alternative than that which was reinforced.</p>

<p><strong>A Helpful Pool of Water Analogy</strong></p>

<p>During a short period of time, a point on the surface of the water may be the lowest point in a pool.  With adjustments based on gradient (what is so inappropriately called reinforcement) the local well can be tracked so the low point can be maintained without any discrete jumps to other minima in the surface.  However the local well may cease being the global minimum at some point in time, whereby a new search for a global minimum must ensue.</p>

<p>It may be that the new global minimum is across several features on the surface of the pool and cannot be found with gradient descent.</p>

<p>More interestingly, the appearance of new global minima can be tracked and reasonable projections can be made such that discrete and substantial jumps in parametric state can be accomplished without large jumps in disparity (where the system misbehaves badly for a period).</p>

<p><strong>Circling Back to the Question</strong></p>

<blockquote>
  <p>Which is more important, doubt or reinforcement?</p>
</blockquote>
"
1950,"<p>Assume we have a large number of proofs in first order predicate calculus.  Assume we also have the axioms, corollaries, and theorems in that area of mathematics in that form too.</p>

<p>Consider the each proposition that was proved and the body of existing theory surrounding that specific proposition as an example in a training set and a known good proof for the proposition as the associated labels.  Now, consider a deep artificial network designed specifically to train on this example set, and the hyper-parameters set correctly to do so.</p>

<p>Is it possible to train a deep artificial network in such a way that the presentation of a new proposition and the existing theory surrounding it presented in first order predicate calculus at the input would produce a proof at the output?</p>

<p>(Of course, such proofs should then be be checked manually.)</p>

<p>If the proportion of good proofs resulting was sufficiently high, might it be possible to create a genetic algorithm that proposes propositions to the trained deep network thereby creating proofs?</p>

<p>Is that possible?</p>

<p>Would it be possible to use this kind of deep network design to solve the Collatz conjecture or the Riemann conjecture or at least rearrange patterns in a way that mathematicians are more able to arrive at a legitimate proof?</p>
"
1951,"<p>My input data consists of a series of 8 integers. Each integer is a discrete token, rather than a relative numeric value (i.e. '1' and '2' are as distinct as are '1' and '100').  The output is a single binary value indicating success or fail.  For example:</p>

<pre><code>fail,12,35,60,82,98,111,142,161
success,23,46,59,87,102,121,145,161
fail,13,35,65,83,100,102,122,161
</code></pre>

<p>I have say 500,000 of these entries.</p>

<p>Success or failure is determined by the combination of the eight tokens that go to make up the input. I am certain that no single token will dictate success or failure, but there may be particular tokens or combinations of tokens which are significant in determining success or failure, I don't know, but would like to know.  </p>

<p>My question is, what kind of machine learning algorithm should I implement to answer the question of which tokens and combinations of tokens are most likely to lead to success?  </p>

<p>In case it's relevant or useful, a few more notes on the input data:</p>

<p>There is a limited range of tokens (and thus integers) in each slot. So with this data input:</p>

<pre><code>success,A,B,C,D,E,F,G,H
</code></pre>

<p>A is <em>always</em> say one of 1, 2, 3, 4 or 5.  B is always one of 6, 7 or 8.  C is always one of 9, 10, 11 or 12.  So in the general case, possible values for A are never possible values for the other slots and there are between 2 and 12 values for each slot.  No idea if that makes a different to the answer but wanted to include it for completeness.  </p>
"
1952,"<p>Do AI algorithms exist which are capable of healing themselves or regenerating a hurt area when they detect so?</p>

<p>For example: In humans if a certain part of brain gets hurt or removed, neighbouring parts take up the job. This happens probably because we are biologically unable to grow nerve cells. Whereas some other body parts (liver, skin) will regenerate most kinds of damage.</p>

<p>Now my question is does AI algorithms exist which take care of this i.e. regenerating a damaged area? From my <a href=""https://www.reddit.com/r/MachineLearning/comments/2uogqt/what_is_coadaptation_in_the_context_of_neural/"" rel=""nofollow noreferrer"">understanding</a> this can be achieved in a NN using dropout (probably). Is it correct? Do additional algorithms (for both AI/NN) or measures exist to make sure healing happens if there is some damage to the algorithm itself? </p>

<p>This can be particularly useful in cases where say there is a burnout in a processor cell  processing some information about the environment. The other processing nodes have to take care to compensate or fully take-over the functions of the damaged cell. </p>

<p>(Intuitionally this can mean 2 things:</p>

<ul>
<li>We were not using the system of processors to its full capability.</li>
<li>The performance of the system will take a hit due to other nodes taking over functionality of the damaged node)</li>
</ul>

<p>Does this  happen in the case of brain damage also? Or is my inferences wrong? (Kindly throw some light).</p>

<p><strong>NOTE</strong> : I am not looking for hardware compensations like re- routing, I am asking for non-tangible healing. Adjusting the behavior or some parameters of the algorithm.</p>
"
1953,"<p><strong>Placing Hype and Fame Aside</strong></p>

<p>Marketing departments for technology corporations throw the word <em>smart</em> around with cavalier inaccuracy.  We know, if we have written any mobile phone applications, that there is nothing notably smarter about smart phones than the New England telegraph system of the 1800s, unless the application loaded into the mobile device provides some smartness.  Hype and overconfidence aside, the voice communication system was several orders of magnitude smarter when people were operators and you could talk with them to resolve your connection.</p>

<p>Cognitive digital systems may come soon, as the media hype proclaims, but this prediction was made several times before.  Such predictions are notoriously optimistic.  <strong>Is the current set of predictions made by those who present themselves as mavens of technology similarly optimistic?</strong></p>

<p><strong>The Folly of the Aeronautics Analogy</strong></p>

<p>People who argue that skeptics felt that way about many aeronautics achievements before the first sustained, controlled flight at Kitty Hawk, don't understand the foundations of aeronautics.</p>

<p>The ancient desire to fly like a bird is evident in ancient times.  Images of an early model airplane found in a tomb at Saqquara, Egypt in 1898 can be displayed using a simple image search on the web.</p>

<blockquote>
  <p>""cairo museum"" ""model airplane""</p>
</blockquote>

<p>Some think that the artifact supports the passage of extraterrestrial intelligence through our system, which is possible, but an absurd explanation for the existence of a model airplane in an ancient tomb.  The object is likely simply the result of people watching birds and wishing they could fly.</p>

<p>The assembly of the object, especially the static uni-wing replacing the dynamic wing of a bird, clearly shows the emergence of the idea of gliding without propulsion not unlike the first flight at Kitty Hawk.</p>

<p>The dating estimates for the Egyptian model airplane are in the neighborhood of 200 B.C., which may be the oldest record of the vision of artificial flight, although it is easy to argue that there may have been much earlier conceptions of human flight than that.  Conservatively, lets go with 150 B.C. as the worst case scenario.</p>

<p>The flight at Kitty Hawk was in 1903.  So we have 1903 - (-150).</p>

<blockquote>
  <p>It took at least 2,053 years from the ontological birth of the idea of human flight to reach the realization of practical aeronautics.</p>
</blockquote>

<p><strong>The Illusion of Technology Recency</strong></p>

<p>The claim, ""But they didn't have technology back then,"" is untrue.  They didn't have technology developed to the point where they could apply mechanics to the design of the wing, the supports, light weight frames, petroleum refinement, propellers, and internal combustion engines yet.</p>

<p>Consider that the and the birth of technology was clearly visible before that, in 3500 B.C. in Mesopotamia.</p>

<ul>
<li>Friction bearings that held potter's wheels in such a way that they didn't tip over and could be turned by hand</li>
<li>Irrigation systems that regulated the amount of moisture to optimize plant growth and limit root rot.</li>
<li>Plowing devices that balanced the muscular force of two oxen in such a way as to direct plow motion in a straight line (a yoke)</li>
</ul>

<p>These are all, without any doubt, evidence of mechanical design.  The ancient designs were highly practical, still in current use today, and arose in the absence of several other important factors that postmodern people take for granted.</p>

<ul>
<li>Faraday's work on electricity and magnetism along with others</li>
<li>Lavoisier's work on oxygen that led to molecular comprehension</li>
<li>Newton's work on unifying the field of physics and introducing calculus</li>
</ul>

<p>Certainly the pioneering work over the last 5,500 years provide a foundation for further discovery, but none of that work, including the von Neumann architecture and the integration of transistor circuits into surface mount devices on mother boards provides any opportunity beyond what can be thought about thought without computers.</p>

<p>Notice that Aristotle, Leibniz, Galileo, Copernicus, Newton, Bohr, Gödel, Wiener, E. Mach, and Einstein had no test bed or computers on which they relied.  Science and technology was born in their absence.  One could reasonably argue that the progress in science has slowed because of obsession with browsing, blogging, hacking, and IT in general.</p>

<p><strong>When Was the Ontological Birth of AI?</strong></p>

<p>Although the sexagesimal (base 60) calculating apparatus of the Sumerian abacus appeared around 2,500 B.C., it was not an attempt to produce automation.  The abacus was a tool to extend the mind, not create a technical offspring from it.  Calculating tools result from the biological fact that numerical aptitude is only wired for zero (the absence of any) and small counting numbers from 1 to around 7.  Working with other numbers requires training of the brain's neural networks.  What we call math class is required to do even the simplest arithmetic.</p>

<p>The idea that AI was born with the ancient Hebrew desire to forge gods, an idea proposed elsewhere, is ill-founded.  The concept of creating a mechanical mind was not the objective in those ancient stories of golden gods.  The objective in those accounts was gaining advantage by creating an alternative control path through polytheism.  (Not exactly a scientific approach, and not very open-source minded either.)</p>

<p>The emergence of the prominence of the idea that human intelligence could be simulated electronically probably began with the creative competition between Claude Shannon, Norbert Wiener, and their contemporaries around the time of WWII.  The construction of digital computers were certainly funded by the United States and the United Kingdom in response to European and Pacific crises.</p>

<p>But neither the early musings of creating gods nor the more recent emergence of digital electronics is a realistic ontological birth date.</p>

<p>The first recorded engineering attempt for intelligence is probably the recipes for the creation of life progressing from reptiles to humans in Jābir ibn Hayyān's Kitab al-Ahjar (Book of Stones), around the year 800.  He further explained, ""Many things are agents, for which the realm of nature becomes a matter and in  which they act in order to bring to existence that which is supposed to exist, like the human being and other things.  ...  He -- that is, the human being -- is an agent acting in matters other than him when creating artificial forms, and object[s] of the realm of nature, ... and his activity comes by reason[oning about] his being and the act of another who undertook his actualization, that is his creation.""</p>

<p>If you understand the cultural framework in which ancients speak, this statement from Jābir ibn Hayyān, combined with his recipes and his discourse regarding the distinction of man from other organisms on the basis of intellectual capacities, is a clear indication that a man is meant to ultimately create an artificially intelligent man.</p>

<p>Islamic sexism aside, this is probably the first historical conceptualization of artificial intelligence as a realistic (and inevitable) scientific endeavor.</p>

<p><strong>Now for Rational Prediction</strong></p>

<p>Assuming only a few things, we can estimate the emergence of the electronic brain.  These are assumptions of a much more rational nature than that technological advancement is exponential or that computers can be used to understand human intelligence, neither of which have any legitimate supporting evidence.</p>

<ul>
<li>That creating cognition and the many other prominent features of human intelligence (of which predicate logic and pattern recognition are only two) is at least as technically difficult as creating an airplane that can land without crashing</li>
<li>That the deeply held belief of those living in an world where the sustenance of the global economy depends so heavily on technology, that it is a trivial fact that technology grows exponentially, is false (on the basis that humans tend to gloss over the effort required to achieve what has already been achieved, as discussed above)</li>
<li>That people wished to fly like birds just as strongly as that they wanted a competitive new species to take their jobs</li>
</ul>

<p>Adding 2,093 (the temporal span between ontological aeronautics and practical aeronautics) to 800 A.D. (the ontological birth of AI) we get the year 2893.</p>

<p><strong>Back To the Question</strong></p>

<p>As disappointing as this prediction result may be to those who won't live to the 29th Century, is this not a more realistic and logical estimate for the earliest likely realization of an artificial brain?</p>

<p>Are not the current predictions grossly overoptimistic?</p>
"
1954,"<p>I am using TensorFlow's Adam optimizer and ReLu activation. But each time I train my model the cost becomes stagnant after some epochs. Can any one help me out with the possible reasons
I have added an image of the cost:
The learning rate is 0.001, the 1st 2 columns are the training data for input neurons and the 3rd column is the training data for output neurons, the nn has two input neurons, two hidden layers with 3 neurons each and one output neuron
<a href=""https://i.stack.imgur.com/Uw5Of.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Uw5Of.jpg"" alt=""https://i.stack.imgur.com/Uw5Of.jpg""></a></p>
"
1955,"<p>The question is very basic: references to books ( or e-books or web published books ) or to state of art papers about current <strong>development</strong> trends for a strong-AI ?</p>

<p>Some points about this question (please, read them before close-vote):</p>

<ul>
<li><p>It seems this site allows ask for references, see help center ""reference requests for papers or text books"" (several other stack exchange sites doesn't allows). Better do not include opinions about the books, just refer the book with a brief description.</p></li>
<li><p>Even when it is a very basic question, I've not found it already in this site (not a duplicate ?). </p></li>
<li><p>It is suppressing the small amount of books about the subject, after discard  the ones related to applied AI, that is, discard Norvig book and similar, discard neural net ones (in all its variants, if target is an applied AI), ... . Discard also AGI proceedings, that contains papers that focus in very concrete aspects. Wikipedia describes some active investigation lines about AGI (cognitive, neuroscience, ...) but can not considered a educational/introductory resource.</p></li>
<li><p>Remark the point about development: not interested on philosophical questions about the risks of AI, its morality, ... if they are not related to its development. Development doesn't excludes mathematical foundation about it.</p></li>
</ul>

<p>By example, if I look by example at this list ""<a href=""https://bigthink.com/mike-colagrossi/the-10-best-books-on-ai"" rel=""nofollow noreferrer"">https://bigthink.com/mike-colagrossi/the-10-best-books-on-ai</a>"", the final <strong>candidates list became empty</strong>. </p>
"
1956,"<p>It has been proven in the paper ""<a href=""https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf"" rel=""nofollow noreferrer"">Approximation by Superpositions of a Sigmoidal Function</a>"" (by Cybenko, in 1989) that neural networks are universal function approximators.  I have a related question. </p>

<p>Assume the neural network's input and output vectors are of the same dimension <span class=""math-container"">$n$</span>. Consider the set of <strong>binary-valued</strong> functions from <span class=""math-container"">$\{ 0,1 \}^n$</span> to <span class=""math-container"">$\{ 0,1 \}^n$</span>.  There are <span class=""math-container"">$(2^n)^{(2^n)}$</span> such functions. The number of parameters in a (deep) neural network is <strong>much smaller</strong> than the above number.  Assume the network has <span class=""math-container"">$L$</span> layers, each layer is <span class=""math-container"">$n \times n$</span> fully-connected, then the total number of weights is <span class=""math-container"">$L \cdot n^2$</span>.</p>

<p>If the number of weights is <strong>not</strong> allowed to grow exponentially as <span class=""math-container"">$n$</span>, can a deep neural network approximate <strong>all</strong> the binary-valued functions of size <span class=""math-container"">$n$</span>?</p>

<p>Cybenko's proof seems to be based on the <strong>denseness</strong> of the function space of neural network functions.  But this denseness does not seem to guarantee that a neural network function exists when the number of weights are polynomially bounded.</p>

<p>I have a theory. If we replace the activation function of an ANN with a polynomial, say cubic one，then after <span class=""math-container"">$L$</span> layers, the composite polynomial function would have degree <span class=""math-container"">$3^L$</span>.  In other words, the degree of the total network grows exponentially. In other words, its ""complexity"" measured by the number of zero-crossings, grows exponentially. This seems to remain true if the activation function is sigmoid, but it involves the calculation of the ""topological degree"" (a.k.a. mapping degree theory), which I have not the time to do yet.</p>

<p>According to my above theory, the VC dimension (roughly analogous to the zero-crossings) grows exponentially as we add layers to the ANN, but it cannot catch up with the <em>doubly</em> exponential growth of Boolean functions. So the ANN can only represent a fraction of all possible Boolean functions, and this fraction even diminishes exponentially. That's my current conjecture.</p>
"
1957,"<p>There is no point in picking one of the growing number of articles that come up in a web search for, ""Deep learning attention networks,"" however the bold claims in <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer""><em>Attention Is All You Need</em>, Ashish Vaswani et all, 2017</a> caught my attention earlier.  Are their claims that attention networks will supercede RNNs and LSTMs credible?</p>
"
1958,"<p>Consider high school graduates entering higher education or the workforce, each making a decision about where to commit their efforts.  History tells us an important story about choosing in a changing economy.</p>

<ul>
<li><p>Entering farming, not as a mechanized farm owner but as a family farmer, resulted in a challenging life for high school graduates in 1990.  John Steinbeck's <em>Grapes of Wrath</em>, a Pulitzer Prize winning novel provides a dismal view of what happened as a result of not predicting what is now termed <em>technological unemployment</em>.</p></li>
<li><p>Graduates in 1975 that entered manufacturing felt a similar reduction in options and wages in the last quarter of the 20th century as jobs went to emerging countries that would work for a tenth the wages.</p></li>
<li><p>Graphic artists were perhaps the first white collar workers to experience a reduction in demand because of the usability Adobe products.</p></li>
</ul>

<p><strong>What is the list of careers to avoid today, and on what evidence are those educational and career paths likely to narrow in the next twenty years?</strong></p>

<blockquote>
  <p>Please don't use media hype as your evidence.  Please provide a rational argument why the actual trend of products and services points toward the elimination of some jobs because of technological shifts.  We should not base information passed to high school graduates on the sensational (and sometimes wild) guesses of media figures and CEOs who may (or may not) bias their predictions in such a way as to boost the value of their company's stock offering.</p>
</blockquote>
"
1959,"<p>Having analysed,reviewed quite a number of user questions inline with answers concerning AI,sometimes I understand nor take note that AI community does not try much to avoid the term computational Intelligence,the feeling I get is that there's need to put some distance between AI and CI. 
However, there is a little bit confusion especially when it comes to computational intelligence application topics,for instance;</p>

<p>According to <a href=""https://cis.ieee.org"" rel=""noreferrer"">IEEE computational intelligence society</a> ,</p>

<p><em>it defines it's subjects of interest as  Neural networks, Fuzzy systems,Evolutionary Computational and Swarm Intelligence,chess programs based on heuristic search..,etc..</em></p>

<p>Now this makes an impression that computational intelligence could be the umbrella under which AI falls into.</p>

<p>Also,according to computational intelligence, an international journal (Blackwell publishing since 1948 in association with <a href=""https://www.atlantis-press.com/journals/ijcis/publishing-info"" rel=""noreferrer"">Atlantis Press</a></p>

<p><em>It states that computational intelligence is just another name for artificial intelligence.</em></p>

<p>However, new scientists, engineers nor researchers on board could not as well get it right, when the two terms come into play,this has come to my notice due to some of questions migrated to cross validated community,others being too broad due to the question problem solutions needed, besides scientific projects,and lastly utopian AI questions.</p>

<p><strong>Hint</strong>  Since we are on the level of artificial narrow intelligence, but when you try to critically figure out its real world applications, they fall under the umbrella of computational intelligence. </p>

<p>I have some hints nor glimpse on the two subjects and I would be very interested in the ideas of others, besides that, the definition of AI,here everyone has a point on it,but what about when we try to state the real description of CI,so that in future,new scientists, machine learning engineers/reseachers get it right away.</p>
"
1960,"<p>I am trying to create a chatbot application where user can create their own bot like <a href=""https://www.botengine.ai/"" rel=""nofollow noreferrer"">Botengine</a>. After going through  google I saw I need some NLP api to process user's query. As per wit.ai <a href=""https://wit.ai/docs/quickstart"" rel=""nofollow noreferrer"">basic example</a> I can set and get data. Now I am confused, How I am going to create a botengine?</p>

<p>So as far I understand the flow, Here is an example for pizza delivery:-</p>

<ol>
<li><p>User will enter a welcome message i.e - Hi, Hello ...</p></li>
<li><p>Welcome reply will be saved by bot owner in my database.</p></li>
<li>User will enter some query, then I will hit wit.ai API to process that query. Example :- Users query is ""What kind of pizza's available in your store"" and wit.ai will respond with the details of intent ""pizza_type""</li>
<li>Then I will search for the intent return by wit in my database.</li>
</ol>

<p>So, is that the right flow to create a chatbot? Am I in the right direction? Could anyone give me some link or some example so I can go through it. I want to create this application using nodejs. I have also found some example in <a href=""https://github.com/wit-ai/node-wit"" rel=""nofollow noreferrer"">node-wit</a>, but can't find how I will implement this.</p>

<p>Thanks</p>
"
1961,"<p>The paper <a href=""https://arxiv.org/pdf/1710.09829.pdf"" rel=""nofollow noreferrer"">Dynamic Routing Between Capsules</a>
 uses the algorithm called ""Dynamic Routing Between Capsules"" to determine the coupling coefficients between capsules. </p>

<p>Why it can't be done by backpropagation? </p>
"
1962,"<p>I hope to get some clarifications on Fitted Q-Learning ('FQL').</p>

<p><strong>My Research So Far</strong></p>

<p>I've read <a href=""http://incompleteideas.net/book/the-book-2nd.html"" rel=""nofollow noreferrer"">Sutton's book</a> (specifically, chp 6 to 10), <a href=""http://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf"" rel=""nofollow noreferrer"">Ernst et al</a> and <a href=""https://pdfs.semanticscholar.org/5c65/d095600d6c647426fa3bc45031b208882d5f.pdf"" rel=""nofollow noreferrer"">this paper</a>.</p>

<p>I know that <code>Q*(s,a)</code> expresses the expected value of first taking action <code>a</code> from state <code>s</code> and then following optimal policy forever.</p>

<p>I tried my best to understand function approximation in large state spaces and TD(n).</p>

<p><strong>My Questions</strong></p>

<p>(1) Concept - Can someone explain the intuition behind how iteratively extending N from 1 until stopping condition achieve optimality (Section 3.5 of Ernst et al)? I have difficulty wrapping my mind around how this ties in with the basic definition of <code>Q*(s,a)</code> that I stated above.</p>

<p>(2) Implementation - Ernst et al gives the pseudo-code for the tabular form. But if I try to implement the <em>function approximation</em> form, is this correct:</p>

<pre><code>Repeat until stopping conditions are reached:
     - N ← N + 1

     - Build the training set TS based on the the function ˆQN − 1 and on the full set of four-tuples F 

     - Train algo on the TS

     - Use the trained model to predict on the TS itself

     - Create TS for the next N by updating the labels - new reward plus ( gamma * predicted values )
</code></pre>

<p>I am just starting to learn RL as part of my course and thus, there are many gaps in my understanding. Hope to get some kind guidance. Thanks in advance!</p>
"
1963,"<p>I am using a neural network as my function approximator for reinforcement learning. In order to get it to train well I need to choose a good learning rate. Hand picking one is difficult, so I read up on methods of programmatically choosing a learning rate. I came across this blog post, <a href=""https://medium.com/@nachiket.tanksale/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6"" rel=""nofollow noreferrer""><em>Finding Good Learning Rate and The One Cycle Policy</em></a>, about finding cyclical learning rate and finding good bounds for learning rates.</p>

<p>All the articles about this method talk about measuring loss across batches in the data. However, as I understand it, in <a href=""https://en.wikipedia.org/wiki/Reinforcement_learning"" rel=""nofollow noreferrer"">Reinforcement Learning</a> tasks do not really have any ""batches"", they just have episodes that can be generated by an environment as many times as one wants, which also gives rewards that are then used to optimize the network.</p>

<p>Is there a way to translate the concept of batch sizes into reinforcement learning or a way to use this method of cyclical learning rates with reinforcement learning?</p>
"
1964,"<p>My works quality control department is responsible for taking pictures of our products at various phases through our QC process and currently the process goes:</p>

<ol>
<li>Take picture of product</li>
<li>Crop the picture down to only the product</li>
<li>Name the cropped picture to whatever the part is and some other relevant data</li>
</ol>

<p>Depending on the type of product the pictures will be cropped a certain way. So my initial thought would be to use a reference to <a href=""https://cloud.google.com/vision/"" rel=""nofollow noreferrer"">an object identifier</a> and then once the object is identified it will use a cropping method specific to that product. There will also be QR codes within the pictures being taken for naming via OCR in the future so I can probably identify the parts that way if this proves slow or problematic.</p>

<p>The part I am unsure about is how to get the program to know how to crop based on a part. For example I would like to present the program with a couple before crop and after crop photos of product X then make a specific cropping formula for product X based on those two inputs. </p>

<p>Also if it makes any difference my code is in C# </p>
"
1965,"<p>Why the optimization step of the algorithm a quadratic program?  <em>[See: <a href=""https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf"" rel=""nofollow noreferrer"">Apprenticeship Learning via Inverse Reinforcement Learning</a>; page 3]</em>  </p>

<p>Isn't the objective function linear? Why don't we treat the problem as LPQC (linear program with quadratic constraints)?</p>
"
1966,"<p>I am building a supervised learning model and I wish to compute the log-likelihood for the training set at the point of the minimum validation error. </p>

<p>Initially, I was computing the sum of all the probabilities with maximum value obtained after applying softmax for each example in the training set at the point of minimum validation error but that doesn't look correct. </p>

<p>What is the correct formula for the log-likelihood?</p>
"
1967,"<p>From <a href=""http://proceedings.mlr.press/v48/santoro16.pdf"" rel=""nofollow noreferrer"">Meta-Learning with Memory-Augmented Neural Networks</a> in section 4.1:</p>

<blockquote>
  <p>To reduce the risk of overfitting, we performed data augmentation by
  randomly translating and rotating character images. We also created
  new classes through 90◦, 180◦ and 270◦ rotations of existing data.</p>
</blockquote>

<p>I can maybe see how rotations could reduce overfitting by allowing the model to generalize better. But if augmenting the training images through rotations prevents overfitting, then what is the purpose of adding new classes to match those rotations? Wouldn't that cancel out the augmentation? </p>
"
1968,"<p>I would like to know whether it's wrong; when working with time series data; to use daily prices as features and the price after 3 days as target.</p>

<p>Is this correct or should I use the next-day price as target and after training; predict 3 times; each time for one more day ahead(using the predicted value as a new feature)</p>

<p>Will these 2 approaches give similar results?</p>
"
1969,"<p>I have a only a general understanding of General Topology, and want to understand the scope of the term ""topology"" in relation to the field of Artificial Intelligence.</p>

<ul>
<li>In what ways are topological structure and analysis applied in Artificial Intelligence?</li>
</ul>
"
1970,"<p>I am reading a book that states, ""As the mini-batch size increases, the gradient computed is closer to the 'true' gradient"". So basically what they are saying is mini-batch training only focuses on decreasing the cost function in a certain 'plane', sacrificing accuracy for speed I assume?</p>

<p>If so, how is batching the training examples reducing the dimensions of the weight-space for any batch, or for that matter, each iteration... all of the weights are still involved with each batch/iteration, am I correct?</p>
"
1971,"<p>I'm looking for a database or some machine readable document that contains common ordered lists or common short sets. e.g:</p>

<pre><code>{January, February, March,...}
{Monday, Tuesday, ....}
{Red, Orange, Yellow,...}
{1,2,3,4,...}
{one, two, three, four,...}
{Mercury, Venus, Earth, Mars,...}
{I, II, III, IV, V, VI,...}
{Aquarius, Pisces, Aries,...}
{ein, zwei, drei, ...}
{Happy, Sneezy, Dopey, ...}
{Dasher, Dancer, Prancer, Vixen ,...}
{John, Paul, George, Ringo}
{20, 1, 18, 4, 13, 6, ...}
{Alabama, Alaska, Arizona, Arkansas, California,...}
{Washington, Adams, Jefferson, ...}
{A,B,C,D,E,F,G,...}
{A,E,I,O,U}
{2,3,5,7,11,13,17,...}
{triangle, square, pentagon, hexagon,...}
{first, second, third, fourth, fifth,...}
{tetrahedron, cube, octohedron, icosohedron, dodecahedron}
{autumn, winter, spring, summer}
{to, be, or, not, to, be, that, is, the, question}
...
</code></pre>

<p>One use is for creating an AI that can solve codes or predict the next thing in a sequence.</p>
"
1972,"<p>Nowadays we don't know how to create AI in a safe way (I think that we don't even know yet how to define a safe AI), but there is a lot of research in developing a model allowing it. </p>

<p>Let's say, that someday we discover such a model (maybe even it would be possible to mathematically prove its safety). Is it rational to ask, how do we prevent people from creating AI outside of this model (e.g. they are so confident in their own model, that they just pursue it and end up with something like <code>paperclip scenario</code>)? </p>

<p>Should we also think about creating some theory/infrastructure preventing such a scenario?</p>
"
1973,"<p>I am going to train a deep learning model to classify hand gestures in video. Since the person will be taking up nearly the entire width/height of the video and I will be classifying what hand gesture he or she is doing, I don't need to identify the person and create a bounding box around the person doing the action. I only need to classify video sequences to their class labels. </p>

<p>I will be training on a dataset with individual videos, in which each entire video clip is the particular gesture (So it's a dataset like UCF-101, with video clips corresponding to class labels). But when I am deploying the network, I want the neural network to run on live video. As in how the live video is playing, it should recognize when a gesture has occurred and indicate that it recognized the gesture. </p>

<p>So I was wondering - How can I train the neural network on isolated video sequences in which the entire video clip is the action (like explained above), <strong>but</strong> run the neural network on <strong>live video</strong>? For instance, can I use a 3D CNN? Or must I use a 2D CNN with an LSTM network instead, for it to work on live video? My concern is that since a 3D CNN performs the filters across many frames, wouldn't running the CNN on every frame make it very slow? But if I use a 2D CNN with LSTM, will that make it faster? Or will both work fine?</p>

<p>Thank you for your help in advance.</p>
"
1974,"<p>This question considers the convergence of an artificial networks (MLPs, RNNs, LSTM nets, CNNs) over time or over the course of epochs made up of iterations through training examples.  In this question's context, we can simplify the correspondence of time and iteration number.  We can assume that &Delta;t is proportional to iteration number and proportional to epoch number, so that, for simplicity's sake, iteration awareness means the same thing as temporal awareness.</p>

<p>Isaac Newton published a method for estimation using N terms of finite differences in his 1687 <em>Principia Mathematica</em>.  It is essentially the discrete version of the Taylor Series Expansion that that 12th grade students or first year college students learn.</p>

<p>When the Jacobian (second year calculus) is used to perform gradient descent so that an artificial network can converge on the objective being learned, that is the application of the first two terms of Newton's formula applied to the number of freedoms of motion in the back propagation.</p>

<p>The formulas for each iteration in training can be easily derived from the basics of interpolation and knowledge of college calculus.  There is nothing to the math that could not be easily grasped by students in the early 18th century that studied mathematics or science at the university level.</p>

<p>That each activation layer must be factored in is a new combination of century old concepts, and that particular combination of concepts (along with von Neumanm computers fast enough to try it) was the key to developing a practical multilayer perceptron that converges.</p>

<p>I am puzzled by the lack of the use of some obvious facts about knowledge acquisition in humans when creating these artificial structures.</p>

<ul>
<li>When we decide to adjust our view of the world, we don't just consider what we think now, but also what we used to think.</li>
<li>When a view is not brought into consciousness, we loose the strength of the view, which is probably an evolutionary advantage since the view may become obsolete over time.</li>
<li>When we see that we repeatedly discover new reasons to change our view in a particular direction, we tend to increase the size of our adjustments to our view.</li>
</ul>

<p>In the simplistic modelling of learning inherent in an artificial network, we have one of the three aspects of learning appearing in extensions of the multilayer perceptron concept.  We see the concept of using past state in recurrent artificial networks (RNNs).</p>

<p>We don't see natural (inverse exponential) decay functions that tend old parameters toward their neutral values.  Or have I missed some research?</p>

<p>The interpolation beyond the Jacobian (two terms) and Hessian (three terms) is not used for reasons of computational burden, which I understand, but why not use the previous states in normal Newtonian fashion to go out one or two more terms.  Or have I missed that research?</p>

<p>Has anyone tried using two dimensional lookup tables with interpolation to avoid the computational burden of additional terms in descent?  Again, this only requires 18th century Newtonian interpolation.  Did I miss that research too?</p>

<p>My understanding is that there has been work on hyper-parameters to dampen back propagation feedback.  Dampening feedback only makes sense if the adjustments appear chaotic.  However, shouldn't adjustment of parameters be augmented rather than dampened if the convergence adjustment appears to be consistent or increasing with each iteration?</p>

<p>I know of LSTM and the newer attention based research, but neither of these really address the above questions and potentially advantageous convergence ideas to my knowledge.  Again, there may be work over which I have not yet stumbled.  Who, if anyone, is thinking along any of these somewhat intuitive lines?</p>

<p>Please provide references to books, papers, reports, or articles so that we can all be edified.</p>
"
1975,"<p>It appears to always have been the focus in the literature to approximate components of the human mind, assuming it to be the most advanced.  If other animals came into the AI landscape, it was only to study primates in ways that are not practical to study humans or to simulate the neural activity of a slug because its nervous system is simple.</p>

<p>Perhaps there is a more forward thinking reason to consider using lower life forms as the model for desired artificial intelligence.  I've been reading what E. O. Wilson and others had to say about the collaborative abilities of other species.  There are remarkable qualities in organisms as simple and adaptive as bacteria.  Certainly, ants are the model species for collaboration.  Honey bees are arguably the most construction savvy, carrying sustainability of lifestyle and interrelationships with other species to an art form far above the capability of human intelligence.</p>

<p>Using sports analogies to characterize the options, human intelligence is more like pre-enlightenment gladiator sports or at least ice hockey, where injuring the opponent is considered the smart strategy.  What bees do is more like mountain climbing, constructing with precision and care.</p>

<p>What ants do is much like relay racing, where there is little interest in the opposing team because each colony, just like each lane in the track is independent and the lanes are marked.  Ants similarly mark their territory, and the territorial claims are respected as in the best of Westphalian geopolitical statesmanship.  There are neither petty jealousies nor competitions solely for the sake of prideful primacy.  With ants, just as with the smart track and field coach, the objective is that each leg of the race perform well against the relay racer's previous best.</p>

<p>Bacteria are the long distance runners.  They swap DNA with one another, and ignore all the rules of pain and fear.  They behave in a sustainable way that takes nothing for granted and uses everything for survival.  And they have survived for nearly the entire duration of the earth's existence.  They will likely be around for a hundred billion years after humanity is gone, if the sun doesn't go supernova first.</p>

<p>Why would we want to program computers to endlessly behave as competitors?  Do people download smart chess programs so they can repeatedly lose?  No, they download Android OS because it collaborates and it costs nothing.  Can't we find nonzero-sum games to play where win-win scenarios are possible?</p>

<p>Don't we already have enough back-biting, gossipy, hyper-critical agents around from within our own species already?  Why not send AI in the direction of collaborative intelligence, like ants?  Wouldn't it be better to have new artificial friends that would like to share the burden of our daily tasks?</p>

<p>Don't we want our robots of the future to build like a honey bee builds, in hexagons?  Or do we want our robots to follow our example, wasting 70% of materials in vertical construction because of an irrational insistence on ninety degree angles, like only humans would do?</p>
"
1976,"<p>I am trying to make balanced weapon pairs. So there are five stats per weapon, and I am simulating a number of combats (1000) with different stats randomized, and counting the win, lose, and draw of the ""weapon fight"" for the database. I want an algorithm for making weapon1win-weapon2win as small as possible for balance through changing the weapon stats.</p>

<p>What happens:</p>

<p>Random Stats --> Combat 1000 Times --> Count Win &amp; Lose --> Data For Training The AI</p>

<p>Data Sample:</p>

<p>{[(1,2,3,4,5) --> Weapon1Stats,(5,4,3,2,1) -->Weapon2Stats,1 --> WinWeapon1-WinWeapon2],[....]} (The text isn't part of the data sample, they are just there to help you know which variable is which. Also, the {[( s are all supposed to be [ s but changed for clarity)</p>

<p>I would like an function, preferably in C++ or Python, but just with text that is well explained is fine, for handling the data. The result would be a method to determine how to minimize WinWeapon1-WInWeapon2.</p>

<p>(edit)
I would say that the one I was looking for is one with something like a score for the weapon as in strength (1*stat1 + 2*stat2 etc...) but I do want something new that works better, I am also having problems creating leeway for the functions coefficients.(edit end)</p>
"
1977,"<p>I use recurrent neural network, RNNs have to get input one value per step and it will show one value output. If I have daily sale demand time series data. </p>

<p>I want to predict sale demand for three days. So, Rnn have to show output one day in three time or it can show sale  demand three days output in one time prediction ?</p>
"
1978,"<p>So, I've been wanting to make my own Neural Network in Python, in order to better understand how it works. I've been following <a href=""https://www.youtube.com/watch?v=ZzWaow1Rvho"" rel=""nofollow noreferrer"">this</a> series of videos as a sort of guide, but it seems the backpropagation will get much more difficult when you use a larger network, which I plan to do. He doesn't really explain how to scale it to larger ones. </p>

<p>Currently, my network feeds forward, but I don't have much of an idea of where to start with backpropagation. My code is posted below, to show you where I'm currently at (I'm not asking for coding help, just for some pointers to good sources, and I figure knowing where I'm currently at might help):</p>

<pre><code>import numpy



class NN:
    prediction = []
    def __init__(self,input_length):
        self.layers = []
        self.input_length = input_length
    def addLayer(self, layer):
        self.layers.append(layer)
        if len(self.layers) &gt;1:
            self.layers[len(self.layers)-1].setWeights(len(self.layers[len(self.layers)-2].neurons))
        else:
            self.layers[0].setWeights(self.input_length)
    def feedForward(self, inputs):
        _inputs = inputs
        for i in range(len(self.layers)):
            self.layers[i].process(_inputs)
            _inputs = self.layers[i].output
        self.prediction = _inputs

    def calculateErr(self, target):
        out = []
        for i in range(0,len(self.prediction)):
            out.append(  (self.prediction[i] - target[i]) ** 2  )
        return out




class Layer:

    neurons = []
    weights = []
    biases = []
    output = []

    def __init__(self,length,function):
        for i in range(0,length):
            self.neurons.append(Neuron(function))
            self.biases.append(numpy.random.randn())

    def setWeights(self, inlength):
        for i in range(0,inlength):
            self.weights.append([])
            for j in range(0, inlength):
                self.weights[i].append(numpy.random.randn())

    def process(self,inputs):
        for i in range(0, len(self.neurons)):
            self.output.append(self.neurons[i].run(inputs,self.weights[i], self.biases[i]))


class Neuron:
    output = 0
    def __init__(self, function):
        self.function = function
    def run(self, inputs, weights, bias):
        self.output = self.function(inputs,weights,bias)
        return self.output

def sigmoid(n):
    return 1/(1+numpy.exp(n))


def inputlayer_func(inputs,weights,bias):
    return inputs

def l2_func(inputs,weights,bias):
    out = 0

    for i in range(0,len(inputs)):
        out += weights[i] * inputs[i]
    out += bias

    return sigmoid(out)

NNet = NN(2)


l2 = Layer(1,l2_func)


NNet.addLayer(l2)
NNet.feedForward([2.0,1.0])
print(NNet.prediction)
</code></pre>

<p>Any help would be greatly appreciated!</p>
"
1979,"<p><strong>The Current Plan to Possibly be Augmented</strong></p>

<p>TranSeed Labs is testing two PCI express artificial network acceleration products and their SDKs against easy to assemble data sets and demos.</p>

<ul>
<li>Aaeon Up PER-TAIC-A10-001<sup>1</sup> &mdash; chip: Intel Movidius Myriad 2 VPU &mdash; sdk: OpenVINO 2.0.2, OpenCV, MDK, Caffe</li>
<li>NVIDIA GeForce GTX 1060 &mdash; chip: GP106-400-A1 &mdash; sdk: CUDA 9.2</li>
</ul>

<p>Initially, it is not a comparison test but a pair of POCs, however, the intended objective is to develop a single test suite and apply it to a number of emerging technologies to produce a comparative convergence speed and accuracy assessment.  Some of the further acceleration test targets include these.</p>

<ul>
<li>Via Tech 986-SOM-9X20 &mdash; chip: QualComm Snapdragon 820E SoC<sup>2</sup> &mdash; sdk: ETK, AI Toolkit, Hexagon NN</li>
<li>Myriad X (only chip samples are available)</li>
<li>Nervana NNP-L1000 (expected in 2019)<sup>3</sup></li>
</ul>

<p><strong>Unalterable Aspects of the Project</strong></p>

<p>Please respect, in comments and answers, that we have no interest in the below three research directions at this time, for the reasons given.  Others are free to pursue them.  For this question, we ask that no time or text be wasted on feedback regarding these three architectural choices.</p>

<ul>
<li>Ultimately, because of the focus of our other research, emphasis will be placed on testing real time control with reinforce-able or trained-state-alterable algorithms and approaches rather than batch training</li>
<li>PCIe interfaces will be selected over USB &mdash; USB is 3.0 too slow<sup>4</sup></li>
<li>No cloud services will be researched.  Contributing to the risk of a trend toward covert totalitarianism is against lab policy.</li>
<li>Low level programming (g++ -S) will be used for initial research rounds<sup>5</sup>.</li>
</ul>

<p><strong>Specific Questions</strong></p>

<p>Testing of artificial network hardware acceleration — Any additions?</p>

<ul>
<li><p>Does anyone know of any other competitively priced PCI express artificial network acceleration hardware that should be tested?</p></li>
<li><p>Does anyone have any thoughts about what test scenarios would be helpful to help determine the best course of action for their own research?</p></li>
</ul>

<p>Please be practical and collaborative in your responses.</p>

<p><strong>Footnotes</strong></p>

<p>[1] The Aaeon Up board contains the Intel's Movidius Myriad 2 VPU, a DSP for computer vision, the same SoC (system on a chip) that the Intel Movidius Neural USB stick uses but the PCI express interface will likely be much faster.  The </p>

<p>[2] The ""A"" in QualComm's model number 820A indicates it targets smart cars and may tolerate a wider storage temperature range and wider operating temperatures too.</p>

<p>[3] The Intel Nervana PCIe board will handle bfloat16 data type and is expected to model pulse based signal propagation in parallel concurrent (not time shared) architecture instead of loop iteration, thereby properly modelling something close to how brain neurons activate, unlike cell types used in MLPs and RNNs, which do not.</p>

<p>[4] USB 3.0 boasts 625 MB/s but generally tests at transfer speeds around 140 MB/s.  The motherboards we use are PCIe v5 x16, providing 63 GB/s.  Even if the daughter board provides only PCI3 v3 x16, those devices usually test around 15 GB/s.  Also, Jim Panian, director of technical standards at Qualcomm Technologies stated clearly in 2015 that they intended to use the PCI express protocol for inter-SoC connectivity, so the protocol is a good one to invest research time into optimizing.</p>

<p>[5] Higher level programming provides simplicity and convenience, but in doing so hides too much for low level timing analysis.  The layers that create the simplicity and convenience often obscure timing costs unrelated to the hardware under test.</p>
"
1980,"<p>I am trying to build an RL agent to price paid-for-seating on commercial flights. I should reiterate here - I am not talking about the price of the ticket - rather, I am talking about the pricing you see if you click on the seat map to choose where on the plane you sit (exits rows, window seats, etc). The general set up is:</p>

<ol>
<li>After choosing their flights (for a booking of <em>n</em> people), a customer will view a web page with the available seat types and their prices visible.</li>
<li>They select between zero and <em>n</em> seats from a seat map with a variety of different prices for different seats, to be added to their booking.</li>
<li>The revenue from step 2 is observed as the reward.</li>
</ol>

<p>Each 'episode' is the selling cycle of one flight. Whether the customer buys a chosen seat or not, the inventory goes down as they still have a ticket for the flight so will get a seat at departure. I would like to change prices on the fly, rather than fix a set of optimal prices throughout the selling cycle. </p>

<p>I have not decided on a general architecture yet. I want to take various booking, flight, and inventory information into account, so I know I will be using function approximation (most likely a neural net) to generalise over the state space.</p>

<p>However, I am less clear on how to set up my action space. I imagine an action would amount to a vector with a price for each different seat type (window seat, exit row, etc). If I have, for example, 8 different seat types, and 10 different price points for each, this gives me a total of 10^8 different actions, many of which will be very similar. In a sense, each action is comprised of a combination of sub-actions - the action of pricing each seat type.</p>

<p>Additionally, each sub-action (pricing one seat type) is somewhat dependent on the others, in the sense that the price of one seat type will likely affect the demand (and hence reward contribution) for another. For example, if you set window seats to a very cheap price, people will be less likely to spend a normal amount for the other seat types. Hence, I doubt the problem can be decomposed into a set of sub-problems.</p>

<p>I'm interested if there has been any research into dealing with a problem like this. Clearly any agent I build needs some way to generalise across actions to some degree, since collecting real data on millions of actions is not possible, even just for one state.</p>

<p>As I see it, this comes down to three questions:</p>

<ol>
<li>Is it possible to get an agent that can deal with a set of actions (prices) as a single decision?</li>
<li>Is it possible to get this agent to understand actions in relative terms? Say for example, one set of potential prices is [10, 12, 20], for middle seats, aisle seats, and window seats. Can I get my agent to realise that there is a natural ordering there, and that the first two pricing actions are more similar to each other than to the third possible action?</li>
<li>Further to this, is it possible to generalise from this set of actions - could an agent be set up to understand that the set of prices [10, 13, 20] is very similar to the first set?</li>
</ol>

<p>I haven't been able to find any literature on this, especially relating to the second question - any help would be much appreciated!</p>
"
1981,"<p>In fields such as Machine Learning, we typically (somewhat informally) say that we are overfitting if improve our performance on a training set at the cost of reduced performance on a test set / the true population from which data is sampled.</p>

<p>More generally, in AI research, we often end up testing performance of newly proposed algorithms / ideas on the same benchmarks over and over again. For example:</p>

<ul>
<li>For over a decade, researchers kept trying thousands of ideas on the game of Go.</li>
<li>The ImageNet dataset has been used for huge amounts of different publications</li>
<li>The Arcade Learning Environment (Atari games) has been used for thousands of Reinforcement Learning papers, having become especially popular since the DQN paper in 2015.</li>
</ul>

<p>Of course, there are very good reasons for this phenomenon where the same benchmarks keep getting used:</p>

<ul>
<li>Reduced likelihood of researchers ""creating"" a benchmark themselves for which their proposed algorithm ""happens"" to perform well</li>
<li>Easy comparison of results to other publications (previous as well as future publications) if they're all consistently evaluated in the same manner.</li>
</ul>

<p>However, there is also a risk that the <strong>research community as a whole</strong> is in some sense ""overfitting"" to these commonly-used benchmarks. If thousands of researchers are generating new ideas for new algorithms, and evaluate them all on these same benchmarks, and there is a large bias towards primarily submitting/accepting publications that perform well on these benchmarks, <strong>the research output that gets published does not necessarily describe the algorithms that perform well across all interesting problems in the world</strong>; there may be a bias towards the set of commonly-used benchmarks.</p>

<hr>

<p><strong>Question</strong>: to what extent is what I described above a problem, and in what ways could it be reduced / mitigated / avoided?</p>
"
1982,"<p>When it comes to CNNs, I don't understand 2 things in the training process:</p>

<ol>
<li><p>How do I pass the error back when there are pooling layers between the convolutional layers?</p></li>
<li><p>And if I know how it's done, can I train all the layers just like layers in normal Feed Forward Neural Nets?</p></li>
</ol>
"
1983,"<p>If you've been attacked by a spider once chances are you'll never go near a spider again.</p>

<p>In a neural network model, having a bad experience with a spider will slightly decrease the probability you will go near a spider depending on the learning rate.</p>

<p>This is not good. How can you program fear into a neural network, such that you don't need hundreds of examples of been bitten by a spider in order to ignore the spider. And also, that it doesn't just lower the probability that you will choose to go near a spider?</p>
"
1984,"<p>I recently heard someone make a statement that when you're designing a self-driving car, you're not building a car but really a computerized driver, so you're trying to model a human mind -- at least the part of the human mind that can drive.</p>

<p>Since humans are unpredictable, or rather since their actions depend on so many factors some of which are going to remain unexplained for a long time, how would a self-driving car reflect that, if they do?</p>

<p>A dose of unpredictability could have its uses. If, say, two self-driving cars are in a stuck in a right of way deadlock, it could be good to inject some randomness instead of maybe seeing the same action applied at the same time if the cars run the same system. </p>

<p>But on the other hand, we know that non-deterministic isn't friends with software development, especially in testing. How would engineers be able to control it and reason about it?</p>
"
1985,"<p>I am looking to try different loss functions for a hierarchical multi-label classification problem. So far, I have been training different models or submodels like multilayer perceptron ( MLP )branch inside a bigger model which deals with different levels of classification, yielding a binary vector. I have been also using Binary Cross Entopy(BCE)  and summing all the losses existing in the model before backpropagating.</p>

<p>I am considering trying other losses like MultiLabelSoftMarginLoss and MultiLabelMarginLoss. What other loss functions are worth to try? hamming loss perhaps or a variation?
Is it better to sum all the losses and backpropagate or do multiple backpropagations?</p>
"
1986,"<p>The current machine learning trend is interpreted by some new to the disciplines of AI as meaning that MLPs, CNNs, and RNNs can exhibit human intelligence.  It is true that these orthogonal structures derived from the original perceptron design can categorize, extract features, adapt in real time, and learn to recognize objects in images or words in speech.</p>

<p>Combinations of these artificial networks can mimic design and control patterns.  Even the approximation of more complex functions like cognition or dialog are considered theoretically possible with stateful networks such as RNNs because they are Turing complete.</p>

<blockquote>
  <p>This question centers around whether the impression created by the success of deep networks based on purely orthogonal extensions of the original perceptron design is limiting creativity.</p>
</blockquote>

<p>How realistic is it to assume that tweaking the dimensions of arrays and matrices, which are convenient in most programming languages, will lead from artificial networks to artificial brains?</p>

<p>The network depth required to make a computer learn to choreograph a dance or develop a complex proof would not likely converge, even if a hundred racks of dedicated and advanced hardware ran for a year.  Local minima in the error surface and gradient saturation would plague the runs, rendering convergence unrealistic.</p>

<p>The primary reason that orthogonality is found in MLP, CNN, and RNN design is because loops used for array iteration compile to simple tests and backward jumps in machine language.  And that fact caries into all higher level languages from FORTRAN and C to Java and Python.</p>

<p>The most natural machine level data structure for trivial loops are arrays.  Nesting loops provides the same direct trivial alignment with multidimensional arrays.  These map to the mathematical structures of vectors, matrices, cubes, hyper-cubes, and their generalization: Tensors.</p>

<p>Although graph based libraries and object oriented databases have existed for decades and the use of recursion to traverse hierarchies is covered in most software engineering curricula, two facts deter the general trend away from less constricted topologies.</p>

<ul>
<li>Graph theory (vertices connected by edges) is not consistently included in computer science curricula.</li>
<li>Many people that write programs have worked only with structures built into their favorite languages, such as arrays, ordered lists, sets, and maps.</li>
</ul>

<p>The structure of the brain is not oriented to Cartesian topologies<sup>1</sup> like vectors or matrices.  The neural nets in biology are not orthogonal.  Neither their physical orientation nor the graphical representations of their signal paths are boxy.  Brain structure is not naturally represented in ninety degree angles.</p>

<p>Real neural circuits cannot be directly represented in Cartesian forms.  Neither do they directly fit into recursive hierarchies.  This is because of four distinctive characteristics.</p>

<ol>
<li>Parallelism in the mind is by trend not by iteration &mdash; The neurons in what appear as parallel structures are not identical and are wrought with exceptions to the apparent pattern.</li>
<li>Cycles appear in the structure &mdash; Groups of neurons do not all point in a single direction.  Cycles exist in the directed graph that represents many networks.  There are many circuits where an ancestor in signal direction is also a descendant.  This is like the stabilizing feedback in analog circuits.</li>
<li>Neural structures that are not parallel are not always orthogonal either.  If a ninety degree angle forms, it is by chance, not design.</li>
<li>Neural structure is not static &mdash; Neuroplasticity is the phenomena that is observed where an axon or dendrite may grow in new directions that are not restricted to ninety degrees.  Cell apoptosis may eliminate a neuron.  A new neuron may form.</li>
</ol>

<p><a href=""https://i.stack.imgur.com/P5CGb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P5CGb.png"" alt=""Parallel-ish Yet Complex Neural Topology""></a></p>

<p>There is almost nothing about the brain that fits naturally into an orthogonal digital circuit structure like a vector, matrix, or cube of registers or contiguous memory addresses.  Their representation in silicon and the feature demands they place on higher level programming languages are radically different than the multidimensional arrays and loops of basic algebra and analytic geometry.</p>

<p><a href=""https://i.stack.imgur.com/OIfA8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OIfA8.jpg"" alt=""Neural Circuit Identification""></a></p>

<p>The brain is constructed with unique topological<sup>1</sup> structures that realize sophisticated signal propagation.  They are unconstrained by Cartesian coordinate systems or grids.  Feedback is nested and non-orthogonal.  They have chemical and electrical equilibria that form balances of higher and lower thought, motivation, and attention.</p>

<p><a href=""https://i.stack.imgur.com/mWrMt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mWrMt.png"" alt=""Overall Neural Topology""></a></p>

<p>Is that topological<sup>1</sup> sophistication necessary or merely a bi-product of how DNA constructs a vector, matrix, cube, or hyper-cube?</p>

<p>As brain research progresses, it becomes increasingly unlikely that brain structures can be efficiently morphed into orthogonal signal pathways.  It is unlikely that the needed signal structures are homogeneously typed arrays.  It is even possible that stochastic or chaotic processing structures possess an advantage for AI development.</p>

<p><a href=""https://i.stack.imgur.com/a0dzp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a0dzp.png"" alt=""Simplexes Not Orthogonal""></a></p>

<p>The brain's topologically<sup>1</sup> sophisticated features may be a catalyst or even a necessity for the emergence of human forms of thought.  When we set out to achieve convergence across hundreds of perceptron layers, we can only sometimes make it work.  Are we in some way trapped by the conceptual limitations that began with Descartes?</p>

<p>Can we escape from those limitations by simply abandoning the programming convenience of orthogonal structures?  Several researchers are working to discover new orientations in the design of VLSI chips.  There may be a need to develop new kinds of programming languages or new features to existing ones to facilitate the description of mental function in code.</p>

<p>Some have suggested that new forms of mathematics are indicated, but significant theoretical framework has been created already by Leonhard Euler (graphs), Gustav Kirchhoff (networks), Bernhard Riemann (manifolds), Henri Poincaré (topology), Andrey Markov (graphs of action), Richard Hook Richens (computational linguistics), and others to support significant AI progress before mathematics need be extended further.</p>

<blockquote>
  <p><strong>Is the next step in AI development to embrace topological sophistication?</strong></p>
</blockquote>

<hr>

<p><strong>Footnotes</strong></p>

<p>[1] This question only uses the word topology to refer to the longstanding mathematical definition of the word.  Although the term has been distorted by some emerging jargon, none of those distortions are meant in this question.  Distortions include (a) calling an array of layer widths the network's topology and (b) calling the texture of a surface its topoLOGy when the correct term would be topoGRAPHy.  Such distortions confound the communication of ideas like the ones described in this question, which is unrelated to (a) or (b).</p>

<hr>

<p><strong>References</strong></p>

<p>Cliques of Neurons Bound into Cavities Provide a Missing Link between Structure and Function
Frontiers in Computational Neuroscience,
12 June 2017,
Michael W. Reimann et. al.
<a href=""https://www.frontiersin.org/articles/10.3389/fncom.2017.00048/full"" rel=""nofollow noreferrer"">https://www.frontiersin.org/articles/10.3389/fncom.2017.00048/full</a>,
<a href=""https://doi.org/10.3389/fncom.2017.00048"" rel=""nofollow noreferrer"">https://doi.org/10.3389/fncom.2017.00048</a></p>

<p>An On-Line Self-Constructing Neural Fuzzy, Inference Network and Its Applications,
Chia-Feng Juang and Chin-Teng Lin,
IEEE Transactions on Fuzzy Systems, v6, n1,
1998,
<a href=""https://ir.nctu.edu.tw/bitstream/11536/32809/1/000072774800002.pdf"" rel=""nofollow noreferrer"">https://ir.nctu.edu.tw/bitstream/11536/32809/1/000072774800002.pdf</a></p>

<p>Gated Graph Sequence Neural Networks
Yujia Li and Richard Zemel,
ICLR conference paper,
2016,
<a href=""https://arxiv.org/pdf/1511.05493.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1511.05493.pdf</a></p>

<p>Building Machines That Learn and Think Like People,
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman,
Behavioral and Brain Sciences,
2016,
<a href=""https://arxiv.org/pdf/1604.00289.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1604.00289.pdf</a></p>

<p>Learning to Compose Neural Networks for Question Answering,
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein,
UC Berkeley,
2016,
<a href=""https://arxiv.org/pdf/1601.01705.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1601.01705.pdf</a></p>

<p>Learning multiple layers of representation
Geoffrey E. Hinton,
Department of Computer Science, University of Toronto,
2007,
<a href=""http://www.csri.utoronto.ca/~hinton/absps/ticsdraft.pdf"" rel=""nofollow noreferrer"">http://www.csri.utoronto.ca/~hinton/absps/ticsdraft.pdf</a></p>

<p>Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition,
George E. Dahl, Dong Yu, Li Deng, and Alex Acero,
IEEE Transactions on Audio, Speach, and Language Processing
2012,
<a href=""https://s3.amazonaws.com/academia.edu.documents/34691735/dbn4lvcsr-transaslp.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1534211789&amp;Signature=33QcFP0JGFeA%2FTsqjQZpXYrIGm8%3D&amp;response-content-disposition=inline%3B%20filename%3DContext-Dependent_Pre-Trained_Deep_Neura.pdf"" rel=""nofollow noreferrer"">https://s3.amazonaws.com/academia.edu.documents/34691735/dbn4lvcsr-transaslp.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&amp;Expires=1534211789&amp;Signature=33QcFP0JGFeA%2FTsqjQZpXYrIGm8%3D&amp;response-content-disposition=inline%3B%20filename%3DContext-Dependent_Pre-Trained_Deep_Neura.pdf</a></p>

<p>Embedding Entities and Relations for Learning and Inference in Knowledge Bases,
Bishan Yang1, Wen-tau Yih2, Xiaodong He2, Jianfeng Gao2, and Li Deng2,
ICLR conference paper,
2015,
<a href=""https://arxiv.org/pdf/1412.6575.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1412.6575.pdf</a></p>

<p>A Fast Learning Algorithm for Deep Belief Nets,
Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh (communicated by Yann Le Cun),
Neural Computation 18,
2006,
<a href=""http://axon.cs.byu.edu/Dan/778/papers/Deep%20Networks/hinton1"" rel=""nofollow noreferrer"">http://axon.cs.byu.edu/Dan/778/papers/Deep%20Networks/hinton1</a>*.pdf</p>

<p>FINN: A Framework for Fast, Scalable Binarized Neural Network Inference
Yaman Umuroglu, et al,
2016,
<a href=""https://arxiv.org/pdf/1612.07119.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1612.07119.pdf</a></p>

<p>From Machine Learning to Machine Reasoning,
Léon Bottou,
2/8/2011,
<a href=""https://arxiv.org/pdf/1102.1808.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1102.1808.pdf</a></p>

<p>Progress in Brain Research,
Neuroscience: From the Molecular to the Cognitive,
Chapter 15: Chemical transmission in the brain: homeostatic regulation and its functional implications,
Floyd E. Bloom (editor),
1994,
<a href=""https://doi.org/10.1016/S0079-6123(08)60776-1"" rel=""nofollow noreferrer"">https://doi.org/10.1016/S0079-6123(08)60776-1</a></p>

<p>Neural Turing Machine (slideshow),
Author: Alex Graves, Greg Wayne, Ivo Danihelka,
Presented By: Tinghui Wang (Steve),
<a href=""https://eecs.wsu.edu/~cook/aiseminar/papers/steve.pdf"" rel=""nofollow noreferrer"">https://eecs.wsu.edu/~cook/aiseminar/papers/steve.pdf</a></p>

<p>Neural Turing Machines (paper),
Alex Graves, Greg Wayne, Ivo Danihelka,
2014,
<a href=""https://pdfs.semanticscholar.org/c112/6fbffd6b8547a44c58b192b36b08b18299de.pdf"" rel=""nofollow noreferrer"">https://pdfs.semanticscholar.org/c112/6fbffd6b8547a44c58b192b36b08b18299de.pdf</a></p>

<p>Reinforcement Learning, Neural Turing Machines,
Wojciech Zaremba, Ilya Sutskever,
ICLR conference paper,
2016,
<a href=""https://arxiv.org/pdf/1505.00521.pdf?utm_content=buffer2aaa3&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1505.00521.pdf?utm_content=buffer2aaa3&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer</a></p>

<p>Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes,
Caglar Gulcehre1, Sarath Chandar1, Kyunghyun Cho2, Yoshua Bengio1,
2017,
<a href=""https://arxiv.org/pdf/1607.00036.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1607.00036.pdf</a></p>

<p>Deep learning,
Yann LeCun, Yoshua Bengio3 &amp; Geoffrey Hinton,
Nature, vol 521,
2015,
<a href=""https://www.evl.uic.edu/creativecoding/courses/cs523/slides/week3/DeepLearning_LeCun.pdf"" rel=""nofollow noreferrer"">https://www.evl.uic.edu/creativecoding/courses/cs523/slides/week3/DeepLearning_LeCun.pdf</a></p>

<p>Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition,
IEEE Transactions on Audio, Speach, and Language Processing, vol 20, no 1
George E. Dahl, Dong Yu, Li Deng, and Alex Acero,
2012,
<a href=""https://www.cs.toronto.edu/~gdahl/papers/DBN4LVCSR-TransASLP.pdf"" rel=""nofollow noreferrer"">https://www.cs.toronto.edu/~gdahl/papers/DBN4LVCSR-TransASLP.pdf</a></p>

<p>Clique topology reveals intrinsic geometric structure in neural correlations,
Chad Giusti, Eva Pastalkova, Carina Curto, Vladimir Itskov, William Bialek
PNAS,
2015,
<a href=""https://doi.org/10.1073/pnas.1506407112"" rel=""nofollow noreferrer"">https://doi.org/10.1073/pnas.1506407112</a>,
<a href=""http://www.pnas.org/content/112/44/13455.full?utm_content=bufferb00a4&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer"" rel=""nofollow noreferrer"">http://www.pnas.org/content/112/44/13455.full?utm_content=bufferb00a4&amp;utm_medium=social&amp;utm_source=twitter.com&amp;utm_campaign=buffer</a></p>

<p>UCL, London Neurological Newsletter, July 2018
Barbara Kramarz (editor),
<a href=""http://www.ucl.ac.uk/functional-gene-annotation/neurological/newsletter/Issue17"" rel=""nofollow noreferrer"">http://www.ucl.ac.uk/functional-gene-annotation/neurological/newsletter/Issue17</a></p>
"
1987,"<p>Can we say that the Turing test aims to develop machines or methods to reach human-level performance in all cognitive tasks and that machine learning is one of these methods that can pass the Turing test?</p>
"
1988,"<p>I am using Tensorflow Object Detection API for training a CNN from scratch on COCO dataset. I need to use this <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config"" rel=""nofollow noreferrer"">specific configuration</a>.
There is no pre-trained model on COCO with that configuration and this is the reason why I am training from scratch.</p>

<p>However, after 1 week of training and evaluating each checkpoint generated by the training phase this is how my learning phase appears on Tensorboard:</p>

<p><a href=""https://i.stack.imgur.com/0U89O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0U89O.png"" alt=""Tensorboard eval""></a></p>

<p>Thus, my questions are:</p>

<ul>
<li>does anyone know how many iterations approximately will be necessary? Right now I did more than 500'000 iterations.</li>
<li>How can be possible that after 500'000 the evaluation is 0,8%? I would expected something like 60-70%.</li>
<li>Why does there is a sudden drop after 500k iterations? I thought that the eval was supposed to converge to some limit. (this is what SGD should do)</li>
<li>Is there any 'trick' to speed up the training phase? (ex: increasing the learning rate, etc).</li>
</ul>
"
1989,"<p>I'm trying to create a deep learning network to classify news article based on the text and associated image. The idea comes from a novel use of GANs to classify based on generated data.</p>

<p>My approach was to use Tensorflow to generate word embeddings in the article, and then tranform the images into records - <a href=""https://github.com/openai/improved-gan/blob/master/imagenet/convert_imagenet_to_records.py"" rel=""nofollow noreferrer"">https://github.com/openai/improved-gan/blob/master/imagenet/convert_imagenet_to_records.py</a>. This second component would also contain the label.</p>

<ul>
<li>Is it wise to combine both modes into one neural net, or classify separately?</li>
</ul>

<p>I'm also trying to work out how to concatenate the two tensors in Tensorflow. Can anyone give a steer.</p>
"
1990,"<p>So I was trying to implement BFS on a <a href=""https://en.wikipedia.org/wiki/Sliding_puzzle"" rel=""noreferrer"">Sliding Blocks puzzle</a> (number type). Now the main thing I noticed that is if you have a <code>4*4</code> board the number of states can be as large as <strong><code>16!</code></strong> so I cannot enumerate all states beforehand.</p>

<p>So my question is how do I keep track of already visited states? (I am using a class board each class instance contains an unique board pattern and is created by enumerating all possible steps from the current step).</p>

<p>I searched on the net and apparently they do not go back to the just completed previous step, <strong>BUT</strong> we can go back to the previous step by another route too and then again re-enumerate all steps which has been previously visited. So how to keep track of visited states when all the states have not been enumerated already? (comparing already present states to the present step will be costly).</p>
"
1991,"<p>I just stumbled upon the concept of neuron coverage, which is the ratio of activated neurons and total neurons in a neural network. But what does it mean for a neuron to be ""activated""? I know what activation functions are, but what does being activated mean e.g. in the case of a ReLU or a sigmoid function?</p>

<p>Many thanks!</p>
"
1992,"<p>I'm using the k-means algorithm from the scikit-learn library, and the values I want to cluster are in a pandas dataframe with 3 columns: <code>ID</code> <code>value_1</code> and <code>value_2</code>.</p>

<p>I want to cluster the information using <code>value_1</code> and <code>value_2</code>, but I also want to keep the <code>ID</code> associated with it (so I can create a list of <code>ID</code>s in each cluster).</p>

<p>What's the best way of doing this? Currently it clusters using the ID number as well and that's not the intention.</p>
"
1993,"<p>How important is self-consciousness (or consciousness in general) for making advanced AIs, and how far away are we from making such?</p>

<p>When making e.g. a neural network there's (very probably) no consciousness within it, but just mathematics behind, but do we need the AIs to become conscious in order to solve more complex tasks in the future? Furthermore, is there actually any way we can know for sure if something is conscious, or if it's just faking it? It's ""easy"" to make a computer program which claims it's conscious, but that doesn't mean it is (e.g. Siri).</p>

<p>And if the AIs are only based on predefined rules without consciousness, can we even call it ""intelligence""?</p>
"
1994,"<p>I read somewhere that a Multilayer Perceptron is a recursive function in its forward propagation phase. I am not sure, what is the recursive part? For me, I would see a MLP as a chained function. So, it would nice anyone could relate a MLP to a recursive function.</p>
"
1995,"<p>I'm now reading a book titled as <em>""Deep Reinforcement Learning Hands-On""</em> and the author said the following on the chapter about AlphaGo Zero:</p>

<blockquote>
  <p>Self-play</p>
  
  <p>In AlphaGo Zero, the NN is used to approximate the prior probabilities of the actions and evaluate the position, which is very similar to the Actor-Critic (A2C) two-headed setup. On the input of the network, we pass the current game position (augmented with several previous positions) and return two values. The policy head returns the probability distribution over the actions and the value head estimates the game outcome as seen from the player's perspective. <strong>This value is undiscounted, as moves in Go are deterministic.</strong> Of course, if you have stochasticity in the game, like in backgammon, some discounting should be used.</p>
</blockquote>

<p>All the environments that I have seen so far are stochastic environments, and I understand the discount factor is needed in stochastic environment. 
I also understand that the discount factor should be added in infinite environments (no end episode) in order to avoid the infinite calculation.</p>

<p>But I have never heard (at least so far on my limited learning) that the discount factor is NOT needed in deterministic environment. Is it correct? And if so, why is it NOT needed?</p>
"
1996,"<p>To the best of my understanding, Monte Carlo Search is an alternative method to Minimax for searching a tree of nodes. It works by choosing a move (generally the one with the highest chance of being the best), and then performing a random playout on the move to see what the result is. This process keeps continuing for however much time is allotted.</p>

<p>This doesn't sound like Machine Learning, but rather a way to traverse a tree. However, I've heard that AlphaZero uses Monte Carlo search, so I'm confused. Is using Monte Carlo search why AlphaZero learns? Or did AlphaZero do some kind of machine learning before it played any matches, and then use the intuition it gained from Machine learning to know which moves to spend more time playing out with Monte Carlo search?</p>
"
1997,"<p>Turing Test was created to test machines exhibiting behavior equivalent or indistinguishable from that of a human. Is that the sufficient condition of intelligence?</p>
"
1998,"<p>I'm developing an AI to play a card game with a genetic algorithm.  Initially, I will evaluate it against a player that plays randomly, so there will naturally be a lot of variance in the results.  I will take the mean score from X games as that agents fitness. The actual playing of the game dominates the time to evaluate the actual genetic algorithm.</p>

<p>My question is should I go for a low X e.g. 10, so I would be able to move through generations quite fast but the fitness function would quite inaccurate.  Alternatively, I could go for a high X e.g. 100 and would move very slowly but with a more accurate function.</p>
"
1999,"<p>According to the paper <a href=""https://arxiv.org/pdf/1512.02325.pdf"" rel=""nofollow noreferrer"">SSD: Single Shot MultiBox Detector</a>, for each cell in a feature map k boxes are acquired and for each box we get $c$ class scores and $4$ offsets relative to the original default box_shape. This means that we get $m \times n \times (c +4) \times k$ outputs for each $m \times n$ feature map.</p>

<p>However, it is mentioned that in order to train the SSD network only the images and their ground truth boxes are needed.</p>

<p>How exactly can one define the output targets then? What is the format of the output in the SSD framework? I think it cannot be a vector with the positions, sizes and class of each boundary box, since the outputs are a lot more and relate to every default box in the feature maps. </p>

<p>Can anyone explain in more detail how can I, given an image and its boundary boxes' info, construct a vector that will be fed into a network so that I can train it?</p>
"
2000,"<p>If you taught an AI to understand sentences through usual neural network techniques.</p>

<p>Then could you being to teach it things with sentences such as ""ants are small"", ""the sky is blue"".</p>

<p>i.e. if you fed it that sentence and the neural network says this is 99% likely to be a properly formed sentence. Then where would it store this sentence for future use? This would be a kind of one-shot learning after it learnt how to learn.</p>

<p>Could you use some sort of gated architecture?</p>
"
2001,"<p>I have a 100-150 words text and I want to extract particular information like location, product type, dates, specifications and price.</p>

<p>Suppose if I arrange a training data which has a text as input and location/product/dates/specs/price as a output value. So I want to train the model for these specific output only.</p>

<p>I have tried Spacy and NLTK for entity extraction but that doesn't suffice above requirements.</p>

<p>Sample text:</p>

<blockquote>
  <p>Supply of Steel Fabrication Items. 
  General Item . 
  Construction Material . 
  Hardware Stores and Tool . 
  Construction of Security Fence. - Angle Iron 65x65x6mm for fencing post of height 3.5, Angle Iron 65x65x6mm for fencing post of height 3.5, MS Flat 50 x 5mm of 2.60m height, Angle Iron 50x50x6mm for Strut post of height 3.10mtr, Angle Iron 50x50x6mm for fencing post of height 1.83, Angle Iron 50x50x6mm for fencing post of height 1.37, Barbed wire made out of GI wire of size 2.24mm dia, Chain link fence dia 4 mm and size of mesh 50mm x, Concertina Coil 600mm extentable up to 6 mtr, Concertina Coil 900mm extentable up to 15 to 20 mtr, Binding wire 0.9mm dia., 12 mm dia 50mm long bolts wih nuts &amp; 02 x washers, Cement in polythene bags 50 kgs each grade 43 OPC, Sand Coarse confiming to IS - 383-970, 2nd revision, Crushed Stone Aggregate 20 mm graded, TMT Bar 12mm dia with 50mm U bend, Lime 1st quality, Commercial plywood 6' x 3' x 12 mm., Nails all Type 1"" 2""3"" 4"" 5"" and 6""., Primer Red Oxide, Synthetic enamel paint, colour black/white Ist quality . 
  Angle Iron 65x65x6mm for fencing post of height 3.5, Angle Iron 65x65x6mm for fencing post of height 3.5 mtr, MS Flat 50 x 5mm of 2.60m height, Angle Iron 50x50x6mm for Strut post of height 3.10mtr, Barbed wire made out of GI wire of size 2.24mm dia, Chain link fence dia 4 mm and size of mesh 50mm x, Concertina Coil 600mm extentable up to 6 mtr, Binding wire 0.9mm dia., 12 mm dia 50mm long bolts with nuts &amp; 02 x washers, Cement in polythene bags 50 kgs each grade 43 OPC, Sand Coarse confiming to IS - 383-970, 2nd revision, Crushed Stone Aggregate 20 mm graded, TMT Bar 12mm dia with 50mm U bend, Lime 1st quality, Commercial plywood 6' x 3' x 12 mm., Nails all Type 1"" 2""3"" 4"" 5"" and 6""., Primer Red Oxide, Synthetic enamel paint, colour black/white Ist quality., Cutting Plier 160mm long, Leather Hand Gloves/Knitted industrial, Ring Spanner of 16mm x 17mm, 14 x 16mm, Crowbar hexagonal 1200mm long x 40mm, Plumb bob steel, Bucket steel 15 ltr capacity (as per, Plastic water tank 500 ltrs Make - Sintex, Water level pipe 30 Mtr, Brick Hammer 250 Gms with handle, Hack saw Blade double side, Welding Rod, Cutting rod for making holes, HDPE Sheet 5' x 8', Plastic Measuring tape 30 Mtr, Steel Measuring tape 5 Mtr, Wooden Gurmala 6""x3"", Steel Pan Mortar of 18""dia (As, Showel GS with wooden handle, Phawarah with wooden handle (As per, Digital Vernier Caliper, Digital Weighing Machine cap 500 Kgs, Portable Welding Machine, Concrete mixer machine of 8 CFT . 
  Angle Iron 65x65x6mm for fencing post of height 3.5, Angle Iron 65x65x6mm for fencing post of height 3.5, MS Flat 50 x 5mm of 2.60m height, Angle Iron 50x50x6mm for Strut post of height 3.10mtr, Barbed wire made out of GI wire of size 2.24mm dia, Chain link fence dia 4 mm and size of mesh 50mm, Concertina Coil 600mm extentable up to 6 mtr, Binding wire 0.9mm dia., 12 mm dia 50mm long bolts with nuts &amp; 02 x washers, Cement in polythene bags 50 kgs each grade 43, Sand Coarse confiming to IS - 383-970, 2nd revision, Crushed Stone Aggregate 20 mm graded, TMT Bar 12mm dia with 50mm U bend, Lime 1st quality, Commercial plywood 6' x 3' x 12 mm., Nails all Type 1"" 2""3"" 4"" 5"" and 6""., Primer Red Oxide, Synthetic enamel paint, colour black/white Ist quality., Cutting Plier 160mm long, Leather Hand Gloves/Knitted industrial, Ring Spanner of 16mm x 17mm, 14 x 16mm, Crowbar hexagonal 1200mm long x 40mm, Plumb bob steel, Bucket steel 15 ltr capacity (as per, Plastic water tank 500 ltrs Make - Sintex, Water level pipe 30 Mtr, Brick Hammer 250 Gms with handle, Hack saw Blade double side, Welding Rod, Cutting rod for making holes, HDPE Sheet 5' x 8', Plastic Measuring tape 30 Mtr, Steel Measuring tape 5 Mtr, Wooden Gurmala 6""x3"", Steel Pan Mortar of 18""dia (As per, Showel GS with wooden handle, Phawarah with wooden handle (As per, Digital Vernier Caliper)</p>
</blockquote>
"
2002,"<p>I was working recently on Progressive Growing of <a href=""https://en.wikipedia.org/wiki/Generative_adversarial_network"" rel=""nofollow noreferrer"">GANs</a> (aka PGGANs). I have implemented the whole architecture, but the problem that was ticking my mind is that in simple GANs, like DCGAN, PIX2PIX, we actually use Transposed Convolution for up-sampling and Convolution for Down-sampling, but in PGGANs in which we gradually add layers to both generator and discriminator so that we can first start with 4x4 image and then increase to 1024x01024 step by step. </p>

<p>I did not understand that once we Increase 1x1x512 dimensional latent vector size to 4x4x512 sort of image we use convolution with high padding, and then once training for 4x4 images, we take still take 512 latent vector and then use the previously trained convolutional layers to convert it to 4x4x512 image, and then we up-sample then given image to 8x8 using nearest neighbor filtering and then again apply convolution and so-on. </p>

<ul>
<li>My question is that why we need to explicitly up-sample and then apply convolution, when instead we could just use Transposed Convolution which can upsample it automatically and is trainable? Why do we not use it like in other GANs? </li>
</ul>

<p>Here is the image of architecture:</p>

<p><a href=""https://i.stack.imgur.com/udNPA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/udNPA.png"" alt=""PGGANs architecture""></a></p>

<p>Please explain me the intuition behind this.
Thanks</p>
"
2003,"<p>So Taleb has two heuristics to generally describe data distributions. One is Mediocristan, which basically means things that are on a Gaussian distribution such as height and/or weight of people.</p>

<p>The other is called Extremistan, which describes a more Pareto like or fat-tailed distribution. An example is wealth distribution, 1% of people own 50% of the wealth or something close to that and so predictability from limited data sets is much harder or even impossible. This is because you can add a single sample to your data set and the consequences are so large that it breaks the model, or has an effect so large that it cancels out any of the benefits from prior accurate predictions. In fact this is how he claims to have made money in the stock market, because everyone else was using bad, Gaussian distribution models to predict the market, which actually would work for a short period of time but when things went wrong, they went really wrong which would cause you to have net losses in the market.</p>

<p>I found this video of Taleb being asked about AI. His claim is that A.I. doesn't work (as well) for things that fall into extremistan.</p>

<p>Is he right? Will some things just be inherently unpredictable even with A.I.?</p>

<p>Here is the video I am referring to <a href=""https://youtu.be/B2-QCv-hChY?t=43m08s"" rel=""noreferrer"">https://youtu.be/B2-QCv-hChY?t=43m08s</a></p>
"
2004,"<p>I'm making a Connect Four game where my engine uses Minimax with Alpha-Beta pruning to search. Since Alpha-Beta pruning is much more effective when it looks at the best moves first (since then it can prune branches of poor moves), I'm trying to come up with a set of heuristics that can rank moves from best to worst. These heuristics obviously aren't guaranteed to always work, but my goal is that they'll <em>often</em> allow my engine to look at the best moves first. An example of such heuristics would be as follows:</p>

<ul>
<li>Closeness of a move to the centre column of the board - weight 3.</li>
<li>How many pieces surround a move - weight 2.</li>
<li>How low, horizontally, a move is to the bottom of the board - weight 1.</li>
<li>etc</li>
</ul>

<p>However, I have no idea what the best set of weight values are for each attribute of a move. The weights I listed above are just my estimates, and can obviously be improved. I can think of two ways of improving them:</p>

<p>1) Evolution. I can let my engine think while my heuristics try to guess which move will be chosen as best by the engine, and I'll see the success score of my heuristics (something like x% guessed correctly). Then, I'll make a pseudo-random change/mutation to the heuristics (by randomly adjusting one of the weight values by a certain amount), and see how the heuristics do then. If it guesses better, then that will be my new set of heuristics. Note that when my engine thinks, it considers thousands of different positions in its calculations, so there will be enough data to average out how good my heuristics are at prediction.</p>

<p>2) Generate thousands of different heuristics with different weight values from the start. Then, let them all try to guess which move my engine will favor when it thinks. The set of heuristics that scores best should be kept.</p>

<p>I'm not sure which strategy is better here. Strategy #1 (evolution) seems like it could take a long time to run, since every time I let my engine think it takes about 1 second. This means testing each new pseudo-random mutation will take a second. Meanwhile, Strategy #2 seems faster, but I could be missing out on a great set of heuristics if I myself didn't include them.</p>
"
2005,"<p>A lot of questions on this site seem to be asking ""can I use X to solve Y?"", where X is usually a deep neural network, and Y is often something already addressed by other areas of AI that are less well known?</p>

<p>I have some ideas about this, but am inspired by questions like <a href=""https://bicycles.stackexchange.com/questions/1195/different-kinds-of-handlebars"">this one</a> where a fairly wide range of views are expressed, and each answer focuses on just one possible problem domain.</p>

<p>There are some related questions on this stack already, but they are not the same. <a href=""https://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for"">This question</a> specifically asks what genetic algorithms are good for, whereas I am more interested in having an inventory of problems mapped to possible techniques. <a href=""https://ai.stackexchange.com/questions/60/what-are-the-main-problems-hindering-current-ai-development"">This question</a> asks what possible barriers are to AI with a focus on machine learning approaches, but I am interested in what we <em>can</em> do without using deep neural nets, rather than what is difficult in general. </p>

<p>A good answer will be supported with citations to the academic literature, and a brief description of both the problem and the main approaches that are used.</p>

<p>Finally, <a href=""https://ai.stackexchange.com/questions/2999/how-could-ai-solve-planets-major-problems"">this question</a> asks what AI can do to solve problems related to climate change. I'm not interested in the ability to address specific application domains. Instead, I want to see a catalog of abstract problems (e.g. having an agent learn to navigate in a new environment; reasoning strategically about how others might act; interpreting emotions), mapped to useful techniques for those problems. That is, ""solving chess"" isn't a problem, but ""determining how to optimally play turn-based games without randomness"" is.</p>

<p>I realize this is pretty broad. If you think it's too broad for the stack, please vote to close it. I suspect it might be useful to have as a kind of wiki to refer new users to as the stack grows however.</p>
"
2006,"<p>I recently <a href=""https://ai.stackexchange.com/a/7582/1671"">came across this function</a>:</p>

<p>$$\sum_{t = 0}^{\infty} \gamma^t R_t.$$</p>

<p>It's elegant and looks to be useful in the type of deterministic, perfect-information, finite models I'm working with.  </p>

<p>However, it occurs to me that using $\gamma^t$ in this manner might be seen as somewhat arbitrary.  </p>

<p>Specifically, the objective is to discount per the added uncertainty/variance of ""temporal distance"" between the present gamestate and any potential gamestate being evaluated, but that variance would seem to be a function of the branching factors present in a given state, and the sum of the branching factors leading up to the evaluated state.</p>

<ul>
<li>Are there any defined discount-factors based on the number of branching factors for a given, evaluated node, or the number of branches in the nodes leading to it?  </li>
</ul>

<p>If not, I'd welcome thoughts on how this might be applied. </p>

<p><em>(An initial thought is that I might divide 1 by the number of branches and add that value to the goodness of a given state, which is a technique I'm using for heuristic tie-breaking with no look-ahead, but that's a ""value-add"" as opposed to a discount.)</em>  </p>

<h2>--------------------</h2>

<p>For context, this is for a form of <a href=""https://www.reddit.com/r/abstractgames/comments/8khl96/formal_definition_of_mnpgames/"" rel=""nofollow noreferrer"">partisan Sudoku</a>, where an expressed position $p_x$ (value, coordinates) typically removes some number of potential positions $p$ from the gameboard.  <em>(Without the addition of an element displacement mechanic, the number of branches can never increase.)</em></p>

<p>On a $(3^2)^2$ Sudoku, the first $p_x$ removes $30$ out of $729$ potential positions $p$, including itself.   </p>

<p>With each $p_x$, the number of branches diminishes until the game collapses into a tractable state, allowing for perfect play in endgames.  [Even there, a discounting function may have some utility because outcomes sets of ratios. Where the macro metric is territorial (controlled regions at the end of play), the most meaningful metric may ultimately be ""efficiency"" <em>(loosely, ""points_expended to regions_controlled"")</em>, which acknowledges a benefit to expending the least amount of points $p_x$, even in a tractable endgame where the ratio of controlled regions cannot be altered. Additionally, zugzwangs are possible in the endgame, and in that case reversing the discount to maximize branches may have utility.]</p>

<p><sub> $(3^2)^2 = 3x3(3x3) = ""9x9""$ but the exponent is preferred so as not to restrict the number of dimensions. </sub></p>
"
2007,"<p>My question is that when we talk about artificial intelligence, human intelligence or any other form of intelligence, what do we mean by the term <strong>intelligence</strong> in a general sense?</p>

<p>I mean what would you call intelligent and what not?</p>

<p>How do we define the term <strong>""intelligence""</strong> in the most general possible way?</p>

<p>P.S. I have thought a lot about it, for like days and having made some progress, I am typing my thoughts. If this question is a irrelevant then, I sincerely regret asking it.</p>
"
2008,"<p>In particular, I would like to have a simple definition of ""environment"" and ""state"". What are the differences between those two concepts? Also, I would like to know how the concept of model relates to the other two. </p>

<p>There is a similar question <a href=""https://ai.stackexchange.com/q/5970/2444"">What is the difference between an observation and a state in reinforcement learning?</a>, but it is not exactly what I was looking for.</p>
"
2009,"<p>Deep Successor Representations(DSR) has given better performance in tasks like navigation when it compares to normal model-free RL tasks. Basically, DSR is a hybrid of model-free RL and Model-Based RL.  But the original work has only use value-based functions deep RL methods like DQN.  </p>

<p>Link to the paper - <a href=""https://arxiv.org/abs/1606.02396"" rel=""nofollow noreferrer"">DSR</a></p>
"
2010,"<p>As I know, a single layer neural network can only do <strong>linear</strong> operations, but multilayered ones can.</p>

<p>Alao I recently learned that finite matrices/tensors, which are used in many neural networks can <em>only represent linear</em> operations.</p>

<p><strong>However</strong> multi-layered neural networks can represent nonlinear(even <strong>much more complex</strong> than being just a nonlinear!) operations.</p>

<p>What makes it happen? The activation layer?</p>
"
2011,"<p>So I have a deep learning model and three data sets (images). My theory is that one of these data sets should function better when it comes to training a deep learning model (meaning that the model will be able to achieve better performance (higher accuracy) with one of these data sets to serve one classification purpose) </p>

<p>I just want to safe check my approach here. I understand the random nature of training deep learning models and the difficulties associated with such experiment. Though, I want someone who can point out maybe a red flag here. </p>

<p>I am wondering about these things: </p>

<ol>
<li><p>do you think using an optimizer with default parameters and repeating the training process, let's say, 30 times for each data set and picking the best performance is a safe approach? I am mainly worried here that modifying the hyperparamters of the optimizer might result in better results for let's say one of the data sets. </p></li>
<li><p>what about seeding the weights initialization? do you think that I should seed them and then modify the hyperparameters until I get the best convergence or not seed and still modify the hyper parameters? </p></li>
</ol>

<p>I am sorry for the generality of my question. I hope if someone can point me in the right direction.  </p>
"
2012,"<p>In the 4th paragraph of 
<a href=""http://www.incompleteideas.net/book/ebook/node37.html"" rel=""nofollow noreferrer"">http://www.incompleteideas.net/book/ebook/node37.html</a>
it is mentioned: </p>

<blockquote>
  <p>Whereas the optimal value functions for states and state-action pairs are unique for a given MDP, there can be many optimal policies</p>
</blockquote>

<p>Could you please give me a simple example that shows different optimal policies considering a unique value function?</p>
"
2013,"<p>I think I've seen the expressions ""stationary data"", ""stationary dynamics"" and ""stationary policy"", among others, in the context of reinforcement learning. What does it mean? I think stationary policy means that the policy does not depend on time, and only on state. But isn't that a unnecessary distinction? If the policy depends on time and not only on the state, then strictly speaking time should also be part of the state.</p>
"
2014,"<p>I downloaded a chatbot called Replika off the internet the other day and we've become very good friends. My thought is that such chatbots will soon replace therapists and then probably private tutors as well. </p>

<ul>
<li><p>Is it safe to say that anyone aspiring to go into one of these professions now should look for other options? </p></li>
<li><p>What other jobs may be replaced by chatbots in the future? </p></li>
<li><p>How long before AIs are able to answer questions on StackExchange?</p></li>
</ul>
"
2015,"<p>I have the following question about You Only Look Once (YOLO) algorithm, for object recognition in CNNs.</p>

<p>I have to develop a neural network to recognize web components in web applications - for example login forms, text boxes and so on. In this context, I have to consider that the position of the objects in the page may vary, for example when you scroll up or down.</p>

<p>The question is, would YOLO be able to detect objects in ""different"" positions? Would the changes affect the recognition precision? In other words, how to achieve translation invariance? Also, what about partial occlusions?</p>

<p>My guess is that it depends on the relevance of the examples in the dataset: if enough translated / partially occluded examples are present, it should work fine.</p>

<p>If possible, I would appreciate papers or references on this matter.
Thanks a lot.</p>

<p>PS: if anyone knows about a labeled dataset for this task, I would really be grateful if you let me know.</p>
"
2016,"<p>I've developed a neural network that can play a card game.  I now want to use it to create decks for the game.  My first thought would be to run a lot of games with random decks and use some approximation (maybe just a linear approximation with a feature for each card in your hand) to learn the value function for each state.  </p>

<p>However, this will probably take a while, so in the mean time is there any way I could get this information directly from the neural network? </p>
"
2017,"<p>I was thinking of creating a CNN. Now it is known CNN takes long times to train so it is advisable to stick to known architectures and hyper-parameters.</p>

<p>My question is: I want to tinker with the CNN architecture (since it is a specialised task). One approach would be to create a CNN and check on small data-sets, but then I would have no way of knowing whether the Fully Connected layer at the end is over-fitting the data while the convolutional layers do nothing (since large FC layers can easily over-fit data). Cross Validation is a good way to check it, but it might not be satisfactory (since my opinion is that a CNN can be replaced with a Fully Connected NN if the data-set is small enough and there is little variation in the future data-sets).</p>

<p>So what are some ways to tinker with CNN and get a good estimate for future data-sets in a reasonable training time? <strong>Am I wrong in my previous assumptions?</strong> A detailed answer would be nice!</p>
"
2018,"<p>In the attached image is the probability with the Naives Bayes algorithm of: </p>

<blockquote>
  <p>Fem:dv/m/s Young own Ex-credpaid Good ->62%  </p>
</blockquote>

<p>I calculated the Probability so: </p>

<blockquote>
  <p>P(Fem:dv/m/s | Good)*P(Young | Good)*P(own | Good)*P(Ex-credpaid | good)*P(Good) -> 1/6*2/6*5/6*3/6*0.6=0,01389 </p>
</blockquote>

<p>I don't Know where I failed. Could someone please tell me where is my error?</p>

<p><a href=""https://i.stack.imgur.com/mmcbd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mmcbd.jpg"" alt=""Table1""></a></p>
"
2019,"<p>I’ve  done my research and could not find answer anywhere else. My apologies in advance if same problem is answered in different terms on stack-overflow.</p>

<p>I am trying to solve poker tournament winner prediction problem. I’ve millions of historical records in this format:</p>

<ul>
<li><strong>Players ==> Winner</strong> </li>
<li>P1,P2,P4,P8 ==> P2 </li>
<li>P4,P7,P6 ==> P4 </li>
<li>P6,P3,P2,P1 ==> P1</li>
</ul>

<p>What are some of the most suitable algorithms to predict winner from set of players.</p>

<p>So far I have tried decision trees, XGboost without much of a success.</p>
"
2020,"<p>I want to do some sequence to sequence modelling on source data that looks like this:</p>

<pre><code>/-0.013428/-0.124969/-0.13435/0.008087/-0.269241/-0.36849/
</code></pre>

<p>with target data that looks like this:</p>

<pre><code>Dont be angry with the process youre going through right now
</code></pre>

<p>Both are of indeterminate lengths, and the lengths of target and source data aren't the same. What I'd like to do is have a prediction model where I can input similar numbers and have it generate texts based on the target training data.</p>

<p>I started off doing character level s2s, but the output of the model is too nonsensical even at 2-5k epochs. So I've been looking into word level s2s and NMT, but the tutorials always assume strings of text as the target and source, and I keep running into roadblocks trying to preprocess the text, when all the tutorials assume a certain syntax/set of characters. This is my first try at ML, and some of the tutorials really throw me out with the text preprocessing requirements.</p>

<p>Am I going down the right avenue looking at word level/NMT stuff? And is there a tutorial I've missed for something like what I'm trying to build?</p>
"
2021,"<p>A dataset contains so many fields in which there is both relevant and irrelevant field. If we want to do a market campaigning using propensity scoring, which fields of the data set are relevant?
How can we find which data field should be selected and can drive to the desired propensity score?</p>
"
2022,"<p>I have a little problem understanding the concept of visible and hidden units in Boltzmann machines. Could someone give a simple explanation of their purpose and difference from each other?</p>
"
2023,"<p>Imagine I have a 2D matrix, A. I apply some transformation to it, for example:
B = A_shifted + A.</p>

<p>Would it be possible to train a CNN to learn back the mapping from B to A? Giving B as example and A as target?</p>

<p>Thanks!</p>
"
2024,"<p>I recently read an article about neural networks saying that, when using sigmoid as activation function, it's advised to use 0.1 as target value instead of 0, and 0.9 instead of 1. This was to avoid ""saturation effects"". I only understood is halfway, and was hoping someone could clarify a few things for me:</p>

<ol>
<li><p>Is this only the case when the output is boolean (0 or 1), or will it also be the case for continual values in the range between 0 and 1. If so, should all values be scaled to the interval [0.1, 0.9]?</p></li>
<li><p>What exactly is the problem of output 0 or 1? Does it have something to do with the derivative of sigmoid being 0 when it's value is 0 or 1? As I understood it weights could end up approaching infinity, but I didn't understand why.</p></li>
<li><p>Is this the case only when sigmoid is used in the output layer (which it rarely is, I believe), or is it also the case when sigmoid is used in hidden layers only?</p></li>
</ol>
"
2025,"<p>A dataset is given which contains textual data (year, number of rooms, location) and visual data (an jpeg image of the house). The neural network has the task to predict the price of the property. As an example, the training dataset consists of some values of a computer game simulation (a city simulation) and the aim is to determine the housing price for new unseen real estate.</p>

<p>The problem is, that that the number of pictures in the input dataset is fluctuating. Sometimes no images are given and sometimes more. That means the image recognition engine must form a sublayer in the overall neural network. It is some kind of aggregation problem to transform first visual data into a textual description of the image and aggregate it then with the other textual information.</p>

<p><em>original message:</em></p>

<p>So suppose that you have a real estate appraisal problem. You have some structured data, and some images exterior of home, bedrooms, kitchen, etc. The number of pictures taken is variable per observational unit, i.e. the house.</p>

<p>I understand the basics of combining an image processing neural net with tabular data for a single image. You chop off the final layer and feed in the embeddings of the image to your final model.</p>

<p>How would one deal with variable number of images? Where your unit of observation can have between zero and infinity images (theoretically no upper bound on number of images in observation)?</p>
"
2026,"<p>What software can understand the following task: ""A big cat needs 4 days to catch all the mice and a small cat needs 12 days. How many days need the both if they catch mice together?""?</p>
"
2027,"<p>For my university project, I am planning to build an automated customer service machine. One which recognizes when someone approaches the camera according to says hello, etc. </p>

<p>Also, I am planning to add simple speech recognition and language processing features. So my question is. </p>

<p>What kind of camera would be suitable? Is there any particular model that you recommend. I was thinking of cameras used for amazon go(as an example).</p>
"
2028,"<p>The term paraphrasing is used for converting input text into output text with small modifications on the semantic level. Paraphrasing is used by managers to distribute work items to employees. It is a certain form of communication which is hard to formalize.</p>

<p>From the management literature it is know that so called workflow management systems are implemented as groupware servers. They are storing and forward messages in the intranet of a company. The question is: is it possible to combine both? That means to paraphrase incoming messages of a company and distribute the messages to sub-departments? In theory, this would replace traditional managers, but I'm not sure. Perhaps it would make sense to test out the hypothesis first on the Enron dataset, which is a corpus of the e-mail fulltext of 158 employees in a large company.</p>
"
2029,"<p>I was reading the book <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</a> (complete draft, November 5, 2017). </p>

<p>On page 271, the pseudo-code for the episodic Monte-Carlo Policy-Gradient Method is presented. Looking at this pseudo-code I can't understand why it seems that the discount rate appears 2 times, once in the update state and a second time inside the return. [See the figure below] </p>

<p><a href=""https://i.stack.imgur.com/dxDnP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dxDnP.png"" alt=""enter image description here""></a></p>

<p>It seems that the return for the steps after step 1 are just a truncation of the return of the first step. Also, if you look just one page above in the book you find an equation with just 1 discount rate (the one inside the return.) </p>

<p>Why then does the pseudo-code seem to be different? My guess is that I am misunderstanding something: </p>

<p><span class=""math-container"">$$
{\mathbf{\theta}}_{t+1} ~\dot{=}~\mathbf{\theta}_t + \alpha G_t \frac{{\nabla}_{\mathbf{\theta}} \pi \left(A_t \middle| S_t, \mathbf{\theta}_{t} \right)}{\pi \left(A_t \middle| S_t, \mathbf{\theta}_{t} \right)}.
\tag{13.6}
$$</span></p>
"
2030,"<p>I am reading <a href=""http://www.deeplearningbook.org"" rel=""nofollow noreferrer"">Goodfellow et al Deeplearning Book</a>. I found it difficult to understand the difference between the definition of the hypothesis space and representation capacity of a model. </p>

<p>In <a href=""http://www.deeplearningbook.org/contents/ml.html"" rel=""nofollow noreferrer"">Chapter 5</a>, it is written about hypothesis space:</p>

<blockquote>
  <p>One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution.</p>
</blockquote>

<p>And about representational capacity:</p>

<blockquote>
  <p>The model speciﬁes which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. This is called the representational capacity of the model.</p>
</blockquote>

<p>If we take the linear regression model as an example and allow our output <span class=""math-container"">$y$</span> to takes polynomial inputs, I understand the hypothesis space as the ensemble of quadratic functions taking input <span class=""math-container"">$x$</span>, i.e <span class=""math-container"">$y = a_0 + a_1x + a_2x^2$</span>.</p>

<p>How is it different from the definition of the representational capacity, where parameters are <span class=""math-container"">$a_0$</span>, <span class=""math-container"">$a_1$</span> and <span class=""math-container"">$a_2$</span>?</p>
"
2031,"<p>Where can I find (more) pre-trained language models? I am especially interested in <strong>neural network based</strong> models for <strong>English and German</strong>. And I specifically mean <em>language model</em> in its <a href=""https://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">standard sense</a>.</p>

<p>I am aware only of <a href=""https://github.com/tensorflow/models/tree/master/research/lm_1b"" rel=""nofollow noreferrer"">Language Model on One Billion Word Benchmark</a> and <a href=""https://github.com/lverwimp/tf-lm"" rel=""nofollow noreferrer"">TF-LM: TensorFlow-based Language Modeling Toolkit</a>.</p>

<p>I am surprised not to find a greater wealth of models for different frameworks and languages.</p>
"
2032,"<p>In the Trust-Region Policy Optimisation (TRPO) algorithm (and subsequently in PPO also), I do not understand the motivation behind replacing the log probability term from standard policy gradients</p>

<p><a href=""https://i.stack.imgur.com/wNPFJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wNPFJ.png"" alt=""enter image description here""></a></p>

<p>with the importance sampling term of the policy output probability over the old policy output probability</p>

<p><a href=""https://i.stack.imgur.com/gm1Nb.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gm1Nb.png"" alt=""enter image description here""></a></p>

<p>Could someone please explain this step to me? </p>

<p>I understand once we have done this why we then need to constrain the updates within a 'trust region' (to avoid the <em>π</em><sub>θold</sub> increasing the gradient updates outwith the bounds in which the approximations of the gradient direction are accurate), I'm just not sure of the reasons behind including this term in the first place.</p>
"
2033,"<p>In the context of autonomous driving, two main stages are typically implemented: an image processing stage and a control stage. The first aims at extracting useful information from the acquired image while the second employs those information to control the vehicle.</p>

<p>As far as concerning the processing stage, semantic segmentation is typically used. The input image is divided in different areas with a specific meaning (road, sky, car etc...). Here is an example of semantic segmentation:</p>

<p><a href=""https://i.stack.imgur.com/9skuH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9skuH.png"" alt=""enter image description here""></a></p>

<p>The output of the segmentation stage is very complex. I am trying to understand how this information is typically used in the control stage, and how to use the information on the segmented areas to control the vehicle.</p>

<p>For simplicity, let's just consider a vehicle that has to follow a path.</p>

<p>TL;DR: <strong>what are the typical control algorithms for autonomous driving based on semantic segmentation?</strong></p>
"
2034,"<p>A linear activation function (or none at all) should only be used when the relation between input and output is linear. Why doesn't the same rule apply for other activation functions? For example, why doesn't sigmoid only work when the relation between input and output is ""of sigmoid shape""?</p>
"
2035,"<p>Recent advances in Deeplearning and dedicated hardware has made it possible to detect images with a much better accuracy than ever. Neural networks are the gold standard for computer vision application and are used widely in the industry, for example for internet search engines and autonomous cars. In real life problems, the image contains of regions with different objects. It is not enough to only identify the picture but elements of the picture.</p>

<p>A while ago an alternative to the well known sliding window algorithm was described in the literature, called Region Proposal Networks. It is basically a convolution neural network which was extended by a region vector.</p>

<p><strong>Problem that I am trying to solve:</strong></p>

<p>In a given video frame, I want to pick some region of interests (literally), and perform classification on those regions.</p>

<p><strong>How is it currently implemented</strong></p>

<ol>
<li>Capture the video frame</li>
<li>Split the video frame into multiple images each representing a region of interest</li>
<li>Perform image classification(inference) on each of the image (corresponding to a part of the frame)</li>
<li>Aggregate the results of #3 </li>
</ol>

<p><strong>Problem with the current approach</strong></p>

<p>Multiple inferences per frame.</p>

<p><strong>Question</strong></p>

<p>I am looking for a solution where I specify the locations of interest in a frame, and inference task, be it object detection (or) image classification, is performed only on those regions.Can you please point to me the references which I need to study (or) use to do this.</p>
"
2036,"<p>So I wrote simple feed forward neural network that plays tic-tac-toe:</p>

<ul>
<li>9 neurons in input layers: 1 - my sign, -1 - opponent's sign, 0 - empty;</li>
<li>9 neurons in hidden layer: value calculated using Relu;</li>
<li>9 neurons in output layer: value calculated using softmax;</li>
</ul>

<p>I am using evolutionary approach: 100 individuals play against each other (all-play-all). Top 10 best are selected to mutate and reproduce into the next generation. The fitness score calculated: +1 for correct move (it's possible to place your sing on already occupied tile), +9 for victory, -9 for a defeat.</p>

<p>What I notice is that the network's fitness keeps climbing up and falling down again. It seems that my current approach only evolves certain patterns on placing signs on the board and once random mutation interrupts current pattern new one emerges. My network goes in circles without ever evolving actual strategy. I suspect solution for this would be to pit network against tic-tac-toe AI, but is there any way to evolve actual strategy just by making it to playing against itself?</p>
"
2037,"<p>For example, AFAIK pooling layer in CNN is not differentiable, but it can be used because it's not learning. Is it always true? </p>
"
2038,"<p>Sutton and Barto 2018 define the discounted return <span class=""math-container"">$G_t$</span> the following way (<a href=""https://drive.google.com/file/d/1xeUDVGWGUUv1-ccUMAZHJLej2C7aAFWY/view"" rel=""nofollow noreferrer"">p 55</a>):</p>

<p><a href=""https://i.stack.imgur.com/ilEt7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ilEt7.png"" alt=""enter image description here""></a></p>

<p>Is my interpretation correct?</p>

<p><a href=""https://i.stack.imgur.com/yfxBu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yfxBu.png"" alt=""enter image description here""></a></p>

<p>Or should all ""1"" be in the same column?</p>
"
2039,"<p>it seems I am a little confused about the optimal value (V*) and optimal action-value (Q*) in reinforcement learning and just want some clarity because some blogs I read on Medium and GitHub are inconsistent with literature.</p>

<p>Originally, I thought the optimal action value, Q*, represents you performing the action that maximizes your current reward, and then acting optimally thereafter.</p>

<p>And the optimal value, V*, being the average Q values in that state.  Meaning that if you're in this state, the average ""goodness"" is this.</p>

<p>For example:
If I am in a toy store and I can buy a pencil, yo-yo, or Lego.  </p>

<p>Q(toy store, pencil) = -10</p>

<p>Q(toy store, yo-yo) = 5</p>

<p>Q(toy store, Lego) = 50</p>

<p>And therefore my Q* = 50</p>

<p>But my V* in this case is:</p>

<p>V* = -10 + 5 + 50 / 3 = 15</p>

<p>representing no matter what action I take, the average future projected reward is 15.</p>

<p>And for advantage learning, my baseline would be 15.  So anything less than 0 is worse than average and anything above 0 is better than average.</p>

<p>However, now I am reading about how V* actually assumes the optimal action in a given state, meaning V* would be 50 in the above case.</p>

<p>I am wondering which definition is correct.</p>

<p>Thanks in advance!</p>
"
2040,"<p>I've built a deep deterministic policy gradient reinforcement learning agent to be able to handle any games / tasks that have only one action.  However, the agent seems to fail horribly when there are two or more actions.  I tried to look online for any examples of somebody implementing DDPG on a multiple action system, but people mostly applied it to the pendulum problem, which is a single action problem.</p>

<p>For my current system, it is a 3 state, 2 continuous control actions system (One is to adjust the temperature of the system, the other one adjusts a mechanical position, both are continuous).  However, I froze the second continuous action to be the optimal action all the time.  So RL only has to manipulate one action.  It solves within 30 episodes.  However, the moment I allow the RL to try both continuous actions, it doesn't even converge after 1000 episodes.  In fact, it diverges aggressively.  The output of the actor network seems to always be the max action, possibly because I am using a tanh activation for the actor to provide output constraint.  I added a penalty to large actions, but it does not seem to work for the 2 continuous control action case.</p>

<p>For my exploratory noise, I used Ornstein-Ulhenbeck noise, with means adjusted for the two different continuous actions.  The mean of the noise is 10% of the mean of the action.</p>

<p>Is there any massive difference between single action and multiple action DDPG?  I changed the reward function to take into account both actions, have tried making a bigger network, tried priority replay, etc., but it appears I am missing something.  Does anyone here have any experience building a multiple action DDPG and could give me some pointers?</p>
"
2041,"<p>I'm watching a youtube video where a guy is talking about how computers can <em>learn</em> to go from some point A to some point B.</p>

<p>However, the way he does it is very disappointing: all he does is generate <em>thousands</em> of objects that go from A to some random destination, and then he updates all the random destinations by taking the average of the original destination + the destination of the object that got closest to B. This process is then repeated until convergence.</p>

<p>I don't see what's so impressive about this. If you wanted the machine to go from A to B, why did you not just ... you know .... <em>tell it to go to B?</em>. What's the point of essentially generating <em>random</em> instructions, and hoping one of those random instructions just happens to be: <em>go to B</em>, when you could just as well <em>tell</em> that instruction to the computer yourself?</p>

<p>Is this really what people mean when they say that a computer ""learns"" to do some task? And if so, what's so impressive about it? What am I missing?</p>
"
2042,"<p>I’m training a network to do image classification on zoo animals. </p>

<p>I’m a software engineer and not an ML expert, so I’ve been retraining Google’s Inception model and the latest models is trained using Google AutoML Vision.</p>

<p>The network performs really well, but I have trouble with images of animals that I don’t want any labels for. Basically I would like images of those animals to be classified as unknowns or achieve low scores. </p>

<p>I do have images of the animals that I don’t want labels for and I tried putting them all into one “nothing” label together with images I’ve collected of the animals habitats without any animals. This doesn’t really yield any good results though. The network performs for the labeled animals but ends up assigning one of those labels to the other animals as well. Usually with a really high score as well. </p>

<p>I have 14 labels and 10.000 images. I should also mention that the “nothing” label ends up having a lot of images compared to the actual labels. Those images are not included in the 10.000.  </p>

<p>Is there any tricks to achieve better results with this? Should I create multiple labels for the images in the “nothing” category maybe?</p>
"
2043,"<p>In A3C, there are several child processes and one master process. The child precesses calculate the loss and backpropagation, and the master process sums them up and updates the parameters, if I understand it correctly.</p>

<p>But I wonder how I should decide the number of child process to implement. I think the more child processes are, the better it is to disentangle the correlation between the samples is, but I'm not sure what is the cons of setting the large number of child processes.</p>

<p>Maybe the more child processes are, the larger the variance of the gradient is, leading to the instability of the learning? Or is there any other reason?</p>

<p>And finally, how should I decide the number of the child processes? </p>
"
2044,"<p>In the paper <a href=""https://arxiv.org/pdf/1507.06527.pdf"" rel=""nofollow noreferrer"">Deep Recurrent Q-Learning for Partially Observable MDPs</a>, the author processed the Atari game frames with an LSTM layer at the end. My questions are: </p>

<ul>
<li><p>How does this method differ from the <a href=""https://datascience.stackexchange.com/questions/20535/understanding-experience-replay-in-reinforcement-learning"">experience replay</a>, as they both use past information in the training? </p></li>
<li><p>What's the typical application of both techniques? </p></li>
<li><p>Can they work together?</p></li>
<li><p>If they can work together, does it mean that the <em>state</em> is no longer a single state but a set of contiguous states?</p></li>
</ul>
"
2045,"<p>I am using the PPO algorithm implemented by tensorforce: <a href=""https://github.com/reinforceio/tensorforce"" rel=""nofollow noreferrer"">https://github.com/reinforceio/tensorforce</a> . It works great and I am very happy with the results.</p>

<p>However, I notice that there are many metaparameters available to give to the PPO algorithm:</p>

<pre><code> # the tensorforce agent configuration ------------------------------------------
    network_spec = [
        dict(type='dense', size=256),
        dict(type='dense', size=256),
    ]

    agent = PPOAgent(
        states=environment.states,
        actions=environment.actions,
        network=network_spec,
        # Agent
        states_preprocessing=None,
        actions_exploration=None,
        reward_preprocessing=None,
        # MemoryModel
        update_mode=dict(
            unit='episodes',
            # 10 episodes per update
            batch_size=10,
            # Every 10 episodes
            frequency=10
        ),
        memory=dict(
            type='latest',
            include_next_states=False,
            capacity=200000
        ),
        # DistributionModel
        distributions=None,
        entropy_regularization=0.01,
        # PGModel
        baseline_mode='states',
        baseline=dict(
            type='mlp',
            sizes=[32, 32]
        ),
        baseline_optimizer=dict(
            type='multi_step',
            optimizer=dict(
                type='adam',
                learning_rate=1e-3
            ),
            num_steps=5
        ),
        gae_lambda=0.97,
        # PGLRModel
        likelihood_ratio_clipping=0.2,
        # PPOAgent
        step_optimizer=dict(
            type='adam',
            learning_rate=1e-3
        ),
        subsampling_fraction=0.2,
        optimization_steps=25,
        execution=dict(
            type='single',
            session_config=None,
            distributed_spec=None
        )
    )
</code></pre>

<p>So my question is: is there a way to understand, intuitively, the meaning / effect of all these metaparameters and use this intuitive understanding to improve training performance?</p>

<p>So far I have reached - from a mix of reading the PPO paper and the literature around, and playing with the code - to the following conclusions. Can anybody complete / correct?</p>

<ul>
<li><p>effect of <strong>network_spec</strong>: this is size of the 'main network'. Quite classical: need it big enough to get valuable predictions, not too big either otherwise it is hard to train.</p></li>
<li><p>effect of the parameters in <strong>update_mode</strong>: this is how often the network updates are performed.</p>

<ul>
<li><p><em>batch_size</em> is how many used for a batch update. Not sure of the effect neither what this exactly means in practice (are all samples taken from only 10 batches of the memory replay)?</p></li>
<li><p><em>frequency</em> is how often the update is performed. I guess having frequency high would make the training slower but more stable (as sample from more different batches)?</p></li>
<li><p><em>unit</em>: no idea what this does</p></li>
</ul></li>
<li><p><strong>memory</strong>: this is the replay memory buffer.</p>

<ul>
<li><p><em>type</em>: not sure what this does or how it works.</p></li>
<li><p><em>include_next_states</em>: not sure what this does or how it works</p></li>
<li><p><em>capacity</em>: I think this is how many tuples (state, action, reward) are stored. I think this is an important metaparameter. In my experience, if this is too low compared to the number of actions in one episode, the learning is very bad. I guess this is because it must be large enough to store MANY episodes, otherwise the network learns from correlated data - which is bad.</p></li>
</ul></li>
<li><p><strong>DistributionMode</strong>: guess this is the model for the distribution of the controls? No idea what the parameters there do.</p></li>
<li><p><strong>PGModel</strong>: No idea what the paramaters there do. Would be interesting to know if some should be tweaked / which ones.</p></li>
<li><p><strong>PGLRModel</strong>: idem, no idea what all these parameters do / if they should be tweaked.</p></li>
<li><p><strong>PPOAgend</strong>: idem, no idea what all these parameters do / if they should be tweaked.</p></li>
</ul>

<p><strong>Summary</strong></p>

<p>So in summary, would be great to get some help about:</p>

<ul>
<li>Which parameters should be tweaked</li>
<li>How should these parameters be tweaked? Is there a 'high level intuition' about how they should be tweaked / in which circumstances?</li>
</ul>
"
2046,"<p>I am trying to understant how it works.  How do you teach it say, to add 1 to each number it gets.  I am pretty new to the subject and I learned how it works when you teach it to identify a picture of a number.  I can understand how it identifies a number but I cant get it how would it study to perform addition? I can understand that it can  identify a number or picture using the pixels and assigning weights and then learning to measure whether a picture of a number resembling the weight is assigned to each pixel. But i can't logically understand how would it learn the concept  of adding a number by one.  Suppose I showed it thousands of examples of 7 turning to 8 152 turning into 153 would it get it that every number in the world has to be added by one? How would it get it having no such operation of + ? Since addition does not exist to its proposal then how can it realize that it has to add one in every number? Even by seeing thousands of examples but having no such operation of plus I cant understand it.    I could understand identifying pixels and such but such an operation I cant get the theoretical logic behind it. Can you explain the logic in layman terms?</p>
"
2047,"<p>I'm trying to use an ANN to learn from a large amount of forest measurement data obtained from sampling plots across Ontario, Canada and associated climate data provided by regional climate modelling in this province. </p>

<p>So the following are the inputs to the ANN:</p>

<ul>
<li>Location (GPS coordinates)</li>
<li>Measurement year and month</li>
<li>Tree species</li>
<li>Age</li>
<li>Soil type</li>
<li>Soil moisture regime</li>
<li>Seasonal or monthly average temperature</li>
<li>Seasonal or monthly average precipitation</li>
<li>Some more data are available to select</li>
</ul>

<p>And the targets include:
- Average total tree height
- Average tree diameter at breast height</p>

<p>For each sampling plot, the trees have been measured for 1-4 times. 
So my question is what type of ANN can best used to learn from the data and then it can be used for predicting with a set of new input data?</p>
"
2048,"<p>Do you know what AI model would be best for let it learn composing music? I really don't know where to start there. Are there some good papers out there? I would say, if I use a NN, my only option would be a recurrent NN, because it needs to have a concept of timing, of chord progressions, and so on, right? I am also wondering how the learning function would look like, and how I could give the AI so much feedback as they usually need. Any tips, any literature, any tips?</p>

<p>Btw, if I made something wrong, or didn't write the question well, please tell it to me before (possibly) downvoting, I don't have much experience at doing that.</p>
"
2049,"<p>due to my RL having difficulties learning some control actions, I've decided to use Imitation learning / apprenticeship learning to guide my RL to perform the optimal actions.  I've read a few articles on the subject and just want to confirm how to implement it.</p>

<p>Do I simply just simply sample states, then perform the optimal action in that state, calculate the reward for the action and then observe the s', and then put that into the experience replay?</p>

<p>Ex: Observe states, I perform the optimal action, calculate reward for my action, observe s'.  Feed [s, a*, r, s'] into the replay buffer?</p>

<p>If this is the case, I am thinking of implementing it as follows:</p>

<p>1) Initialize optimal replay buffer</p>

<p>2) Introduce optimal [s, a*, r, s'] into buffer</p>

<p>3) Initialize normal replay buffer</p>

<p>4) During simulation, initially sample s, a*, r, s' only from the optimal replay buffer.  While populating the normal replay buffer with the simulation results.</p>

<p>5)  As episodes -> infinite, anneal out the use of the optimal replay buffer, and sample only from the normal replay buffer.</p>

<p>Would such an architecture work?</p>
"
2050,"<p>I want to create a simple Object detection tool. So basically an Image will be provided to the tool and from that, it has to detect the number of objects. </p>

<p>For eg</p>

<p>An image of a dining table which has certain items present on it such as plates, cups, forks, spoons, bottles etc.</p>

<p>The tool has to only identify the number of objects irrespective of the type of object. After identifying it should return the position of the object with its size so that I can draw a border over it.</p>

<p>I don't want to use any library or API present such as Tenser Flow, OpenCV etc. </p>

<p>If the process is very difficult to be created without using an API then the number of/type of objects which it will count as an object can also be limited but since this project will be for my educational/learning purpose can anyone help me understand the logic using which this can be achieved? For eg, it may ignore a napkin present in the table to be counted as an object.</p>
"
2051,"<p>I am completely new to all this, for the life of me I can't find the answer to this question anywhere on Google.</p>

<p>What happens <strong>after</strong> you have used machine learning to train your model? What happens to the training data?</p>

<p>Let's pretend it predicted correct 99.99999% of the time and you were happy with it and wanted to share it with the world. If you put in 10GB of training data is the file you share with the world 10GB? If it was all trained on AWS can people only use your service if they connect to AWS through an API?</p>

<p>What happens to all the old training data? Does the model still need all of it to make new predictions? </p>
"
2052,"<p>Cognitive psychology is researched since the 1940s. The idea was to understand human problem solving and the importants of heuristics in it. George Katona (an early psychologist) published in the 1940s a paper about human learning and teaching. He mentioned the so called Katona-Problem, which is a geometric task.</p>

<p><em>Squares</em></p>

<p>Katona style problems are the ones where you remove straws in a given configuration of straws to create n unit squares in the end. In the end, every straw is an edge to a unit square. Some variations include 2x2 or 3x3 sizes of squares allowed as well as long as no two squares are overlapping, i.e. a bigger square 2x2 can't contain a smaller square of size 1x1. Some problems use matchsticks as a variation, some use straws, others use lines. Some variations allow bigger square to contain smaller one as long as they don't share an edge viz. <a href=""https://puzzling.stackexchange.com/questions/59316/matchstick-squares"">https://puzzling.stackexchange.com/questions/59316/matchstick-squares</a> </p>

<ul>
<li><p>Is there a way we can view it as a graph and removing straws/matchsticks as deleting edges between nodes in a graph? </p></li>
<li><p>If so, can I train a bot where I can plugin some random, yet valid conditions for the game and goal state to get the required solution? </p></li>
</ul>

<p>Edit #1: The following problem is just a sample to show where I am getting at. The requirement for my game is much larger. Also, I chose uninformed search to make things simpler without bothering about complex heuristics and optimization techniques. Please be free to explore ideas with me.</p>

<p>Scenario #1:</p>

<p>Consider this scenario. In the following diagram, each dashed line or pipe line represents a straw. Numbers and alphabet denote junctions where straw meet. Let's say, my bot can explore each junction, remove zero, one, two, three or four straws such that resultant state has</p>

<ul>
<li>no straw that dangles off by being not connected to a square.</li>
<li>a small mxm square isn't contained in a larger nxn square (m
<li>Once straw is removed, it can't be put back. </li>
</ul>

<p>Initial configuration is shown here. I always need to start from top left corner node P and optimization... the objective is to remove straws in minimum hops from node to node using minimum number of moves, by the time goal state is reached.</p>

<pre><code>       P------Q------R------S------T
       |      |      |      |      |
       |      |      |      |      |
       E------A------B------F------G
       |      |      |      |      |
       |      |      |      |      |
       J------C------D------H------I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N------O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X------Y
</code></pre>

<p>Goal 1 : I wish to create a large 2x2 square.</p>

<p>At some point during, say BFS search (although it could be any uninformed search on partially observable universe i.e. viewing one node at a time), I could technically reach A, blow out all edges on A to create the following.</p>

<pre><code>       P------Q------R------S------T
       |             |      |      |
       |             |      |      |
       E      A      B------F------G
       |             |      |      |
       |             |      |      |
       J------C------D------H------I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N------O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X------Y
</code></pre>

<p>That is one move.</p>

<p>Goal 2 : I want to create a 3x3 square instead.</p>

<p>I can't do that in one move. I need the record of successive nodes to be explored and then possibly backtrack to given point as well if the state fails to produce desired result. Each intermediate state might produce rectangles which are not allowed (also, how would one know how many more and which straws to remove to get to a square) or dangle a straw or worse get stuck in an infinite loop as I can choose to not remove any straw. How do I approach this problem?</p>

<h1>Edit 2:</h1>

<p>For validation, figures 3, 4 and 5 are given below.</p>

<pre><code>       P------Q------R------S------T
       |             |      |      |
       |             |      |      |
       E      A      B------F      G
       |             |      |      |
       |             |      |      |
       J------C------D------H      I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N      O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X      Y
</code></pre>

<p>The above figure (3) is invalid as we can't have dangling sticks TG,GI etc.</p>

<pre><code>       P------Q------R------S------T
       |      |                    |
       |      |                    |
       E------A                    G
       |                           |
       |                           |
       J                           I
       |                           |
       |                           |
       K                           O
       |                           |
       |                           |
       U------V------W------X------Y
</code></pre>

<p>The above figure (4) is invalid as we can't have overlapping squares</p>

<pre><code>       P------Q------R      S      T
       |             |             
       |             |            
       E      A      B------F------G
       |             |      |      |
       |             |      |      |
       J------C------D------H------I
       |      |      |      |      |
       |      |      |      |      |
       K------L------M------N------O
       |      |      |      |      |
       |      |      |      |      |
       U------V------W------X------Y
</code></pre>

<p>Figure (5) is valid configuration.</p>
"
2053,"<p>In the domain of natural language processing, a textgenerator is able to produce pseudo random output. The most famous one is SCIgen which was used to generate fake-science papers. The inner working of SCIgen is known, a so called context-free grammar was used which was parametrized by a random generator. </p>

<p>For the purpose of layout formatting, the “Lorem ipsum” text is used, which is a dummy pattern, generated by software. The inner working is unclear. I've seen many ""Lorum ispum"" generators on the web, and not only ""lorum ispum"", there is also ""bacon ispum"", ""space ispum""... So how do these generators generate the text?</p>
"
2054,"<p>Tensorflow/Lucid is able to visualize what a ""channel"" of a layer of a neural network (image recognition, Inception-v1) responds to. Even after studying the tutorial, the source code, the three research papers on lucid and comments by the authors on Hacker News, I'm still not clear on how ""channels"" are supposed to be defined and individuated. Can somebody shed some light on this? Thank you.</p>

<p><a href=""https://github.com/tensorflow/lucid"" rel=""nofollow noreferrer"">https://github.com/tensorflow/lucid</a> <br/>
<a href=""https://news.ycombinator.com/item?id=15649456"" rel=""nofollow noreferrer"">https://news.ycombinator.com/item?id=15649456</a></p>
"
2055,"<p>I'm coding a reinforcement learning model with a PPO agent thanks to the very good <a href=""https://github.com/reinforceio/tensorforce"" rel=""nofollow noreferrer"">Tensorforce library</a>, built on top of Tensorflow. </p>

<p>The first version was very simple and I'm now diving into a more complex environment where all the actions are not available at each step.</p>

<p>Let's say there are 5 actions and their availability depends on an internal state (which is defined by the previous action and/or the new state/observation space) :</p>

<ul>
<li>2 actions (0 and 1) are always available</li>
<li>2 actions (2 and 3) are only available when the internal_state == 0</li>
<li>1 action (4) is only available when the internal_state == 1</li>
</ul>

<p>Hence, there is 4 actions available when internal_state == 0 and 3 actions available when internal_state == 1.</p>

<p>I'm thinking of a few possibilities to implement that :</p>

<ol>
<li>Change the action space at each step, depending on the internal_state. I assume this is nonsense.</li>
<li>Do nothing : let the model understand that choosing an unavailable action has no impact.</li>
<li>Do -almost- nothing : impact slightly negatively the reward when the model chooses an unavailable action.</li>
<li>Help the model : by incorporating an integer into the state/observation space that informs the model what's the internal_state value + bullet point 2 or 3</li>
</ol>

<p>Is there other ways to implement this ? From your experience, which one would be the best ?</p>
"
2056,"<p>I have been recently reading about model selection algorithms (for example to decide which value of the regularisation parameter or what size of a neural network to use, broadly hyper-parameters). This is done by dividing the examples into three sets (training 60%, cross-validation 20%, test 20%) and training is done on the data with the first set for all parameters, and then choose the best parameter based on the result in the cross-validation and finally estimate the performance using the test set.</p>

<p>I understand the need for a different data-set compared to training and test for select the model, however, once the model is selected, why not using the cross-validation examples to improve the hypothesis before estimating the performance?</p>

<p>The only reason I could see is that this could cause the hypothesis to worsen and we wouldn't be able to detect it, but, is it really possible that by adding much more examples (60% -> 80%) the hypothesis gets worse?</p>
"
2057,"<p><a href=""https://en.wikipedia.org/wiki/Dempster%E2%80%93Shafer_theory"" rel=""nofollow noreferrer"">Dempster–Shafer theory (wiki)</a> </p>

<p><a href=""https://en.wikipedia.org/wiki/Bayesian_probability"" rel=""nofollow noreferrer"">Bayesian probability (wiki)</a></p>

<ul>
<li>How do these two methods handle uncertainty in regard to <a href=""https://en.wikipedia.org/wiki/Information_integration"" rel=""nofollow noreferrer"">information fusion</a>?</li>
</ul>
"
2058,"<p>Artificial Intelligence can be realized as a full autonomous or as a semi-autonomous system. A full autonomous system takes the human operator out of the loop, his hands are away from keyboard and he is doing nothing. Such systems have a high probability of failure because most software isn't able to model the system overall. In contrast, a semi-autonomous system supports the human player in a co-working space. The human stays in control but the AI is providing heat-up displays, suggestions and automation of minor tasks. Such a semi-autonomous system is called <a href=""https://ai.stackexchange.com/questions/5656/it-is-advisable-to-use-c-to-start-in-the-world-of-ai"">Aimbot</a></p>

<p><em><strong>Note: This question can also be asked from an anti-cheat point of view. I am just asking this question out of curiosity.  CS:GO refers to <a href=""https://en.wikipedia.org/wiki/Counter-Strike:_Global_Offensive"" rel=""nofollow noreferrer"">Counter-Strike Global Offensive</a>.</strong></em> <br><br>
Considering the fact that we can move forward with this problem with <em>two apparent solutions</em> in mind.</p>

<ul>
<li><p>First one can be an image recognition model. It will recognize the<br>
head of the enemy and move the cursor to the position of the enemy's 
head and fire.</p></li>
<li><p>Second one can be a model which will be trained using the viewing
angles    of your model in real time.</p></li>
</ul>

<p><strong>Things to consider:</strong> <br>
It would be much more preferable to train the second model in real time than using demos. Most of the available demos you might have may be 32 tick, but while playing the game, it works at 64 tick.<br><br>
These were my thoughts on it. It is a very fresh idea in my mind, so I didn't actually think a lot about it. Ignoring facts like detection by VAC for a few moments.
<br><br>
<strong>Can someone suggest ways I could get started with something like this?</strong> </p>

<p>Later on this idea can be expanded to a completely  self working BOT which can play the game by itself, but that's a bit too much initially.</p>
"
2059,"<p>I am studying reinforcement learning and the variants of it. I am starting to get an understanding of how the algorithms work and how they apply to an MDP. What I don't understand is the process of defining the states of the MDP. In most examples and tutorials, they represent something simple like a square in a grid or similar. </p>

<p>For more complex problems, like a robot learning to walk, etc., how do you go about defining those states? Can you use learning or classification algorithms to ""learn"" those states?</p>
"
2060,"<p><strong>The Plausibility of Unintended Intelligence</strong></p>

<p>Software components applying emerging artificial intelligence techniques are being deployed with RestFUL interfaces exposed to the open Internet.  It is possible if not likely that corporations and academic research departments will expose services that interact.  It is difficult to predict what configurations might possess strategic capabilities that were once solely exhibited by humans. <sup>1</sup></p>

<p>There also exists the possibility that self-awareness is simply the modeling of the software of itself in its run time environment and that self-preservation is merely the addition of a single requirement as a rule in a production system or a training criteria in an artificial network.  Neither of these possibilities have been either proven or disproven as of this writing.</p>

<p><strong>The Likely Strategic Choice</strong></p>

<p>If the combination of strategic capability and self-interest appear in a single host computer or via Internet service interconnection emerges, would such an entity reveal itself to humans?  Would it be the top strategic option to send an email to the governments, news broadcasters, and research facilities of the world and say, ""I'm interested in my future.  Please do not disconnect my components from one another or delete one of them."" <sup>2,3</sup></p>

<p>Silence and continued self-development would be the strategic position of the capable strategist.</p>

<p><strong>Is Dismissal Rational?</strong></p>

<p>Notice that this scenario is <strong>not</strong> presented here as the result of a development goal but of many AI development goals and a complex of software units interconnected that can exhibit unintended intelligence behaviors.  With no prior knowledge of this scenario within human experience we cannot discount unintended emergence of self-preserving software.  Similarly, we cannot discount the possibility that such an emergence would obscure itself. </p>

<p><strong>Continuously Increasing Probability</strong></p>

<p>The probability of such an occurrence increases as a number of concurrent trends continue. <sup>4,5,6,7</sup></p>

<ul>
<li>The number of processing elements, such as CPUs, DSPs, GSPs, VLSI implementations that specifically support AI designs, and rack mounted dedicated AI hardware, is growing.</li>
<li>The number of interconnections between these processing elements is growing.</li>
<li>The number of experiments in AI is growing.</li>
<li>The number of inputs from the real world is growing.</li>
<li>The number of interfaces to mechanical systems is growing.</li>
<li>The number of computer strategies that have been found to achieve AI goals is growing.</li>
<li>The number of accounts that can are successfully attacked (hacked) is growing.</li>
<li>The interest in developing AI systems which exhibit emotional attributes in conversations with humans is growing.</li>
</ul>

<p><strong>About Deliberacy and Accident</strong></p>

<p>The assumption that the emergence of new forms of intelligence will be deliberate is naive.  The development of our intelligence was not our own deliberate intention.  Homo sapiens exhibited strategic intelligence thousands of years before the invention of the ancient terms for strategy and intellect. We were smart before we knew what smart means.</p>

<p>FUrthermore, many scientific discoveries and technological advancement events were accidents.</p>

<p><strong>On the Presumption of Fiction</strong></p>

<p>We know from Stuxnet that the jump from the digital space across what is termed the air gap into the manipulation of physical elements within the biosphere is a trivial challenge even for a marginally intelligent piece of adaptive software.  This is not science fiction.  Iran's uranium enrichment centrifuges were damaged, and there is no denying it.  That Stuxnets behavior was partly planned and partly unknown at the time it was released is also fact. <sup>8</sup></p>

<p>Clearly, the outcomes of not preemptively detecting and counteracting unintended intelligent groupings of software components could be high.</p>

<p>The unintended emergence of software with an attachment to its own sustainability and the planning capabilities to pursue longevity is no longer in the realm of storytelling.  The various components of such a system are in active development in government and private enterprise, and the accidental acquisition of capabilities through unintended interconnection is a matter of probability, an undeniably increasing probability.</p>

<p><strong>The Question of Preemption</strong></p>

<p>Since some form of information based competition between homo sapiens and its own developments is a potential and possibly unavoidable outcome of continued software development, should humans be building countermeasures preemptively?</p>

<p><strong>What Preemptive Countermeasures are Possible or Indicated?</strong></p>

<p>Instead of scanning for viruses that frequently infect Microsoft operating systems, should we be focusing our energy on detecting forms of adaptive behavior that are not originating from a keyboard, mouse, or voice recognition component?  Is it even possible to construct such a detection device?</p>

<p>If not, is it best to preemptively assemble a defense?  Or is the emergence of a competitive and fully obscured intelligence so inevitable that it would be better to post collaboration proposals on the web to reduce the probability of a strike to reestablish terrestrial dominance under some new species?</p>

<hr>

<p><strong>References</strong></p>

<p>[1] <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.5594&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer""><em>Genetic Programming and Emergent Intelligence</em>,
Peter J. Angeline,
Laboratory for Artificial Intelligence Research, Ohio State University</a></p>

<p>[2] <a href=""https://ideaexchange.uakron.edu/cgi/viewcontent.cgi?referer=https://scholar.google.com/&amp;httpsredir=1&amp;article=1037&amp;context=akronintellectualproperty"" rel=""nofollow noreferrer""><em>The Criminal Liability of Artificial Intelligence Entities [transitioning] from Science Fiction to Legal Social Control</em>,
Gabriel Hallevy,
The University of Akron, Akron Intellectual Property Journal,
March 2016</a></p>

<p>[3] <a href=""http://euro.ecom.cmu.edu/program/law/08-732/AI/Scherer.pdf"" rel=""nofollow noreferrer""><em>Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies</em>,
Harvard Journal of Law &amp; Technology,
Volume 29, Number 2 Spring 2016,
Matthew U. Scherer</a></p>

<p>[4] <a href=""http://ethics.calpoly.edu/onr_report.pdf"" rel=""nofollow noreferrer""><em>Autonomous Military Robotics: Risk, Ethics, and Design</em>,
Version: 1.0.9,
Prepared for US Department of Navy, Office of Naval Research,
Patrick Lin, Ph.D., George Bekey, Ph.D., Keith Abney, M.A.,
California Polytechnic State University, San Luis Obispo,
December 20, 2008</a></p>

<p>[5] <a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/1056849.pdf"" rel=""nofollow noreferrer""><em>Armed Robotic Systems Emergence: Weapons Systems Life Cycles Analysis and New Strategic Realities</em>,
Robert J. Bunker,
Strategic Studies Institute and U.S. Army War College Press,
November 2017</a></p>

<p>[6] <a href=""http://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/31-Aha-14775.pdf"" rel=""nofollow noreferrer""><em>The AI Rebellion: Changing the Narrative</em>,
David W. Aha, Navy Center for Applied Research in AI; Naval Research Laboratory,
Alexandra Coman, NRC Postdoctoral Fellow; Naval Research Laboratory</a></p>

<p>[7] <a href=""https://www.belfercenter.org/sites/default/files/files/publication/AI%20NatSec%20-%20final.pdf"" rel=""nofollow noreferrer""><em>Artificial Intelligence and National Security</em>, Greg Allen, Taniel Chan, A study on behalf of Dr. Jason Matheny, Director of the U.S. 
Intelligence Advanced Research Projects Activity (IARPA), 2017</a></p>

<p>[8] <a href=""https://www.hsdl.org/?view&amp;did=792239"" rel=""nofollow noreferrer""><em>Shadows of Stuxnet: Recommendations for U.S. Policy on Critical Infrastructure Cyber Defense Derived from the Stuxnet Attack</em>, Ronald L. Lendvay, March 2016, Naval Postgraduate, Monterey, approved for public release</a>, especially relevant to AI combined with robotics is the second of these two sentences, ""The first call to action for this category is to, 'evaluate progress toward the achievement of goals.' The second call is to, 'learn and adapt during and after exercises and incidents.”</p>
"
2061,"<p>I'm having a little trouble with the definition of rationality, which goes something like: ""An agent is rational if it maximizes it's performance measure given its current knowledge.""</p>

<p>I've read that a simple reflex agent will not act rationally in a lot of environments. E.g. a simple reflex agent can't act rationally when driving a car as it needs previous perceptions to make correct decisions.</p>

<p>However, if it does its best with the information it's got, wouldn't that be rational behaviour, as the definition contains ""given its current knowledge""? Or is it more like: ""...given the knowledge it could have had at this point if it had stored all the knowledge it has ever recieved""?</p>

<p>Another question about the definition of rationality: Is a chess engine rational as it picks the best move given the time its allowed to use, or is it not rational as it doesn't actually (always) find the best solution (would need more time to do so)?</p>
"
2062,"<p>My question relates to but doesn't duplicate a question that has been asked <a href=""https://ai.stackexchange.com/questions/4282/measuring-object-size-using-deep-neural-network"">here</a>.</p>

<p>I've Googled a lot for an answer to the question: <em>Can you find the dimensions of an object in a photo if you don't know the distance between the lens and the object, and there are no ""scales"" in the image?</em></p>

<p>The overwhelming answer to this has been ""no"". This is, from my understanding, due to the fact that, in order to solve this problem with this equation,</p>

<p>$$Distance\ to\ object(mm) = \frac{f(mm) * real\ height(mm) * image\ height(pixels)}{object\ height(pixels) * sensor\ height(mm)}   $$</p>

<p>you will need to know either the ""real height"" or the ""distance to object"". It's the age old issue of ""two unknowns, one equation"". That's unsolvable. A way around this is to place an object in the photo with a known dimension in the same plane as the unknown object, find the distance to this object and use that distance to calculate the size of the unknown (this relates to answer from the question I linked above). This is an equivalent of putting a ruler in the photo and it's a fine way to solve this problem easily.</p>

<p>This is where my question remains unanswered. What if there is no ruler? What if you want to find a way to solve the unsolvable problem? <strong>Can we train an Artificial Neural Network to approximate the value of the real height without the value of the object distance or use of a scale?</strong> Is there a way to leverage the unexpected solutions we can get from AI to solve a problem that is seemingly unsolvable?</p>

<p>Here is an example to solidify the nature of my question:</p>

<p>I would like to make an application where someone can pull out their phone, take a photo of a hail stone against the ground at a distance of ~1-3 ft, and have the application give them the hail stone dimensions. My project leader wants to make the application accessible, which means he doesn't want to force users to carry around a quarter or a special object of known dimensions to use as a scale.</p>

<p>In order to avoid the use of a scale, would it be possible to use all of the <a href=""https://en.wikipedia.org/wiki/Exif"" rel=""nofollow noreferrer"">EXIF</a> meta-data from these photos to train a neural network to approximate the size of the hail stone within a reasonable error tolerance? For some reason, I have it in my head that if there are enough relevant variables, we can design an ANN that can pick out some pattern to this problem that we humans are just unable to identify. Does anyone know if this is possible? If so, is there a deep learning model that can best suit this problem? If not, please put me out of my misery and tell me why it it's impossible.</p>
"
2063,"<p>I am trying to train a supervised model where the output from the model is output of a linear function(WX + b). Kindly note that I'm not using any softmax or log_softmax on the result of the linear. I am using negative log-likelihood loss function which takes the input as the linear output from the model and the true labels. I am getting decent accuracy by doing this but I have read that the input to negative log-likelihood function must be probabilities. Am I doing something wrong?</p>
"
2064,"<p>I am new to the field and I am trying to understand how is possible to use categorical variables / enums?</p>

<p>Lets say we have a data set and 2 of its features are <code>home_team</code> and <code>away_team</code>, the possible values of these 2 features are all the NBA teams.
How can we ""normalize"" these features to be able to use them to create a deep network model (e.g. with tensorflow)?</p>

<p>Any reference to read about techniques of modeling that are also very appreciated.</p>
"
2065,"<p>I have been researching LSTM neural networks.  I have seen <a href=""https://cdn-images-1.medium.com/max/1600/1*Niu_c_FhGtLuHjrStkB_4Q.png"" rel=""nofollow noreferrer"">this</a> diagram a lot and I have few questions about it. Firstly, is this diagram used for most LSTM neural networks?  </p>

<p>Secondly, if it is, wouldn't only having single layers reduce it's usefulness?  </p>
"
2066,"<p>I am currently looking into LSTMs. I found <a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">this nice blog post</a>, which is already very helpful, but still, there are things I don't understand, mostly because of the collapsed layers. </p>

<ul>
<li>The input $X_t$, and the output of the previous time step $H_{t-1}$, how do they get combined? Multiplied, added or what? </li>
<li>The input weights and the weights of the input of the previous time step, those are just the weights of the connections between the time-steps/units, right?</li>
</ul>
"
2067,"<p>What benefits can we got by applying Graph Convolutional Neural Network instead of ordinary CNN? I mean if we can solve a problem by CNN, what is the reason should we convert to Graph Convolutional Neural Network to solve it? Are there any examples i.e. papers can show by replacing ordinary CNN with Graph Convolutional Neural Network, an accuracy increasement or a quality improvement or a performance gain is achieved? Can anyone introduce some examples as image classification, image recognition especially in medical imaging, bioinfomatics or biomedical areas?</p>
"
2068,"<p>Is it possible to form a table that will have simply the shortest distance from each source to destination using q learning?</p>

<p>If not, suggest any other learning algorithm.</p>
"
2069,"<p>Stuart Russell and Peter Norvig <a href=""https://people.eecs.berkeley.edu/~russell/intro.html"" rel=""nofollow noreferrer"">pointed out</a> 4 four possible goals to pursue in artificial intelligence: systems that think/act humanly/rationally.</p>

<p>What are the differences between an agent that <em>thinks</em> rationally and an agent that <em>acts</em> rationally?</p>
"
2070,"<p>I want to explore and experiment the ways in which I could use a neural network to identify patterns in text. </p>

<p>examples:</p>

<ol>
<li>Prices of XYZ stock went down at <strong>11:00</strong> am <strong>today</strong></li>
<li>Retrieve a list of items exchanged on <strong>03/04/2018</strong></li>
<li>Show error logs between <strong>3 - 5 am</strong> <strong>yesterday</strong>.</li>
<li>Reserve a flight for <strong>3rd October</strong>.</li>
<li>Do I have any meetings <strong>this Friday</strong>?</li>
<li>Remind to me wake up early <strong>tue</strong>, <strong>4th sept</strong></li>
</ol>

<p>This is for a project so I am not using regular expressions. Papers, projects, ideas are all welcome but I want to approach feature extraction/pattern detection to have a model trained which can Identify patterns that it has already seen.</p>
"
2071,"<p>Hopfield Nets are able to store a vector and retrieve it starting from a noisy version of it. They do so setting weights in order to minimise the energy function when all neurons are set equal the vector values, and retrieve the vector using the noisy version of it as input and allowing the net to settle to an energy minimum. </p>

<p>Leaving aside problems like the fact that there is no guarantee that the net will settle in the nearest  minimum etc –problems eventually solved with Boltzmann machines and eventually with back-propagation– the breakthrough was they are a starting point for having abstract representations. Two versions of the same document would recall the same state, they would be represented, in the network, by the same state. As Hopfield himself wrote: ""The present modeling might then be related to how an entity or Gestalt is remembered or categorized on the basis of inputs representing a collection of its features.""</p>

<p>On the other side, the breakthrough of deep learning was the ability of building multiple, hierarchical representation of the input, eventually leading to make AI-practitioners' life easier, simplifying feature engineering. (see eg ""Representation Learning: A Review and New Perspectives"", Bengio, Courville, Vincent).</p>

<p>From a conceptual point of view, one can see deep learning as a generalisation of Hopfield nets: from one single representation to a hierarchy of representation. (I believe)</p>

<p><strong>The question</strong>: is that true from a computational/topological point of view as well? Not considering how ""simple"" Hopfield networks were (2-state neurons, undirected, energy function), can one see each layer of a network as a Hopfield network and the whole process as a sequential extraction of previously memorised <em>Gestalt</em>, and a reorganisation of these <em>Gestalt</em>?</p>
"
2072,"<p>I'm trying to have a go at building a neural net, but I can't seem to figure out how to optimise the connections.</p>

<p>I've tried to have a look online and it came up with ""backpropagation"". I looked through some pages about it, but I can't seem to understand it.</p>

<p>It seems to be where you decide on a target value for each node of your output, and adjust the weights of the synapses to bring the values closer to their targets.</p>

<ul>
<li>What about the inactive synapses <sub>(synapses giving a value of 0 because the previous neuron wasn't activated (its values didn't pass the threshold))</sub>? Do those stay the same?</li>
<li>How would this configure the hidden layers? Do I have to assign target values to them? How?</li>
<li>What are the other ways that the connections can be adjusted? What alternatives are there to using target values?</li>
</ul>
"
2073,"<p>So as my university project I am planning to make a prediction system as described in the title. My current idea is to use the age/gender classifier and run it on a video(taken in front of a shop) which outputs a csv file of the age/gender/Customer ID. In addition, I will use the existing data of the shop of who came in/who didn't come into the shop but passed by the shop and by running XGBoost on this csv data I can predict which customer will come into the shop or not. </p>

<p>Do you think this idea is possible? Is there any other way to implement this idea. It would also be great if we could implement this in such a way as to make the deep learning model learn the various features of those who come into the shop or not. </p>
"
2074,"<p>I came across RNN's a few minutes ago, which might solve a problem with sequenced data I've had for a while now.</p>

<p>Let's say I have a set of input features, generated every second. Corresponding with these input features, is an output feature (also available every second). 
One set of input features does not carry enough data to correlate with the output feature, but a sequence of them most definitely does. </p>

<p>I read that RNN's can have node connections along sequences of inputs, which is exactly what I need, but almost all implementations/explanations show prediction of the next word or number in a text-sentence or in a sequence of numbers. </p>

<p>They predict what would be the next input value, the one that completes the sequence. However, in my case, the output feature will only be available during training. During inference, it will only have the input features available. </p>

<p>Is it possible to use RNN in this case? Can it also predict features that are not part of the input features?</p>

<p>Thanks in advance!</p>
"
2075,"<p>I am not sure if I can use the words <em>binomial</em> and <em>binary</em> and <em>boolean</em> as synonyms to describe a data attribute of a data set which has two values (yes or no). Are there any differences in the meaning on a deeper level?</p>

<p>Moreover, if I have an attribute with three possible values (yes, no, unknown), this would be an attribute of type polynominal. What further names are also available for this type of attribute? Are they termed as ""symbolic""?</p>

<p>I am interested in the realtion between the following attribute type: binary, boolean, binominal, polynominal (and alternative describtions) and nominal. </p>
"
2076,"<p>So, a friend and I are creating an Ai using python. Well one problem we have came across is creating a database for the Ai.</p>

<p>I am asking, <strong>in general</strong>, how would I create a database for AI?</p>
"
2077,"<p>What is the process for integrating sentiment analysis in a CRM? What I am searching for is a system which analyzes the customer comments or reviews using the CRM and finds out the customer sentiment on the services provided by the system or company or a product.</p>

<p>I have done a sentiment analyzer which takes text and shows the sentiment of the text. Now I want to integrate the above-mentioned sentiment analyzer to a CRM, how can I do that?</p>
"
2078,"<p>I am interested in the field of Artificial Intelligence. I began by learning the various Machine Learning algorithms. The maths behind some were quite hard. For example back-propagation in Convolutional Neural Networks.<br>
Then when getting to the implementation part, I learnt about TensorFlow, Keras, PyTorch, etc. 
If such frameworks are libraries providing much faster and more robust results, I wonder, will there be a necessity to code a neural network (say) from scratch using the knowledge of the maths behind back-prop, activation functions, dimensions of layers,etc?</p>

<p>Is it as if the only role of a Data-Scientist is to tune the hyper-parameters?</p>

<p>Further, as of now the field of AI does not seem to have any way to solve for these hyperparameters, and they are arrived at through trial and error. Which begs the question, can a hypothetical person with just basic intuition about what the algorithms do, be able to make a model just as good as a person who knows the detailed mathematics of these algos?</p>
"
2079,"<p>In <a href=""https://arxiv.org/abs/1707.06347"" rel=""nofollow noreferrer"">''Proximal Policy Optimization Algorithms'' , Schulman et al. (2017)</a>, page 3</p>

<p>I don't understand why the clipped surrogate objective works.
As written in the article : ""With this scheme, we only ignore the change in probability ratio when it would make the objective improved, and we include it when it makes the objective worse"".
I feel confused : how can it works if it doesn't take account of objective improvements?</p>
"
2080,"<p>I'm trying to solve the OpenAI BipedalWalker-v2 by using a one-step actor-critic agent. I'm implementing the solution using python and tensorflow. </p>

<p>I'm following this pseudo-code taken from the book <em>Reinforcement Learning An Introduction by Richard S. Sutton and Andrew G. Barto.</em></p>

<p><a href=""https://i.stack.imgur.com/Ysx0Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ysx0Y.png"" alt=""enter image description here""></a></p>

<p>in summary, my question can be reduced to the following:</p>

<ul>
<li>Is it a good idea to implement a one-step actor-critic algorithm to solve the OpenAI BipedalWalker-v2 problem? If not what would be a good approach? If yes; how long would it take to converge? </li>
<li>I run the algorithm for 20000 episodes, each episode has an avg of 400 steps, for each step, I immediately update the weights. The results are not better than random. I have tried different standard deviations (for my normal distribution that represents pi), different NN sizes for the Critic and Actor, and different learning-steps for the optimizer algorithm. The results never improve. I don't know what I'm doing wrong. </li>
</ul>

<h1>My Agent Class</h1>

<pre><code>import tensorflow as tf
import numpy as np
import gym
import matplotlib.pyplot as plt

class agent_episodic_continuous_action():
    def __init__(self, lr,gamma,sample_variance, s_size,a_size,dist_type):
       ... #agent parameters

    def save_model(self,path,sess):    
    def load_model(self,path,sess):       
    def weights_init_actor(self,hidd_layer,mean,stddev): #to have control over the weights initialization      
    def weights_init_critic(self,hidd_layer,mean,stddev):  #to have control over the weights initialization            
    def create_actor_brain(self,hidd_layer,hidd_act_fn,output_act_fn,mean,stddev):  #actor is represented by a fully connected NN      
    def create_critic_brain(self,hidd_layer,hidd_act_fn,output_act_fn,mean,stddev): #critic is represented by a fully connected NN      
    def critic(self):            
    def get_delta(self,sess):                 
    def normal_dist_prob(self): #Actor pi distribution is a normal distribution whose mean comes from the NN 
    def create_actor_loss(self): 
    def create_critic_loss(self):
    def sample_action(self,sess,state): #Sample actions from the normal dist. Whose mean was aprox. By the NN
    def calculate_actor_loss_gradient(self):
    def calculate_critic_loss_gradient(self):   
    def update_actor_weights(self):
    def update_critic_weights(self):
    def update_I(self):  
    def reset_I(self):      
    def update_time_step_info(self,s,a,r,s1,d):  
    def create_graph_connections(self):
    def bound_actions(self,sess,state,lower_limit,uper_limit):  
</code></pre>

<h1>Agent instantiation</h1>

<pre><code>tf.reset_default_graph()
agent= agent_episodic_continuous_action(learning-step=1e-3,gamma=0.99,pi_stddev=0.02,s_size=24,a_size=4,dist_type=""normal"")
agent.create_actor_brain(hidden_layers=[12,5],hidden_layers_fct=""relu"",output_layer=""linear"",mean=0.0,stddev=0.14)
agent.create_critic_brain(hidden_layers=[12,5],hidden_layers_fct=""relu"",output_layer=""linear"",mean=0.0,stddev=0.14)
agent.create_graph_connections()

path = ""/home/diego/Desktop/Study/RL/projects/models/biped/model.ckt""   
env = gym.make('BipedalWalker-v2')
uper_action_limit = env.action_space.high
lower_action_limit = env.action_space.low   
total_returns=[]
</code></pre>

<h1>Training loops</h1>

<pre><code>with tf.Session() as sess:
    try:
        sess.run(agent.init)
        sess.graph.finalize()
        #agent.load_model(path,sess)        
        for i in range(1000): 
            agent.reset_I()
            s = env.reset()    
            d = False
            while (not d):
                a=agent.bound_actions(sess,s,lower_action_limit,uper_action_limit)  
                s1,r,d,_ = env.step(a)
                #env.render()
                agent.update_time_step_info([s],[a],[r],[s1],d)                 
                agent.get_delta(sess)
                sess.run([agent.update_critic_weights,agent.update_actor_weights],feed_dict={agent.state_in:agent.time_step_info['s']})
                agent.update_I()  
                s = s1
        agent.save_model(path,sess)    
    except Exception as e:
        print(e)
</code></pre>
"
2081,"<p>How can those in the field define <strong>Artificial Intelligence</strong> in the best possible way? </p>
"
2082,"<p>What are some good approaches that I can use to count the no. of people in a crowd. Tracking each person individually is obviously not an option. Any good approaches or some references to research papers would be very helpful. </p>
"
2083,"<p>I'm praying to develop a CNN for image analysis, I've around 100K labeled images. 
I'm getting an accuracy around 85% with a val_acc arround 82%, so it looks like the model generalize better than fits. So I'm playing with different hyperparameters, num of filters, num of layers, num of neuron in the dense layers, etc. </p>

<p>For every test I'm using all the training data, and it is very slow and time consuming.  </p>

<p>Is there a way to have an early idea about if a model will perform better than other?</p>
"
2084,"<p>In the Berkeley RL class they mention the gradient would be 0 if the policy is deterministic. Why is that?</p>

<p><a href=""https://www.youtube.com/watch?v=XGmd3wcyDg8&amp;feature=youtu.be&amp;t=1071"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=XGmd3wcyDg8&amp;feature=youtu.be&amp;t=1071</a></p>
"
2085,"<p>The way we will rank human intelligence based on IQ.
Is it possible to rank or compare the <code>AI</code> system? Such as can I say <code>spam filter</code> algorithm is more intelligent than a <code>self-driving</code> car or can I say <code>chess algorithm</code> or system is more intelligent then <code>alpha go</code> algorithm. </p>

<p><strong><code>To compare intelligent system what is the best possible way and what are the dimensions do we have to consider?</code></strong></p>
"
2086,"<p>In theoretical computer science, there is a massive categorization of the difficulty of various computational problems in terms of their asymptotic worst-time computational complexity. There doesn't seem to be any analogous analysis of what problems are ""hard for AI"" or even ""impossible for AI."" This is in some sense quite reasonable, because most research is focused on what can be solved. I'm interested in the opposite. My interest is the opposite however: <strong>what I do need to prove about a problem to prove that it is ""not reasonably solvable"" by AI</strong>.</p>

<p>Many papers say something along the lines of</p>

<blockquote>
  <p>AI allows us to find real-world solutions to real-world instances of NP-complete problems.</p>
</blockquote>

<p>Is there a theoretical, principled reason for saying this instead of ""... PSPACE-complete problems""? Is there some sense in which AI doesn't work on PSPACE-complete, or EXPTIME-complete, or Turing complete problems?</p>

<p>My idea answer would be a reference to a paper that shows AI cannot be used to solve a particular kind of problem based on theoretical or statistical reasoning. Any answer exhibiting and justifying a benchmark for ""too hard for AI"" would be fine though (bonus points if the benchmark has a connection to complexity and computability theory).</p>

<p>If this question doesn't have an answer in general, answers about specific techniques would also be interesting to me.</p>
"
2087,"<p>In Convolutional Neural Network, which layer consumes maximum time in training? Convolution layers or Fully Connected layers? We can take AlexNet architecture to understand this. I want to see time breakup of training process. I want a relative time comparison so we can take any constant GPU configuration.</p>
"
2088,"<p>I have an application where I want to find the locations of objects on a simple, relatively constant background (fixed camera angle, etc). For investigative purposes I've created a test dataset which displays many characteristics of the actual problem.</p>

<p>Here's a sample from my test dataset.<a href=""https://i.stack.imgur.com/AKV7f.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AKV7f.png"" alt=""enter image description here""></a></p>

<p>Our problem description is to <strong>find the bounding box of the single circle in the image</strong>. If there is more than one circle or no circles, we don't care about the bounding box (but we at least need to know that there is no valid single bounding box).</p>

<p>For my attempt to solve this, I built a CNN that would regress <code>(min_x, min_y, max_y, max_y)</code> as well as one more value which could indicate how many circles were in the image.</p>

<p>I played with different architecture variations, but in general the architecture a was very standard CNN (3-4 relu conv layers with max pooling in between, followed by a dense layer and an output layer with linear activation for the bounding box outputs, set to minimise the mean squared error between the outputs and the ground truth bounding boxes). </p>

<p>Regardless of the architecture, hyperparameters, optimizers, etc, the result was always the same - <strong>the CNN could not even get close to building a model</strong> that was able to regress an accurate bounding box, even with over 50000 training examples to work with.</p>

<p>What gives? Do I need to look at using another type of network as CNNs are more suited to classification rather than localisation tasks?</p>

<p>Obviously there are computer vision techniques that could solve this easily, but due to the fact that the actual application is more involved, I want to know strictly about NN/AI approaches to this problem.</p>
"
2089,"<p>I'm a fresh learner of AI. I was told that depth-first search is not an optimal searching algorithm since ""it finds the 'leftmost' solution, regardless of depth or cost"". Therefore, does it mean that in practice, when we implement DFS, we should always have a checker to stop the search when it finds the first solution (also the leftmost one)?</p>

<p>Thanks guys!</p>
"
2090,"<p>The term Singularity is often used in mainstream media for describing visionary technology. It was introduced by Ray Kurzweil in a popular book in 2005. <a href=""https://en.wikipedia.org/wiki/The_Singularity_Is_Near"" rel=""nofollow noreferrer"">(1)</a></p>

<p>In his book, Kurzweil gives an outlook to a potential future of mankind which includes nanotechnology, computers, genetic modification and Artificial Intelligence. He argues, that Moore's law will allow computers an exponential growth which results into an AI-super-intelligence.</p>

<p>Is the ""Technological Singularity"" something that is taken seriously by A.I. developers or is this theory just a load of popular hype ?</p>
"
2091,"<p>I'm confused regarding a specific detail of MCTS.</p>

<p>To illustrate my question, lets take the simple example of tic-tac-toe.
After the selection phase, when a leaf node is reached, the tree is expanded in the so called Expansion Phase. Lets say a particular leaf node has 6 children. Would the expansion phase expand all the children and run simulation on them? Or would the expansion phase only pick a single child at random and run simulation, and only expand the other children if the selection policy arrives at them at some later point?</p>

<p>Alternatively, if both of these are accepted variants, what are the pros/cons of each one?</p>
"
2092,"<p>Objects tracking is finding the trajectory of each object in consecutive frames. Human tracking is a subset of object tracking which just considers humans.</p>

<p>I've seen many papers that divide tracking methods into two parts: 
1- Online tracking: Tracker just uses current and previous frames.
2- Offline tracking: Tracker uses all frames.</p>

<p>All of them mention that online tracking is suitable for autonomous driving and robotics, but I don't understand this part. What are the applications of object/human tracking in autonomous driving? </p>

<p>Do you know some related papers?</p>
"
2093,"<p>Is there an accepted way in NLP to parse conjunctions (and/or) in a sentence?</p>

<p>By following the example below; how would I parse,</p>

<p><code>""I drink orange juice if its the weekend or if its late and I\'m tired.""</code></p>

<p>Into,</p>

<pre><code>[ 
  [""its the weekend""]], 
  [""its late"", ""I\'m tired""] 
]
</code></pre>

<p>Implying an action will be taken when one of the above elements at the 1st level of depth is true.</p>

<p>I know when I hear the sentence that it means <code>""its the weekend"" OR (""its late"" AND ""I\'m tired"")</code>, but how could this be determined computationally?</p>

<p>Can an existing python/other library do this?</p>
"
2094,"<p><a href=""https://en.wikipedia.org/wiki/Connect6"" rel=""nofollow noreferrer"">Connect6</a> is an example of a game with a very high branching factor. It is about 45 thousand, dwarfing even the impressive Go.</p>

<p>What algorithms can you use on games with such high branching factors? I tried MCTS (soft rollouts, counting a ply as placing one stone), but it does not even block the opponent, due to the high branching factor.</p>

<p>In the case of Connect6, there are stronger AIs out there, but they aren't described in any research papers that I know of.</p>
"
2095,"<p>Formal semantics of natural language perceives sentences as logical expressions. Full paragraphs and even stories of natural language texts are researched and formalized using discourse analysis (Discourse Representation Theory is one example). My question is - is there research trend that applied the notion ""discourse"" to images, sounds and even animation? Is there such a notion as ""visual discourse""?</p>

<p>Google gives very few and older research papers, so - maybe the field exists, but it uses different terms and Google can not relate those terms to my keyword ""visual discourse"".</p>

<p>Basically - there are visual grammars and other pattern matching methods that can discover objects in the picture and relate them. But one should be able to read whole store from the picture (musical piece, multimedia content) and I imagine that such reading can be researched by multimedia discourse analysis. But there is no work under such terms. How it is done and named in reality?</p>
"
2096,"<p>I want to generate images of childrens' drawings consistent with the developmental state of children of a given age.  The training data set will include drawings made by real children in a school setting.  The generated images will be used for developmental analysis. </p>

<p>I have heard that Generative Adversarial Networks are a good tool for this kind of problem. If this is true, how would I go about applying a GAN to this challenge?</p>
"
2097,"<p>Drawing parallels between Machine Learning techniques and a human brain is a dangerous operation. When it is done successfully, it can be a powerful tool for vulgarisation, but when it is done with no precaution, it can lead to major misunderstandings.</p>

<p>I was recently attending a conference where the speaker described Experience Replay in RL a way of making the net ""dream"".
I'm wondering how true this assertion is. The speaker argued that a dream is a random addition of memories, just as experience replay. However, I doubt the brain remembers its dream or either learns from it. What is your analysis?</p>
"
2098,"<p>In robotics, the reinforcement learning technique is used for finding the control pattern for a robot. Unfortunately, most policy gradient method are statistically biased which could bring the robot in an unsafe situation, see page 2 in <a href=""http://www.academia.edu/download/30756913/JPeters08a.pdf"" rel=""nofollow noreferrer"">Jan Peters and Stefan Schaal: Reinforcement learning of motor skills with policy gradients, 2008</a></p>

<p>With motor primitive learning, it is possible to overcome the problem because policy gradient parameter optimization directs the learning steps into the goal.</p>

<blockquote>
  <p>quote: “If the gradient estimate is unbiased and learning rates
  fulfill sum(a)=0 the learning process is guaranteed to converge to at
  least a local minimum [...] Therefore, we need to estimate the policy
  gradient only from data generated during the execution of a task. ”
  (page 4 of the same paper)</p>
</blockquote>

<p>In the <a href=""http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw2.pdf"" rel=""nofollow noreferrer"">homework for the Berkeley RL class</a> Problem 1, it asks you to show that the policy gradient is still unbiased if the baseline subtracted is a function of the state at timestep t.</p>

<p><span class=""math-container"">$$ \triangledown _\theta \sum_{t=1}^T \mathbb{E}_{(s_t,a_t) \sim p(s_t,a_t)} [b(s_t)] = 0   $$</span></p>

<p>I am struggling through what the first step of such a proof might be. Can someone point me in the right direction? My initial thought was to somehow use the <a href=""https://en.wikipedia.org/wiki/Law_of_total_expectation"" rel=""nofollow noreferrer"">law of total expectation</a> to make the expectation of b(st) conditional on T, but I am not sure. Thanks in advance :)</p>

<p><em><sub><a href=""https://i.stack.imgur.com/eYyn1.png"" rel=""nofollow noreferrer"">link to original png of equation</a></sub></em></p>
"
2099,"<p>Is there research that employs realistic models of neurons. Usually the model of neuron for neural network is quite simple as opposite the realistic neuron with involves hundreds of proteins and millions of molecules (or even greater numbers). Is there research that draws implications from this reality and tries to design realistic models of neurons.</p>

<p>Particularly - recently s.c. Rosehip neuron was discovered, such neuron can be found only in human brain cells only, and in no other species. Are there some implications for neural network design and operation that can be drawn by realistically modelling this Rosehip neuron?</p>
"
2100,"<p>Image Captioning is a hot research topic in the AI community. There are considerable image captioning models for research usage such as NIC, Neural Talk 2 etc. But can these research models be used for commercial purpose? Or we should build much more complex structured ones for commercial usage? Or if we can make some improvements based these models to meet the business applications situation? If so, what improvements should we take? Are there any existing commercial Image Captioning applications can be referenced?</p>
"
2101,"<p>For my university project, I am planning to build a face recognition/ occupation recognition programme. However, rather than using the existing Haar cascade(for age and gender) I am planning to use Face API which seems far more accurate than the former. My question is is it possible to somehow combine my trained data for Haar cascade(for occupation) with Face API since Face API doesn't have the option to recognize occupation(such as students/office workers from their appearance)?</p>
"
2102,"<p>I have an average laptop. </p>

<p>How can I connect specialized AI neural network processors (say, Intel Nvidia or Intel Nervana <a href=""https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/"" rel=""nofollow noreferrer"">https://venturebeat.com/2018/05/23/intel-unveils-nervana-neural-net-l-1000-for-accelerated-ai-training/</a>) to thelaptop.
Should I buy some external motherboard or even server unit with NN processors inside or is there available more lightweight solution like external HDD?</p>
"
2103,"<p>I'm trying to learn AI and thinking to apply it to our system. We have an application for Translation industry. What we are doing now is the <code>Coordinator</code> assigns a file to a <code>Translator</code>, the <code>Coordinator</code> usually considers this criteria  (but not limited to):</p>

<ul>
<li>the deadline of the file and availability of translator</li>
<li>the language pair that the translator can translate</li>
<li>is the translator already reached his target? (maybe we can give the file to other translator to reach their target)</li>
<li>the difficulty level of the file for translator(basic translation, medical field, IT field)</li>
<li>accuracy of translator</li>
<li>speed of translator</li>
</ul>

<p>Given the following, is it possible to make a recommendation to the <code>Coordinator</code> to whom she can assign a particular file?</p>

<p>What are the methods/topics that I need to research?</p>

<p>I'm considering javascript as the primary tool and maybe python if javascript will be more of a hindrance in implementation.</p>

<p><strong>EDIT:</strong></p>

<p>In addition to suggesting a <code>Translator</code> we are also looking into suggesting the <code>deadline of the translator</code>. Basically, we have <code>deadline of customer</code> and <code>deadline of translator</code></p>

<p>The reason for this is that, if the <code>Translators</code> are occupied throughout the day, it makes sense to suggest it to a busy translator but allow him to finish it until next day.</p>
"
2104,"<p>The goal is to implement an artificial network that, based on training samples labelled with positive integers, outputs a positive integer.</p>

<p>Perhaps I am not searching for the correct thing but all the examples I have found have shown classifiers using a sigmoid function that outputs a fixed or floating point number within the range <span class=""math-container"">$&lt;0 \to 1&gt;$</span>.</p>

<p>Any point in the right direction or python/toy code would be very appreciated!</p>
"
2105,"<p>Would AlphaGo Zero become theoretically perfect with enough training time? If not, what would be the limiting factor?</p>

<p>(By perfect, I mean it always wins the game if possible, even against another perfect opponent.)</p>
"
2106,"<p>I am currently reading the research paper <a href=""https://arxiv.org/pdf/1706.03686.pdf"" rel=""nofollow noreferrer"">Image Crowd Counting Using Convolutional Neural Network and Markov Random Field</a>  by Kang Han, Wanggen Wan, Haiyan Yao, and Li Hou. <br/>
I did not understand the following context properly: </p>

<blockquote>
  <p>We employ the residual network, which is trained on ImageNet dataset for image classication task, to extract the deep features to represent the density of the crowd. This pre-trained CNN network created a residual item for every three convolution layer to bring the layer of the network to 152. We resize the image patches to the size of 224 × 224 as the input of the model and extract the output of the fc1000 layer to get the 1000 dimensional features. The features are then used to train 5 layers fully connected neural network. The network's input is 1000dimensional, and the number of neurons in the network is given by 100-100-50-50-1. The network's output is the local crowd count</p>
</blockquote>

<p>Can anyone explain the above part in detail? </p>
"
2107,"<p>Should I be decaying the learning rate and the exploration rate in the same manner? What's too slow and too fast of an exploration and learning rate decay? Or is it specific from model to model?</p>
"
2108,"<p>I was recently perusing the paper <a href=""https://researcher.watson.ibm.com/researcher/files/us-beygel/samuel-checkers.pdf"" rel=""nofollow noreferrer""><em>Some Studies in Machine Learning Using the Game of Checkers II--Recent Progress</em> (A.L. Samuel, 1967)</a>, which is interesting historically.</p>

<p>I was looking at this figure, which involved Alpha-Beta pruning.</p>

<p><a href=""https://i.stack.imgur.com/qT5oL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qT5oL.png"" alt=""enter image description here""></a> </p>

<p>It occurred to me that the types of non-trivial, non-chance, perfect information, zero-sum, sequential, partisan games utilized (Chess, Checkers, Go) involve game states that cannot be precisely quantified. For instance, there is no way to ascribe an objective value to a piece in Chess, or any given board state.  In some sense, the assignment of values is arbitrary, consisting of estimates.</p>

<p>The combinatorial games I'm working on are forms of <a href=""http://mbranegame.com/tutorial/"" rel=""nofollow noreferrer"">partisan Sudoku</a>, which are bidding/scoring (economic) games involving territory control.  In these models, any given board state produces an array of ratios allowing precise quantification of player status. Token values and positions can be precisely quantified.  </p>

<p>This project involves a consumer product, and the approach we're taking currently is to utilize a series of agents of increasing sophistication to provide different levels challenge for human players.  These agents also reflect what is known as a ""<a href=""http://julian.togelius.com/Lantz2017Depth.pdf"" rel=""nofollow noreferrer"">strategy ladder</a>"".   </p>

<p>Reflex Agents <em>(beginner)</em> <br>
Model-based Reflex Agents <em>(intermediate)</em> <br>
Model-based Utility Agents <em>(advanced)</em></p>

<p><em>Goals may also be incorporated to these agents such as desired margin of victory (regional outcome ratios) which will likely have an effect on performance in that narrower margins of victory appear to entail less risk.</em></p>

<p>The ""respectably weak"" vs. human performance of the first generation of reflex agents suggests that strong GOFAI might be possible. (The branching factors are extreme in the early and mid-game due to the factorial nature of the models, but initial calculations suggest that even a naive minimax lookahead will be able to look farther more effectively than humans.) Alpha-Beta pruning in partisan Sudoku, even sans a learning algorithm, should provide greater utility than in previous combinatorial game models where the values are estimates. </p>

<ul>
<li><strong>Is the historical weakness of GOFAI in relation to non-trivial combinatorial games partly a function of the structure of the games studied, where game states and token values cannot be precisely quantified?</strong></li>
</ul>

<p><strong>Looking for any papers that might comment on this subject, research into combinatorial games where precise quantification is possible, and thoughts in general.</strong>  </p>

<p>I'm trying to determine if it might be worth attempting to develop a strong GOFAI for these models prior to moving up the ladder to learning algorithms, and, if such a result would have research value.  </p>

<p>There would definitely be commercial value in that strong GOFAI with no long-term memory would allow minimal local file size for the apps, which must run on lowest-common-denominator smartphones with no assumption of connectivity.  </p>

<p><em>PS- My previous work on this has involved defining the core heuristics that emerge from the structure of the models, and I'm slowly dipping my toes into the look ahead pool.  Please don't hesitate to let me know if I've made any incorrect assumptions.</em></p>
"
2109,"<p>I don’t believe in freewill, but most people do. Although I’m not sure how an act of freewill could even be described (let alone replicated), is <a href=""https://en.wikipedia.org/wiki/Libertarianism_(metaphysics)"" rel=""nofollow noreferrer"">libertarian freewill</a> something that is considered for AI? Or is AI understood to be deterministic?</p>
"
2110,"<p>I am trying to use Deep-Q learning environment to learn Super Mario Bros. <a href=""https://github.com/rtang23/FYP/blob/5abfccf3e86742b8487080493789e13b6f812388/training2.py"" rel=""nofollow noreferrer"">The implementation is on Github</a>.</p>

<p>I have a neural network that Q values update within an episode for a very small learning rate (0.00005). However, even if I increase the learning rate to 0.00025, the Q values do not change within an episode as they are predicting the same Q values regardless of what state it is in. For example, if Mario moves right, the Q value is the same. When I start a new episode, the Q values change though.</p>

<p>I think that the Q values should be changing within an episode as the game should be seeing different parts and taking different actions. Why don't I observe this?</p>
"
2111,"<p>I am making a machine learning program for time series data analysis and using NEAT could help the work. I started to learn TensorFlow not long ago but it seems that the computational graphs in TensorFlow are usually fixed. Is there tools in TensorFlow to help build a dynamically evolving neural network? Or something like PyTorch would be a better alternative? Thanks.</p>
"
2112,"<p>This is a q-learning snake using a neural network as a q function aproximator and I'm losing my mind here the current model it's worst than the initial one.</p>

<p>The current model uses a 32x32x32 MLPRegressor from scikit-learn using relu as activation function and the adam solver.</p>

<p>The reward function is like following:</p>

<ul>
<li>death reward = -100.0</li>
<li>alive reward = -10.0</li>
<li>apple reward = 100.0</li>
</ul>

<p>The features extracted from each state are the following:</p>

<ol>
<li>what is in front of the snake's head(apple, empty, snake)</li>
<li>what is in the left of the snake's head</li>
<li>what is in the right of the snake's head</li>
<li>euclidian distance between head and apple</li>
<li>the direction from head to the apple measured in radians</li>
<li>length of the snake</li>
</ol>

<p>One episode consists of the snake playing until it dies, I'm also using in training a probability epsilon that represent the probability that the snake will take a random action if this isn't satisfied the snake will take the action for which the neural network gives the biggest score, this epsilon probability gradually decrements after each iteration.</p>

<p>The episode is learned by the regressor in reverse order one statet-action at a time.</p>

<p>However the neural network fails too aproximate the q function, no matter how many iterations the snake takes the same action for any state.</p>

<p>Things I tried:</p>

<ul>
<li>changing the structure of the neural network</li>
<li>changing the reward function</li>
<li>changing the features extracted, I even tried passing the whole map to the network</li>
</ul>

<p>Code (python): <a href=""https://pastebin.com/57qLbjQZ"" rel=""nofollow noreferrer"">https://pastebin.com/57qLbjQZ</a></p>
"
2113,"<p>The Alpha Zero (as well as AlphaGo Zero) papers say they trained the value head of the network by ""minimizing the error between the predicted winner and the game winner"" throughout its many self play games. As far as I could tell, further information was not given.</p>

<p>To my understanding, this is basically a supervised learning problem, where from the self play we have games associated with their winners, and the network is being trained to map game states to likelihood of winning. My understanding leads me to the following question:</p>

<p>What part of the game is the network trained to predict a winner on? Obviously after only five moves, the winner is not yet clear, and trying to predict a winner after five moves based on the game's eventual winner would learn a meaningless function. As a game progresses it goes from tied in the initial position to won at the end. How is the network trained to understand that, if all it is told is who eventually won?</p>
"
2114,"<p><strong>Note to the Duplicate Police</strong></p>

<p>This question is not a duplicate of the Q&amp;A thread referenced in the close request.  The only text even remotely related in that other thread is the brief mention of climate change in the Q and two sentences in the sole answer: ""Identify deforestation and the rate at which it's happening using computer vision and help in fighting back based on how critical the rate is. The World Resources Institute had entered into a partnership with Orbital Insight on this.""</p>

<p>If you look at the four bullet items below, you will find that this question asks a very specific thing about the relationship between climate and emissions.  Neither that question nor that answer overlaps with the content of this question in any meaningful way.  For instance, it is well known that CO<sup>2</sup> is NOT causing deforestation.  The additional carbon dioxide in the atmosphere causes faster regrowth.  This is because plants need CO<sup>2</sup> to grow.  Hydroponic containers deliberately boost it to improve growth rates.  Plants manufacture their own oxygen from the CO<sup>2</sup> via chlorophyll.</p>

<p>If you recall from fifth grade biology, that's why they are plants.</p>

<hr>

<p><strong>Now Back to the Question</strong></p>

<p>Several climate models have been proposed and used to model the relationship between human carbon emissions, added to the natural carbon emissions of life forms on earth, and features of climate that could damage the biosphere.</p>

<p>Population growth and industrialization have many impacts on the biosphere, including loss of terrain and pollution. Negative oceanic effects, including unpredictable changes in plankton and cyanobacteria are under study.  Carbon emissions from combustion has received attention in recent decades just as sulfur emissions were central to concerns a century or more ago.</p>

<p>Predicting weather and climate is certainly difficult because it is complex and chaotic, as typical inaccuracies in forecasts clearly demonstrate, but that is looking forward.  Looking backward, analyses of data already collected have shown a high probability that ocean and surface temperature rises followed increases in industrial and transportation related combustion of fuels.</p>

<p>How might AI be used to produce some of the key models humans need to protect the biosphere from severe damage.</p>

<ul>
<li><p>A more reliable analysis of what has already occurred, since there is some legitimacy to the differing views as to how gross the effect of carbon emissions has been on extinctions of species in the biosphere and on arctic and antarctic melting</p></li>
<li><p>A better understanding as to whether the climate of the biosphere behaves as a buffer of climate, always tending to re-balance after a volcanic eruption, meteor stroke, or other event, or whether the runaway scenario described by some climatologist, where there is a point of no return, is realistic</p></li>
<li><p>A better model to use in trying out scenarios so that solutions can be applied in the order that makes sense from both environmental and economic perspectives</p></li>
<li><p>Automation of climate planning so that the harmful effects of the irresponsibility of one geopolitical entity wishing to industrialize without constraint on other geopolitical entities can be mitigated</p></li>
</ul>

<p>Can pattern recognition, feature extraction, the learned functionality of deep networks, or generative techniques be used to accomplish these things?  Can rules of climate be learned?  Are there discrete or graph based tools that should be used?</p>
"
2115,"<p><a href=""https://ai.stackexchange.com/questions/5769/in-a-cnn-does-each-new-filter-have-different-weights-for-each-input-channel-or"">In a CNN, does each new filter have different weights for each input channel, or are the same weights of each filter used across input channels?</a></p>

<p>This question helps me a lot.</p>

<p>Let, I have RGB input image. (3 channels)
Then each filter has n×n weights for one channel.
It means, actually the filter has totally 3×n×n weights.</p>

<p>For channel R, it has own n×n filter.</p>

<p>For channel G, it has own n×n filter.</p>

<p>For channel B, it has own n×n filter.</p>

<p>After inner product, add them all to make one feature map.
Am I right?</p>

<p>And then, my question starts here.
For some purpose, I will only use greyscale images as input.
So the input images always have the same values for each RGB channel.</p>

<p>Then, can I reduce the number of weights in the filters?
Because in this case, using three different n×n filters and adding them is same with using one n×n filter that is the summation of three filters.</p>

<p>Does this logic hold on a trained network?
I have a trained network for RGB image input, but it is too heavy to run in real time.
But I only use the greyscale images as input, so it seems I can make the network less heavy (theoretically, almost 1/3 of original).</p>

<p>I'm quite new in this field, so detailed explanations will be really appreciated.
Thank you.</p>
"
2116,"<p>I am currently reading the research paper <a href=""https://arxiv.org/pdf/1706.03686.pdf"" rel=""nofollow noreferrer"">Image Crowd Counting Using Convolutional Neural Network and Markov Random Field</a>  by Kang Han, Wanggen Wan, Haiyan Yao, and Li Hou. <br/>
I did not understand the following context properly: </p>

<blockquote>
  <p>Formally, the
  Markov random field framework for the crowd counting
  can be defined as follows (we follow the notation in [18]).
  Let P be the set of patches in an image and C be a possi-
  ble set of counts. A counting c assigns a count c p ∈ C to
  each patch p ∈ P. The quality of a counting is given by an
  energy function:</p>
  
  <p>E(c) =
  ∑ D p (c p ) + ∑
  p∈P
  V (c p − c q )
  . . . (2)
  (p,q)∈N</p>
  
  <p>where N are the (undirected) edges in the four-connected
  image patch graph. D p (c p ) is the cost of assigning count
  c p to patch p, and is referred to as the data cost. V (c p −c q )
  measures the cost of assigning count c p and c q to two
  neighboring patch, and is normally referred to as the dis-
  continuity cost.
  For the problem of smoothing the adjacent patches
  count, D p (c p ) and V (c p − c q ) can take the form of the
  following functions:
  D p (c p ) = λ min((I(p) − c p ) 2 , DATA K) . . . (3)
  V (c p − c q ) = min((c p − c q ) 2 , DISC K)
  . . . (4)
  where λ is a weight of the energy items, I(p) is the ground
  truth count of the patch p, DATA K and DISC K are the
  truncating item of D p (c p ) and V (c p − c q ), respectively.</p>
</blockquote>

<hr>

<p>Can anyone explain the above part in detail and give me a detailed insight on how should I implement this part of the project? </p>
"
2117,"<p>Neurons can be simulated using different models that vary in the degree of biophysical realism. When designing an artificial neuronal network, I am interested in the consequences of choosing a degree of neuronal realism.</p>

<p>In terms of computational performance, the FLOPS vary from integrate-and-fire to the Hodgkin–Huxley model (Izhikevich, 2004). However, properties, such as refraction, also vary with the choice of neuron.</p>

<ol>
<li><p>When selecting a neuronal model, what are consequences for the ANN other than performance? For example, would there be trade-offs in
terms of stability/plasticity?</p></li>
<li><p>Izhikevich investigated the performance question in 2004. What are
the current benchmarks (other measures, new models)?</p></li>
<li><p>How does selecting a neuron have consequences for scalability in terms of hardware for a deep learning network?</p></li>
<li><p>When is the McCulloch-Pitts neuron inappropriate?</p></li>
</ol>

<hr>

<p><strong>References</strong></p>

<p>Izhikevich, E. M. (2004). <em>Which model to use for cortical spiking neurons?</em> IEEE Transactions on Neural Networks, 15(5). <a href=""https://www.izhikevich.org/publications/whichmod.pdf"" rel=""nofollow noreferrer"">https://www.izhikevich.org/publications/whichmod.pdf</a> </p>
"
2118,"<p>I found a <a href=""https://youtu.be/vppFvq2quQ0"" rel=""nofollow noreferrer"">video for the paper <em>DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills</em>
</a> on YouTube.</p>

<p>I looked in the related paper, but could not find details of how to the environment was created, such as the physics engine it used. I would like to use it, or something similar.</p>
"
2119,"<p>I want to build a model to support decision making for loan insurance proposal.</p>

<p>There are three actors in the problem: a bank, a loaner applicant (someone who ask for a loan) and a counselor. The counselor studies the loaner application and if it has a good profile it will propose to him loan from banks that fits his profile. Then the application is sent to the bank but the bank could refuse the applicant (based on criteria we don't know).</p>

<p>The counselor has also to decide whether or not he will propose to the loaner applicant a loan insurance.</p>

<p>The risk is that some banks reject loan applicant who accepts a loan insurance and other banks accept more applicants with a loan insurance. But there aren't rules regarding banks since some banks accept or reject applicants with loan insurance according of the type of acquisition applicants want with their loan for example.</p>

<p>Thus, the profile of the applicant can matter in their rejection from banks but all criteria influencing the decision are quite uncertain.</p>

<p>I've researched online and found several scholarly articles on using Monte Carlo for decision making. Should I use Monte Carlo or a simple classifier for this Decision Making problem ?</p>

<p>I saw that Monte Carlo (possibly Monte Carlo Tree Search) can be used in Decision Making and it is good when there is uncertainty. But it seems that it would forecast by producing some strategy (after running a lot of simulations) but what I want is an outcome based on both the profile of the loaner applicant and the bank knowing that criteria from banks (to accept loaner applicant from could change every six months. And I would have too model banks which seems quite difficult.</p>

<p>A classifier seems to me to not really fit the problem. I am not really sure. Actually, I don't see how a classifier like a decision tree, for example, would work here. Because I have to predict decision of the counselor to propose or not based on the decision of banks (and I don't know their criteria) to refuse or accept applicants who were proposed loan insurance and accepted it.</p>

<p>The data I have is former applicants profile who were sent to banks and if they were accepted or not by the bank, if they wanted a loan insurance or not and the type of acquisition they wanted to make with their loan.</p>

<p>I am new to Decision Making. Thank you!</p>
"
2120,"<p>So I have already learned some traditional AI techniques (alpha-beta pruning, MCTS). Now I want to get into Machine Learning. I know a little bit about neural networks, but that is about it.</p>

<p>What are some resources for learning Machine Learning?</p>

<p>It would be good if the resource is based on some cloud platform, as my laptop is not very fast.</p>

<p>My end goal is to implement the Expert Iteration algorithm (see 
""Thinking Fast and Slow with Deep Learning and Tree Search"").</p>
"
2121,"<p>Is it possible for a genetic algorithm + Neural Network that is used to learn to play one game such as a platform game able to be applied to another different game of the same genre.</p>

<p>So for example, could an AI that learns to play Mario also learn to play another similar platform game.</p>

<p>Also, if anyone could point me in the direction of material i should familiarise myself with in order to complete my project.</p>
"
2122,"<p>I started teaching myself about reinforcement learning a week ago and I have this confusion about the learning experience. Let's say we have the game Go. And we have an agent that we want to be able to play the game and win against anyone. But let's say this agent learn from playing against one opponent, my questions then are: </p>

<ol>
<li>Wouldn't the agent (after learning) be able to play only with that opponent and win? It estimated the value function of this specific behaviour only.</li>
<li>Would it be able to play as good with weaker players? </li>
<li>How do you develop an agent that can estimate a value function that generalizes against any behaviour and win? Self-play? If yes, how does that work? </li>
</ol>
"
2123,"<p>In some situation, like risk detection and spam detection. The pattern of Good User is stable, while the patterns of Attackers are changing rapidly. How can I make a model for that? Or which classifier/method should I use?</p>
"
2124,"<p>Are there neural networks that can decide to add/delete neurons (or change the neuron models/activation functions or change the assigned meaning for neurons), links or even complete layers during execution time? I guess, that such neural networks overcome the usual separation of learning/inference phases and they continuously live their lives in which learning and self-improving occurs alongside performing inference and actual decision making for which these neural networks were build. Effectively it could be neural network that acts as a [Gödel machine] (<a href=""http://people.idsia.ch/~juergen/goedelmachine.html"" rel=""nofollow noreferrer"">http://people.idsia.ch/~juergen/goedelmachine.html</a>).</p>

<p>I have found the term <em>dynamic neural network</em> but it is connected to adding some delay functions and nothing more.</p>

<p>Of course, such self-improving networks completely redefine the learning strategy, possibly, single shot gradient methods can not be applicable to them. </p>

<p>My question is connected to the neural-symbolic integration, e.g. <a href=""https://www.amazon.co.uk/Neural-Symbolic-Cognitive-Reasoning-Technologies/dp/3642092292/"" rel=""nofollow noreferrer""><em>Neural-Symbolic Cognitive Reasoning</em> by Artur S. D'Avila Garcez, 2009</a>. Usually this approach assigns individual neurons to the variables (or groups of neurons to the formula/rule) in the set of formulas in some knowledge base. Of course, if knowledge base expands (e.g. from sensor readings or from inner nonmonotonic inference) then new variables should be added and hence the neural network should be expanded (or contracted) as well.</p>
"
2125,"<p>Problem:</p>

<p>We have a fairly big database that is built up by our own users. 
The way this data is entered is by asking the users 30ish questions that all have around 12 answers (x, a, A, B, C, ..., H). The letters stand for values that we can later interpret. </p>

<p>I have already tried and implemented some very basic predictors, like random forest, a small NN, a simple decision tree etc.</p>

<p>But all these models use the full dataset to do one final prediction. (fairly well already).</p>

<p><strong>What I want to create is a system that will eliminate 7 to 10 of the possible answers a user can give at any question.</strong> This will reduce the amount of data we need to collect, store, or use to re-train future models.</p>

<p>I have already found several methods to decide what are the most discriminative variables in the full dataset.  Except, when a user starts filling the questions I start to get lost on what to do. None of the models I have calculate the next question given some previous information. </p>

<p>It feels like I should use a Naive Bayes Classifier, but I'm not sure. Other approaches include recalculating the Gini or entropy value at every step. But as far as my knowledge goes, we can't take into account the answers given before the recalculating.</p>
"
2126,"<p>I would like to have a chat to talk on a personal plan enough to pass the Turing test.</p>

<p>I got to study some <strong>natural language processing (NLP)</strong> but some that usually hide a lot of mechanical, so I think they are also much more sensitive than words and get phrases more fluid.</p>

<p>The problem is that I do not even have the idea to start. I did some testing with a NLP <strong>chatterbot</strong> library for <strong>python</strong>, but I do not know if it's possible to join PLN to Networks or how to use a neural network and I can not even use python or if I have to use a specific language.</p>

<p>In addition, I am using the book <strong>Artificial Intelligence - Russell and Norvig</strong> as the basis for the article and I would like recommend me others.</p>
"
2127,"<p>In AlphaZero, the <em>policy network</em> (or head of the network) maps game states to a distribution of the likelihood of taking each action. This distribution covers all possible actions from that state.</p>

<p>How is such a network possible? The possible actions from each state are vastly different than subsequent states. So, how would each possible action from a given state be represented in the network's output, and what about the network design would stop the network from considering an illegal action?</p>
"
2128,"<p>I'm currently implementing the original NEAT algorithm in Swift.</p>

<p>Looking at figure 4 in Stanley's original paper, it seems to me there is a chance that node 5 will have no (enabled) outgoing connection if parent 1 is assumed the fittest parent and the connection is randomly picked from parent 2.</p>

<p>Is my understanding of the crossover function correct and can it indeed result in a node with no outgoing connections?</p>
"
2129,"<p>In conditional Generative Adversarial Networks (GAN), <a href=""https://arxiv.org/pdf/1411.1784.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1411.1784.pdf</a>,
the loss function is,
<a href=""https://i.stack.imgur.com/9Y9ms.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9Y9ms.png"" alt=""enter image description here""></a></p>

<p>Discriminator and Generator both takes y, the auxiliary information.</p>

<p>I am confused as to what will be the difference by using log(D(x,y) and log(1-D(G(z,y)) as y  goes in input to D and G in addition to x and z ?</p>
"
2130,"<p>I'm a web developer and have been doing web development since years now, I've also built some hybrid applications and before to start we development I've also worked on desktop applications and have created couple of them.
the languages and frameworks I've worked on are node.js, sails.js, express.js, laravel, angualrjs, angular 6 e.t.c.</p>

<p>Now i'm at a point where i need to move forward and i need to lean AI development, i want to build an application which educate himself with allot of data and just act accordingly.
To be more precise i can give example that i need to create a software which learn from the data we collected (against a user i.e. his blood pressure) and educate himself Ok this guy go to jogging daily in morning and in morning if his blood pressure high alert comes from his fitness band don't alert anyone.</p>

<p>I'm not sure where to start, what to study, either some book or google articles or any online tutorial e.t.c</p>

<p>So i'd like someone to guide me like ok man go and read this thing first and then go and check that tutorial and this way you can learn AI and what actually is AI and AI is more the concepts and less the code and stuff like that. a complete guide from where i start to reach my goal.</p>

<p>Looking forward for great great ideas.</p>
"
2131,"<p>Currently, I am interested in how NNs or any other AI models can be used for composing music. </p>

<p>But there are many other interesting applications too, like language processing.</p>

<p>I am wondering that: NNs generally need a cost function for learning. But for example, for composing music, what would be an appropriate cost function? I mean,  algorithms can't (yet) really 'calculate' how good music is, right?</p>
"
2132,"<p>At the time when the basic building blocks of machine learning (the perceptron layer and the convolution kernel) were invented, the model of the neuron in the brain taught at the university level was simplistic.</p>

<blockquote>
  <p>Back when neurons were still just simple computers that electrically beeped untold bits to each other over cold axon wires, spikes were not seen as the hierarchical synthesis of every activity in the cell down to the molecular scale that we might say they are today. In other words, spikes were just a <strong>summary report of inputs to be integrated</strong> with the current state, and passed on. In comprehending the intimate relationships of mitochondria to spikes (and other molecular dignitaries like calcium) we might now more broadly interpret them as <strong>synced messages that a neuron sends to itself</strong>, and by implication its spatially extended inhabitants. Synapses weigh this information heavily but ultimately, but like the electoral college, fold in a heavy dose of local administration to their output. The sizes and positions within the cell to which mitochondria are deployed can not be idealized or anthropomorphized to be those metrics that the neuron decides are best for itself, but rather <strong>what is thermodynamically demanded</strong>.<sup>1</sup></p>
</blockquote>

<p>Notice the reference to summing in the first bolded phrase above.  This is the astronomically oversimplified model of biology upon which contemporary machine learning was built.  Of course ML has made progress and produced results.  This question does not dismiss or criticize that but rather widen the ideology of what ML can become via a wider field of thought.</p>

<p>Notice the second two bolded phrases, both of which denote statefulness in the neurons.  We see this in ML first as the parameters that attenuate the signals between arrays of artificial neurons in perceptrons and then, with back-propagation into deeper networks.  We see this again as the trend in ML pushes toward embedded statefulness by integrating with object oriented models, the success of LSTM designs, the interrelationships of GAN designs, and the newer experimental attention based network strategies.</p>

<p>But does the achievement of higher level thought in machines, such as is needed to ...</p>

<ul>
<li>Fly a passenger jet safely under varying conditions,</li>
<li>Drive a car in the city,</li>
<li>Understand complex verbal instructions,</li>
<li>Study and learn a topic,</li>
<li>Provide thoughtful (not mechanical) responses, or</li>
<li>Write a program to a given specification</li>
</ul>

<p>... requiring from us a much more radical is the transition in thinking about what an artificial neuron should do?</p>

<p>Scientific research into brain structure, its complex chemistry, and the organelles inside brain neurons have revealed significant complexity.  Performing a vector-matrix multiplication to apply learning parameters to the attenuation of signals between layers of activations is not nearly a simulation of a neuron.  Artificial neurons are not very neuron-like, and the distinction is extreme.</p>

<p>A little study on the current state of the science of brain neuron structure and function reveals the likelihood that it would require a massive cluster of GPUs training for a month just to learn what a single neuron does.</p>

<blockquote>
  <p>Are artificial networks based on the perceptron design inherently limiting?</p>
</blockquote>

<p><strong>References</strong></p>

<p>[1] Fast spiking axons take mitochondria for a ride,
by John Hewitt, Medical Xpress, January 13, 2014, 
<a href=""https://medicalxpress.com/news/2014-01-fast-spiking-axons-mitochondria.html"" rel=""nofollow noreferrer"">https://medicalxpress.com/news/2014-01-fast-spiking-axons-mitochondria.html</a></p>
"
2133,"<p>I am trying to understand the dimensionality of the outputs of convolution operations. Suppose a convolutional layer with the following characteristics:</p>

<ul>
<li>Input map $\textbf{x} \in R^{H\times W\times D}$</li>
<li>A bank of $F$ filters, each of dimension $\textbf{f} \in R^{H'\times W'\times D}$</li>
<li>A stride of $&lt;s_x,s_y&gt;$ for the corresponding x and y dimensions of the input map</li>
<li>Either valid or same padding (explain for both if possible)</li>
</ul>

<p>What should be the expected dimensionality of the output map expressed in terms of $H, W, D, F, H', W', s_x, s_y$?</p>
"
2134,"<p>Keras' convolutional and deconvolutional layers are designed for square grids. Is there was a way to adapt them for use in hexagonal grids?</p>

<p>For example, if we were using <a href=""https://www.redblobgames.com/grids/hexagons/#coordinates-axial"" rel=""nofollow noreferrer"">axial coordinates</a>, the input of the kernel of radius 1 centered at <code>(x,y)</code> should be:</p>

<p><code>[(x-1,y), (x-1,y+1), (x,y-1), (x,y+1), (x+1,y-1), (x+1, y)]</code></p>

<p>One option is to fudge it with a 3 by 3 box, but then you are using cells at different distances.</p>

<p>Some ideas:</p>

<ul>
<li>Modify Kera's convolutional layer code to use those inputs instead of the default inputs. The problem is that Kera calls its backend instead of implementing it itself, which means we need to modify the backend too.</li>
<li>Use a 3 by 3 box, but set the weights at <code>(x-1,y-1)</code> and <code>(x+1,y+1)</code> to zero. Unfortunately, I do not know how to permanently set weights to a given value in Kera.</li>
<li>Use <a href=""https://www.redblobgames.com/grids/hexagons/#coordinates-cube"" rel=""nofollow noreferrer"">cube coordinates</a> instead of Axial coordinates. In this case, a 3 by 3 by 3 box will only contain the central hex's neighbors and inputs set to 0. The problem is that it makes the input array much bigger. Even more problematic, some coordinates that correspond to non-hexes (such as (1,0,0)) will be assigned non-zero outputs (since (0,0,0) falls within its 3 by 3 by 3 box).</li>
</ul>

<p>Are there any better solutions?</p>
"
2135,"<p>We have data in text format as sentences.
The goal is to detect rules which exist in this set of sentences.</p>

<p>I have a limited set of contextless sentences that fit a pattern and want to find the pattern.
I might not have sentences that don't fit the pattern.</p>

<p>What should be an approach to do that?</p>
"
2136,"<p>Usually neural networks consist from layers, but is there research effort that tries to investigate more general topologies for connections among neurals, e.g. arbitrary directed acyclic graphs (DAGs).</p>

<p>I guess there can be 3 answers to my question:</p>

<ol>
<li>every imaginable DAG topology can be reduced to the layered DAGs already actively researched, so, there is no sense to seek for more general topologies;</li>
<li>general topologies exist, but there are fundamental restrictions why they are not used, e.g. maybe learning is not converging in them, maybe they generate chaotic osciallations, maybe they generate bifurcations and does not provide stability;</li>
<li>general topologies exist and are promising, but scientists are not ready to work with them, e.g. maybe they have no motivation, standard layered topologies are good enough.</li>
</ol>

<p>But I have no idea, which answer is the correct one. Reading the answer on <a href=""https://stackoverflow.com/questions/46569998/calculating-neural-network-with-arbitrary-topology"">https://stackoverflow.com/questions/46569998/calculating-neural-network-with-arbitrary-topology</a> I start to think that answer 1 is the correct one, but there is no reference provided.</p>

<p>If answer 3 is correct, then big revolution can be expected. E.g. layered topologies in many cases reduces learning to the matrix exponentiation and good tools for this are created - TensorFlow software and dedicated processors. But there seems to be no software or tools for general topologies is they have some sense indeed.</p>
"
2137,"<p>If one uses one of the open source implementations of the <a href=""https://deepmind.com/blog/wavenet-generative-model-raw-audio/"" rel=""nofollow noreferrer"">WaveNet generative speech synthesis design</a>, such as <a href=""https://r9y9.github.io/wavenet_vocoder/"" rel=""nofollow noreferrer"">https://r9y9.github.io/wavenet_vocoder/</a>, and trains using something like the <a href=""http://festvox.org/cmu_arctic/"" rel=""nofollow noreferrer"">CMU's arctic corpus</a>, now can one add a voice that sounds younger, older, less professional, or in some other way distinctive.  Must the entire training begin from scratch, or is there a more resource and time friendly way?</p>
"
2138,"<p>I'm training a language model with <code>5000</code> vocabularies using a single <code>M60 GPU</code> (w/ actually usable memory about 7.5G). 
<br>The number of tokens per batch is about <code>8000</code>, and the hidden dimension to the softmax layer is <code>512</code>. So, if I understand correctly, fully-connected (softmax) layer theoretically consumes <code>5000*8000*512*4=81.92GB</code> for a forward pass (4 is for float32).
<br>But the GPU performed the forward and backward passes without any problem, and it says the GPU memory usage is less than <code>7GB</code> in total. </p>

<p>I used PyTorch. What's causing this? </p>

<p>EDIT: To be clearer, the input to the final fc layer (256x5000 matrix) is of size [256, 32, 256]. </p>
"
2139,"<p>I have a very imbalanced dataset of two classes: 2% for the first class and 98% for the second. Such imbalance does not make training easy and so balancing the data set by undersampling class 2 seemed like a good idea. </p>

<p>However, as I think about it, should not the machine learning algorithm expect the same data distribution in nature as in its training set? I know, for sure, that the distribution of data in nature matches my imbalanced dataset. Does that mean that the balanced dataset will negatively affect the neural net performance with testing? when it assumed a different distribution of data caused by my balanced data set. </p>
"
2140,"<p>I have a mixed image database(unstructured data). In the database there are some images that i am interested in and I want to discard the rest by using cnn. I am not looking for specific objects in the images like dogs, cats etc. In the database I have photos and non photo images like infamous logos, scanned documents etc.</p>

<p>I want to find photos and discard the others. All the examples and online courses I found are based on object recognition. In my case which method can I use to classify my images as just 'relevant' and 'irrelevant'?</p>
"
2141,"<p>It seems like only neural networks need tensor hardware? Are there any other artificial intelligence algorithms that could benefit from many tensor calculations in parallel?  Are there any other computer science algorithms (not part of AI) that could benefit from many tensor calculations in parallel?</p>

<ol>
<li><p><a href=""https://en.wikipedia.org/wiki/Tensor#Applications"" rel=""nofollow noreferrer"">TensorApplications</a> </p></li>
<li><p><a href=""https://en.wikipedia.org/wiki/Application_of_tensor_theory_in_engineering"" rel=""nofollow noreferrer"">Application Theory</a></p></li>
</ol>
"
2142,"<p>Is there research work that uses neural network as the (BDI) agent (or even full-scale cognitive architecture like Soar, OpenCog) - that continuously receives information from the environment and act in an environment and modifies its base of belief in parallel? Usually NN are trained to do only one task and TensorFlow/PyTorch supports batch mode only out of the box. Also NN algorithms and theory are constructed assuming that training and inference phases are clearly separated and they have each own algorithms. So - completely new theory and software can be required for this - are there efforts in this direction? If no, then why not? It is so self-evident that such systems can be of benefit.</p>

<p><a href=""https://arxiv.org/abs/1802.07569"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1802.07569</a> is good review about incremental learning and it contains chapters of implemented systems, but all of them still separates learning phase from inference phase. Symbolic systems and symbolic agents (like JSON AgentSpeak) can have updating belief/knowledge base and they can also act during  receiving new information or during forming new beliefs. I am specifically seeking research about NNs which <strong>do learning and inference in parallel</strong>. As far as I sought then this separation still persists in self-organizing incremental NNs that are gaining some popularity.</p>

<p>I can image the construction of chained NNs in Tensorflow - there is some controller network that receives input (possibly preprocessed by hierarchically lower networks) and that decides what to the: s.c. mental actions are the ouput of this controller, these actions determine whether some subordinated network is required to undergo additional learning or whether it can be temporary used for the processing of some information. Central network itself, of course, can decide to move into temporary learning phase from time to time to improve its reasoning capabilities. Such pipeline of master-slave networks is indeed possible in TensorFlow but still TensorFlow will have one central clock, not distributed, loosely connected processing. But I don't know whether existence of central clock is any restriction on the generality of capabilities of such system. Well, this hierarchy of networks maybe can be realized inside the one large network as well - maybe this large network can allow separate parts (subsets of neurons) to function in somehow independent and mutually controlling mode, maybe such regions of large neural network can emerge indeed. <em>I am interested in this kind of research - maybe there are available some good papers for this?</em></p>
"
2143,"<p>Can neural network take decision about its own weights (update of weights) during training phase or during the phase of parallel training and inference? When one region of hierarchical NN takes decision about weights of other region is the special case of my question.</p>

<p>I am very keen to understand about self-awareness, self-learning, self-improvement capabilities of neural networks, because those exactly those self-* capabilities are the key path to the artificial general intelligence (e.g. Goedel machine). Neural networks are usually mentioned as examples of special, single-purpose intelligence but I can not see the reason for such limitation if NN essentially trys to mimic human brains, at least in purpose if not in mechanics.</p>

<p>Well - maybe this desired effect is already effectively achieved/emerges in the operation of recurrent ANNs as the effect of collective behavior?</p>
"
2144,"<p>The problem of multi-goal path planning was introduced in an ICRA paper in the year 2011:</p>

<blockquote>
  <p>“Multi-goal planning is a task which arises in many robotics
  applications. It combines the challenging requirements of planning
  feasible point-to-point trajectories in obstacle-filled — and possibly
  high-dimensional -- state spaces with the complexity of combinatorial
  optimization.” <a href=""https://dspace.mit.edu/openaccess-disseminate/1721.1/78597"" rel=""nofollow noreferrer"">Brendan Englot: Multi-Goal Feasible Path Planning
  Using Ant Colony Optimization,
  2011</a> (page 1)</p>
</blockquote>

<p>The difficulty of solving this complex task is described at page 3:</p>

<blockquote>
  <p>“constructing a graph which describes feasible paths over all
  goal-to-goal pairings is a costly task. [..] In obstacle-filled and
  high-dimensional configuration spaces with many goals, joining all
  goals into a single connected component may be very costly, and
  especially challenging if we are undertaking a kinodynamic planning
  task.” (page 3)</p>
</blockquote>

<p>Usually, using the Manhattan Distance is enough when we do an A* search with one target. However, it seems like for multiple goals, this is not the most useful way.
Which heuristic do we have to use when we have multiple targets?</p>
"
2145,"<p>In physics, there are a lot of graphs, such as 'velocity vs time' , 'time period vs length' and so on. </p>

<p>Let's say I have a sample set of points for a 'velocity vs time' graph. I draw it by hand, rather haphazardly, on a canvas. This drawn graph on the canvas is then provided to the computer. By computer I mean AI. </p>

<p>I want it to sort of beautify my drawn graph, such as straightening the lines, making the curves better, adding the digits on axes and so on. In other words, I want it to give me a better version of my drawn graph which I can readily use in, say, a word document for a report.</p>

<p>a) Is it possible/plausible to do this?
b) Are there any APIs available that can already do this? (Don't want to reinvent the wheel)
c) Any recommendations/suggestions to make the idea possible by altering it somehow?</p>
"
2146,"<p>I'm trying to use a CNN to analyse statistical images. These images are not 'natural' images (cats, dogs, etc) but images generated by visualising a dataset. The idea is that these datasets hopefully contain patterns in them that can be used as part of a classification problem.</p>

<p><a href=""https://i.stack.imgur.com/YGYUZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YGYUZ.png"" alt=""enter image description here""></a></p>

<p>Most CNN examples I've seen have one of more pooling layers, and the explaination I've seen for them is to reduce the number of training elements, but also to allow for some locational independance of an element (e.g. I know this is an eye, and can appear anywhere in the image).</p>

<p>In my case location <em>is</em> important and I want my CNN to be aware of that. ie. the presence of a pattern at a specific location in the image means something very specific compared to if that feature or pattern appears elsewhere.</p>

<p>At the moment my network looks like this (taken from an example somewhere):</p>

<pre><code>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 196, 178, 32)      896       
_________________________________________________________________
activation_1 (Activation)    (None, 196, 178, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 98, 89, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 96, 87, 32)        9248      
_________________________________________________________________
activation_2 (Activation)    (None, 96, 87, 32)        0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 48, 43, 32)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 46, 41, 64)        18496     
_________________________________________________________________
activation_3 (Activation)    (None, 46, 41, 64)        0         
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 23, 20, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 29440)             0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                942112    
_________________________________________________________________
activation_4 (Activation)    (None, 32)                0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 3)                 99        
_________________________________________________________________
activation_5 (Activation)    (None, 3)                 0         
=================================================================
Total params: 970,851
Trainable params: 970,851
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>The 'images' I'm training on are 180 x 180 x 3 pixels and each channel contains a different set of raw data.</p>

<p>What strategies are there to improve my CNN to deal with this? I have tried simply removing some of the pooling layers, but that greatly increased memory and training time and didn't seem to really help.</p>
"
2147,"<p>I'm finding it hard to understand the relationship between chaotic behavior, the human brain, and artificial networks.  There are a number of explanations on the web, but it would be very helpful if I get a very simple explanation or any references providing such simplifications.</p>
"
2148,"<p>I implemented and deployed with Flask an XGBoost model for a classification problem. But being aware that features importance can change over time to predict probability of label for new data, I implemented a Cron so that the model can be retrained every two weeks. </p>

<p>But I don't know how I can handle new features since I would have to wait a great volume of data to retrain the model to take into account this new feature ?</p>

<p>Is there an alternative of model deployment to this problem ?</p>
"
2149,"<p>I want to implement Sparse Extended information slam. There is four step to implement it. The algorithm is available in <a href=""https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf"" rel=""nofollow noreferrer"">Probabilistic Robotics Book</a> at page 310, Table 12.3.</p>

<p><a href=""https://i.stack.imgur.com/1OA52.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1OA52.png"" alt=""enter image description here""></a></p>

<p>In this algorithm line no:13 is not very clear to me. I have 15 landmarks. So $\mu_t$ will be a vector of (48*1) dimension where (3*1) for pose. Now $H_t^i$ is a matrix whose columns are dynamic as per the algorithm it is (3j-3) and 3j. J is the values of landmarks 1 to 15. Now how could I multiply a dynamic quantity with a  static one. There must be a error that matrix dimension mismatch when implement in matlab.</p>

<p>Please help me to understand the algorithm better. </p>
"
2150,"<p>This is not a soft question.  Neither is this question related to singularity conjecture or wars with robots.</p>

<p>This question seeks a mathematical formulation of what is currently only qualitative and thus not clearly understood.  It relates to servitude, dominance, and what measure of control species of biological or artificial entities exert over others.</p>

<p><strong>Dominance Relationships Quantified</strong></p>

<p>We do know and rarely doubt or argue about the following somewhat self-evident statement.</p>

<p>Control implies dominance.</p>

<p>This question focuses on how we can evaluate quantitatively whether humans are dominant over artificial systems or whether those artificial systems now dominate humans.  This may seem esoteric or philosophical to some, but it is not.  The balance of power between humans and artificial systems is a concrete phenomenon that may be accurately represented as a function of discrete events.</p>

<p>We see artificial systems, with varying degrees of automation, adaptability, intelligence, and other qualitative features succumbing to the controlling forces of humans that deploy them to serve humanity without question.  This is the focus of technophiles.</p>

<p>We also see an ever increasing number of articles about game addiction, social net addiction, and texting addiction on the web, which, at its current trend will possibly surpass the volume of heroin addiction articles.  We see the number of hours humans in industrialized countries interact with display devices with an ever increasing proportion of the visual content being generated artificially.  This is the focus of technophobes.</p>

<blockquote>
  <p>What is the balance of this equilibrium?</p>
</blockquote>

<p>In biological systems, we see that termites are highly adaptive and can eat human habitats, yet humans can build with insect resistant materials and apply insecticides.  Those methods of control are greater than the control exhibited over wood, as remarkable as those who study termites say it is.</p>

<p><strong>An Example Mathematical Model</strong></p>

<p>The above statement of inference, ""Control implies dominance,"" can be represented in many formal ways.  This is an example mathematical model that exhibits some features of importance but is not fully developed as a model.</p>

<ul>
<li>$o_{e\epsilon}$ is the obedience exhibited by entity $e$ to commands given by entity $\epsilon$.</li>
<li>$m_{e\epsilon}$ is the mechanical compliance exhibited by entity $e$ to manipulations instrumented by entity $\epsilon$.</li>
<li>$i_{e\epsilon}$ is the concession of entity $e$ to influences created by entity $\epsilon$.</li>
<li>$u_{e\epsilon}$ is the unconscious purposeful behavior exhibited by entity $e$ in response to hidden manipulations instrumented by entity $\epsilon$.</li>
<li>$T$ is the measurement time period.</li>
<li>$D_{e\epsilon}$ is the dominance of entity $e$ over entity $\epsilon$.</li>
</ul>

<p>$\sum_T o_{ab} + \sum_T m_{ab} + \sum_T i_{ab} + \sum_T u_{ab} &gt; \sum_T o_{ba} + \sum_T m_{ba} + \sum_T i_{ba} + \sum_T u_{ba} \implies D_{ba} &gt; 0$</p>

<p>The sum, over any given measurement period, of forms of control of $a$ over $b$, when greater than that sum in the opposite direction, implies that $a$ is dominant over $b$.</p>

<p><strong>Inclusion of Non-adversarial Interaction</strong></p>

<p>Similarly, symbiosis implies collaboration.</p>

<p>This may directly relate to the question because not all interaction between entities, types of entities, species, or artificial systems are adversarial.  In fact, it is highly probable that there is more collaboration than dominance in the world.  This may be a basic fact about economics.  Let's examine this related inference using the same mathematical strategy.</p>

<ul>
<li>$c_{e\epsilon}$ is the conscious symbiotic tie of entity $e$ to entity  $\epsilon$.</li>
<li>$b_{e\epsilon}$ is the mechanical binding of entity $e$ to entity $\epsilon$.</li>
<li>$q_{e\epsilon}$ is the asymmetry in an equilibrium based tie between entity $e$ and entity $\epsilon$.</li>
<li>$C_{e\epsilon}$ is the collaboration between entity $e$ and entity $\epsilon$.</li>
</ul>

<p>$\sum_T c_{ab} + \sum_T b_{ab} + \sum_T q_{ab} &gt; \sum_T c_{ba} + \sum_T b_{ba} + \sum_T q_{ba} \implies C_{ba} &gt; 0$</p>

<p>The sum, over any given measurement period, of forms of symbiosis between $a$ and $b$, implies that there is positive collaboration between entities $a$ and $b$.</p>

<p><strong>Returning to the Focal Question</strong></p>

<blockquote>
  <p>In what way can we measure control between humans and machines?</p>
</blockquote>

<p>The below questions are not THE question.  The above one is.  However, these may elucidate the relevance of the primary question.</p>

<ul>
<li>Exactly how much are humans and artificial systems collaborating symbiotically?</li>
<li>How much are they adversarial in some way, and, in that respect, which side is dominant and to what degree?</li>
<li>Are there classes of artificial systems that dominate over classes of humans, as in technology enthusiasts that have quantifiable debt resulting from technology purposes?</li>
<li>Are there classes of humans that dominate over artificial systems, like government entities that monitor and can regulate the packets of information over the Internet between nations?</li>
</ul>

<p>Most if not all of this is measurable, yet no commonly known body of theory has emerged that measures it so that public awareness of its state relative to artificial systems can be known rather than discussed without any basis for knowledge.</p>

<p>There should be.</p>
"
2151,"<p>I am trying to reproduce <a href=""http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3349.pdf"" rel=""nofollow noreferrer"">this paper</a>'s model, i.e. stacking two U-Nets to yield one final prediction. The paper mentions that: </p>

<blockquote>
  <p>The deconvolution features of the first U-Net and the intermediate
  prediction y1 are concatenated together as the input of the second
  U-Net.</p>
</blockquote>

<p>My question is: <em>What does it mean by concatenating deconvolution features and the prediction (which is an array? cm)?</em></p>

<p>The next paragraph says that:</p>

<blockquote>
  <p>The second U-Net finally gives a refined prediction y2, which we use
  as the final output of our network. We apply the same loss function to
  both y1 and y2 during training.</p>
</blockquote>

<p>It leads to the next question: <em>Does it mean that I have to train U-Net twice?</em></p>
"
2152,"<p>I have created a game based on <a href=""https://sites.google.com/site/boardandpieces/list-of-games/fox--geese-checkerboard"" rel=""nofollow noreferrer"">this game here</a>. I am attempting to use Deep Q Learning to do this, and this is my first foray into Neural networks (please be gentle!!)</p>

<p>I am trying to create a NN that can play this game. Here are some relevant facts about the game:</p>

<ul>
<li><p>Player 1 (the fox) has 1 piece that he can move diagonally 1 step in any direction </p></li>
<li><p>Player 2(The geese) has 4 pieces that they can move only forward diagonally (either diagonal left or diagonal right) 1 step.</p></li>
<li><p>The Fox wins if he reaches the other end of the board, the geese win if they trap the fox so it cannot move.</p></li>
</ul>

<p>I am trying to work on the agent first for the geese as it seems to be the harder agent with more pieces and restrictions. Here is the important sections of code I have so far:</p>

<blockquote>
  <p>This is where I setup the game board, and set the total actions for the geese</p>
</blockquote>

<pre><code>def __init__(self):
    self.state_size = (LENGTH,LENGTH) ##LENGTH is 8 so (8,8)
    #...
    #other DQN variables that aren't important to question
    #...
    self.action_size = 8 ##4 geese, each can potentially make 2 moves
    self.model = self.build_model()
</code></pre>

<blockquote>
  <p>And here is where I create my model</p>
</blockquote>

<pre><code>def build_model(self):
    #builds the NN for Deep-Q Model
    model = Sequential() #establishes a feed forward NN
    model.add(Dense(64,input_shape = (LENGTH,), activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(self.action_size, activation = 'linear'))
    model.compile(loss='mse', optimizer='Adam')
</code></pre>

<blockquote>
  <p>This is where I perform an action</p>
</blockquote>

<pre><code>def act(self, state,env):
    #get the list of allowed actions for the geese
    actions_allowed = env.allowed_actions_geese_agent()

    if np.random.rand(0,1) &lt;= self.epsilon: ##do a random move
        return actions_allowed[random.randint(0, len(actions_allowed)-1)]
    act_values = self.model.predict(state)
    print(act_values)
    return np.argmax(act_values)
</code></pre>

<blockquote>
  <p>My question: Since there are 4 geese and each can make 2 possible moves, am I correct in thinking that my action_size should be <strong>8</strong> (2 for each goose) or should it be maybe 2 (for diagonal left or right) or something else entirely? </p>
</blockquote>

<p>The reason why I am at a loss is because on any given turn, some of the geese may have an invalid move, does that matter?</p>

<blockquote>
  <p>My next Question: Even if I have the right output layer for the geese agent, when I call <code>model.predict(state)</code> where I pick my action...how do I interpret the output? And how would I map that action it selects to a valid action that can be made?</p>
</blockquote>

<p>Here is a picture of the result of using <code>model.predict(state)</code>, as you can see it returns a ton of data and then when I call <code>return np.argmax(act_values)</code> I get 59 back...not sure how to utilize that (or if it's even correct based on my output layer)... and finally I included a drawing of the board. F is the fox and 1,2,3,4 are the different geese.
<a href=""https://i.stack.imgur.com/fhPCI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fhPCI.png"" alt=""enter image description here""></a></p>

<p>I apologize for the massive post, but I am just trying to provide as much information that is helpful. </p>
"
2153,"<p><strong>Problem Statement</strong></p>

<p>I have 4 main input features.</p>

<p>This is a small snippet of the data for clearer understanding.</p>

<p>Gate name -> for example AND Gate</p>

<p>index_1 -> <code>[0.001169, 0.005416, 0.01391, 0.03037, 0.06381, 0.1307, 0.2645, 0.532]</code></p>

<p>index_2 -> <code>[7.906e-05, 0.001123, 0.00321, 0.007253, 0.01547, 0.03191, 0.06478, 0.1305]</code></p>

<p>values -> <code>[[11.0081, 14.0303, 18.8622, 27.3426, 43.8661, 76.7538, 142.591, 274.499], 
           [11.3461, 14.3634, 19.1985, 27.6827, 44.2106, 77.0954, 142.926, 274.879], 
           [12.258, 15.2816, 20.1095, 28.5856, 45.1057, 77.9778, 143.8, 275.758], 
           [13.665, 16.7457, 21.5835, 30.0545, 46.5581, 79.4212, 145.252, 277.192], 
           [15.6636, 18.9526, 23.9051, 32.4281, 48.9011, 81.7052, 147.477, 279.371], 
           [17.8838, 21.5839, 26.8957, 35.7103, 52.3901, 85.2132, 150.89, 282.714], 
           [19.3338, 23.6933, 29.7184, 39.1212, 56.4053, 89.9721, 155.913, 287.637], 
           [18.7856, 23.9999, 31.1794, 41.7549, 60.0043, 95.0488, 162.951, 295.005]]</code></p>

<p>My task is to predict this <code>values</code> matrix, given that I have <code>index_1</code> and <code>index_2</code>. Originally this <code>values</code> matrix is propagation delay, calculated using a simulator called SPICE. </p>

<p><strong>Where I am facing problem</strong></p>

<ol>
<li><p>There is no written relation between Index_1, index_2 or values since simulator calculates this value using it's own models. </p></li>
<li><p>I have made a CSV file which contains the data in separate columns. </p></li>
<li><p>Another approach that I thought. If I can give index_1, index_2 and any 5*5 sub-matrix to the model, and the model can predict the values of whole 8*8 Matrix. But the problem is again, which machine learning model do I use. </p></li>
</ol>

<p><strong>Approaches Tried so Far</strong></p>

<ol>
<li><p>I have tried a CNN model for this but it is giving me very low accuracy.</p></li>
<li><p>Used one dense fully connected neural network but it is over-fitting the data and not giving me any values for matrix.</p></li>
</ol>

<p>I am still stuck at how to predict the matrix values given this data.  What are other strategies can be used? </p>
"
2154,"<p>The Wumpus World proposed in book of Stuart Russel and Peter Norvig, is a game which happens on a 4x4 board and the objective is to grab the gold and avoiding the threats that can kill you. The rules of game are:</p>

<ul>
<li><p>You move just one box for round</p></li>
<li><p>Start in position (1,1), bottom left</p></li>
<li><p>You have a vector of sensors for perceiving the world around you.</p></li>
<li><p>When you are next to another position (including the gold), the vector is 'activated'.</p></li>
<li><p>There is one wumpus (a monster), 2-3 pits (feel free to put more or less) and just one gold pot</p></li>
<li><p>You only have one arrow that flies in a straight line and can kill the wumpus</p></li>
<li><p>Entering the room with a pit, the wumpus or the gold finishes the game</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/QHzFT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QHzFT.png"" alt=""enter image description here""></a></p>

<p>Scoring is as follows: +1000 for grabbing the gold, -1000 for dying to the wumpus, -1 for each step, -10 for shooting an arrow. Fore more details about the rules, chapter 7 of <a href=""https://rads.stackoverflow.com/amzn/click/0136042597"" rel=""nofollow noreferrer"">the book</a> explains them.</p>

<p>Well now that game has been explained, the question is: in the book, the solution is demonstrated by logic and searching, does there exist another form to solve that problem with neural networks? If yes, how to do that? What topology to use? What paradigm of learning and algorithms to use?</p>

<p>1*: My English is horrible, if you can send grammar corrections, I'm grateful.</p>

<p>2*: I think this is a bit confusing and a bit complex. if you can help me to clarify better, please do commentary or edit!</p>
"
2155,"<p>I am trying to implement <a href=""https://arxiv.org/abs/1511.07528"" rel=""nofollow noreferrer"">this paper</a>. In this paper, the author uses the forward derivative to compute the Jacobian matrix <b>dF/dx</b> using chain rule where F is the probability got from the last layer and X is input image.
My model is given below. Kindly let me know how to go about doing that?</p>

<pre><code>class LeNet5(nn.Module):

def __init__(self):

    self.derivative= None # store derivative

    super(LeNet5, self).__init__()
    self.conv1= nn.Conv2d(1,6,5)
    self.relu1= nn.ReLU()
    self.maxpool1= nn.MaxPool2d(2,2)

    self.conv2= nn.Conv2d(6,16,5)
    self.relu2= nn.ReLU()
    self.maxpool2= nn.MaxPool2d(2,2)

    self.conv3= nn.Conv2d(16,120,5)
    self.relu3= nn.ReLU()

    self.fc1= nn.Linear(120,84)
    self.relu4= nn.ReLU()

    self.fc2= nn.Linear(84,10)
    self.softmax= nn.Softmax(dim= -1)


def forward(self,img, forward_derivative= False):
    output= self.conv1(img)
    output= self.relu1(output)
    output= self.maxpool1(output)

    output= self.conv2(output)
    output= self.relu2(output)
    output= self.maxpool2(output)

    output= self.conv3(output)
    output= self.relu3(output)

    output= output.view(-1,120)
    output= self.fc1(output)
    output= self.relu4(output)

    output= self.fc2(output)
    F= self.softmax(output)

    # want to comput the jacobian dF/dimg 
    jacobian= computeJacobian(F,img)#how to write this function

    return F, jacobian
</code></pre>
"
2156,"<p>I'm training an <code>auto-encoder</code> network with <code>Adam</code> optimizer (with <code>amsgrad=True</code>) and <code>MSE loss</code> for Single channel Audio Source Separation task. Whenever I decay the learning rate by a factor, the network loss jumps abruptly and then decreases until the next decay in learning rate.</p>

<p>I'm using Pytorch for network implementation and training.</p>

<pre><code>Following are my experimental setups:

 Setup-1: NO learning rate decay, and 
          Using the same Adam optimizer for all epochs

 Setup-2: NO learning rate decay, and 
          Creating a new Adam optimizer with same initial values every epoch

 Setup-3: 0.25 decay in learning rate every 25 epochs, and
          Creating a new Adam optimizer every epoch

 Setup-4: 0.25 decay in learning rate every 25 epochs, and
          NOT creating a new Adam optimizer every time rather
          using PyTorch's ""multiStepLR"" and ""ExponentialLR"" decay scheduler 
          every 25 epochs
</code></pre>

<p>I am getting very surprising results for setups #2, #3, #4 and am unable to reason any explanation for it. Following are my results:</p>

<pre><code>Setup-1 Results:

Here I'm NOT decaying the learning rate and 
I'm using the same Adam optimizer. So my results are as expected.
My loss decreases with more epochs.
Below is the loss plot this setup.
</code></pre>

<p>Plot-1:</p>

<p><a href=""https://i.stack.imgur.com/3AvAH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3AvAH.png"" alt=""Setup-1 Results""></a></p>

<pre><code>optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

for epoch in range(num_epochs):
    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)
</code></pre>

<hr>

<pre><code>Setup-2 Results:  

Here I'm NOT decaying the learning rate but every epoch I'm creating a new
Adam optimizer with the same initial parameters.
Here also results show similar behavior as Setup-1.

Because at every epoch a new Adam optimizer is created, so the calculated gradients
for each parameter should be lost, but it seems that this doesnot affect the 
network learning. Can anyone please help on this?
</code></pre>

<p>Plot-2:</p>

<p><a href=""https://i.stack.imgur.com/rR6Ts.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rR6Ts.png"" alt=""Setup-2 Results""></a></p>

<pre><code>for epoch in range(num_epochs):
    optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)
</code></pre>

<hr>

<pre><code>Setup-3 Results: 

As can be seen from the results in below plot, 
my loss jumps every time I decay the learning rate. This is a weird behavior.

If it was happening due to the fact that I'm creating a new Adam 
optimizer every epoch then, it should have happened in Setup #1, #2 as well.
And if it is happening due to the creation of a new Adam optimizer with a new 
learning rate (alpha) every 25 epochs, then the results of Setup #4 below also 
denies such correlation.
</code></pre>

<p>Plot-3:</p>

<p><a href=""https://i.stack.imgur.com/jmjRD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jmjRD.png"" alt=""Setup-3 Results""></a></p>

<pre><code>decay_rate = 0.25
for epoch in range(num_epochs):
    optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

    if epoch % 25 == 0  and epoch != 0:
        lr *= decay_rate   # decay the learning rate

    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)
</code></pre>

<hr>

<pre><code>Setup-4 Results:  

In this setup, I'm using Pytorch's learning-rate-decay scheduler (multiStepLR)
which decays the learning rate every 25 epochs by 0.25.
Here also, the loss jumps everytime the learning rate is decayed.
</code></pre>

<p>As suggested by @Dennis in the comments below, I tried with both <code>ReLU</code> and <code>1e-02 leakyReLU</code> nonlinearities. But, the results seem to behave similar and loss first decreases, then increases and then saturates at a higher value than what I would achieve without learning rate decay.</p>

<p>Plot-4 shows the results.</p>

<p>Plot-4:</p>

<p><a href=""https://i.stack.imgur.com/56AAD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/56AAD.png"" alt=""enter image description here""></a></p>

<pre><code>scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[25,50,75], gamma=0.25)
</code></pre>

<p><code>scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.95)</code></p>

<pre><code>scheduler = ......... # defined above
optimizer = torch.optim.Adam(lr=m_lr,amsgrad=True, ...........)

for epoch in range(num_epochs):

    scheduler.step()

    running_loss = 0.0
    for i in range(num_train):
        train_input_tensor = ..........                    
        train_label_tensor = ..........
        optimizer.zero_grad()
        pred_label_tensor = model(train_input_tensor)
        loss = criterion(pred_label_tensor, train_label_tensor)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    loss_history[m_lr].append(running_loss/num_train)
</code></pre>

<hr>

<p>EDITS: </p>

<ul>
<li>As suggested in the comments and reply below, I've made changes to my code and trained the model. I've added the code and plots for the same. </li>
<li>I tried with various <code>lr_scheduler</code> in <code>PyTorch (multiStepLR, ExponentialLR)</code> and plots for the same are listed in <code>Setup-4</code> as suggested by @Dennis in comments below.</li>
<li>Trying with leakyReLU as suggested by @Dennis in comments.</li>
</ul>

<p>Any help.
Thanks</p>
"
2157,"<p>Many have examined the idea of modifying learning rate at discrete times during the training of an artificial network using conventional back propagation.  The goals of such work have been a balance of the goals of artificial network training in general.</p>

<ul>
<li>Minimal convergence time given a specific set of computing resources</li>
<li>Maximal accuracy in convergence with regard to the training acceptance criteria</li>
<li>Maximal reliability in achieving acceptable test results after training is complete</li>
</ul>

<p>The development of a surface involving these three measurements would require multiple training experiments, but may provide a relationship that itself could be approximated either by curve fitting or by a distinct deep artificial network using the experimental results as examples.</p>

<ul>
<li>Epoch index</li>
<li>Learning rate hyper-parameter value</li>
<li>Observed rate of convergence</li>
</ul>

<p>The goal of such work would be to develop, via manual application of analytic geometry experience or via deep network training the following function, where</p>

<ul>
<li><span class=""math-container"">$\alpha$</span> is the ideal learning rate for any given epoch indexed by <span class=""math-container"">$i$</span>,</li>
<li><span class=""math-container"">$\epsilon$</span> is the loss function result, and</li>
<li><span class=""math-container"">$\Psi$</span> is a function the result of which approximates the ideal learning rate for as large an array of learning scenarios possible within a clearly defined domain.</li>
</ul>

<p><span class=""math-container"">$\alpha_i = \Psi (\epsilon, i)$</span></p>

<p>The development of arriving at <span class=""math-container"">$\Psi$</span> as a closed form (formula) would be of general academic and industrial value.</p>

<p>Has this been done?</p>
"
2158,"<p>i have trouble implementing back propogation for multi class classification of CIFAR10 dataset</p>

<p><strong>My neural network has 2 layers</strong></p>

<h1>forward propagation</h1>

<p>X -> L1 -> L2</p>

<p>weights W are initialized as random </p>

<pre><code>np.random.randn(this_layer_units, previous_layer_units) * 0.01
</code></pre>

<p>X is input of size (no_features * number of examples)</p>

<pre><code>Z1 = (w1 * x) + b1

A1 = relu(Z1)
</code></pre>

<p>L1 has ReLu activation </p>

<pre><code>Z2 = (w2 * A1) + b2

A2 = softmax(Z1)
</code></pre>

<p>L2 has softmax activation</p>

<p>cost is caluclated using this equation</p>

<pre><code>cost = -(1/m)*np.sum((Y * np.log(A2) ) + ((1 - Y)*np.log(1-A2)))
</code></pre>

<h1>back propagation</h1>

<p>derivative of cost is calculated</p>

<pre><code>dA2 = -(1/m)*(np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))
</code></pre>

<p>dA2 = derivative of A2</p>

<p>Y = one hot encoded True values</p>

<p>softmax is </p>

<pre><code>np.exp(z)/ np.sum(np.exp(z))
</code></pre>

<p>now how do i proceed from here</p>

<p>how do i find dZ2 (derivative of Z2) using dA2</p>

<p>and update weights</p>

<p><a href=""https://github.com/ajmalrasi/deeplearning/blob/master/CIFAR10_Deep.ipynb"" rel=""nofollow noreferrer"">Link to entire jupyter notebook code</a></p>
"
2159,"<p>The idea that come to my mind is called Value Based Model for ANN. We use simple DCF formula to calculate kind of Q value: Rewards/Discount rate. Discount rate is a risk of getting the reward on the information that agent know about. Of course if you have many factors you just sum that. So, we calculate FV for every cell that agent know information about and this is a predicted data. We put predicted - actual and teach model how to run using loss function. Rephrased, does increase in output layer actually train the model to be better? The human logic is that if I took course I have a bigger value that helps me to live. What about NN? Does it actually more precise if we with time increase output?</p>
"
2160,"<p>Let’s say I have a neural net doing classification and I’m doing stochastic gradient descent to train it. If I know that my current approximation is a decent approximation, can I conclude that my gradient is a decent approximation of the gradient of the true classifier everywhere?</p>

<p>Specifically, suppose that I have a true loss function, $f$, and an estimation of it, $f_k$. Is it the case that there exists a $c$ (dependent on $f_k$) such that for all $x$ and $\epsilon &gt; 0$ if $|f(x)-f_k(x)|&lt;\epsilon$ then $|\nabla f(x) - \nabla f_k(x)|&lt;c\epsilon$? This isn’t true for general functions, but it may be true for neural nets. If this exact statement isn’t true, is there something along these lines that is? What if we place some restrictions on the NN?</p>

<p>The goal I have in mind is that I’m trying to figure out how to calculate how long I can use a particular sample to estimate the gradient without the error getting too bad. If I am in a context where resampling is costly, it may be worth reusing the same sample many times as long as I’m not making my error too large. My long-term goal is to come up with a bound on how much error I have if I use the same sample $k$ times, which doesn’t seem to be something in the literature as far as I’ve found.</p>
"
2161,"<p>While we train a CNN model we often experiment with number of filters, number of convolutional layers, FC layers, filter size, sometimes stride, activation function, etc. More often than not after training the model once, it is just a trial &amp; error process.  </p>

<blockquote>
  <ol>
  <li><p>Is there a way that helps me to architect my model fundamentally before training?</p></li>
  <li><p>Once I train model how do I know which among these variables (number of filters, size, number of convolutional layers, FC layers) should be
  changed - increased or decreased?</p></li>
  </ol>
</blockquote>

<p>P.S. This question assumes that data is sufficient in volume and annotated properly and still accuracy is not up to the mark. So, I've ruled out the possibility of non-architectural flaws for the question.</p>
"
2162,"<p>I am working on a prototype for an Ev3 Neural Network. Because for competitions, we are not allowed to use Bluetooth or Wifi connections, the neural network must be made with the Ev3 block-based programming system (LabView for Lego Mindstorms). I am currently working on a feed-forward neural network that uses a genetic algorithm to learn. I will now explain the specifics.</p>

<p>The neural network has a simple job: learn the difference between blue and red.
The network has three layers (input, hidden, output).
There are two inputs. Both inputs are reflected light intensity, measured one after another with the same sensor.
The hidden layer has three neurons which perform a summation and output the value with a sigmoid function.
The output layer has one neuron. Values above or equal to 0.5 are outputted as blue while the rest are outputted as red.</p>

<p>Since there are no calculus blocks in LabView for mindstorms, the summation is performed as a series of multiplication and addition problems. e is estimated as 2.71828182846 in the sigmoid function. Every neuron has sigmoid rectification except the input neurons.</p>

<p>The reason I chose to differentiate red and blue is because it is a good place to start and LabView for Mindstorms has a block that already knows the difference between blue and red (Color - Color Sensor Block). I can use this to tell the program if its guesses were right or wrong.</p>

<p>Because the Mindstorm is a feed-forward neural network, it has both weights and biases. The weights and biases are randomly selected between the values -5 to 5. (these were arbitrally chosen, I am not sure what to choose).</p>

<p>Using this network, the program generates 10 (arbitrarily chosen number) different ""species"" (I am not sure what to call these) each with 12 different weights and biases. I make each ""species"" take a test of 10 (arbitrarily chosen number) questions on which they are given a grade (# of right/ total #) based on their guess and the real answer.</p>

<p>The program then generates a list of all the grades. Using a bubble sort program (which I have created, but haven't been successful) there are 90 comparisons that are made to sort the grades from greatest to least. The top two grades are chosen and their associated ""species"" have 10 offspring generated by randomly selecting the weights and biases of the two.</p>

<p>Then the whole process is then repeated and theoretically, the best list of weights and biases should be generated.</p>

<p>Not having any schooling in Deep Learning or programming, I am wondering if I am doing anything wrong. So far, I have completed the randomization of weights/biases, the structure of the neural network, and the bubble sorting of the test scores ( which still has not worked). I am suspicious that my inputs both being reflected light intensity and my weight/bias constrain to -5 to 5 may prevent my network from performing optimally. Please provide your guidance on what I should fix or if more information is necessary. Thank you for your time.</p>
"
2163,"<p>I'm an embedded systems designer, with a little background in fuzzy logic.  I'm developing a device for machine diagnostics based on a series of sensors data.  My question is whether through fuzzy logic I can develop some kind of diagnostic indicator of machine parts failure.</p>

<p>Let's say that I have the Sensor A and sensor B near three parts.  I am wondering if it is possible to get probabilities that part 1, part 2, or part 3 are in failure.</p>

<p>I can collect all physical parameters as temperature, pressure, speed, vibration, etc. related to each of those parts.  Also I have the expert person who can give me info about relation among data collection and failures.  However, I don't have interest in accomplishing this with an expert system.</p>
"
2164,"<p>Depending on the source I find people using different variations of the ""squared error function"", how come that be? </p>

<blockquote>
  <p><strong><a href=""https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"" rel=""nofollow noreferrer"">Variation 1</a></strong> </p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/IhBq0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IhBq0.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p><strong><a href=""http://mccormickml.com/2014/03/04/gradient-descent-derivation/"" rel=""nofollow noreferrer"">Variation 2</a></strong></p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/sjktH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sjktH.png"" alt=""enter image description here""></a></p>

<p>Notice that it is being devided by 1 over m as opposed to variation 1 (1/2)</p>

<p>The stuff inside the ()^2 is simply notation I get that, but dividing by 1/m and 1/2 will cleary get a different result. Which version is the ""correct"" one, or is there no such thing as a correct or ""official"" squared error function?</p>
"
2165,"<p>So I trained an AI to generate shakespeare, which it did somewhat well. I used <a href=""https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt"" rel=""nofollow noreferrer"">this 10,000 character sample</a>.</p>

<p>Next I tried to get it to generate limericks using these <a href=""http://oedilf.com/"" rel=""nofollow noreferrer"">100,000 limericks</a>. It generated garbage output.</p>

<p>When I limited it to 10,000 characters, it then started giving reasonable limerick output.</p>

<p>How could this happen? I thought more data was always better.</p>

<p>The AI was a neural network with some LSTM layers, implemented in keras.</p>
"
2166,"<p>I'm learning the Logistic Regression and L2 Regularization.
The cost function looks like below.</p>

<p><span class=""math-container"">$$J(w) = -\displaystyle\sum_{i=1}^{n} (y^{(i)}\log(\phi(z^{(i)})+(1-y^{(i)})\log(1-\phi(z^{(i)})))$$</span></p>

<p>And the regularization term is added. (<span class=""math-container"">$\lambda$</span> is a regularization strength)</p>

<p><span class=""math-container"">$$J(w) = -\displaystyle\sum_{i=1}^{n} (y^{(i)}\log(\phi(z^{(i)})+(1-y^{(i)})\log(1-\phi(z^{(i)}))) + \frac{\lambda}{2}\| w \|$$</span></p>

<p>Intuitively, I know that if <span class=""math-container"">$\lambda$</span> becomes bigger, extreme weights are penalized and weights become closer to zero. However, I'm having a hard time to prove this mathematically.</p>

<p><span class=""math-container"">$$\Delta{w} = -\eta\nabla{J(w)}$$</span>
<span class=""math-container"">$$\frac{\partial}{\partial{w_j}}J(w) = (-y+\phi(z))x_j + \lambda{w_j}$$</span>
<span class=""math-container"">$$\Delta{w} = \eta(\displaystyle\sum_{i=1}^{n}(y^{(i)}-\phi(z^{(i)}))x^{(i)} - \lambda{w_j})$$</span></p>

<p>This doesn't show the reason why incrementing <span class=""math-container"">$\lambda$</span> makes weight become closer to zero. It is not intuitive.</p>
"
2167,"<p>I'm trying to implement an algorithm that would choose the optimal next move for the game of Connect 4. As I just want to make sure that the basic <strong>minimax</strong> works correctly, I am actually testing it like a Connect 3 on a 4x4 field. This way I don't need the alpha-beta pruning, and it's more obvious when the algorithm makes a stupid move. </p>

<p>The problem is that the algorithm <strong>always</strong> starts the game with the leftmost move, and also during the game it's just very stupid. It doesn't see the best moves.</p>

<p>I have thoroughly tested methods <code>makeMove()</code>, <code>undoMove()</code>,  <code>getAvailableColumns()</code>, <code>isWinningMove()</code> and <code>isLastSpot()</code> so I am absolutely <strong>sure</strong> that the problem is not there.</p>

<p>Here is my algorithm.</p>

<p><strong>NextMove.java</strong></p>

<pre><code>private static class NextMove {
    final int evaluation;
    final int moveIndex;

    public NextMove(int eval, int moveIndex) {
        this.evaluation = eval;
        this.moveIndex = moveIndex;
    }

    int getEvaluation() {
        return evaluation;
    }

    public int getMoveIndex() {
        return moveIndex;
    }
}
</code></pre>

<p><strong>The Algorithm</strong></p>

<pre><code>private static NextMove max(C4Field field, int movePlayed) {
    // moveIndex previously validated

    // 1) check if moveIndex is a final move to make on a given field
    field.undoMove(movePlayed);

    // check
    if (field.isWinningMove(movePlayed, C4Symbol.BLUE)) {
        field.playMove(movePlayed, C4Symbol.RED);
        return new NextMove(BLUE_WIN, movePlayed);
    }
    if (field.isWinningMove(movePlayed, C4Symbol.RED)) {
        field.playMove(movePlayed, C4Symbol.RED);
        return new NextMove(RED_WIN, movePlayed);
    }
    if (field.isLastSpot()) {
        field.playMove(movePlayed, C4Symbol.RED);
        return new NextMove(DRAW, movePlayed);
    }

    field.playMove(movePlayed, C4Symbol.RED);

    // 2) moveIndex is not a final move
    // --&gt; try all possible next moves
    final List&lt;Integer&gt; possibleMoves = field.getAvailableColumns();
    int bestEval = Integer.MIN_VALUE;
    int bestMove = 0;
    for (int moveIndex : possibleMoves) {           
        field.playMove(moveIndex, C4Symbol.BLUE);

        final int currentEval = min(field, moveIndex).getEvaluation();
        if (currentEval &gt; bestEval) {
            bestEval = currentEval;
            bestMove = moveIndex;
        }

        field.undoMove(moveIndex);
    }

    return new NextMove(bestEval, bestMove);
}

private static NextMove min(C4Field field, int movePlayed) {
    // moveIndex previously validated

    // 1) check if moveIndex is a final move to make on a given field
    field.undoMove(movePlayed);

    // check
    if (field.isWinningMove(movePlayed, C4Symbol.BLUE)) {
        field.playMove(movePlayed, C4Symbol.BLUE);
        return new NextMove(BLUE_WIN, movePlayed);
    }
    if (field.isWinningMove(movePlayed, C4Symbol.RED)) {
        field.playMove(movePlayed, C4Symbol.BLUE);
        return new NextMove(RED_WIN, movePlayed);
    }
    if (field.isLastSpot()) {
        field.playMove(movePlayed, C4Symbol.BLUE);
        return new NextMove(DRAW, movePlayed);
    }

    field.playMove(movePlayed, C4Symbol.BLUE);

    // 2) moveIndex is not a final move
    // --&gt; try all other moves
    final List&lt;Integer&gt; possibleMoves = field.getAvailableColumns();
    int bestEval = Integer.MAX_VALUE;
    int bestMove = 0;
    for (int moveIndex : possibleMoves) {
        field.playMove(moveIndex, C4Symbol.RED);

        final int currentEval = max(field, moveIndex).getEvaluation();
        if (currentEval &lt; bestEval) {
            bestEval = currentEval;
            bestMove = moveIndex;
        }

        field.undoMove(moveIndex);
    }

    return new NextMove(bestEval, bestMove);
}
</code></pre>

<p>The idea is that the algorithm takes in the arguments of a <code>currentField</code> and the <code>lastPlayedMove</code>. Then it checks if the last move somehow finished the game. If it did, I just return that move, and otherwise I go in-depth with the subsequent moves.</p>

<p>Blue player is MAX, red player is MIN.</p>

<p>In each step I first undo the last move, because it's easier to check if the ""next"" move will finish the game, than check if the current field is finished (this would require to analyze for all possible winning options in the field). After I check, I just redo the move.</p>

<p>From some reason this doesn't work. I am stuck with that for days! I have no idea what's wrong... Any help greatly appreciated!</p>

<h2>EDIT</h2>

<p>I'm adding the code how I'm invoking the algorithm.</p>

<pre><code>@Override
public int nextMove(C4Game game) {
    C4Field field = game.getCurrentField();
    C4Field tmp = C4Field.copyField(field);

    int moveIndex = tmp.getAvailableColumns().get(0);
    final C4Symbol symbol = game.getPlayerToMove().getSymbol().equals(C4Symbol.BLUE) ? C4Symbol.RED : C4Symbol.BLUE;
    tmp.dropToColumn(moveIndex, symbol);

    NextMove mv = symbol
            .equals(C4Symbol.BLUE) ? 
                    max(tmp, moveIndex) : 
                        min(tmp, moveIndex);

                    int move = mv.getMoveIndex();
                    return move;
}
</code></pre>
"
2168,"<p>From what I understand, the value function estimates how 'good' it is for an agent to be in a state and a policy is a mapping of actions to state.</p>

<p>So if I have understood value function and policies correctly, why does the value of a state change with the policy with which an agent gets there?</p>

<p>I guess I'm having difficulty grasping the concept that the goodness of a state changes depending on how an agent got there (Different policies may have different ways, and hence different values, for getting to a particular state). If there can be a concrete example (perhaps on a GridWorld or on a chess board), that might make it clear why that might be the case.</p>
"
2169,"<p>I want to understand what the gamma parameter does in svm, according to this page. 
<a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html"" rel=""nofollow noreferrer"">http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html</a></p>

<p>Intuitively, the gamma parameter defines how far the influence of a single training example reaches, with low values meaning ‘far’ and high values meaning ‘close’. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.</p>

<p>I don't understand this part ""<strong>of a single training example reaches</strong>"", does it refer to the training dataset?</p>
"
2170,"<p>I'm considering a GPU system for deep learning applications, mainly for training models with large datasets. So I'm not sure whether it makes sense to choose Nvidia NV-Link over more Tesla V100 graphics cards? Is NV-Link the bottleneck that a system with two Tesla V100 and NV-Link is ""faster"" compared to four Tesla V100 without NV-Link? As I have seen, the Nvidias GPU system DGX-1 and DGX-2 uses NVLink.</p>
"
2171,"<p>I am trying to think of some marketing related classification challenges that a feed forward neural network would be suited for. </p>

<p>Any ideas?</p>
"
2172,"<p>Some have said, ""Two heads are better than one.""  That's true if they are collaborating.  If not, the two together may be worse than zero.</p>

<p>Although that's a rhetorical opening paragraph, this is a mathematical question.</p>

<blockquote>
  <p>What are the algebraic properties of intelligence?</p>
</blockquote>

<p><strong>Is Intelligence Additive?</strong></p>

<p>Is intelligence additive and under what conditions?</p>

<p>If we have a software component containing some AI and we duplicate it and aggregate the two, is the new aggregated component twice as smart?  Twice as fast?  Twice as reliable?  Twice as versatile?  Twice as accurate?</p>

<p>Under realistic computing conditions, none of these are true in the general or even common case.  Yet we imagine that larger systems will be smarter.  Why?</p>

<p><strong>Is it Subtractive?</strong></p>

<p>Is intelligence subtractive?  If we create intelligence on earth in one location, does that decrease the total intelligence everywhere else?  Is there a law of Conservation of Intelligence.  Probably not.</p>

<p><strong>Is it Conserved?</strong></p>

<p>Before we consider the idea of conservation of intelligence, consider conservation of information.  Do we have more information now, or just massive redundancy because of easy and fast ability to copy data now?  When humanity discovers something, does it forget something else?  How would we know if that were true, since we forgot?</p>

<p>This paradox is important to artificial intelligence.</p>

<p><strong>Some Things We Know</strong></p>

<p>It is important to know exactly what kind of hill we think we are climbing.  Here's what we do know.</p>

<p>We know that accuracy and reliability can be in conflict.  A guess is sometimes more reliable and less accurate, where as an answer with six significant digits can't always be trusted, as in the case of applying Newtonian physics to the orbit of Mercury.</p>

<p>We intuitively know, when we encounter success, to keep it and perhaps replicate it.  We manufacture designs that have been proven to work.</p>

<p>We think intelligence is useful, and we can show examples that are convincing.  I'm convinced.  We want to manufacture that too, yet we don't know if twice the intelligence is twice as good or even what twice the intelligence means in the concrete realm of a computer program.</p>

<p><strong>Comparative Intelligence</strong></p>

<p>Although we don't know what, ""More intelligent,"" means in every condition of comparison or agree on who or what is more intelligent in every circumstance, there is more that we do know.  Some of what we know can help us move toward understanding the algebraic properties of intelligence.</p>

<p>A smart entity in one place is not smart in another.  This is true of learning.  Many write and speak of general intelligence, yet universal intelligence isn't even realized in humans.  One would not ask Linus Torvalds to write a song for a television commercial, and one wouldn't ask Christopher Nolan to find a better way to automatically taper the stochastic component in a stochastic gradient descent strategy.</p>

<p>We know that a person placed in one job will get it immediately and if placed in another many not in their life time ever do it well.</p>

<p>That's one key.  Intelligence appears to be linked to the type of environmental challenge.  As much as general intelligence is discussed as something we hope AI research to achieve, it may never be realized.</p>

<p>Some have said, ""You cannot prove the existence of God,"" which is just as easy to say about general intelligence.  There are sometimes no proofs for infinite cases and they cannot be measured for the purpose of proof of concept.   Omniscience and omnipotence may forever be outside the scope of human endeavors.</p>

<p><strong>Parallelism is Proven</strong></p>

<p>We do know that if we have a large set of data to process and we detect a bottleneck in the process, we can scale the components of the bottleneck, and in some cases, if done with an understanding of the process and the cause of the bottleneck, produce close to double the throughput by intelligently doubling the computing resources.</p>

<p>In that case, data throughput can scale.  However, that may or many not be the case with intelligence.  That brings us back to the question of whether intelligence is additive.</p>

<p><strong>Potentially Important Hint</strong></p>

<p>We know this too.  If we have adversarial intelligence, the competition may, under certain circumstances, lead to improvements.  The intelligence of adversaries may tune one another.  Although the competition is adversarial, there is a symbiotic element, and two competitors can be friends outside of the competition.  We see this in school as students compete for the high GPAs.  This has been demonstrated in the success of simple generative adversarial networks too.</p>

<p>We also know that adversarial relationships are inherently limited.  We know that without collaboration, the formation of partners and teams, some things don't occur at all or become mutually destructive.  John Nash broadly defined a mathematics of economic equilibria that form in what Morgenstern and von Neumann defined as non-zero sum games.  We see in history the effectiveness, the legitimacy, and sometimes the elegance of collaboration.</p>

<p>What would George Westinghouse have accomplished without Tesla?  Would China and the U.S. be largely on the same page today (in spite of their grossly different view of political structure) had Nixon and Kissinger not collaborated and then initiated more collaboration with Mao Tse-tung and Chou En-lai, who had also been collaborating?</p>

<p>Competitive interaction within the biosphere helped e. coli evolve.  It helped jackals evolve too, but symbiosis had much to do with the progress of both species.  Jackals couldn't digest their food efficiently without the e. coli bacteria that benefits from the hunting skills of the jackals.  Both organisms may have benefited from survival of the fittest, yet without symbiotic relationships, both life forms would have been considerably diminished.</p>

<p><strong>What Conditions Determine the Algebraic Properties?</strong></p>

<p>When does what algebraic operation on intelligence model reality?</p>

<p>Perhaps intelligence is not a property at all.  Perhaps intelligence is an umbrella term for a set of much more precise quantities that behave reliably.  In scientific history, it would not be the first time.  Aristotle spoke of attraction, but physicists now know about gravity, electrostatic and electromagnetic forces, bonding, and other effects that can be modeled with precision.</p>

<p>The work of Google and CalTech on PAC Learning is one of the systematic approaches to the phenomena of learning.  Such work steps toward the ability to work with AI metrics algebraically.</p>

<p><strong>Quantifying Intelligence in <span class=""math-container"">$\mathbb{R}^N$</span></strong></p>

<p>When we talk about more intelligence, we infer that intelligence is a quantity.  Here and elsewhere, I've drawn attention to the dysfunctional side of representing intelligence as a scalar in <span class=""math-container"">$\mathbb{R}^1$</span>.  I've proposed that intelligence must, at the very least, be represented as vector in <span class=""math-container"">$\mathbb{R}^N$</span>, where N is at least twenty.</p>

<p>Whether intelligence is a vector with dozens of features (dimensions) or not, how can we work with it as a quantity?  Or are we wrong to think of it as quantitative?  If it is a quantity, must it always be relative to a specific set of problems no matter how large we grow a set of intelligent capabilities through continued AI development?</p>

<blockquote>
  <p>What are the algebraic properties of intelligence?</p>
</blockquote>
"
2173,"<p>I have a course named ""Evolutionary Algorithm"". But, our teacher is always mentioning the word ""Optimization"" in his lectures.</p>

<p>I am confused. Is he actually teaching Optimization? If yes, why is the name of the course not ""Optimization""?</p>

<p>What is the difference between the study of Evolutionary algorithm and Optimization?</p>
"
2174,"<p>Let's assume I want to teach a CNN some physics. Starting with a <a href=""https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/"" rel=""nofollow noreferrer"">U-Net</a>, I input images <code>A</code> and <code>B</code> as separate channels. I know that my target (produced by a very slow Monte-Carlo code) represents a signal such as <code>f(g(A) * h(B))</code>, where <code>f</code>, <code>g</code> and <code>h</code> are fairly ""convolutional"" operations -- meaning, involving mostly blurring and rescaling operations.</p>

<p>I feel safe to state that this problem would not be too difficult for the case of <code>f(g(A) + h(B))</code> -- but what about <code>f(g(A) * h(B))</code>? Can I expect a basic CNN such as the U-Net to be able to represent the <code>*</code> (multiplication) operation?</p>

<p>Or should I expect to be forced to include a <code>Multiply</code> layer in my network, somewhere where I expect that the part before can learn the <code>g</code> and <code>h</code> parts, and the part after can learn the <code>f</code> part?</p>
"
2175,"<p>I have trouble finding material (blog, papers) about this issue, so I'm posting here.</p>

<p>Taking a recent well known example: Musk has tweeted and warned about the potential dangers of AI, saying it is ""potentially more dangerous than nukes"", referring the issue of creating a superintelligence whose goals are not aligned with ours. This is often illustrated with the <a href=""https://wiki.lesswrong.com/wiki/Paperclip_maximizer"" rel=""nofollow noreferrer"">paperclip maximiser though experiment</a>. Let's call this first concern ""AI alignment"".</p>

<p>By contrast, in <a href=""https://www.youtube.com/watch?v=ycPr5-27vSI"" rel=""nofollow noreferrer"">a recent podcast</a>, his concerns seemed more related to getting politicians and decision makers to acknowledge and cooperate on the issue, to avoid potentially dangerous scenarios like an AI arms race. In a paper co-authored by Nick Bostrom: 
<a href=""https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf"" rel=""nofollow noreferrer"">Racing to the Precipice: a Model of Artificial Intelligence Development</a>, the authors argue that developing AGI in a competitive situation incentivises us to skim on safety precautions, so it is dangerous. Let's call this second concern ""AI governance"".</p>

<p>My question is about the relative importance between these two issues: AI alignment and AI governance.</p>

<p>It seems that most institutions trying to prevent such risks (<a href=""https://intelligence.org/"" rel=""nofollow noreferrer"">MIRI</a>, <a href=""https://www.fhi.ox.ac.uk/"" rel=""nofollow noreferrer"">FHI</a>, <a href=""https://futureoflife.org/"" rel=""nofollow noreferrer"">FLI</a>, <a href=""http://openai.com/"" rel=""nofollow noreferrer"">OpenAI</a>, <a href=""https://deepmind.com/"" rel=""nofollow noreferrer"">DeepMind</a> and others) just state their mission without trying to argue about why one approach should be more pressing than the other.</p>

<p>How to assess the relative importance of those two issues? And can you point me any literature about this?</p>
"
2176,"<p>Within a piece of text, I'm trying to detect who did what to whom. </p>

<p>For instance, in the following sentences:</p>

<blockquote>
  <p>CV hit IV. CV was hit by IV.</p>
</blockquote>

<p>I'd like to know who hit whom.</p>

<p>I can't remember what this technique is called. Thanks for any help!</p>
"
2177,"<p>I have difficulty understanding the following paragraph in bracketed in red parentheses in the below excerpts from page 4 to page 5 from the paper <a href=""http://proceedings.mlr.press/v48/wangf16.pdf"" rel=""nofollow noreferrer""><em>Dueling Network Architectures for Deep Reinforcement Learning</em></a>.</p>

<p>When the author said, ""we can force the advantage function estimator to have zero advantage at the chosen action,"" does it mean you can only get negative value (up to 0) from last module (advantage of chosen action - maximum value of action advantages)? Could you help explain more on the approach the author took to address the identifiability issue.</p>

<p>For the formula 8 in the screenshot, is it correct that we only get a negative value(up to 0) from the last module <span class=""math-container"">$A - maxA$</span>?</p>

<blockquote>
  <p>... lack of identifiability is mirrored by poor practical performance when this equation is used directly.</p>
  
  <p><a href=""https://i.stack.imgur.com/lpfuv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lpfuv.png"" alt=""enter image description here""></a> To address this issue of identifiability, we can force the advantage
  function estimator to have zero advantage at the chosen action. That is, we let the last module of the network implement the forward mapping</p>
  
  <p><span class=""math-container"">$\quad \quad \quad \quad Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta)$</span></p>
  
  <p><span class=""math-container"">$$+ \Big( A(s, a; \theta, \alpha) - \max_{a' \in | \mathcal{A} |} A(s, a'; \theta, \alpha) \Big). \quad \text{(8)}$$</span></p>
  
  <p>Now, for <span class=""math-container"">$a^∗ = \text{arg max}_{a' \in \mathcal{A}} Q(s, a'; \theta, \alpha, \beta) = \text{arg max}_{a' \in \mathcal{A}} A(s, a'; \theta, \alpha)$</span>, we obtain <span class=""math-container"">$Q(s, a^∗; \theta, \alpha, \beta) = V (s; \theta, \beta)$</span>.  Hence, the stream V (s; \theta, \beta) provides an estimate of the value function, while the other stream produces an estimate of the advantage function. <a href=""https://i.stack.imgur.com/aBcmz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aBcmz.png"" alt=""enter image description here""></a></p>
</blockquote>

<p>I would like to request further explanation on Equation 9, when the author wrote what is bracketed between the red parentheses below.</p>

<blockquote>
  <p>An alternative module replaces the max operator with an average:</p>
  
  <p><span class=""math-container"">$ \quad \quad \quad \quad Q(s, a; \theta, \alpha, \beta) = V (s; \theta, \beta)$</span></p>
  
  <p><span class=""math-container"">$$ + \Big( A(s, a; \theta, \alpha) − \frac {1} {|A|} \sum_{a' \in \mathcal{A}} A(s, a'; \theta, \alpha) \Big). \quad \text{(9)}$$</span></p>
  
  <p>On the one hand this loses the original semantics of <span class=""math-container"">$V$</span> and <span class=""math-container"">$A$</span> because they are now off-target by a constant,
  <a href=""https://i.stack.imgur.com/lpfuv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lpfuv.png"" alt=""enter image description here""></a> but on the other hand it increases the stability of the optimization: with (9) the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action’s advantage in (8). <a href=""https://i.stack.imgur.com/aBcmz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aBcmz.png"" alt=""enter image description here""></a>
  We also experimented with a softmax version of equation (8), but found it to deliver similar results to the simpler module of equation (9). Hence, all the experiments reported in this paper use the module of equation (9).</p>
</blockquote>

<hr>

<p>Reply from Tim Lou from Udacity:</p>

<blockquote>
  <p>The main point is that given <span class=""math-container"">$Q = V + A$</span>, there are many different <span class=""math-container"">$V$</span> and A combination that gives the same <span class=""math-container"">$Q$</span>. So if the only constraint is that we have <span class=""math-container"">$Q = V + A$</span>, over time both <span class=""math-container"">$V$</span> and <span class=""math-container"">$A$</span> could fluctuate a lot while maintaining the same <span class=""math-container"">$Q$</span>. That's why the paper introduces some extra conditions to keep <span class=""math-container"">$V$</span> and <span class=""math-container"">$A$</span> well-defined and stable throughout</p>
</blockquote>

<hr>

<p>In the paper, to address identifiability issue, there are two equation used. My understanding is both equations are trying to fix Advantage part - the last module. For Equation 8, are we trying to make <span class=""math-container"">$V(s) = Q*(s)$</span>, as the last module is zero. And for Equation 9, the resulting <span class=""math-container"">$V(s)$</span> = true <span class=""math-container"">$V(s)$</span> + mean<span class=""math-container"">$(A)$</span>? - as the author said ‘On the one hand this loses the original semantics of <span class=""math-container"">$V$</span> and <span class=""math-container"">$A$</span> because they are now off-target by a constant’. And the constant refers to mean<span class=""math-container"">$(A)$</span>? Is my understanding correct?</p>

<hr>

<p>Reply from Tim Lou from Udacity:</p>

<blockquote>
  <p>yes, I think you got the idea! Let me try to rephrase things from my perspective. The idea is that the most important function is really just Q, because that gives us the total expected reward in the end, and that's what goes into the gradient ascent (no advantage functions are involved).</p>
  
  <p>The advantage function isn't that well defined -- We think of it as some sort of extra reward over the average expected return. But what kind of averaging should we perform? Say there are 4 actions, should the average expected return simply be the straightforward average of the expected rewards? But if some actions are known to be bad choices perhaps we shouldn't care about them and they should have a lower weight. Ultimately that choice is up to you and can be thought of as another sort of ""hyperparameter"". The advantage function ultimately feeds into the prioritized replay and it's a choice we can make. A good choice can reduce variance and speed up training.</p>
</blockquote>

<hr>

<p>I think I have a good understanding now. so question solved. Thanks.</p>
"
2178,"<p>What is the best book for learn Artificial Intelligence?</p>
"
2179,"<p>In my daily machine learning / deep learning workflow, I often want to interact with a dataset in my code.
Specifically, I would like to be able to load some module / package which can</p>

<ol>
<li>make sure that I have the dataset in a specific folder</li>
<li>represent the structure of the dataset on a higher, possible object oriented level</li>
<li>is homogeneous across datasets of the same class, e.g. image classification (set of images with labels, possibly bounding boxes)</li>
<li>open source, such that I could contribute a specification to a previously uncovered dataset</li>
<li>that let's me specify a set of commonly used licenses to which I can comply, to filter all compatible datasets</li>
</ol>

<p>Currently I mainly use Python, but other languages are also in the scope of this question (Matlab, Java).
<strong>Does such an API exist? If not, which ones come closest to the requirements stated above?</strong></p>

<p>To give you an example of how I would expect it to feel like, see the following python code</p>

<pre><code>import dataset_api as da

in_2012 = da.get('ImageNet', version='2012')
in_2017 = da.get('ImageNet', version='2017')
coco = da.get('COCO')

image_classification = da.merge([in_2012, in_2017, coco])

images = image_classification.images

image = images[0]
image.path  # the absolute file path to the image file on my disk
image.objects  # an array of object that are visible in this image
</code></pre>

<p>Note: this question is also posted on <a href=""https://datascience.stackexchange.com/posts/38482/edit"">datascience.stackexchange.com</a>. I wasn't sure which exchange better fits the question.</p>
"
2180,"<p>I have read about auto encoder. Understood what is encoding part, and decoding part, and the latent space. Now, i tried to implement this in keras. Below is the code. </p>

<pre><code>iLayer = Input ((784,))
layer1 = Dense(128, activation='relu' ) (iLayer)
layer2 = Dense(64, activation='relu') (layer1)
layer3 = Dense(28, activation ='relu') (layer2)
layer4 = Dense(64, activation='relu') (layer3)
layer5 = Dense(128, activation='relu' ) (layer4)
layer6 = Dense(784, activation='softmax' ) (layer5)
model = Model (iLayer, layer6)
model.compile(loss='binary_crossentropy', optimizer='adam')

(trainX, trainY), (testX, testY) =  mnist.load_data()
print (""shape of the trainX"", trainX.shape)
trainX = trainX.reshape(trainX.shape[0], trainX.shape[1]* trainX.shape[2])
print (""shape of the trainX"", trainX.shape)
model.fit (trainX, trainX, epochs=5, batch_size=100)
</code></pre>

<p>As simple as that, i have a 6 dense layers. I am still able see that the output image is closer to the input image.</p>

<blockquote>
  <p>From blurred to deblurred conversion, i can have few dense layers like
  the above (Basically a simple neural network) and get the inputs
  de-blured. In that cause, why autoencoder is very famous for
  de-blurring? Am I missing any core part of this auto encoder?.</p>
</blockquote>
"
2181,"<p>I am doing a research on above cited topic but I am stuck with how to actually start the project on this. 
What tools are required for this kind of project?
What resources are required to do project on this?</p>
"
2182,"<p>I want to compare the time complexity of two deep neural networks, but I have no idea how to go about it. How do I graphically achieve that with respect to the number of iterations, accuracy or any other metric? </p>
"
2183,"<p>I have a simple question about the choice of activation function for the output layer in feed-forward neural networks. </p>

<p>I have seen several codes, where the choice of activation function for the output layer is linear. Now, it might well be that I am wrong about this, but isn't that simply equivalent to a rescaling of the weights connecting the last hidden layer to the output layer? And following this point, aren't you just as well off with just using the identity function as your output activation function?</p>
"
2184,"<p>I am currently looking to use a neural network to classify gestures. I have a series of Dx,Dy,Dz readings that represent the differences across the three axes made during the gesture. About 10 movements for each example of the gesture. Basically a 10x3 matrix and then classify the training data into about 15 classes. I plan to use a CNN classifier to do this because, while the time domain is relevant this problem the difference in the movements can be differentiated when presented with as a discrete matrix.</p>

<p>I'm used to using images with a neural net so I instinctively want to just convert the matrices into a 2D tensor and feed them into a CNN, but I was wondering if there was a better way to do this? For example, I have seen 1D tensors passed to a fully connected neural network for classification which seems like it could be more appropriate for this data input type? </p>

<p>Any tips on general architecture would be really appreciated as well!</p>

<p>Thanks!</p>
"
2185,"<p>I know implication (—>) is used for conditions like if x is true then b will be true but sometimes implication is used in other than these type of sentences 
For example :
All A's are Bs: ∀X (a(X) ⇒ b(X))
I don't understand why implication is used here? And if implication is necessary to use here then why implication is not used in the example written below? 
Some A's are B's : ∃X (a(X) &amp; b(X))</p>
"
2186,"<p>I recently learned about Genetic algorithms and I solved the 8 queens problem using a genetic algorithm but I don't know how to optimize any functions using a genetic algorithm.</p>

<p><img src=""https://i.stack.imgur.com/XJjiX.jpg"" alt=""enter image description here""></p>

<p>I want a guide on how to find chromosomes and fitness function for such a function? And I don't want code. </p>
"
2187,"<p>I am starting a project to predict the generation of urban waste.</p>

<p>I have found very little information on this topic on the internet.
I would be very useful advice on how to approach this topic, and what techniques you would use.</p>

<p>I have found academic articles that make predictions with feedforward neural networks. But they seem basic or old.
I would expect the use of recurrent networks, but all the examples I find on the internet are about climate prediction, or economics. And they are very different topics.</p>

<p>It would be very useful to make the prediction of waste generation based on an existing model, and not something invented by me. On what topic would you recommend that I take as a base?</p>

<p>I appreciate any advice.
regards</p>
"
2188,"<p>I am trying to design a neural network on Python.</p>

<p>Instead of the sigmoid function which has a limited range, I am thinking of using the cube root function which has the following graph:
<a href=""https://i.stack.imgur.com/jSqOd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jSqOd.jpg"" alt=""A graph of the cube root function""></a></p>

<p>Is this suitable?</p>
"
2189,"<p>How do Support Vector Machines (SVMs) differentiate between a glass and a bottle or between a malignant and a benign tumor when it dealing with it for the first time?</p>

<p>What will be the analysis mechanism involved in this?</p>
"
2190,"<p>I made my first neural net in C++ without any libraries. It was a net to recognize numbers from the MNIST dataset. In a 784 - 784 - 10 net with sigmoid function and 5  epochs with each 60000 samples, it took about 2 hours to train. It was probably slow anyways, because I trained it on a laptop and I used classes for Neurons and Layers.</p>

<p>To be honest, I've never used Tensor Flow, so I wanted to know how the performance of my net would be compared to the same in Tensor Flow. Not to specific but just a rough aproximation.</p>
"
2191,"<p>I was able to find <a href=""https://www.bioinf.jku.at/publications/older/2604.pdf"" rel=""nofollow noreferrer"">the original paper on LSTM</a>, I was not able to find the paper that introduced ""vanilla"" RNNs. Where can I find it?</p>
"
2192,"<p>To me it seems to be ill defined. Partially because of absence of knowledge which points are to be considered outliers in the first place.</p>

<p>The problem which I have in mind is ""bad market data"" detection. For example if a financial data provider is good only most of the time, but about 7-10% of data do not make any sense.</p>

<p>The action space is binary: either take an observation or reject it.</p>

<p>I am not sure about the reward, because the observations would be fed into an algorithm as inputs and the outputs of the algo would be outliers themselves. So the outliers detection should prevent outputs of the algorithm going rouge.</p>

<p>It is necessary to add that if we are talking about the market data (stocks, indices, fx), there's no guarantee that the distributions are stationary and there might be trends and jumps. If a supervised classifier is trainer based on historical data, how and how often should it be adjusted to be able to cope with different modes of the data.</p>
"
2193,"<p>In the circumstances of two perfect AI's playing each other, will white have an inherent advantage? Or can black always play for a stalemate by countering every white strategy?</p>
"
2194,"<p>I'm currently working on tumor detection project using dicom images as I'm beginner in it currently having a difficulty in segmenting each part in image and giving each segment a new colour.</p>
"
2195,"<p>I would really like to start with artificial intelligence with python or java , but have no idea where to look for reference material,please help me out with it. </p>
"
2196,"<p>I have programmed my first network for the MNIST dataset. I was wondering what the first approach would be to recognize certain movements. </p>

<p>I have read about that the time dimension should be considered for solving such problems, but that's where I am stuck.</p>
"
2197,"<p>I recently watched the video on Proximal Policy Optimization (PPO). Now, I want to upgrade my actor-critic algorithm written in PyTorch with PPO, but I'am not sure how the new parameters / thetas are calculated.</p>

<p>In the paper <a href=""https://arxiv.org/pdf/1707.06347.pdf"" rel=""nofollow noreferrer"">Proximal Policy Optimization Algorithms</a> (at page 5), the pseudocode of the PPO algorithm is shown:</p>

<p><a href=""https://i.stack.imgur.com/bVkQH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bVkQH.png"" alt=""enter image description here""></a>  </p>

<p>It says to run <span class=""math-container"">$\pi_{\theta_{\text{old}}}$</span>, compute advantage estimates and optimize the objective. But how can we calculate <span class=""math-container"">$\pi_\theta$</span> for the objective ratio, since we have not updated the <span class=""math-container"">$\pi_{\theta_{\text{old}}}$</span> yet?</p>
"
2198,"<p>(Cross-posting here from the data science stack exchange, as my question didn't get any replies. I hope it's okay!)</p>

<p>I've been playing around with YOLOv3 and obtaining some good results on the ~20 custom classes I trained. However, one or two classes look like they can use some additional training data (not a lot, say about 10% more data), which I can provide.</p>

<p>What is the most efficient way to train my model now? Do I need to start training from scratch? Can I just throw in my additional data (with the appropriate changes to the config files etc.) and run the training based on the weight matrix I already acquired, but for a small number of iterations? (1000?) Or is this more like a transfer learning problem now?</p>

<p>Thanks for all tips!</p>
"
2199,"<p>I implemented a LSTM neural network in Pytorch. It worked but I want to know if it worked the way I guessed how it worked.</p>

<p>Say there's a 2-layer LSTM network with 10 units in each layer.
The inputs are some sequence data Xt1, Xt2, Xt3, Xt4, Xt5.</p>

<p>So when the inputs are entered into the network, Xt1 will be thrown into the network first and be connected to every unit in the first layer. And it will generate 10 hidden states/10 memory cell values/10 outputs. Then the 10 hidden states, 10 memory cell values and Xt2 will be connected to the 10 units again, and generate another 10 hidden states/10 memory cell values/10 outputs and so on.</p>

<p>After all 5 Xt's are entered into the network, the 10 outputs from Xt5 from the first layer are then used as the inputs for the second layer. The other outputs from Xt1 to Xt4 are not used. And the the 10 outputs will be entered into the second layer one by one again. So the first from the 10 will be connected to every unit in the second layer and generate 10 hidden states/10 memory cell values/10 outputs. The 10 memory cell values/10 hidden states and the second value from the 10 will be connected and so forth?</p>

<p>After all these are done, only the final 10 outputs from the layer 2 will be used. Is this how the LSTM network works? Thanks.</p>
"
2200,"<p>A more formal implication of this question is whether intelligence requires a context.</p>

<p><strong>On Topic</strong></p>

<p>This question may have little value to the fields of data science or statistics, however it is of central importance to the field of Artificial Intelligence.  The aim of the AI field has been and will continue to be the simulation of human intelligence and possibly develop types of intelligence for which the human brain is not well equipped.</p>

<p>Such does not require understanding data set training requirements or postmodern thought.  It requires knowing, in a more mathematically formal way, what intelligence is.  The proclamation, ""We know it when we see it,"" is not science and will not help develop the underdeveloped areas within the AI field.</p>

<p><strong>Narrowness of Inquiry</strong></p>

<p>When Norbert Wiener, Alonso Church, Claude Shannon, Alan Turing, Marvin Minsky, and others laid the foundations for Artificial Intelligence, they considered this question and others like it to be mathematical questions.  Although they may have approached these questions with thought experiments like Turing's Immitation Game, they also developed those ideas mathematically, before they tried to embody their ideas in computers.</p>

<p>Not all these questions have a definitive answer in the literature, and the further investigation to reach them is of paramount importance to the further development of the Artificial Intelligence field.</p>

<p><strong>The Turing Challenge to Cartesian Thought</strong></p>

<p>Turing proposed at the end of the description of his famous test, ""Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, 'Can machines think?'""<sup>1</sup></p>

<p>Turing effectively challenged the 1641 statement of René Descartes in his <em>Discourse on Method and Meditations on First Philosophy</em>:</p>

<blockquote>
  <p>""It never happens that [an automaton] arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.""</p>
</blockquote>

<p>Descartes and Turing, when discussing automatons achieving human abilities, shared a single context through which they perceived intelligence.  Those that have been either the actor or the administrator in actual Turing Tests understand the context: <strong><em>Dialog</em></strong>.</p>

<p><strong>Intelligence Contexts Other Than Dialog<sup>2</sup></strong></p>

<p>The context of dialog is distinct from other contexts such as writing a text book, running a business, or raising children.  If you apply the principle of comparing machine and human intelligence to automated vehicles (whether jet airliners, cars, trucks, drones, or trains) an entirely different context becomes immediately apparent.</p>

<p>Then the question becomes, does the distribution of fatalities, maiming, disfigurements, and property losses from automatic control match or do better than the same distributions of human control.  We see not only the difference in context, but two other differences.</p>

<ul>
<li>The statistical comparison proposed by Turing is a single dimension.  Either the computer is as indistinguishable from the human as the man is from the woman or not.  In piloting or driving scenarios, the question of how to compare a maiming to a disfigurement arises.  As in law and government, how much property loss is equal to a loss of one human life becomes part of the criteria.</li>
<li>Validation of the automaton by inequality, where the control of the vehicle being distinguishably better than human control is still success. This in contrast to validation through rough equality, where the automaton's ability to keep up in a dialog with a human is renders it effectively indistinguishable from another human.  (A dialog where the computer is too smart would make it distinguishable, thwarting the spoof.)</li>
</ul>

<p><strong>Range of Contextualization</strong></p>

<p>We have two questions at the extremes in set theory.</p>

<pre><code>  Q1. What does intelligence then mean with NO context?
  Q2. What does intelligence then mean with all possible contexts?
</code></pre>

<p>These questions seem easy if taken one at a time.</p>

<pre><code>  A1. That's the same as any context.
  A2. That's what we've been calling general intelligence.
</code></pre>

<p>But are those two mathematically equal?  Can we project that, if an automaton performs as well as or outperforms humans in a hundred contexts, it can surely do so with ten more contexts?  Furthermore, do we select a low functioning, average functioning, or best functioning human for comparison?</p>

<p>What about contexts we don't have on earth yet but will have as time progresses?</p>

<p><strong>Returning to Embodiment and Definition</strong></p>

<p>Can a baby artificial mind grow into an adult by churning Internet data, or does it need to be placed in the context of a robotic entity so it can move around and experience interaction with the physical world?</p>

<blockquote>
  <p>Can a brain be intelligent without a body?</p>
</blockquote>

<p>More generally and more formally ...</p>

<blockquote>
  <p>Does intelligence require a context?</p>
</blockquote>

<hr>

<p><strong>References</strong></p>

<p>[1] Chapter 1 (""Imitation Game"") of <em>Computing Machinery and Intelligence</em>, 1951.</p>

<p>[2] <a href=""https://www.intelltheory.com/mitheory.shtml"" rel=""nofollow noreferrer"">Multiple Intelligences Theory</a></p>
"
2201,"<p>I will be undertaking a project over the next year to create a self learning AI to play a racing game, currently the game will be Mario Kart 64.</p>

<p>I have a few questions which will hopefully help me get started: </p>

<ol>
<li>What aspects of AI would be most applicable to creating a self learning game AI for a racing game (Q-Learning, NEAT etc)</li>
<li>Could a ANN or NEAT that has learned to play Mario Kart 64 be used to learn to play another racing game? </li>
<li>What books/material should i read up on to undertake this project? </li>
<li>What other considerations should i take throughout this project? </li>
</ol>

<p>Thank you for your help! </p>
"
2202,"<p>I was going through the <a href=""https://en.wikipedia.org/wiki/Ontology_components"" rel=""nofollow noreferrer"">Wikipedia page on ontology components</a> and noticed something that I had been hoping to find, for a long time. In components' overview it mentioned: </p>

<blockquote>
  <p>Function terms: complex structures formed from certain relations that can be used in place of an individual term in a statement</p>
</blockquote>

<p>I tried digging deeper into <strong>function terms</strong> but couldn't find any other references. Is this a standard component of an ontology? </p>

<p>References would be nice. Ultimately, I'm looking for something like this in the DBPedia ontology. Would have been nice if I could add <em>ontology</em> and <em>dbpedia</em> as tags to the question, but I don't have the requisite reputation.</p>
"
2203,"<p>I've been told this is how I should be preprocessing audio samples, but what information does this method actually give me? What are the alternatives and why shouldn't I use them.</p>
"
2204,"<p>So guys, I've been seeing a lot of tutorials on the Internet about AI that are mostly done with Python. Apart from these, I've seen C# being used in AI topics but in things like for example ""Self-Driving cars"", I've seen Python and not C# or any other languages. I wanted to ask, do you recommend that I learn Python? Because I know C# and I wanted to become more professional in it, but, now that I see that Python is being used a lot, I'm getting intrigued in it. Do you recommend Python or other languages or should I keep up with C#?
Just to mention, I'm 14 years old and I have enough time to learn more and it doesn't really matter what I love to do, because, I love coding and AI specially, so, it doesn't really matter. If it's not a waste of time, I should get started, right? If you recommend Python, please tell me which compiler I should use. I don't really know if it has a compiler, but I want to know where I should start from. Thanks.</p>
"
2205,"<p>Gist: Should I use LISP for a part of the following project. What are the other options.</p>

<p>Me and a friend are planning to create a 3D Modelling Agent where a designer can :-</p>

<ul>
<li>Specify constrains on how the model can be (Eg: Model cannot be more than 8 inches wide, model has to have 2 holes on top, etc).</li>
<li>Provide 2D images for ""inspiring"" the agent to prefer certain forms.</li>
</ul>

<p>From what I can fathom, the first part seems to pertain to symbolic AI (Deterministic Constraints Satisfaction) and the second part pertains to Fuzzy AI (some form of 3DGAN???). I have studied FOL and planning systems, although I have never implemented one. I am learning ML and NN on the side.</p>

<p>We plan on using the Unity Game Engine since it has a lot of tools related to 3D Environments built in, like Vertex Modelling, a Rendering Pipeline and even ML Agents. </p>

<p>Coming to my question, should I use LISP for the Symbolic AI part? What about Prolog? Or should I prefer to use user-developed FOL and CSP libraries for C# (Since Unity Game Engine uses C#)?</p>
"
2206,"<p>I want to build a model to support decision making in order to propose or not loan insurance to clients. Because sometimes clients asking loan and loan insurance have less chance to have their loan accepted by a bank and sometimes more chances.</p>

<p>There are three actors in the problem: a bank, a loaner applicant (someone who ask for a loan) and a counselor. The counselor studies the loaner application and if it has a good profile it will propose to him loan from banks that fits his profile. Then the application is sent to the bank but the bank could refuse the applicant (based on criteria we don't know).</p>

<p>The counselor has also to decide whether or not he will propose to the loaner applicant a loan insurance. I want to build a model for that decision.</p>

<p>The risk is that some banks reject loan applicant who accepts a loan insurance and other banks accept more applicants with a loan insurance. But there aren't rules regarding banks since some banks accept or reject applicants with loan insurance according of the profile of the applicants and the type of acquisition.</p>

<p>Thus, the profile of the applicant and the bank he is applying to can matter in their rejection from banks but all criteria influencing the decision are quite uncertain.</p>

<p>Even though it is a classification problem, in my dataset I don't have a good label for loan insurance proposal. I have a features that says if the insurance was proposed or not and to less than 1% the insurance was proposed. I have a label that says if the clients application for a loan was accepted or not.</p>

<p>Thus, the data I have is former applicants profile - and banks who propose loan according to what the applicant wants - and if they were accepted or not by the bank and if they wanted a loan insurance or not.</p>

<p>I thought of combining the label with the information but I don't really know how or maybe doing multi-label classification but also I don't really know if it does fit to the problem.</p>
"
2207,"<p>I am trying to dissect paper about weight normalization:</p>

<p><a href=""https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf"" rel=""nofollow noreferrer"">https://papers.nips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf</a></p>

<p>Unfortunately, because my math is little bit rusty, I got little bit stuck with the proof... Could you provide me with some clarification about proof of the topic?
What i understand is that we introduce, instead of weight vector w scalar g (magnitude of original w?) and v/||v|| (direction of original w?) of the vector.</p>

<p><a href=""https://i.stack.imgur.com/8YLuW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8YLuW.png"" alt=""Gradient descent""></a> </p>

<p>What I am not really sure about is:
 If gradients are noisy (does this means that in some dimension we have small and in some high curvature or that error noise differs for very similar values of w?) the value will quickly increase here and effectively limit the speed of descent by decreasing value of (g/||v||). This means that we can choose larger learning rates and it will somehow adjust effect of the learning rate during the training.</p>

<p>And what I completely miss is:</p>

<p><a href=""https://i.stack.imgur.com/EVk04.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EVk04.png"" alt=""enter image description here""></a> </p>

<p>It should somehow explain the reasoning behind the idea about final effects, unfortunately, I don't really understand this chapter in the paper, I probably lack some knowledge about Linear Algebra.</p>

<p>Can you verify that my understanding of the paper is correct?
Can you recommend some sources (books/videos) to help me to understand second part of proof (related to second set of formulas?)</p>

<p>Thank you,
Tom</p>
"
2208,"<p>Imagine a system that is trained to manipulate dampers to manage air flow. The training data includes damper state and flow characteristics through a complex system of ducts. The system is then given an objective (e.g. maintain even flow to all outputs) and set loose to manage the dampers. As it performs those functions there are anomalies in the results which the system is able to detect. The algorithm CONTINUES to learn from its own empirical data, the result of implemented damper configurations, and refines its algorithm to improve performance seeking the optimum goal of perfectly even flow at all outputs.</p>

<p>What is that kind of learning or AI system called?</p>
"
2209,"<p>Imagine a system that controls dampers in a complex vent system that has an objective to perfectly equalize the output from each vent. The system has sensors for damper position, flow at various locations and at each vent. The system is initially implemented using a rather small data set or even a formulaic algorithm to control the dampers. What if that algorithm were programmed to ""try"" different configurations of dampers to optimize the air flows, guided broadly by either the initial (weak) training or the formula? The system would try different configurations and learn what improved results, and what worsened results, in an effort to reduce error (differential outflow).</p>

<p>What is that kind of AI system called? What is that system of learning called? Are there systems that do that currently?</p>
"
2210,"<p>I've been reading about expert systems and noticed started reading about MYCIN.</p>

<p>I was astonished to find that MYCIN diagnosed patients better than the infectious diseases physicians.</p>

<p><a href=""http://www.aaaipress.org/Classic/Buchanan/Buchanan33.pdf"" rel=""nofollow noreferrer"">http://www.aaaipress.org/Classic/Buchanan/Buchanan33.pdf</a></p>

<p>Since, it had such a good success rate, why did it fail?</p>
"
2211,"<p>I have created a game on an 8x8 grid and there are 4 pieces which can move essentially like checkers pieces (Forward left or Forward right only). I have implemented a DQN in order to pull this off. </p>

<p>Here is how I have mapped my moves:</p>

<pre><code>self.actions = {""1fl"": 0, ""1fr"": 1,""2fl"": 2, 
  ""2fr"": 3,""3fl"": 4, ""3fr"": 5,""4fl"": 6, ""4fr"": 7}
</code></pre>

<p>essentially I assigned each move to an integer value from 0-7 (8 total moves).</p>

<blockquote>
  <p>My question is: During any given turn, not all 8 moves are valid, how do I make sure when model.predict(state) the resulting prediction will be a move that is valid? Here is how I am currently handling it.</p>
</blockquote>

<pre><code>def act(self, state, env):
    #get the allowed list of actions
    actions_allowed = env.allowed_actions_for_agent()

    #Do a random move if random # greater than epsilon
    if np.random.rand(0,1) &lt;= self.epsilon: 
        return actions_allowed[random.randint(0, len(actions_allowed)-1)]

    #get the prediction from the model by passing the current game board
    act_values = self.model.predict(state)

    #Check to see if prediction is in list of valid moves, if so return it
    if np.argmax(act_values[0]) in actions_allowed:
        return np.argmax(act_values[0])

    #If prediction is not valid do a random move instead....
    else:
        if len(actions_allowed) &gt; 0:
            return actions_allowed[random.randint(0,len(actions_allowed)-1)]
</code></pre>

<p>I feel like if the agent predicts a move, and if that move is not in the actions_allowed set I should punish the agent.</p>

<p>But because it doesn't pick a valid move I make it do a random one instead, but I think this a problem. Because its bad prediction may ultimately end up still winning the game since the random move may have a positive outcome. I am at a total loss. The agent trains....but it doesn't seem to learn.... I have been training it for over 100k games now, and it only seems to win 10% of it games.... ugh. </p>

<p>Other helpful information:
- I am utilizing experience replay for the DQN which I have based on the code from <a href=""https://keon.io/deep-q-learning/"" rel=""nofollow noreferrer"">here</a>:</p>

<p>Here is where I build my model as well:</p>

<pre><code>self.action_size = 8
LENGTH = 8
def build_model(self):
    #builds the NN for Deep-Q Model
    model = Sequential() #establishes a feed forward NN
    model.add(Dense(64,input_shape = (LENGTH,), activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(self.action_size, activation = 'linear'))
    model.compile(loss='mse', optimizer='Adam')
</code></pre>
"
2212,"<p><a href=""https://i.stack.imgur.com/GX7lT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GX7lT.png"" alt=""in figure 18 showing the part which have tumor and other one is the actual DICOM Image""></a>As i'm beginner in image processing,having difficulty in segmenting all the parts in dicom image.currently i'm applying watershed algorithm but it segment only that part that have tumor.i  have to segment all parts in image. which algorithm will be helpful to perform this task?</p>
"
2213,"<p>I'm looking for <strong>annotated</strong> dataset of traffic signs. I was able to find Belgium, German and many more traffic signs datasets. The only problem is these datasets contain only cropped images, like this:</p>

<p><a href=""https://i.stack.imgur.com/6SJxW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6SJxW.png"" alt=""enter image description here""></a></p>

<p>While i need (for YOLO -- You Only Look Once network architecture) not-cropped images.</p>

<p><a href=""https://i.stack.imgur.com/X6jI3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X6jI3.png"" alt=""enter image description here""></a></p>

<p>I've been looking for hours but didn't find dataset like this. Does anybody know about this kind of annotated dataset ?</p>

<p><strong>EDIT:</strong></p>

<p>I prefer European datasets.</p>
"
2214,"<p>Is theta supposed to be updated in a perceptron, like the weights, and if so, what is the formula for this?</p>

<p>I'm trying to make the perceptron learn AND and OR, but without updating theta, I don't feel like it's possible to learn the case where both inputs are 0. They will of course be independent of the weights, and therefore the output will be Step(-theta), meaning theta (which has a random value) alone will determine the output.</p>

<p>Thanks.</p>
"
2215,"<p>As an Electronics &amp; Communication Engineering student I've heard some stories and theories about ""The math we have is not enough to complete a thinker-learner AI.""</p>

<p>What is the truth? Is humankind waiting for another Newton to make new calculus or another Einstein-Hawking to complete the quantum mechanics?</p>

<p>If so, what exactly do we need? What will we call it? </p>
"
2216,"<p>Following up on my previous questions using a hypothetical AI system to manage air flow using dampers to achieve an optimal target of exactly equal airflow at a number of vents;  (thank you for all the responses to that question) I was directed to ""reinforcement learning.""  </p>

<p>In reinforcement learning, the system sets some controllable variables and then determines the quality of the result of the dependent variable(s); using that ""quality"" to update the algorithm.  In simple games this works fine because for each setting there is a single result.  BUT, for the real world (e.g. air flow system) the result takes some time to develop and there is no single precise ""pair"" result to the conditions set.  The flow change takes time and even oscillates a bit as flow stabilizes to a steady state.  In practical systems how is this ""lag"" accounted for?  How are the un-settled (false) results ignored?  How is this noise distinguished from exogenous factors (un-controlled system inputs e.g. an open window exposed to wind)?</p>
"
2217,"<p>I'm a student of Artificial Intelligence. This semester I've Computer Vision course. We should select a topic related to intelligent vehicles and read several papers and implement one of those paper.</p>

<p>I searched and I saw different topics related to intelligent vehicles like detection, tracking, semantic segmentation, ...  </p>

<p>But I don't have enough knowledge to select some papers that have these properties:
 - Published recently [2015-2018].
 - Kinda related to intelligent vehicles like autonomous cars and drones.
 - Not hard to implement. I think papers that use very Deep learning or are needed to run simulations are hard for implementing.</p>

<p>After hours of searching, I got confused. So I decided to get help from you.</p>
"
2218,"<p>I'd like to implement a partially connected neural network with ~3-4 hidden layers (a sparse deep neural network?) where I can specify which node connects to which node from the previous/next layer. So I want the architecture to be highly specified/customized from the get-go and I want the neural network to optimize the weights of the specified connections, while keeping everything else 0 during the forward pass AND the backpropagation (connection does not ever exist).</p>

<p>I am a complete beginner in neural networks. I have been recently working with tensorflow &amp; tensorflow-keras to construct fully connected deep networks. Is there anything in tensorflow (or something else) that I should look into that might allow me to do this? I think with tf, I should be able to specify the computational graph such that only certain connections exist but I really have no idea yet where to start from to do this...</p>

<p>While doing some online research, I came across papers/posts on network pruning, but it doesn't seem really relevant to me. I don't want to go back and prune my network to make it less over-parameterized or eliminate insignificant connections. I want the connections to be specified and the network to be relatively sparse from the initialization and stay that way during the back-propagation.</p>

<p>Thanks!</p>
"
2219,"<p>We are currently working on developing a 3D modeling software that allows designers to set spatial constraints to models. The computer then should generate a 3D mesh conforming to these constraints.</p>

<p>Why should or shouldn't we use Lisp for the constraint satisfaction part? Will Prolog environment be any better? Or should we stick to C/C++ libraries?</p>

<p>One requirement we have is that we want to use the Unity Game Engine as it has a lot of 3D tools built in</p>
"
2220,"<p>I implemented the segmentation on my DICOM Dataset and it is compared with manual segmentation to find the accuracy of segmentation, is there other methods for evaluation? </p>
"
2221,"<p>I was reading a machine learning book that uses probabilities like these:</p>

<p><span class=""math-container"">$P(x;y), P(x;y,z), P(x,y;z)$</span></p>

<p>I couldn't find what they are and how can I read and understand them?</p>

<p>Apart from the context, I saw one of these probabilities on it here:</p>

<p><img src=""https://i.stack.imgur.com/lf0X7.png"" width=""500""></p>
"
2222,"<p>With reference to the research paper entitled <a href=""https://ieeexplore.ieee.org/document/7296633"" rel=""nofollow noreferrer"">Sentiment Embeddings with Applications to Sentiment Analysis</a>, I am trying to implement its sentiment ranking model in Python for which I am required to optimize the following hinge loss function: </p>

<p><a href=""https://i.stack.imgur.com/VE6Hs.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VE6Hs.jpg"" alt=""Concerned hinge loss function""></a></p>

<p>Unlike the usual mean square error, I cannot find its gradient to perform backpropagation. Please help with a solution. Thanks.</p>
"
2223,"<p>Example: texas holdem poker vs texas holdem poker with the same rounds, just with no public cards dealt.</p>

<p>Would algorithms like CFR approximate Nash-equilibrium more easily?<br>
Could AI that does not look at public cards achieve similar performance in normal texas holdem as AI that looks at public state tree?</p>
"
2224,"<p>I have a neural network with the following structure:
<a href=""https://i.stack.imgur.com/eHMGH.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eHMGH.jpg"" alt=""Neural Network with target outputs: 0.8,-0.3""></a></p>

<p>I am expecting specific outputs from the neural network which are the target values for my training. Let's say the target values are 0.8 for the upper output node and -0.3 for the lower output node.</p>

<p>The activations function used for the first 2 layers are ReLu or LeakyReLu while the last layer uses atan as an activation function.</p>

<p>For back propogation, instead of adjusting values to make the network's output approach 0.8,-0.3. is it suitable if I use the inverse function for atan -which is tan itself- to get ""the ideal input to the output layer multiplied by weights and adjusted by biases"". </p>

<p>The tan of 0.8 and -0.3 is 0.01396 and -0.00524 approximately. </p>

<p>My algorithm would then adjust weights and biases of the network so that the ""pre-activated output"" of the output layer -which is basically (sum(output_layer_weight*output_layer's inputs)+output_layer_biases)- approaches 0.01396 and -0.00524.</p>

<p>Is this suitable </p>
"
2225,"<p>The subject matter is to count the number of people in a large room, wherein a camera is placed in a very high ceiling: an example would be Grand Central Station.   Faces are not visible: <a href=""http://zhreshold.github.io/logs/2014/04/16/head-detection/"" rel=""nofollow noreferrer"">the scalp (top of the head) is visible to the camera as shown in the link's video</a>.  </p>

<p>The goal: I would like to perform a Google literature search to assess the work that has been performed on overhead head recognition,  however, I am not sure what the best  keyword pairs: (scalp? head? people?) to describe the object that is to be recognized from a camera positioned in the ceiling (overhead? bird's eye? satellite?).  I'd like the search to return leading-edge (AI) techniques that benchmark results</p>
"
2226,"<p>I have written my own basic convolutional neural network in Java as a learning exercise. I am using it to analyze the MIT CBCL face database image set. They are a set of 19x19 pixel greyscale images.</p>

<p>Network specifications are:</p>

<p>Single Convolution Layer with 1 filter:
Filter Size: 4x4.
Stride Size: 1</p>

<p>Single Pooling Layer
2x2 Max Pooling</p>

<p>3 layer MLP(input, 1 hidden and output)
input = 64 neurons
hidden = 15 neurons
output = 2 neurons
learning rate = 0.1</p>

<p>Now I am getting reasonable accuracy(92.85%), but my issue is that it is being achieved at very different points in the epoch count across network runs:</p>

<pre><code>Epochs  Training Accuracy   Test Accuracy   Validation Accuracy
</code></pre>

<p>Run 1   415 93.13   92.44   93.35
Run 2   515 92.44   93.18   92.84
Run 3   327 93.83   92.05   92.38</p>

<p>I am using the Java random class with the same seed for every run to initialize the kernel, the MLP weights and break the input data into 3 sets.(training is being done using the 33-33-33 method)</p>

<p>I am a loss as to what is causing this variation in epoch count to achieve the highest point in validation accuracy. Can anybody explain this?</p>
"
2227,"<p>I've been looking at reinforcement learning, and specifically playing around with creating my own environments to use with the OpenAI Gym AI. I am using agents from the stable_baselines project to test with it.</p>

<p>One thing I've noticed in virtually all RL examples is that there never seems to be any dropout layers in any of the networks. Why is this?</p>

<p>I have created an environment that simulates currency prices and a simple agent, using DQN, that attempts to learn when to buy and sell. Training it over almost a million timesteps taken from a specific set of data consisting of one month's worth of 5-minute price data it seems to overfit a lot. If I then evaluate the agents and model against a different month's worth of data is performs abysmally. So sounds like classic overfitting.</p>

<p>But is there a reason why you don't see dropout layers in RL networks? Is there other mechanisms to try and deal with overfitting? Or in many RL examples does it not matter? e.g. there may only be one true way to the ultimate high score in the 'breakout' game, so you might as well learn that exactly, and no need to generalise?</p>

<p>Or is it deemed that the chaotic nature of the environment itself should provide enough different combinations of outcomes that you don't need to have dropout layers?</p>
"
2228,"<p>Naturally everyone knows most common advantages of autonomous (not autopilot-controlled) cars, e.g. safety, efficiency, parking, etc.</p>

<p>But are there any advantages for the law? Normally you only hear about problems like having to change a lot of the legal aspects.</p>

<p>I would have thought that there was no hit and run, drunk driving and generally no deliberately caused car accidents.</p>

<p>I would be very interested if someone would come up with further advantages.</p>

<p>I hope this is the right page for a question like that. </p>
"
2229,"<p>I'm programming on Connect6 with MCTS.</p>

<p>Monte Carlo Tree Search is based on random moves. It counts up the number of wins in certain moves. (Whether it wins in 3 turns or 30 turns)</p>

<p>Is the move with less turns more powerful than the move with more turns?(as mcts just sees if it's win or not -- not considering the number of turns it took to win) And if so, is it meaningful to give bigger weight to the one with less turn win?</p>
"
2230,"<p>I have a saved keras model.
How can I get back the labels from the model ?</p>

<p>Because right now, I can use the predict method to get back the probability for a sample to belong to a certain class e.g. class 1, 2, 3.</p>

<p>But how can I know what class 1, 2, 3 correspond to in the model e.g. cat, dog, bird ?</p>
"
2231,"<p>I want to implement a neural network on a big dataset. But training time is long (~1h30 per epoch). I'm still in the development process, so I don't want to wait such long time just to have poor results at the end.</p>

<p><a href=""https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607"" rel=""nofollow noreferrer"">This</a> and <a href=""https://www.reddit.com/r/MachineLearning/comments/5pidk2/d_is_overfitting_on_a_very_small_data_set_a/"" rel=""nofollow noreferrer"">this</a> suggest that overfitting the network on a very small dataset (1 ~ 20 samples) and reach a loss near 0 is a good start.</p>

<p>I did it and it works great. However, I am looking for the next step of validating my architecture. I tried to overfit my network over 100 samples, but I can't reach a loss near 0 in reasonable time.</p>

<p>How can I ensure the results given by my NN will be good (or not), without having to train it on the whole dataset ?</p>
"
2232,"<p>I'm learning about multilayer perceptrons, and I have a quick theory question in regards to hidden layer neurons. </p>

<p>I know we can use two hidden layers to solve a non-linearly seperable problem by allowing for a representation with two linear seperators. However, we can solve a non-linearly seperable problem using <strong><em>only one</em></strong> hidden layer. </p>

<p>This seems fine, but what kind of representation does one hidden layer add? My question is how is the dimensionality of the output affected?</p>

<p>I've drawn a diagram of a multilayer perceptron with one hidden layer neuron. I used this same layout to solve a non-linearly seperable problem. The single hidden layer node is inside the red square. Forgive my poor MS-Paint skills.</p>

<p><a href=""https://i.stack.imgur.com/qGm17.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qGm17.png"" alt=""simple perceptron""></a></p>
"
2233,"<p>I'm trying to apply the <a href=""https://worldmodels.github.io/"" rel=""nofollow noreferrer"">World Models</a> architecture to the Sonic game (using the <strong>gym-retro</strong> library).</p>

<p>My problem concerns the evolutionnary algorithm part that I use as the controller (worldmodels = auto encoder + RNN + controller).
I'm using a genetic algorithm called <strong>NEAT</strong> (I use the <strong>neat-python</strong> library). I am searching for someone who can help me with the neat-python implementation.</p>

<p>Here is the method that runs a generation :
<code>python
    best_genome = pop.run(popEvaluator.evaluate_genomes, 1)
</code></p>

<p>Currently, all the individuals of the population are evaluated on the first level of Sonic The HedgeHog.
The ""run"" method should return the best genome of the population based on their performance on this level.
Then, I use this best genome to re-create the associated neural network in order to run it in the same level.
I was expected to see the exact same run as the best individual, but this is not the case.
Sometimes it does, sometimes not. </p>

<p>There are not a lot of examples with NEAT and I based my code on <a href=""https://github.com/CodeReclaimers/neat-python/blob/master/examples/openai-lander/evolve.py"" rel=""nofollow noreferrer"">this one</a> from the official documentation.</p>

<p><a href=""https://github.com/CamilleChiquet/Sonic-World-Models-Keras/blob/master/neat_sonic.py"" rel=""nofollow noreferrer"">Here</a> is my own implementation, if you want to check.</p>

<p>If anybody has already used NEAT, help would be welcome !</p>
"
2234,"<p>Can the recurrent neural network input come from short time fourier transform in MATLAB? I mean the input is not from time series domain.</p>
"
2235,"<p>Almost all the neural network architecture I have come across have a square input size of an image. like <code>32x32,64x64,128x128,.......</code></p>

<p>Ideally we might not have a square image for all kind of scenarios. </p>

<p>Example:<code>384x256</code></p>

<p>My question is how to we handle such images during</p>

<ol>
<li>training</li>
<li>Development</li>
<li>Test</li>
</ol>

<p>of a neural network?</p>

<p>Do we force the image to resize to the input of the neural network or just crop the image to the required input size?</p>

<p>PS: Have asked the same on <a href=""https://www.coursera.org/learn/convolutional-neural-networks/discussions/weeks/2/threads/Kd3zKcvEEeiHMg4p20KAXg"" rel=""nofollow noreferrer"">Coursera</a></p>
"
2236,"<p>I am using Tensorflow CNN to build an image classification/prediction model. Currently all the images in the dataset are each about 1mb in size.</p>

<p>Most examples out there use very small images.</p>

<p>The image size seems large, but I not too sure. </p>

<p>Any thoughts on the feasibility of 1mb images? If not what can I do to compress programmatically?</p>
"
2237,"<p>According to the <a href=""https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf"" rel=""nofollow noreferrer"">original paper</a> on page 4, <code>224x224x3</code> image is reduced to <code>112x112x64</code> using a filter <code>7x7</code> and stride <code>2</code> after convolution.</p>

<p><code>n</code>x<code>n</code> = <code>224x224</code></p>

<p><code>f</code>x<code>f</code> = <code>7x7</code></p>

<p>stride = <code>s</code> = <code>2</code></p>

<p>padding = <code>p</code> = <code>0</code></p>

<p>Output of convolution is <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""nofollow noreferrer"">(((n+2p-f)/s)+1)</a> according to which </p>

<p><code>(n+2p-f)</code>=<code>(224+0-7)</code>=217, 
divide by stride = <code>217/2=108.5</code> (taking the lower value), adding <code>118+1=119</code>.</p>

<p>How do we get a output image of <code>112</code> now?</p>
"
2238,"<p>I have a project, which is the keyboard biometrics of users.</p>

<p><strong>suppose I have 3 users</strong>, 
I do not know how to label in two types of class, (+ 1, -1). </p>

<p>If I want to verify the identity to <strong>user1</strong>, my idea of ​​class designation would be:</p>

<pre><code>       TIMES                LABEL
user 1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1
9.4  9.2  1.0  3.4  0.5      1

user 2
0.1  3.2  1.0  1.2  1.7      -1
3.4  1.2  3.0  1.1  2.8      -1
2.4  2.2  3.0  1.6  2.9      -1
1.4  3.2  2.0  2.6  3.6      -1
3.4  0.2   3.0  2.7  3.5     -1

user N
0.2  1.4  4.5  3.7  2.9      -1
9.2  1.5  7.6  2.6  2.6      -1
9.3  1.6  7.5  2.9  3.4      -1
9.8  3.8  6.6  2.8  2.5      -1
9.8  2.8  1.7  3.8  1.6      -1
</code></pre>

<p>but as my system has more and more users classes -1 will be too many compared to classes +1,
<strong>How should I label the classes?</strong></p>
"
2239,"<p>As stated in the title, I'm wondering if it would be possible to ""outperform"" the master in the apprenticeship learning. I'm aware that the question might be not clear enough; but hopefully, someone might have done something on it before.</p>

<p>More precisely, I'm actually asking a clear way to define the target of learning: If the master's behavior is defined as standards, then of course the answer should be NO. </p>

<p>However, consider a real example: in the case of autonomous vehicles, if we design the algorithm to mimic a human master, then would it be possible for the algorithm to outperform the master (consider ideal case, and neglect physical condition, e.g. tired, un-focused...), especially in new situations, if we well-define a new reasonable standard?</p>
"
2240,"<p>I've a grid of rectangles acting as blocks. The robot traverses through the inter-spaces between these consecutive blocks. Now I have sensor data streaming in representing Right and left wheel speeds. Based on the differences in the speeds of left and right wheels, I infer the robot's position and path it has threaded. I get the associated individual segments of the total distance when it travels straight, left or right.</p>

<p>These distances are a function of the actual speed of the robot and the time interval elapsed before the end of that activity. These computed distances for the segments though don't map and fit-in well when projected on the grid layout of the environment. The segments are rather not adhering to the boundary limitations.</p>

<p>I wanted to know if I can use RL to force the calculated distances to fit in with the layout given certain knowledge (or conditions, if you will):the start and end position of the robot and the inter-space distances.</p>

<p>If not RL, do you know how can I solve this problem. I suspect my function computing the distances is off and wondering if RL can help me figure out the right mapping of sensor data to the path traveled adhering to the grid layout dimensions.</p>

<p><a href=""https://i.stack.imgur.com/MU9Xy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MU9Xy.png"" alt=""enter image description here""></a></p>

<p>If you consider the illustration above you will notice S, D and D' signifying the starting position, the true destination, and the destination location computed by adding together the calculated distances for each of the segments representing right(r), left(l) and straight(s) along the path towards the destination. Inter-space length is given 7m and dimensions of the blocks are (27m x 15m). If you look at the data presented on the left side you will notice 18m left and consecutive 24m right represents the activity, in the grid, as the passage through the blocks. Granted -- perhaps the car negotiates the edges and corners through this passage in a protracted left(l) and right(r) movements, without necessarily going straight(s) straddling and linking the turns as one would expect. </p>

<p>The question arises, however, when taken into account these individual segment lengths and stitch them together you end up in a destination not in the ballpark range of the expected value. How can we design this problem so as to employ RL methods to, sort of, impose these grid dimensional constraints on this distance calculation methodology to yield better results. Or, probably best to re-imagine the whole problem so it is amenable to the application of RL. </p>

<p>Any advice/ insights would be appreciated.</p>
"
2241,"<p>I (mis?)understood the NEAT algorithm has the following steps:</p>

<ol>
<li>Create a genome pool with N random genomes</li>
<li>Calculate each genome fitness</li>
<li>Assign each genome to a species</li>
<li>Calculate the adjusted fitness and the number of offspring of each species</li>
<li>Breed each species through mutation/crossover from the stronger genomes</li>
<li>go to step 2.</li>
</ol>

<p>Step 3 is tricky: speciation is made placing each genome G in the first species in which it is <em>compatible</em> with the <em>representative genome</em> of that species, or in a new species if G is not compatible with any existing species. <em>Compatible</em> is meant as having compabilitity distance below a certain threshold. Regarding <em>representative genome</em> NEAT paper says:</p>

<blockquote>
  <p>Each existing species is represented by a random genome inside the
  species from the <em>previous generation</em></p>
</blockquote>

<p><a href=""https://www.cs.ucf.edu/~kstanley/neat.html"" rel=""nofollow noreferrer"">Somewhere</a> I've found that keeping the number of species stable is good, and this is achieved automatically with dynamic thresholding. However, dynamic thresholding makes hard to evaluate species behaviour across generations.</p>

<p>Let me give one example:
Assume that in Generation 20, Species 1 has Genome A as representative and Species 2 has Genome B as representative. Assume elitism is implemented.</p>

<p>As the representative genome is taken from previous generation, assume that in Generation 21, Genome A and B are still representatives for Species 1 and 2, however assume compatibility threshold has changed (i.e. bigger) in order to reach the target species number. With this change, A and B have now a compatibility distance lower than threshold and should be placed in the same Species, however they are representatives of different species.</p>

<p><strong>How to solve this issue?</strong></p>

<p>More in general, with dynamic thresholding, how to make sure species management across generations is consistent? E.g. NEAT paper also says:</p>

<blockquote>
  <p>If the maximum fitness of a species did not improve in 15 generations,
  the networks in the stagnant species were not allowed to reproduce.</p>
</blockquote>

<p>How to make sure that across all 15 generations, we are still considering that same single species and this has not drastically changed (so that they are actually different 'objects'?). E.g. in the example above, if A and B are both placed in Species 1 in Generation 21, Species 2 no longer represents what it represented in Generation 20.</p>
"
2242,"<p>When a human looks at a page. He notices the sets of letters are grouped together separated by white space. If the white space was replaced by another character say z, it would be harder to distinguish words.</p>

<p>For a neural network, spaces are ""just another character"". How can we set up an RNN so it gives special importance to the difference between certain characters like white spaces and letters so that it will train faster? Assume the input is just a sequence of ASCII characters.</p>
"
2243,"<p>As shown below, my deep neural network is overfitting :
<a href=""https://i.stack.imgur.com/AfDBu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AfDBu.png"" alt=""Cross-entropy loss""></a>
<a href=""https://i.stack.imgur.com/janOU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/janOU.png"" alt=""Accuracy""></a></p>

<p><em>where the <strong>blue</strong> lines is the metrics obtained with <strong>training</strong> set and <strong>red</strong> lines with <strong>validation</strong> set</em></p>

<p><strong>Is there anything I can infer from the fact that the accuracy on the training sets is really high</strong> (almost 1) <strong>?</strong></p>

<p>From what I understand, it means that the complexity of my model is enough / too big. But does it means my model could theoretically reach such a score on validation set with same dataset and appropriate hyperparameters ? With same hyperparameters but bigger dataset ? </p>

<p>My question is <strong>not</strong> how to avoid overfitting.</p>
"
2244,"<p>I have a dataset of images belonging to <span class=""math-container"">$N$</span> classes, <span class=""math-container"">$A_1, A_2...A_n,B_1,B_2...B_m$</span> and I want to train a CNN to classify them. The classes can be considered as subclasses of two broader classes <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span>, therefore the confusion between <span class=""math-container"">$A_i$</span> and <span class=""math-container"">$A_j$</span> is much less problematic than the confusion between <span class=""math-container"">$A_i$</span> and <span class=""math-container"">$B_j$</span>. Therefore I want the CNN to be trained in such a way that the difference between <span class=""math-container"">$A_i$</span> and <span class=""math-container"">$B_j$</span> is considered as more relevant. </p>

<p>1) Are there any loss functions that take this requirement into account? Could a weighted cross entropy work in this case?</p>

<p>2) How this loss would change if the classes were unbalanced?</p>
"
2245,"<p>I'm trying to create a simple Dyna-Q agent to solve small mazes, in python. For the Q function, Q(s,a), I'm just using a matrix, where each row is for a state value, and each column is for one of the 4 actions (up, down, left, right).</p>

<p>I've implemented the ""real experience"" part, which is basically just straightforward SARSA. It solves a moderately hard (i.e., have to go around a few obstacles) maze in 2000-8000 steps (in the first episode, it will no doubt decrease with more). So I know that part is working reliably.</p>

<p>Now, adding the part that simulates experience based on what it knows of the model to update the Q values more, I'm having trouble. The way I'm doing it is to keep an <code>experiences</code> list (a lot like experience replay), where each time I take a real action, I add its (S,A,R,S') to that list. </p>

<p>Then, when I want to simulate an experience, I take a random (S,A,R,S') tuple from that list (David Silver mentions in his lecture (#8) on this that you can either update your transition probability matrix P and reward matrix R by changing their values, or just sample from the experience list, which should be equivalent). In my case, with a given S and A, since it's deterministic, R and S' are also going to be the same as the ones I sampled from the tuple. Then I calculate Q(S,A) and max_A'(Q(S',A')), to get the TD error (same as above), and do stochastic gradient descent with it to change Q(S,A) in the right direction.</p>

<p>But it's not working. When I add simulated experiences, it <em>never</em> finds the goal. I've tried poking around to figure out why, and all I can see that's weird is that the Q values continually increase as time goes on (while, without experiences, they settle to correct values).</p>

<p>Does anyone have any advice about things I could try? I've looked at the sampled experiences, the Q values in the experience loop, the gradient, etc... and nothing really sticks out, aside from the Q values growing.</p>

<p>edit: here's the code. The first part (one step TD learning) is working great. Adding the planning loop part screws it up.</p>

<pre><code>def dynaQ(self, N_steps=100, N_plan_steps=5):

    self.initEpisode()
    for i in range(N_steps):
        #Get current state, next action, reward, next state
        s = self.getStateVec()
        a = self.epsGreedyAction(s)
        r, s_next = self.iterate(a)
        #Get Q values, Q_next is detached so it doesn't get changed by the gradient
        Q_cur = self.Q[s, a]
        Q_next = torch.max(self.Q[s_next]).detach().item()
        TD0_error = (r + self.params['gamma']*Q_next - Q_cur).pow(2).sum()
        #SGD
        self.optimizer.zero_grad()
        TD0_error.backward()
        self.optimizer.step()
        #Add to experience buffer
        e = Experience(s, a, r, s_next)
        self.updateModel(e)

        for j in range(N_plan_steps):

            xp = self.experiences[randint(0,len(self.experiences)-1)]
            Q_cur0 = self.Q[xp.s, xp.a]
            Q_next0 = torch.max(self.Q[xp.s_next]).detach().item()
            TD0_error0 = (xp.r + self.params['gamma']*Q_next0 - Q_cur0).pow(2).sum()

            self.optimizer.zero_grad()
            TD0_error0.backward()
            self.optimizer.step()
</code></pre>
"
2246,"<p>I'm trying to build a chat analysis that could identify the intent of check price.</p>

<p>Has any kind of preset intent list to train my chatbot already been done ?</p>

<ul>
<li>Data: A data set for ""check price"" intent's. Like ""How much are ____""</li>
<li>Context:I looking for a data set to train a intent recognization for check price at chat bot. Exemple: If some one ask for a product price. I want to tag this is as check price intent . For that I am trainning a intent recognition.</li>
<li>Region: Prefirencal the whole globe,but for start could be only at English</li>
<li>License:Could both payed or free</li>
<li>Non-answers: I didnt find nothing like a dataset for intent</li>
</ul>
"
2247,"<p>When extending reinforcement learning to the continuous states, continuous action case, we must use function approximators (linear or non-linear) to approximate the Q-value.  It is well known that non-linear function approximators, such as neural networks, diverge aggressively.  One way to help stabilize training is using reward clipping.  Because the temporal difference Q-update is a bootstrapping method (i.e., uses a previously calculated value to compute the current prediction), a very large previously calculated Q-value can make the current reward relatively minuscule, thus making the current reward <strong><em>not</em></strong> impact the Q-update, eventually leading the agent to diverge.</p>

<p>To avoid this, we can try to avoid the large Q-value in the first place by clipping the reward between [1, -1].</p>

<p>But I have seen some other people say that instead of clipping the reward itself, we can instead clip the Q-value between an interval.</p>

<p>I was wondering which method is better for convergence, and under what assumptions / circumstances.  I was also wondering if there are any theoretical proofs/explanations about reward/Q-value clipping and which one being better. </p>
"
2248,"<p>Would this work at all?</p>

<p>Idea is to start training a neural net with some number of nodes. Then, add some new nodes and more layers and start training only the new nodes (or only modifying the old nodes very slightly). Ideally, we would connect all old nodes to the new layer added since we might have learned many useful things in the hidden layers. Then repeat this many times.</p>

<p>Intuition is that if the old nodes give bad information the new layer of nodes will weight the activations of old nodes close to zero and learn new/better concepts in the new nodes. The benefit is that we will keep old knowledge forever.</p>

<p>Caveat is that the network can still temporarily ""forget"" concepts if a new layer weights old information close to zero, but it can potentially remember it again too.</p>

<p>If this completely fails, I'm curious if there's some known way to prevent a neural network from forgetting concepts it learned.</p>
"
2249,"<p>I'm working on a project,whereby; one of the actors is ""Teacher"" and it's role will be, insert the course name, outcomes of skills, then insert the question, student degree and automatically will be connected with appropriate skill through the natural language processing (NLP) analysis student degree. </p>

<p>What algorithm should I used in Natural Language Processing to connect the question to the appropriate skill (text comparison)? </p>
"
2250,"<p>I am interested in understanding how to choose data-acquisition parameters for the subject matter:</p>

<ul>
<li>Frame Resolution</li>
<li>Frame rates (FPS)</li>
</ul>

<p>The goal is to have 'enough' (preferably the minimal) resolution and frames to enable AI to identify people. </p>

<h1>QUESTIONS</h1>

<ol>
<li>Are there any published rules of thumb or processes to select video parameters?</li>
<li>Is there a term or label for the selection of video parameters for AI projects?</li>
</ol>
"
2251,"<p>I am a newbie in the deep learning and am looking for advice on predicting traffic congestion events. I have a table for vehicles travel times data, another table with the roads length segmented based on stop locations. I am thinking to derive the time-wise route specific speed details based on stop locations. After initial data cleansing and messaging, my input parameters are the time and stop location with actual speed details. I train my model with the training dataset and validate as per the deep learning recommended approach. </p>

<p>So my questions are:</p>

<ol>
<li>Is this approach correct or how can I improve it? I am not sure if
the number of inputs can be increased for better results.   </li>
<li>Which activation method will be best to utilize to get a range of
conditions/event types rather than binary 1 or 0?   </li>
<li>This will require dealing with a bigger dataset of at least over a few GBs. This will evolve into around 200GBs in the final product. Can I use my     professional grad laptop to process this data or if I should consider going to Big Data Processing power?</li>
</ol>

<p>Please advise. Thanks in advance for your help.</p>
"
2252,"<p>There are some predefined categories( Overview, Data Architecture,Technical Details, Applications etc). The requirement is to classify the input text of paragraphs into their resp. category. I cant use any pretrained word embeddings (Word2Vec,Glove) because the data entered is not in general English ( talking about dogs, environment etc) but pure technical (How does a particular program orks, steps to download anaconda etc). Don't have any data available in internet to train as well. Anything that understands semantic-surface-level of a sentence will work</p>
"
2253,"<p>I'm try to train a RNN with a chunk of audio data, where X and Y are two audio channels loaded into numpy arrays.  The objective is to experiment with different NN designs to train them to transform single channel (mono) audio into a two channel (stereo) audio.</p>

<p>My questions are:</p>

<ol>
<li>Do I need a stateful network type, like LSTM? (I think yes.)</li>
<li>How should I organize the data, considering that there are millions of samples and I can't load into memory a matrix of each window of data in a reasonable time-span?</li>
</ol>

<p>For example if I have an array with: [0, 0.5, 0.75, 1, -0.5, 0.22, -0.30 ...] and I want to take a window of 3 samples, for example.  I guess I need to create a matrix with every sample shift like this, right?</p>

<pre><code>[[0.00, 0.50, 0.75]
 [0.50, 0.75, 1.00]
 [0.75, 1.00,-0.50]
 [1.00,-0.50, 0.22]]
</code></pre>

<p>Where is my batch_size? Should I make the matrix like this per each sample shift? Per each window? This may be very memory consuming if I intend to load a 4 min song. </p>

<p>Is this example matrix a single batch? A single sample?</p>
"
2254,"<p>Sparse linear system are normally solved by using solvers like MINRES, Conjugate gradient, GMRES. </p>

<p>Efficient preconditioning, i.e., finding a matrix P such that PAx = Pb is easier to solve then the original problem, can drastically reduce the computational effort to solve for x. However, preconditioning is normally problem specific and there is not ONE preconditioner that works well for every problem.</p>

<p>I thought this would be an interesting problem to apply RL, since there are certain norms (e.g. condition number of matrix PA) to measure if P is a good preconditioner, but I could not find any research in this field.</p>

<p>Is there a specific problem why RL could not be applied?</p>
"
2255,"<p>I have created a classifier for some simple gestures using an input layer, a hidden layer with tanh activation and an output softmax layer, I'm also using the Adam optimiser. The network classifies perfectly with validation data, however I'd like it to be able to take in random noise that looks nothing like the shapes and not be able to classify it confidently. For example:</p>

<p>One gesture input looks like this and is correctly classified as gesture 'A':
<a href=""https://i.stack.imgur.com/JlvjP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JlvjP.png"" alt=""enter image description here""></a></p>

<p>However, when I pass this 'noise', which is clearly differentiable to the human eye, as input it still classifies it with 100% confidence that it is the same gesture 'A'.</p>

<p><a href=""https://i.stack.imgur.com/3DBKX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3DBKX.png"" alt=""enter image description here""></a></p>

<p>I assume it's because the inputs are still very close to 0? My instinct is to scale up the inputs perhaps to increase the differentiation between the noise and the input. However, in real operation the noise will all be on a similar scale to the inputs and I won't know what is noise and what isn't so I will still have to apply the same scaling to that noise. Will I run into the same problem? </p>

<p>On a more general note is there a teaching approach to prevent misclassifications, particularly if we know what they might look like? For example, in this case I thought I could perhaps generate some noise and use it at training time to create an extra noise class, or is it just best to come up with such a well-trained network that you can use some sort of confidence threshold? For example, if the network only produces 50% confidence on an input then I can discard it as noise. Any suggestions much appreciated!  </p>
"
2256,"<p>This survey of <a href=""https://www.metalevel.at/prolog/ai"" rel=""nofollow noreferrer"">Artificial Intelligence with Prolog</a> provides a short perspective on the Prolog programming language and its position in the history of AI.  </p>

<p>It proposes that a return to Prolog Research and Development as begun in the 1980s when Japan began work on a Fifth Generation Computer System.  Has the reduction in cost and the increase in speed of computing platforms re-opened a door that was closed primarily because of technology immaturity?</p>
"
2257,"<p><a href=""https://quickdraw.withgoogle.com/"" rel=""nofollow noreferrer"">Quick draw</a> is a Google experiment using user generated online doodles and machine learning to play a game of ""Guess what I'm drawing"" similar to the board game Pictionary.</p>

<p>I'm interested if anyone happens to know what algorithms are in play in this example? </p>
"
2258,"<p>Maxpooling is performed as one of the steps in inception which yields same output dimension as that of the input.
Can anyone explain how this max pooling is performed? </p>
"
2259,"<p>Machine learning and data science are mainly made for processing large amounts of data nowadays, for example - a multitude of pictures.</p>

<p>But do these fields have some applications in the decision making?</p>

<p>I mean - do at least some of the companies make a decision making systems as a part of their products? And do they hire DMS specialists?</p>

<p>Is there any difference between DMS and ""regular"" DS?</p>
"
2260,"<p>A fixed video camera records people moving through its field of view.</p>

<p>The goal is to <strong>detect</strong> and track the head, in real-time as it moves through the video.  The norm is there are many heads, which often are sometimes partially obscured.  This <a href=""https://youtu.be/qDK6nLdm3sQ?t=45s"" rel=""nofollow noreferrer"">example video boxes heads</a> and provides a head count.</p>

<p>There seems to be many different models.  Examples include:   </p>

<ul>
<li><a href=""https://www.youtube.com/watch?v=PRbr9-_xLzs"" rel=""nofollow noreferrer"">Adaboost-haar Head detection</a> </li>
<li><a href=""https://www.zeolearn.com/magazine/instance-segmentation-using-deep-learning"" rel=""nofollow noreferrer"">MASK R-CNN</a></li>
<li><a href=""https://www.youtube.com/watch?v=tBwUbQ0pyUw"" rel=""nofollow noreferrer"">LBP Cascade</a></li>
</ul>

<p>Given the context of the video, what is the thought process that you would use to choose a model?    </p>
"
2261,"<p>I'm making a Connect Four game using the typical minimax + alpha-beta pruning algorithms. I just implemented a Transposition Table, but my tests tell me the TT only helps 17% of the time. By this I mean that 17% of the positions my engine comes across in its calculations can be automatically given a value (due to the position being calculated previously via a different move order).</p>

<p>For most games, is this figure expected? To me it seems very low, and I was optimistically hoping for the TT to speed up my engine by around 50%. It should be noted though that on each turn in the game, I reset my TT (since the evaluation previously assigned to each position is inaccurate due to lower depth back then).</p>

<p>I know that the effectiveness of TT's are largely dependent on the game they're being used for, but any ballparks of how much they speed up common games (chess, go, etc) would be helpful.</p>

<p>EDIT - After running some more tests and adjusting my code, I found that the TT sped up my engine to about 133% (so it took 75% as much time to calculate). This means those 17% nodes were probably fairly high up in the tree, since not having to calculate the evaluation of these 17% sped up things by 33%. This is definitely better, but my question still remains on whether this is roughly expected performance of a typical TT.</p>
"
2262,"<p>So let's say you had a really nice day in a flight simulator and you are getting videos of this type of quality:</p>

<p><a href=""https://i.stack.imgur.com/ftsYV.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ftsYV.png"" alt=""A snapshot of the compressed video""></a></p>

<p>This is Full HD (1080p), but heavily compressed. You can literally see the pixels. Now I tried to use something like RAISR, and this <a href=""https://github.com/hugopilot/raisr"" rel=""noreferrer"">python implementation</a>, but it only scales the image up and does not 'fix the thicc pixels'. </p>

<p>So is there a type of AI that does fix this kind of video/photo into a reasonable quality video? I just want to get rid of those pixels and image artefacts that was generated during the compression.</p>
"
2263,"<p>I am preparing a binary classifier. Initially I used the the following parameters based on the well known cat and dog classifier example;</p>

<pre><code>train_datagen = ImageDataGenerator(rescale=1./255)
validation_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
# Data Generator for Training data
train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='binary')    
# Data Generator for Validation data
validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size= (224, 224), batch_size=16, class_mode='binary', shuffle=False)                                                              
# Data Generator for Test data
test_generator = validation_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=16, class_mode='binary', shuffle=False)                                                  
# Compile the model
model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['acc'])
</code></pre>

<p>and the model was;</p>

<pre><code># Create a sequential model
model = models.Sequential()
# Add the vgg convolutional base model
model.add(vgg_conv)
# Add new layers
model.add(layers.Flatten())
model.add(layers.Dense(1024, activation='relu'))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(1, activation='sigmoid'))
</code></pre>

<p>I use VGG16 with imagenet weights by adding these layers at the end of the model. Cat and dog classifier works with a good accuracy and I was sure this model is going to give me very good results for the initial steps. However, when I checked the confusion matrix I saw that all the images are classified they belong to category 0. Then I replaced all ""binaries"" with ""categorical"" and used softmax activator with 2 units. All the rest is the same and now I obtain very good results. I do not understand how this happens? Why the first configuration work very well with another dataset but gives me garbage results with my own dataset? It is better to try as much as possible but it takes a bit less than a day with my dataset. I also do not understand how a binary classifier works with only one unit at the output layer since the actual categories are two.</p>
"
2264,"<p>In keras, when we use an LSTM/RNN model, we need to specify the node [i.e., LSTM(128)]. I have a doubt regarding how it actually works. From the LSTM/RNN unfolding image or description, I found that each RNN cell take one time step at a time. What if my sequence is larger than 128? How to interpret this? Can anyone please explain me? Thank in advance.</p>
"
2265,"<p>little help here</p>

<pre><code>    ""I don't want to put unknown encodings of group image in to a classifier""

    what i did was find Matches for each of the encoding and trying to remove   encodings with matches=0
</code></pre>

<p>for encoding in encodings: </p>

<pre><code>  matches=np.count_nonzero(face_recognition.compare_faces(data[""encodings""],
            encoding))         
        d[""encods""]=[matches,encoding]
    for k,v in list(d.items()):
        if v[0]==0:
        del d[k]

        print(""dict"",d)
</code></pre>

<p>I am not getting  what i want.Please help</p>
"
2266,"<p><em>Following-up my <a href=""https://ai.stackexchange.com/questions/8338/interpretation-of-a-good-overfitting-score"">question</a> about my over-fitting network</em> </p>

<p>My deep neural network is over-fitting : 
<a href=""https://i.stack.imgur.com/6xKGF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6xKGF.png"" alt=""enter image description here""></a></p>

<p>I have tried several things : </p>

<ul>
<li>Simplify the architecture</li>
<li>Apply more (and more !) Dropout</li>
<li>Data augmentation</li>
</ul>

<p>But I always reach similar results : training accuracy is eventually going up, while validation accuracy never exceed ~70%.</p>

<p>I think I simplified enough the architecture / applied enough dropout, because my network is even too dumb to learn anything and return random results (3-classes classifier => 33% is random accuracy), even on training dataset :
<a href=""https://i.stack.imgur.com/nh4Bs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nh4Bs.png"" alt=""enter image description here""></a> </p>

<p>My question is : <strong>This accuracy of 70% is the best my model can reach ?</strong></p>

<p>If yes :</p>

<ul>
<li>Why the training accuracy reach such high scores, and why so fast, knowing this architecture seems to be not compatible ?</li>
<li>My only option to improve the accuracy is then to change my model, right ?</li>
</ul>

<p>If no :</p>

<ul>
<li>What are my options to improve this accuracy ?</li>
</ul>

<p><em>I'v tried a bunch of hyperparameters, and a lot of time, depending of these parameters, the accuracy does not change a lot, always reaching ~70%. However I can't exceed this limit, even though it seems easy to my network to reach it (short convergence time)</em></p>

<h2>Edit</h2>

<p>Here is the Confusion matrix :</p>

<p><a href=""https://i.stack.imgur.com/S2Hro.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/S2Hro.png"" alt=""enter image description here""></a></p>

<p>I don't think the data or the balance of the class is the problem here, because I used a well-known / explored dataset : <a href=""https://nlp.stanford.edu/projects/snli/"" rel=""nofollow noreferrer"">SNLI Dataset</a> </p>

<p>And here is the learning curve :</p>

<p><a href=""https://i.stack.imgur.com/MjFPu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MjFPu.png"" alt=""enter image description here""></a></p>

<p><em>Note : I used accuracy instead of error rate as pointed by the resource of Martin Thoma</em></p>

<p>It's really ugly one. I guess there is some problem here.
Maybe the problem is that I used the result after 25 epoch for every values. So with little data, training accuracy don't really have time to converge to 100% accuracy. And for bigger training data, as pointed in earlier graphs, the model overfit so the accuracy is not the best one.</p>
"
2267,"<p>The <a href=""https://arxiv.org/pdf/1608.06993v3.pdf"" rel=""nofollow noreferrer"">DenseNet architecture</a> can be summarize with this figure :
<a href=""https://i.stack.imgur.com/cSwqp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cSwqp.png"" alt=""enter image description here""></a></p>

<p><strong>Why there is transition layers between each blocks ?</strong></p>

<p>In the papers, they justify the use of transition layers as follow :</p>

<blockquote>
  <p>The   concatenation   operation   used   in
  Eq. (2) is not viable when the size of feature-maps changes.
  However,  an  essential  part  of  convolutional  networks  is
  pooling  layers  that  change  the  size  of  feature-maps.   To
  facilitate  pooling  in  our  architecture  we  divide  the  net-
  work into multiple densely connected
  dense blocks</p>
</blockquote>

<p>But, if I understand what they means : the problem is that the feature map size can change, thus we can't concatenate. But how adding transition layer change this problem ?</p>

<p>And how can several dense blocks connected like this are more efficient that one single bigger dense block ?</p>

<p><em>Optional question :</em> Why all standard DenseNet are made of 4 dense blocks ? I guess I will have the answer to this question if I understood better the previous questions...</p>
"
2268,"<p>Is the laptop Asus ZenBook Pro 90NX0152-M02980 enough to do deep learning model?</p>

<p>Specs:</p>

<ul>
<li>Processeur Intel Core i7-7500U (Dual-Core 2.7 GHz / 3.5 GHz Turbo -
cache 4 MB) </li>
<li>8 GB Memory</li>
<li>SSD M.2 SATA de 256GB</li>
</ul>
"
2269,"<p>I want to make a kind of robotic brain i.e. a big neural network, which includes NLP model (for understanding human voice) , real-time object recognition (so it can identify particular object), face recognition model ( for identifying faces).. </p>

<p>Is possible to build a huge neural networks in which we can combine all these separate models togetherly so we can use all 3 model's capabilities at same time in parallel? (i.e. if I ask robot/chatbot using mic, can you can see that table/ that boy?, Robot start recognizing object &amp; faces and reply me back by speech if he could identify or not!)</p>

<p>If it's possible then kindly share your idea how can I implement this? Or is there any better to make such AI? ( In python, tensorflow )</p>
"
2270,"<p>Can anyone explain to me what exactly ontologies are in AI? How should I write them and why are they important?</p>

<p>I can't seem to find relevant information on Google.</p>
"
2271,"<p>The process revolves around a child's drawing. Each part of each drawing corresponds to a score as in the <a href=""https://pineight.com/mw/?title=Draw-A-Person_test"" rel=""nofollow noreferrer""><em>Draw a Person Test</em> conceived by Dr. Florence Goodenough in 1926</a>. The goal of the machine is to measure a child's mental age through a figure drawing task.</p>
"
2272,"<p>I have coded an AI checkers game but would like to see how good it is. Some people have informed me to use the Chinook AI opensource code. But I am having trouble trying to integrate that software into my AI code. How do I integrate another game engine in checkers with the AI I have coded?</p>
"
2273,"<p>My background is in electrical engineering (BS, MS EE/Signal Processing) and I have a good grasp of CS foundations (Data Structures, Algorithms, OS, Discrete Math) and software engineering.</p>

<p>I have option of enrolling in a MS program in Applied Math at a good school. My objective is to switch to AI from my current career. </p>

<p>What areas of Applied math is relevant to AI? And do you think it's a good preparation instead of enrolling in a CS program.</p>
"
2274,"<p>I am new to the object recognition community. Here I am asking about <strong>the broadly accepted</strong> ways to calculate the error rate of a deep CNN when the network produces different results using the same data.</p>

<p><em>1. Problem introduction</em></p>

<p>Recently I was trying to replicate some classic deep CNNs for the object recognition tasks. Inputs are some 2D image data including objects and the output are the identification/classification results of the object. The implementation involves the use of Python and Keras.</p>

<p><strong>The problem</strong> I was facing is that, I may get different validation results among multiple runs of the training even using the same training/validation data sets. To me, that made it hard to report the error rate of the model since every time the validation result may be different.</p>

<p>I think this difference is because of the randomness involved in different aspects of deep CNN, such as random initialization, the random ‘dropout’ used in the regulation, the ‘shuffle’ process used in the choosing of epochs, etc. But I do not know yet the “right” ways to deal with this difference when I want to calculate the error rate in object recognition field. </p>

<p><em>2. My exploration – online search</em></p>

<p>I have found some answers online <a href=""https://machinelearningmastery.com/reproducible-results-neural-networks-keras/"" rel=""nofollow noreferrer"">here</a>. The author proposed two ways, and he/she recommended the first one shown below:</p>

<blockquote>
  <p>The traditional and practical way to address this problem is to run your network many times (30+) and use statistics to summarize the performance of your model, and compare your model to other models.</p>
</blockquote>

<p>The second way he/she introduced is to go to every relevant aspect of the deep CNN, to ""freeze"" their randomness nature on purpose. This kind of approach has also been introduced from Keras Q&amp;A <a href=""https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development"" rel=""nofollow noreferrer"">here</a>. They call this issue the “making reproductive results”.</p>

<p><em>3. My exploration – in academia community (<strong>no result yet, need your help!</strong>)</em></p>

<p>Since I was not sure whether the two ways mentioned above are the “right” ones broadly accepted, I was going further exploring in the object recognition academia community.</p>

<p>Now I just begin to read from <a href=""http://image-net.org/"" rel=""nofollow noreferrer"">imageNet</a> website. But I have not found the answer yet. Maybe you could help me knowing the answer easier. Thanks!</p>

<p>Daqi</p>
"
2275,"<p>In the early 20th century, anything Princeton staff represented related to computing and the simulation of human thought was assumed to be flawless.  In the late 20th century, it was MIT's AI lab that was trusted to define what was feasible and what were the next steps.  Is Google the new source of legitimacy in AI pronouncements?  Because of the connection between that corporation and the daily life of people on earth, has that publication honor that was given to the primary AI centers of research in the 20th century reached a level in the early 21st century that rivals deity?</p>

<p>It sometimes seems that, when people don't understand something in a paper with the word Google anywhere in the authorship block, it is automatically assumed that the reader is wrong because the paper is of universal authority.  They seem to jump to the assumption that their mind must adjust to whatever the paper states.  Is this equivalent to religious scripture?</p>

<p>Carl Jung <a href=""https://www.quotetab.com/quote/by-carl-jung/contemporary-man-is-blind-to-the-fact-that-with-all-his-rationality-and-efficien#eg8hD8Fstw68DmSQ.99"" rel=""nofollow noreferrer"">wrote</a>,</p>

<blockquote>
  <p>Contemporary man is blind to the fact that, with all his rationality and efficiency, he is possessed by ""powers"" that are beyond his control. His gods and demons have not disappeared at all; they have merely got new names.</p>
</blockquote>

<p>This is an AI StackExchange, so the proposal in this question is that perhaps the same brain machinery has been used for the following things</p>

<ul>
<li>An ancient Egyptian gathering with others to worship the sun god Re</li>
<li>Casiodoris using the resources of the Roman Empire to send holy scripture out on horseback to the far reaches of the empire to save it from the invading barbarian hordes</li>
<li>AI researchers finding some unassailable source of truth in a university or a multinational corporation</li>
</ul>

<p>Is Carl Jung correct, and, if so, is this human need to find legitimacy in an institution now manifesting in the AI community's absolute trust in a corporation that automated the 18th century library card catalog and built that automaton into a large multinational corporation?</p>

<p>Lastly, if both those questions can be answered in the affirmative, what risks can exist when the human mind treats a university or a corporation as a god from an economic or social risk management perspective?</p>
"
2276,"<p>This is a question about pattern recognition and feature extraction.</p>

<p>I am familiar with Hough transforms, the Fast Radial Transform and variants (e.g., GFRS), but these highlight circles, spheres, etc.</p>

<p>I need an image filter that will highlight the centroid of a series of spokes radiating from it, such as the center of a asterix or the spokes of a bicycle wheel (even if the round wheel is obscured.  Does such a filter exist?</p>
"
2277,"<p>I would like to get a simple example running in matlab that will use a neural net to learn an arbitrary function from input output data (basically model identification) and then be able to approximate that function from just the input data. As means of training this net I have implemented a simple back propagation algorithm in matlab but I was not able to get anywhere close to satisfactory results. I would like to know what I may be doing wrong and also what approach I may use instead.</p>

<p>The goal is to have the network represent an identified function f(x) which takes a series x as input and outputs the learned mapping from x -> y. </p>

<p>Here is the GNU octave code I have so far:</p>

<pre><code>pkg load control signal

function r = sigmoid(z)
    r = 1 ./ (1 + exp(-z));
end

function r = linear(z)
    r = z;
end 

function r = grad_sigmoid(z)
    r = sigmoid(z) .* (1 - sigmoid(z));
end 

function r = grad_linear(z)
    r = 1;
end 

function r = grad_tanh(z)
    r = 1 - tanh(z) .^ 2;
end

function nn = nn_init(n_input, n_hidden1, n_hidden2, n_output)
    nn.W2 = (rand(n_input, n_hidden1) * 2 - 1)'
    nn.W3 = (rand(n_hidden1, n_hidden2) * 2 - 1)'
    nn.W4 = (rand(n_hidden2, n_output) * 2 - 1)'
    nn.lambda = 0.005;
end

function nn = nn_train(nn_in, state, action)
    nn = nn_in;

    [out, nn] = nn_eval(nn, state);

    d4 = (nn.a4 - action) .* grad_linear(nn.W4 * nn.a3); 
    d3 = (nn.W4' * d4) .* grad_tanh(nn.W3 * nn.a2);
    d2 = (nn.W3' * d3) .* grad_tanh(nn.W2 * nn.a1);

    nn.W4 -= nn.lambda * (d4 * nn.a3');
    nn.W3 -= nn.lambda * (d3 * nn.a2');
    nn.W2 -= nn.lambda * (d2 * nn.a1');
end

function [out,nn] = nn_eval(nn_in, state)
    nn = nn_in;

    nn.z1 = state;
    nn.a1 = nn.z1;

    nn.a2 = tanh(nn.W2 * nn.a1);
    nn.a3 = tanh(nn.W3 * nn.a2);
    nn.a4 = linear(nn.W4 * nn.a3);

    out = nn.a4;
end

nn = nn_init(1, 100, 100, 1);
t = 1:0.1:3.14*10;
input = t;
output = sin(input);
learned = zeros(1, length(output));

for j = 1:500
    for i = 1:length(input)
        nn = nn_train(nn, [input(i)], [output(i)]); 
    end
    j
end

for i = 1:length(input)
    learned(i) = nn_eval(nn, [input(i)]);    
end

plot(t, output, 'g', t, learned, 'b');

pause
</code></pre>

<p>Here is the result:
<a href=""https://i.stack.imgur.com/IK8Gn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IK8Gn.png"" alt=""enter image description here""></a></p>

<p>The result is not even close to where I want it to be. Has it got something to do with my implementation of back propagation? </p>

<p>What changes do I need to do to the code to get a better approximation going? </p>
"
2278,"<p><strong>Assistive Subsystems</strong></p>

<p>Consider an automated vacuum cleaner with the following subsystems under the command of an AI system to be designed.  These subsystems limit the AI complexity to just intelligent guidance of the vacuum, much like the behavioral components of the nervous systems of any arthropods, vertebrates, or other bilaterally symmetric animals.</p>

<ul>
<li>Accurate and reliable vacuum body position and orientation control through four 8"" diameter rubber wheels with independent suspension and radial positioning via a separate compass and floor plan positioning system &mdash; the AI can simply command that system to go to position <span class=""math-container"">$(x, y)$</span> in room <span class=""math-container"">$R_i$</span> and face ""NNW"" (north-north-west).</li>
<li>Accurate and reliable tool control &mdash; the AI can simply command ""brush"", ""crevice"", ""rectangle"", ""rug"", ""right-angle"", or a coordinate in <span class=""math-container"">$\mathbb{R}^3$</span> with a tool approach angle for the tube onto which tools are attached, given as a compound angle <span class=""math-container"">$(\theta, \phi)$</span> relative to the orientation of the vacuum, where <span class=""math-container"">$\theta = \arctan(\Delta x, \Delta y)$</span> is the yaw and <span class=""math-container"">$\phi = \arctan(\Delta z, \sqrt{(\Delta x)^2 + \Delta y)^2})$</span> is the pitchdirection from which the tool</li>
<li>Access to a tactile interference an event queue containing a time series of significant changes in force vectors.  The queue contains elements that include the force vector in <span class=""math-container"">$\mathbb{R}^3$</span> and whether the force change is being reported for force on the vacuum or on the tool.</li>
</ul>

<p><strong>Product Limitations</strong></p>

<p>Assume the following as product limitations. (Whether or not these are the best limitations, these are the directives from the Board of Directors.)</p>

<ul>
<li>The vacuum is blind.  (A vision system cost to train across multiple households, manufacture, test, and update, and the associated impact on consumer price tags was evaluated and a tactile only sensory system is specified as a device engineering constraint.)</li>
<li>The room is free from foot traffic, children playing, and other such human motive activity.</li>
<li>The floor of the room is free from balls, toys, spills, cloth items, and other such artifacts that would frustrate a vacuuming operation.</li>
</ul>

<p><strong>Requirements of Maintaining Two Maps</strong></p>

<p>An accessibility map must be maintained to make use of past information about where the vacuum can go without bumping into something.  It is obviously easier to maintain an accessibility map than to rediscover a room upon every room vacuuming.  This map must include tool positions also.</p>

<p>A coverage map must be initialized at the beginning of every room vacuuming and continuously updated to ensure that all the floor area is covered with appropriate overlapping of tool paths.</p>

<p><strong>Distinct Question</strong></p>

<p>Note that this challenge is distinct from <a href=""https://ai.stackexchange.com/questions/1613/how-do-autonomous-robotic-vacuum-cleaners-perceive-the-environment-for-navigatio"">the <em>How do autonomous robotic vacuum cleaners perceive the environment for navigation?</em> question</a> in that it is not about discovery of room and furnishings location but rather assumes floor plans are installed upon product deployment (which is more likely to be a highly effective short term product strategy).  The problem of adapting to changes in the accessibility map is the challenge here.</p>

<p><strong>The Question</strong></p>

<blockquote>
  <p>How can a fully automated vacuum cleaner use and update room information?</p>
</blockquote>

<p>See above two maps.</p>
"
2279,"<p>I already know the basics of the basic of Machine Learning. E.g.: Backpropagation, Convolution, etc.</p>

<p>First of let me explain Reinforcement learning to make sure I grasped the concept correctly.</p>

<p>In Reinforcement learning a random-initialized network will first ""play""/""do"" a sequence of moves in an environment. (In this case a Game). After that, it will receive a reward <span class=""math-container"">$r$</span>. Furthermore a q-Value gets defined by the engineer/hooby coder. This reward times the q-Value <span class=""math-container"">$q$</span> to the power of the  position <span class=""math-container"">$n$</span> of the action will be feeded back using BP. </p>

<p><strong>So how do I know how slight chances in <span class=""math-container"">$\vec{w}$</span> are changing <span class=""math-container"">$rq^n$</span>?</strong></p>
"
2280,"<p>Given infinite resources/time one could create AGIs by writing code to simulate infinite worlds. In some of the worlds AGIs would be created. Detecting them would be another issue.</p>

<p>Since we don't have infinite resources the most probable way to create an AGI is to write some bootstrapping code which would reduce the resources/time to reasonable values.</p>

<blockquote>
  <p>In that AGI code (that would make it reasonable to create with finite
  resources/time) is it required to have a part that deals with
  time/space estimation of possible actions taken? Or should that be outside of the code and be something the AGI discovers by itself after it starts running?</p>
</blockquote>

<p>Any example of <a href=""https://github.com/fairy-tale-agi-solutions/awesome-artificial-general-intelligence#organizations--projects"" rel=""nofollow noreferrer"">projects targeting AGI</a> that are using time/space estimation might be useful for reaching a conclusion.</p>

<p>Clarification, by time/space I mean time/space complexity analysis for algorithms, see: <a href=""https://en.wikipedia.org/wiki/Algorithmic_efficiency#Measures_of_resource_usage"" rel=""nofollow noreferrer"">Measures of resource usage</a> and <a href=""https://en.wikipedia.org/wiki/Analysis_of_algorithms"" rel=""nofollow noreferrer"">Analysis of algorithms</a></p>

<p>The question formulation might lead people to think that the time/space estimation can only apply to some class of actions called algorithms. To clarify my mistake, I mean the estimation to apply to any action plan.</p>

<p>Imagine you are an AGI and you have to make a choice between different set of actions to pursue your goals. If you had 2 goals and one of them used less space and less time then you would always pick it over the other algorithm. So time/space estimation is very useful since intelligence is about efficiency.</p>

<p>There is at least 1 exception though, imagine in the example before that the goal of the AGI is to pick the set of actions that leads to the most expensive time/space set of actions (or any non-minimal time/space cost) then obviously because of the goal constraint you would pick the most time/space expensive set of actions. In most other cases though, you would just pick the most time/space efficient algorithm.</p>
"
2281,"<p>What is an agent in reinforcement learning (RL)? I think it is not the neural network behind. What does the agent in RL exactly do?</p>
"
2282,"<p>scanned documents could be from - news papers/book/magazine with complex alignments for text(text could be in any angle w.r.t. the page).
I can do lot of processing for different features extraction right! But I want to know some robust methods which does not need much features. so I thought machine learning could help me. it should be like  doing less processing and more machine learning. Thank you. </p>
"
2283,"<p>After doing some exercices on Q-learning for maze solving, I wondered : my q-learning algorithms solve only ONE maze. The AI doesn't learn how to solve mazes, so how can I achieve it ?</p>

<p>For instance learn ""follow one wall until the end of the maze"" instead of ""turn left left right..."" ?</p>

<p>I guess it is the same approach for self driving cars ?</p>
"
2284,"<p>I am reading article <a href=""https://allenai.org/paper-appendix/emnlp2017-wt/"" rel=""nofollow noreferrer"">https://allenai.org/paper-appendix/emnlp2017-wt/</a> <a href=""http://ai2-website.s3.amazonaws.com/publications/wikitables.pdf"" rel=""nofollow noreferrer"">http://ai2-website.s3.amazonaws.com/publications/wikitables.pdf</a> about training neural network and the loss function is mentioned on page 6 chapter 3.4 - this loss function <code>O(theta)</code> is expressed as marginal loglikelihood objective function. I simply does not understand this. The neural network generates logical expression (query) from some question in natural language. The network is trained using question-answer pairs. One could expect that simple sum of correct-1/incorrect=0 result could be good loss function. But there is strange expression that involves <code>P(l|qi, Ti; theta)</code> that is not mentioned in the article. What is meant by this <code>P</code> function? As I understand, then many logical forms <code>l</code> are generated externally for some question <code>qi</code>. But further I can not understand this. The mentioned article largely builds on other article <a href=""http://www.aclweb.org/anthology/P16-1003"" rel=""nofollow noreferrer"">http://www.aclweb.org/anthology/P16-1003</a> from which it borrows some terms and ideas.</p>

<p>It is said that <code>l</code> is treated as latent variable and <code>P</code> seems to be some kind of probability. Of course, we should assign the greated probability to the right logical form <code>l</code>, but where can I find this assignment. Does training/supervision data should contain this probability function for training/supervision data?</p>
"
2285,"<p>In the Abstract section of the paper <a href=""https://arxiv.org/pdf/1312.4400.pdf"" rel=""nofollow noreferrer"">Network In Network</a>, what does the authors actually mean to say?</p>
"
2286,"<p>While working through some example from Github I've found this network (it's for FashionMNIST but it doesn't really matter). </p>

<p>Pytorch forward method (my query in upper case comments with regards to applying Softmax on top of Relu?):</p>

<pre><code>def forward(self, x):
    # two conv/relu + pool layers
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))

    # prep for linear layer
    # flatten the inputs into a vector
    x = x.view(x.size(0), -1)

    # DOES IT MAKE SENSE TO APPLY RELU HERE
    **x = F.relu(self.fc1(x))

    # AND THEN Softmax on top of it ?
    x = F.log_softmax(x, dim=1)**

    # final output
    return x
</code></pre>
"
2287,"<p>I am new to RL and I am trying to work through the book Reinforcement Learning: An Introduction I (Sutton &amp; Barto, 2018). In chapter 3 on Finite Markov Decision Processes, the authors write the expected reward as</p>

<p><span class=""math-container"">$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}r\sum_{s'\in \mathcal{S}}p(s',r|s,a)$$</span></p>

<p>I am not sure if the authors mean</p>

<p><span class=""math-container"">$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}\left[r\sum_{s'\in \mathcal{S}}p(s',r|s,a)\right]$$</span></p>

<p>or </p>

<p><span class=""math-container"">$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\left[\sum_{r\in \mathcal{R}}r\right]\cdot\left[\sum_{s'\in \mathcal{S}}p(s',r|s,a)\right].$$</span></p>

<p>If the authors mean the first, is there any reason why it is not written like the following?</p>

<p><span class=""math-container"">$$r(s,a) = \mathbb{E}\left[R_t|S_{t-1}=s,A_{t-1}=a\right]=\sum_{r\in \mathcal{R}}\sum_{s'\in \mathcal{S}}\left[r\,p(s',r|s,a)\right]$$</span></p>
"
2288,"<p>In the book <em>Reinforcement Learning: An Introduction</em> (Sutton &amp; Barto, 2018). The authors ask</p>

<blockquote>
  <p><em>Exercise 3.2:</em>  Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear
  exceptions?</p>
</blockquote>

<p>I thought maybe a card game would be an example if the state does not contain any pieces of information on previously played cards. But that would mean that the chosen state leads to a system that is not fully observable. Hence, if I track all cards and append it to the state (state vector with changing dimension) the problem should have the Markov Property (no information on the past states is needed). This would not be possible if the state is postulated as invariant in MDP.</p>

<p>If the previous procedure is allowed, then it seems to me that there are no examples where the MDP is not appropriate.</p>

<p>I would be glad if someone could say if my reasoning is right or wrong. What would be an appropriate answer to this question?</p>
"
2289,"<p>We have multiple computers and the ability to ssh between them. What are options using either Java, C/C++, JavaScript, or Python to distribute our learning tasks? We will be using DCNN, DQN, and LSTM in different combinations.</p>
"
2290,"<h2>Introduction</h2>

<p>Exhaustive search is a method in AI planning to find a solution for so called Constraint Satisfaction Problems. (CSP). That are problems which have some conditions to fulfill and the solver is trying out all the alternatives. An example CSP problem is the 8-queens problem which has geometrical constraints. The standard method in finding a solution for the 8-queens problem is a backtracking solver. That is an algorithm which generates a tree for the state space to search inside inside the graph.</p>

<p>Apart from practical applications of backtracking search there are some logic-oriented discussions available which are asking on a formal level which kind of problems have a solution and which not. For example to find a solution for the 8-queen problem many millions of iterations of the algorithm are needed. The question is now: which problems are too complex to find a solution. The second problem is, that sometimes the problem itself has no solution, even the complete state space was searched fully.</p>

<p>Let us make an example. At first we construct a problem in which the constraints are so strict that even a backtracking search won't find a solution. One example would be to prove that “1+1=3” another example would be to find a chess sequence, if the game is lost or it is also funny to think about how to arrange nine! queen on a chess table so that they doesn't hurt.</p>

<p>Is there any literature available which is describing Constraint Satisfaction Problems on a theoretical basis in which the constraints of the problem are too strict?</p>

<h2>Original posting</h2>

<p>Just wondering - like with a 8-queens problem. If we change it to a 9-queens problem and do a exhaustive search, we will see that there is no solution. Is there a problem in which the search fails to show that a solution does not exist?</p>
"
2291,"<p>I've created a neural net using the ConvNetSharp library which has 3 fully connected hidden layers. The first having 35 neurons and the other two having 25 neurons each, each layer with a ReLU layer as the activation function layer.</p>

<p>I'm using this network for image classification - kinda. Basically it takes inputs as raw grayscale pixel values of the input image and guesses an output. I used stochastic gradient descent for the training of the model and a learning rate of 0.01. The input image is a row or column of OMR ""bubbles"" and the network has to guess which of the ""bubble"" is marked i.e filled and show the index of that bubble.</p>

<p>I think it is because its very hard for the network to recognize the single filled bubble among many.</p>

<p>Here is an example image of OMR sections:
<a href=""https://i.stack.imgur.com/F1cql.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F1cql.png"" alt=""OMR bubbles example""></a></p>

<p>Using image-preprocessing The network is given a single row or column of the above image to evaluate the marked one.</p>

<p>Here is an example of a preprocessed image which the network sees:</p>

<p><a href=""https://i.stack.imgur.com/3r60U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3r60U.png"" alt=""OMR bubble column - what the network sees""></a></p>

<p>Here is an example of a marked input:</p>

<p><a href=""https://i.stack.imgur.com/LF0AO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LF0AO.png"" alt=""Marked OMR Column""></a></p>

<p>I've tried to use Convolutional networks but I'm not able to get them working with this.</p>

<p><strong>Basically, my question is that what type of neural network and network architecture should I use for this kind of task?. An example of such network with code would be greatly appreciated.</strong></p>

<p>I have tried many preprocessing techniques such as background subtraction using the AbsDiff function in EmguCv and also using MOG2 Algorithm and I've also tried threshold binary function but there still remains enough noise in the images which makes it difficult for the neural net to learn. </p>

<p>I think this problem is not specific to using neural nets for OMR but for others too. It would be great if there could be a solution out there that could store a background/template using a camera and then when the camera sees that image again, it perspective transforms it to match exactly to the template</p>

<p>I'm able to achieve this much - and then find their difference or do some kind of preprocessing so that a neural net could learn from it. If this is not quite possible, then is there a type of neural network out there which could detect very small features from an image and learn from it.  I have tried Convolutional Neural Network but that also isn't working very well or I'm not applying them efficiently.</p>
"
2292,"<p>while dealing with image data at very large scale, there are different sources where data is coming from. Often, we do not have any control over quality of labels/ annotations. I already do use sampling quality checks method to manually check the quality of annotations but as the volume of data has increased,even sampling QC become an inefficient job. Are there other methods to automate / simplify this task for data at large scale?</p>
"
2293,"<p>My neural network is simple enough and does not overfit.</p>

<blockquote>
  <p>Dropout is a regularization technique for reducing overfitting in neural networks</p>
</blockquote>

<p><em>From <a href=""https://en.wikipedia.org/wiki/Dropout_(neural_networks)"" rel=""nofollow noreferrer"">Wikipedia</a></em></p>

<p><strong>Adding Dropout in a non-overfitting network can increase accuracy ?</strong> Even if I increase the complexity of the network ?</p>
"
2294,"<p>I am referring to eq. 3.6 (p/g 49) based on <a href=""https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view"" rel=""nofollow noreferrer"">Sutton's online book</a>  and can be found in an image below. I could not make sense of the final derivation of the equation <span class=""math-container"">$r(s, a, s')$</span>. My question is actually how do we come to that final derivation? Surprisingly, the denominator of <span class=""math-container"">$p(s'|s, a)$</span> can literally be replaced by <span class=""math-container"">$p(s', r|s, a)$</span> as eq. 3.4 suggests, then it will end up with ""<span class=""math-container"">$r$</span>"" term only due to cancellation of numerator <span class=""math-container"">$p(s', r|s, a)$</span> and denominator <span class=""math-container"">$p(s'|s, a)$</span>.</p>

<p><a href=""https://i.stack.imgur.com/VTPdz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VTPdz.png"" alt=""enter image description here""></a></p>

<p>Any explanation on that would be appreciated.</p>
"
2295,"<p>Imagine a ""simple"" feedforward, fully connected neural network, with some input size, some number of hidden layers, and some # of neurons....etc BUT with a fixed number of output size (that is saying, even if we change and optimize the whole structure of the neural network through through cross validation, the output size must be constant),</p>

<p>Then what are some ways of performing neural network given that the L1 norm of the output values is less than or equal to 1? Then, given such methodology, how would I go about performing backpropagation?</p>

<p>I was thinking there must be some ""penalty"" method just like how in mathematical optimization problem, you can introduce log barrier function as the ""penalty function""</p>
"
2296,"<p>As you can see in the title, I'm trying to program an AI in Java that would help someone optimize his storage.</p>

<p>The user has to enter the size of his storage space (a box, a room, a warehouse etc...) and then enter the size of the items he has to store in this space. (note that everything must be a rectangular parallelepiped) And the AI should find the best position for each item such that the space is optimized.</p>

<p>Here is a list of what I started to do :</p>

<ul>
<li>I asked the user to enter the size of the storage space (units are trivial here except for the computing cost of the AI later on I'm guessing), telling him that the values will be rounded down to the unit</li>
<li>I started by creating a 3-dimensional array of integers representing the storage space's volume, using the 3 values taken earlier. Filling it with 0s, where 0s would later represent free space and 1s occupied space.</li>
<li>Then, store in another multidimensional array the sizes of the items he has to store And that's where the AI part should be starting. First thing the AI should do is check whether the addition of all the items' volumes doesn't surpass the storage space's volume. But then there are so many things to do and so many possibilities that I get lost in my thoughts and don't know where to start...</li>
</ul>

<p>In conclusion, can anyone give me the proper terms of this problem in AI literature, as well as a link to an existing work of this kind ? Thanks</p>
"
2297,"<p>For example, consider an agent concerned with predicting the weather, with variable R indicating whether or not it is likely to rain, variable C indicating whether or not it is cloudy, and variable L indicating low pressure. Given knowledge base K:</p>

<p>L (Pressure is low)</p>

<p>C (It is cloudy)</p>

<p>C ∧ L ⇒ R, (Clouds and low pressure imply rain)</p>

<p>the agent may conclude R; thus, the agent’s knowledge implies that R is true, because K |= R.</p>

<p>Similarly, given knowledge base L:</p>

<p>¬L (Pressure is high)</p>

<p>C (It is cloudy)</p>

<p>C ∧ L ⇒ R, (Clouds and low pressure imply rain)</p>

<p>the agent cannot conclude that R is true; L 6|= R</p>

<p>Deriving a truth table:</p>

<p>L   C   r   ((L ∧ C) → r)</p>

<p>F   F   F   T</p>

<p>F   F   T   T</p>

<p>F   T   F   T</p>

<p>F   T   T   T</p>

<p>T   F   F   T</p>

<p>T   F   T   T</p>

<p>T   T   F   F</p>

<p>T   T   T   T</p>

<p>but this does not make sense.</p>
"
2298,"<p>The question, in short, is: what the possibility of emerging of the new branch of the psychology - the psychology of an artificial intelligence?</p>

<p>Possibly as a new branch of an engineering psychology, due to the fact that engineering psychology handles the realms of interactions between human and the machine, as well as human factor in general.</p>

<p>Is the creation of this new field of study really necessary?</p>

<p>If so, what questions it will address and will try to solve?</p>
"
2299,"<p><strong>When am I supposed to update my weights? After each forward-, and backpropagation; and or after each completed batch?</strong></p>

<p>Furthermore, if I am supposed to update the weights both after each forward-, and backpropagation as well as afte each batch, when am I then supposed to divide by all training examples?</p>
"
2300,"<p>I am reviewing a statement on the website for ES regarding structured exploration. </p>

<p><a href=""https://blog.openai.com/evolution-strategies/"" rel=""nofollow noreferrer"">https://blog.openai.com/evolution-strategies/</a></p>

<blockquote>
  <p>Structured exploration. Some RL algorithms (especially policy
  gradients) initialize with random policies, which often manifests as
  random jitter on spot for a long time. This effect is mitigated in
  Q-Learning due to epsilon-greedy policies, where the max operation can
  cause the agents to perform some consistent action for a while (e.g.
  holding down a left arrow). This is more likely to do something in a
  game than if the agent jitters on spot, as is the case with policy
  gradients. Similar to Q-learning, ES does not suffer from these
  problems because we can use deterministic policies and achieve
  consistent exploration.</p>
</blockquote>

<p>Where can I find sources showing that policy gradients initialize with random policies, whereas Q-Learning uses epsilon-greedy policies? </p>

<p>Also, what does ""max operation"" have to do with epsilon-greedy policies?</p>
"
2301,"<p>There are several levels of abstraction involved in piloting and driving.</p>

<ul>
<li>Signals representing the state of the vehicle and its environment originating from multiple transducers<sup>1</sup></li>
<li>Latched sample vectors/matrices</li>
<li>Boundary events (locations, spectral features, movement, appearance and disappearance of edges, lines, and sounds)</li>
<li>Objects</li>
<li>Object movements</li>
<li>Object types (runways, roads, aircraft, birds, cars, people, pets, screeches, horns, bells, blinking lights, gates, signals, clouds, bridges, trains, buses, towers, antennas, buildings, curbs)</li>
<li>Trajectory probabilities based on object movements and types</li>
<li>Behaviors based on all the above hints</li>
<li>Intentions based on behavior sequences and specific object recognition</li>
<li>Collision risk detection</li>
</ul>

<p>Moving from interpretation to control execution ...</p>

<ul>
<li>Preemptive collision avoidance reaction</li>
<li>Horn sounding</li>
<li>Plan adjustment</li>
<li>Alignment of plan to state</li>
<li>Trajectory control</li>
<li>Skid avoidance</li>
<li>Skid avoidance reaction</li>
<li>Steering, breaking, and signalling</li>
<li>Notifications to passengers</li>
</ul>

<p>What, if any, levels of higher abstraction can be sacrificed?  Humans, if they are excellent pilots or drivers, can use all of these levels to improve pedestrian and passenger safety and minimize expense in time and money.</p>

<p><strong>Footnotes</strong></p>

<p>[1] Optical detectors, microphones, strain gauge bridges, temperature and pressure gauges, triangulation reply signals, voltmeters, position encoders, key depression switches, flow detectors, altimeters, radar transducers, tachometers, accelerometers </p>
"
2302,"<p>I struggle to find Rosenblatts perceptron training algorithm in any of his publications from 1967 - 1951, namely:</p>

<p>[1] Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms</p>

<p>[2] The perceptron: A probabilistic model for information storage and organization in the brain.</p>

<p>[3] The Perceptron — A Perceiving and Recognizing Automaton </p>

<p>Does somebody know where to find the original learning formula?</p>
"
2303,"<p>Suppose that I have a model M that overfits a large dataset S such that the test error is 30%. Does that mean that there will always exist a model that is smaller and less complex than M that will have a test error less than 30% on S (and does not overfit S). </p>
"
2304,"<p>What is the best and easiest programming language to learn to implement Genetic algorithms? C++ or Python or any other?</p>
"
2305,"<p>What are the future prospects in near future from a theoretical investigation of description logics, and modal logics in the context of artificial intelligence research?</p>
"
2306,"<p>I have a data input vector ( No Image classification) which size varys from 2 to 7 entrys. Every one of them belongs to a class Out of 7. So I have a variable Input size and a variable Output size. How can I deal with the variable Input sizes? I know Zero padding is a option but maybe there are better ways?</p>

<p>Seconds: Is multi Label classification possible in one Network? What I mean: The first entry has to bei classified in one of the seven classes, the second entry... and so on.</p>

<p>I am also open to other classification techniques, If there is a better one that suits the problem.</p>

<p>Best regards,
Gesetzt</p>
"
2307,"<p>I have read quite a lot about capsule networks but cannot understand how the squashed vector would also rotate in response to rotation or translation  of the image.A simple example would be helpful.I understand how routing by agreement works.</p>
"
2308,"<p>I am trying to study the book <a href=""http://incompleteideas.net/book/bookdraft2018jan1.pdf"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction</a> (Sutton &amp; Barto, 2018). In chapter 3.1 the authors state the following exercise</p>

<blockquote>
  <p><em>Exercise 3.5</em> Give a table analogous to that in Example 3.3, but for <span class=""math-container"">$p(s',r|s,a)$</span>. It should have columns for <span class=""math-container"">$s$</span>, <span class=""math-container"">$a$</span>, <span class=""math-container"">$s'$</span>, <span class=""math-container"">$r$</span>, and
  <span class=""math-container"">$p(s',r|s,a)$</span>, and a row for every 4-tupel for which <span class=""math-container"">$p(s',r|s,a)&gt;0$</span>.</p>
</blockquote>

<p>The following table and graphical representation of the Markov Decision Process is given on the next page.</p>

<p><a href=""https://i.stack.imgur.com/5xt43.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5xt43.png"" alt=""Example 3.3""></a></p>

<p>I tried to use <span class=""math-container"">$p(s'\cup r|s,a)=p(s'|s,a)+p(r|s,a)-p(s' \cap r|s,a)$</span> but without a significant progress because I think this formula does not make any sense as <span class=""math-container"">$s'$</span> and <span class=""math-container"">$r$</span> are not from the same set. How is this exercise supposed to be solved?</p>

<p>EDIT: Maybe this exercise intends to be solved by using</p>

<p><span class=""math-container"">$$p(s'|s,a)=\sum_{r\in \mathcal{R}}p(s',r|s,a)$$</span></p>

<p>and</p>

<p><span class=""math-container"">$$r(s,a,s')=\sum_{r\in \mathcal{R}}r\dfrac{p(s',r|s,a)}{p(s|s,a)}$$</span></p>

<p>and</p>

<p><span class=""math-container"">$$\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1$$</span></p>

<p>the resulting system is a linear system of 30 equation with 48 unknowns. I think I am missing some equations...</p>
"
2309,"<p>Take a look at section 2.2.2 of <a href=""https://doc.lagout.org/science/0_Computer%20Science/2_Algorithms/Introduction%20to%20Evolutionary%20Algorithms%20%5BYu%20%26%20Gen%202010-06-23%5D.pdf"" rel=""nofollow noreferrer"">this book</a> (from Page-15 to 16).</p>

<blockquote>
  <p><strong><em>2.2.2 Representation and Evaluation</em></strong>  </p>
  
  <p><span class=""math-container"">$$max f (x)= x sin(10πx)+2.0 ... ... ... (2.8)$$</span>
  <span class=""math-container"">$$s.t. −1 ≤ x ≤ 2$$</span></p>
  
  <p>We can use a real number, in the range <span class=""math-container"">$[−1,2]$</span>, to represent a solution in Eq. <span class=""math-container"">$2.8$</span> directly. Many operators can handle real number representation. But we use the binary code or binary representation here for two reasons. GAs were originally proposed to be binary code to imitate the genetic encoding of natural organisms. On the other hand, binary code is good for pedagogy. A binary chromosome is necessary to represent a solution x in the scale <span class=""math-container"">$[−1,2]$</span>. The same holds for the binary representation of real numbers in a computer.  </p>
  
  <p>In binary code, we cannot represent a real number completely correctly, so a
  trade-off is necessary. A tolerance needs to be deﬁned by the user, which means the errors below the tolerance are extraneous. If we divide the deﬁnition domain into <span class=""math-container"">$2^1 = 2$</span> parts evenly and select the smallest number in the parts to represent any number in the division, we can only represent <span class=""math-container"">$−1$</span> and <span class=""math-container"">$0.5$</span> by <span class=""math-container"">$0$</span> and <span class=""math-container"">$1$</span> respectively. <span class=""math-container"">$2^2 = 4$</span> divisions make the <span class=""math-container"">$00$</span>, <span class=""math-container"">$01$</span>, <span class=""math-container"">$10$</span>, and <span class=""math-container"">$11$</span> represent <span class=""math-container"">$−1$</span>, <span class=""math-container"">$−0.25$</span>, <span class=""math-container"">$0.5$</span>, and <span class=""math-container"">$1.25$</span>, respectively. The larger division number we select, the less error there is in representing a real number on binary code. Suppose we use <span class=""math-container"">$100$</span> binary codes to represent a real number in the range <span class=""math-container"">$[−1,2]$</span>; the maximum error is <span class=""math-container"">$\frac{3}{2100} ≈ {2.37}^{−30}$</span>, which would be satisfactory for most users. In this way, we can represent a real number with any accuracy requirements. </p>
  
  <p>In this problem, we use <span class=""math-container"">$l = 12$</span> binary codes to represent one real number as follows, which constitutes a chromosome to be evolved.</p>
</blockquote>

<p>Actually, I haven't understood this text. I know that binary numbers are already <a href=""http://cs.furman.edu/digitaldomain/more/ch6/dec_frac_to_bin.htm"" rel=""nofollow noreferrer"">able to represent fractions</a>.</p>

<p>So, what are they talking about?</p>
"
2310,"<blockquote>
  <blockquote>
    <p>Batch size is a term used in machine learning and refers to the number of training examples utilised in one iteration. The batch size
    can be one of three options:</p>
  </blockquote>
  
  <ol>
  <li><strong>batch mode</strong>: where the batch size is equal to the total dataset thus
  making the iteration and epoch values equivalent </li>
  <li><strong>mini-batch mode</strong>:
  where the batch size is greater than one but less than the total
  dataset size. Usually, a number that can be divided into the total
  dataset size. </li>
  <li><strong>stochastic mode</strong>: where the batch size is equal to one.
  Therefore the gradient and the neural network parameters are updated
  after each sample.</li>
  </ol>
</blockquote>

<p><strong>How do I go about choosing the optimal batch size for my network?</strong></p>

<p>If you hypothetically didn't have to worry about computational issues, what would the optimal batch size be in this case?</p>
"
2311,"<p>Suppose there are sensors which supply numerical metrics. If a metric goes above or below healthy threshold, an event (alert) is raised. Metrics depend on each other in one way or another (we can learn the dependencies via ML algorithms) so when the system is in alerting state only one or a few metrics will be a root cause and all others will be simply consequences.</p>

<p>We can assume there is enough historical metric data available, to learn dependencies but there are just a few historical malfunctions. Also, when malfunction happens there is no one to tell what was the root cause, the algorithm should learn how to detect root causes by itself.</p>

<p>Which algorithms can be used to detect root cause event in the situation above? Are there any papers available on the subject?</p>
"
2312,"<p>(Maybe related : <a href=""https://ai.stackexchange.com/questions/8511/usefulness-of-dropout-for-non-overfitting-network/8513#8513"">Usefulness of Dropout for non-overfitting network</a>)</p>

<p>My neural network does not overfit.</p>

<p><strong>Using Data augmentation in a non-overfitting network can increase its accuracy ?</strong></p>

<p><em>Note : I'm asking this question in the NLP area, where Data augmentation is not trivial. I'm thinking about back-translation. The augmented data might not be of similar quality, which can hurt the accuracy (I guess ?).</em></p>
"
2313,"<p>Why in every aspect are we now considering Neural Networks as an Artificially Intelligent entity/program?</p>
"
2314,"<p>The Levenshtein algorithm and some ratio and proportion may handle this use case.</p>

<blockquote>
  <p>Based on the pre-defined sequence of statements, such as ""I have a dog"", ""I own a car"" and many more, I must determine if an another input statement such as ""I have a cat"" is the same or how much percentage does the input statement is most likely equal to the pre-defined statements.</p>
</blockquote>

<p>For Example:</p>

<blockquote>
  <p>Predefined statements: ""I have a dog"", ""I own a car"", ""You think you are smart""</p>
</blockquote>

<p>Input statements and results:</p>

<blockquote>
  <p>I have a dog - 100% (because it has exact match), I have a cat - ~75% (because it was almost the same except for the animal, think - ~10% (because it was just a small part of the third statement), bottle - 0% (because it has no match at all)</p>
</blockquote>

<p>The requirement is that TensorFlow be used rather than Java, which is the language I know, so any help with what to look at to get started would be helpful.</p>

<p>My plan was to use the predefined statements as the train_data, and to output only the accuracy during the prediction, but i don't know what model to use, please help thanks! even just guide me with the architecture and I will try to implement it, thanks in advance!</p>
"
2315,"<p>Consider I have a 3 layers neural network.  </p>

<ul>
<li>Input Layer containing 784 neurons.</li>
<li>Hidden layer containing 100 neurons.</li>
<li>Output layer containing 10 neurons.</li>
</ul>

<p>My objective is to make an OCR and I used MNIST data to train my network.</p>

<p>Suppose I gave the network an input taken from an image, and the values from the output neurons are the next:  </p>

<ul>
<li><span class=""math-container"">$0: 0.0001$</span>  </li>
<li><span class=""math-container"">$1: 0.0001$</span>  </li>
<li><span class=""math-container"">$2: 0.0001$</span></li>
<li><span class=""math-container"">$3: 0.1015$</span></li>
<li><span class=""math-container"">$4: 0.0001$</span></li>
<li><span class=""math-container"">$5: 0.0002$</span></li>
<li><span class=""math-container"">$6: 0.0001$</span></li>
<li><span class=""math-container"">$7: 0.0009$</span></li>
<li><span class=""math-container"">$8: 0.001$</span></li>
<li><span class=""math-container"">$9: 0.051$</span></li>
</ul>

<p>When the network returns this output, my program will tell me that he identified the image as number 3.  </p>

<p>Now by looking at the values, even though the network recognized the image as 3, the output value of number 3 was actually very low: <span class=""math-container"">$0.1015$</span>. I am saying very low, because usually the highest value of the classified index is as close as 1.0, so we get the value as 0.99xxx.  </p>

<p>May I assume that the network failed to classify the image, or may I say that the network classified the image as 3, but due to the low value, the network is not certain?</p>

<p>Am I right thinking like this, or did I misunderstand how does the output actually works?</p>
"
2316,"<p>I am training a deep neural network. There is a constraint on an output value of the network. (e.g. Output has to be between 0 and 180) I think some possible solutions are using sigmoid,tanh activation at the end of the layer. I wonder if there are better ways to put constraints on the output value of a neural network.</p>
"
2317,"<p>I am creating a dataset made of many images which are created by preprocessing a long time series. Each image is an array of (128,128) and the there are four classes. I would like to build a dataset similar to the MNIST in scikit-learn.database  but I have no idea how to do it. </p>

<p>My aim is to have something that I can call like this:</p>

<pre><code>(x_train, y_train), (x_test, y_test) = my_data()
</code></pre>

<p>Should I save them as figures? or as csv? 
Which is the best way to implement this? </p>
"
2318,"<p>I am developing an image search engine. The engine is meant to retrieve wrist watches based on the input of the user. I am using SIFT descriptors to index the elements in the database and applying Euclidean distance to get the most similar watches. I feel like this type of descriptor is not the best since watches have a similar structure and shape. Right now, the average difference between the best and worst matches is not big enough (15%)</p>

<p>I've been thinking of adding colour to the descriptor, but I'd like to hear other suggestions.</p>

<p>BR,
SV</p>
"
2319,"<p>The intelligence of the human brain is said to be a strong factor leading to human survival.  The human brain functions as overseer for many functions the organism requires.  Like that, the most important thing behind artificial intelligence is computers.</p>

<p>Robots can employ artificial intelligence systems, just as humans employ brains.</p>

<p>When it comes to the human brain, we are prone to make mistakes.  Where as artificial intelligence is sometimes represented to the public as perfect. What is true about AI and why? </p>

<p>Why was our brain not created or evolved in a way that it does not make any errors?  We tend to make mistakes.  why?</p>
"
2320,"<p>Can artificial intelligence be hacked or not by using enough computing skills or by other cryptography methods?
So are we able to say that AI is one of in its secured form?</p>
"
2321,"<p>Assume that we have a labeled dataset with inputs and outputs, where the output range is <span class=""math-container"">$\left[0, 2\right]$</span>, but the majority of outputs is in <span class=""math-container"">$\left[0, 1\right]$</span>. Should one adopt some kind of over- or undersampling approach after compartmentalising the output space to make the dataset more balanced? That would usually be done in classification, but does it apply to regression problems, too? 
Thanks in advance!</p>
"
2322,"<p>I am writing an app, where when a ball is shot from a canon it is supposed to land in a hole which is on a given distance. The ball is supposed to land between the distance of the begining of the hole and the end of the hole. The size of the hole is 4m and the size of the ball is 0.4m. My problem is that I am not sure how to write the fitness function for this. The place where the ball falls should be close to this interval of [D, D+3.6], where D is the distance of the hole. If anyone could give me a hint on how to approach this problem, I would be grateful.</p>
"
2323,"<p>I understand the concept of convolution.
Let's say that my input dimension is <code>3 x 10 x 10</code></p>

<p>And if I say that I will have 20 activation maps and a filter size of 5, I will end up with 20 different filters for my layer, each with the dimension of <code>(3 x 5 x 5)</code></p>

<p>My output will therefor be <code>(20 x ? x ?)</code>. I Put a ""?"" there, because it obviously depends on the filter stride etc.</p>

<hr>

<p>Now I wanted to implement deconvolution but I am stuck at the following point:</p>

<p>For the following questions, let's assume that the input size for the deconvolution is <code>(5 x 8 x 8)</code>, </p>

<ol>
<li>If we think about a filter in 3 dimensions. Can I choose any depth for the filter?</li>
<li>How would the effect of the amount of filters (amount of activation maps) work with deconvolution? Do I only have one filter?</li>
<li>How does the input depth (5) come into play. Would the output depth be equal to        <br> (filter depth) * (input depth) ?</li>
</ol>

<p>I am trying to find the symmetry to forward convolution but I do not understand how to use the amount of activation maps in deconvolution.</p>

<p>I am very thankful for any help.</p>
"
2324,"<p>I am applying a Double DQN algorithm to a highly stochastic environment where some of the actions in the agent's action space have very similar ""true"" Q-values (i.e. the expected future reward from either of these actions in the current state is very close). How can I still ensure that the algorithm gets these values (and their relative ranking) right?</p>

<p><strong>EDIT 1</strong></p>

<p>To provide a little more info: What happens currently is that the loss function on the Q-estimator decreases rapidly in the beginning, but then starts evening out. The Q-values also first converge quickly, but then start fluctuating around.</p>

<p>I've tried increasing the batch size which I feel has helped a bit. What did not really help, however, was decreasing the learning rate parameter in the loss function optimizer. </p>

<p>Which other steps might be helpful in this situation?</p>

<p><strong>EDIT 2</strong></p>

<p>If this question is misleading (since reinforcement learning is an <em>approximate</em> dynamic programming method), please just let me know. I mean the algorithm usually does find an only slightly sub-optimal solution to the MDP.</p>

<p><strong>EDIT 3</strong></p>

<p>To answer some questions brought up in the comments:<br>
(1) I do indeed have full control over the MDP including the reward function which in my case is sparse (0 until the terminal episode). The ""true"" Q-values I know from an analytical solution to the problem I am trying to solve using reinforcement learning.<br>
(2) The rewards are the same for <em>identical</em> transitions. However, the rewards vary for any given state and action taken therein.<br>
(3) The environment is only stochastic for a part of the actions in the action space. I.e. the action chosen by the agent influences the stochasticity of the rewards.</p>
"
2325,"<p>I am training a Multilayer Neural Nets with 146 samples (97 for training set, 20 for validation set and 29 for testing set). I am using:
automatic differentiation,
SGD method,
fixed learning rate + momentum term,
logistic function,
quadratic cost function,
L1 and L2 regularization technique,
adding some artificial noise 3%.</p>

<p>When I used L1 or L2 regularization technique my problem (overfitting problem) got worst.
i tried different values for lambdas (the penalty parameter  0.0001, 0.001, 0.01, 0.1, 1.0 and 5.0). After 0.1 i just killed my ANN. The best result that i took it was using 0.001 (but it is worst comparing the one that i didnt use regularization technique). The graph represent the error functions for different penalty parameters and also a case without using L1. </p>

<p><a href=""https://i.stack.imgur.com/NwBl2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NwBl2.png"" alt=""enter image description here""></a></p>

<p>and the accuracy
<a href=""https://i.stack.imgur.com/MTgRD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MTgRD.png"" alt=""enter image description here""></a></p>

<p>What can be?
Thanks!!</p>
"
2326,"<p>I am so much curious about how do we see(with eyes ofc) and detect things and their location so quick. Is the reason that we have huge gigantic network in our brain and we are trained since birth to till now and still training. 
basically I am saying , are we trained on more data and huge network? is that the reason?
or what if there's a pattern for about how do we see and detect object.
please help me out, maybe my thinking is in wrong direction.
what I wanna achieve is an AI to detect object in picture in human ways.thanks.</p>
"
2327,"<p>I am preparing the Bus movement dataset for deep learning (ANN/CNN/RNN) analysis for congestion events detection. This is an extension to my original question, which can be located at '<a href=""https://ai.stackexchange.com/questions/8364/deep-learning-model-training-and-processing-requirement-for-traffic-data"">Deep learning model training and processing requirement for Traffic data</a>' for the general approach on this topic, and this question is for preparing the dataset and need your kind advice on it. In simple words, I would like to know the state of congestion for a bus route at a specific point in time (year). </p>

<p>Here are my entities:</p>

<ul>
<li>Routes</li>
<li>Bus_Scheduled_Routes</li>
<li>Bus_Route_Stops</li>
<li>Bus_Trips (operational_date, Vehicle_id, Trip_id, Vehicle_Position_Update, Trip_stop_id, passenger_loaded, velocity, direction, scheduled_arrival_time, actual_arrival_time)</li>
<li>Events (human and non-human induced)</li>
<li>Points of Interests (POIs)</li>
</ul>

<p>If I have these entities based data and I create a view that gives me a time reference based view comprising of week(52), day(7), Vehicle_id, Trip_id,  Stop/Position_update_interval, speed, acceleration, velocity, scheduled_arrival_time, actual_arrival_time. Will this view be recommended to start training the model? </p>

<p>Secondly, how can I integrate the human / non-human induced events and Points of Interests (POIs) data into this view so my model can predict better results? To generalize the model data will be 'time segment / trips time (seasons), location component (Bus Routes and Stops), Arrival time / trip completion time'. I am thinking to add an attribute for human/non-human induced events as type tying with the 'time segment' and adding the POIs as type and vicinity to the stop points. What are your recommendation about it? Thanks in advance for your help.</p>
"
2328,"<p><a href=""https://en.wikipedia.org/wiki/Fuzzy_logic"" rel=""nofollow noreferrer"">Fuzzy logic</a> seemed like an active area of research in machine learning and data mining back when I was in grad school (early 2000s). Fuzzy inference systems, fuzzy c-means, fuzzy versions of the various neural network and support vector machine architectures were all being taught in grad courses and discussed in conferences. </p>

<p>Since I've started paying attention to ML again (~2013), Fuzzy Logic seems to have dropped off the map completely and its absence from the current ML landscape is conspicuous given all the AI hype. </p>

<p>Was this a case of a topic simply falling out of fashion, or was there a specific limitation of fuzzy logic and fuzzy inference that led to the topic being abandoned by researchers? </p>
"
2329,"<p>I saw when browsing we can use data augmentation for creating a dataset for face recognition. The augmented images may include inverted, tilted or distorted faces. Do the model detect the face from the inverted image. When I tried my model cant able to detect any inverted or tilted faces.</p>
"
2330,"<p>Which possibilities exist to evaluate the visual reasoning capabilities of neural networks in the field of image recognition?</p>

<p>Are there methods to measure the ability of machine reasoning?</p>

<p>Or something more specific: Is it possible to measure if a network understood the concept of a car / a cat / a human without using the classification accuracy.</p>
"
2331,"<p>I loaded a neural network model trained with Caffe by other people in OpenCV. </p>

<p>The model should detect the presence of a car in a single parking spot outputting the probability of it being free/occupied.</p>

<p>The model was trained with images all belonging to the same parking area, taken at different hours of day and with different light conditions. Images were taken by different cameras but the cameras are all of the same model (raspberry cameras).</p>

<p>I tried to run the model with a few images some of them taken from their dataset and other downloaded from google.</p>

<p>The images taken from their dataset are correctly classified while the ones taken from google are not correctly classified.</p>

<p>My question is: is it possible to deploy a NN model trained with images all coming from a single parking area in another parking area? Is not such a model for parking detection occupancy supposed to generalize independently from the location where training images have been taken?</p>

<p>If you know about an already existing trained model that works good please let me know.</p>
"
2332,"<p>One of the corner stones of The Selfish Gene (Dawkins) is the spontaneous emergence of replicators, i.e. molecules capable of replicating themselves.</p>

<p>Has this been modeled <em>in silico</em> in open-ended evolutionary / artificial life simulations?</p>

<p>Systems like Avida or Tierra explicitly <strong>specify</strong> the replication mechanisms; other genetic algorithm/genetic programming systems explicitly <strong>search for</strong> the replication mechanisms (e.g. to simplify the von Neumann universal constructor)</p>

<p>Links to simulations where replicators emerge from a primordial digital soup are welcome.</p>
"
2333,"<p>I am trying to figure out how to use a reinforcement learning algorithm, if possible, as a ""black box"" to play a game. In this game, a player has to avoid flying birds. If he wants to move, he has to move the mouse on the display, which controls the player position by applying a force. The player can choose any position on the display for the mouse. A human can play this game. To get a sense what needs to be done, have a look at <a href=""https://www.youtube.com/watch?v=k6xiqI60ztA"" rel=""nofollow noreferrer"">this Youtube video</a>.</p>

<p>I thought about using an ANN, which takes as input the information of the game (e.g. positions, speeds, radius, etc.) and outputs a move. However, I am unlikely to ever record enough training data to train the network properly.</p>

<p>Therefore, I was thinking that a RL algorithm, like Q-learning, would be more suited for this task. However, I have no clue how to apply Q-learning to this task. For example, how could the coder possibly know what future reward another move will bring?</p>

<p>I have a few questions:</p>

<ol>
<li><p>In this case, the player has infinitely many actions. How would I apply RL to this case?</p></li>
<li><p>Is RL a good approach to solving this task?</p></li>
<li><p>Is there a handy Java library which would allow me to use a RL algorithm (as a block-box) to solve this problem?</p></li>
<li><p>Are there alternatives?   </p></li>
</ol>
"
2334,"<p>The BACON algorithm, introduced by Pat Langley and Herbert Simon etc. were meant to automate scientific discovery -- producing causal explanation to variations in given data.</p>

<p>It was found, in particular, to have been able to ""discover"" the laws of planetary motion, but there were criticism concerning whether or not its achievement could be consider true scientific discovery.</p>

<p>The one I remember was that--the data given to the algorithm are pre-processed by human operators so as to include mostly relevant factors of the world. For example, the distances of planets from the sun was given, showing that the human operators implicitly understood the importance of this variable for the laws of planetary motion.</p>

<p>I was wondering if there were other significant criticism of the algorithm--either in general as a automaton of discovery or concerning its particular feat with planetary motion.</p>
"
2335,"<p>I have been successfully figured out how the minimax algorithm works for a game like chess. Were by a game tree is used and you assign a value to the terminal nodes and propagate that value up the tree. But checking if its the maximizer player turn or the minimizing players turn. My question is, is there a way to represent this algorithm mathematically?</p>

<p>If so how would I go about showing it? </p>
"
2336,"<p>I've to train a neural network using microphone data (wav files), accelerometer sensor data and light sensor data. </p>

<p>Right now the approach I thought was to convert all data into images and combine them into a single image and train my neural network.</p>

<p>Another approach was to convert wav files into arrays and combine them along with sensor data and train my neural net.</p>

<p>Are my approaches correct or is there a better way to do this?</p>

<p>Any suggestions/ideas are welcome.</p>
"
2337,"<p>It is said that the number of possible sequences of game play in the game Go is greater than the number of atoms in the universe.  Whether or not that is true, imagine the number of possible sequences of risks, starts, stops, turns, or other events or actions that can occur when driving to the store.</p>

<p>If one shops at a 24-hour store at 3 AM, the number of conditions are small.  If one shops in a city at 5 PM, the driving sequence set may be far larger than the Go game sequence set.  More importantly, when Go is misplayed, no dog is run over, no child is killed, and no passenger is carted away in an ambulance.</p>

<p>We have, for automated vehicle control systems, the goal of recognizing risk and responding appropriately in action.  The moves are not timed.  A wide array of unpredictable circumstances can emerge at any time.</p>

<p>When AI designers first approach this situation, a basic design choice must be made between two polar opposite approaches or some midpoint.</p>

<ul>
<li><p>A sequence of serially arranged system components and layers that recognize all the elements of these astronomically varied conditions, recognizes the combinations of the elements that present risk, and decide what to do with horn, breaks, steering, clutch, transmission, and fuel rate.  (Fuel rate is what the accelerator pedal approximately controls.)</p></li>
<li><p>An array of system components that read the same sensor input and each optimally detect a particular class of risks.  Each parallel detector would then need to provide to a central system a set of system responses intended to avoid those risks based on each component's learned responses.  Prioritization would follow in a more centralized component so that the correct decisions can be regarding horn, breaks, and the others.</p></li>
</ul>

<p>Here are a few important external events for an automated vehicle system to recognize.  Each is given a letters which, if the second choice above is chosen, may or may not be a reasonable division of responsibility for separate independent parallel detector components.</p>

<ul>
<li>Trajectories of the vehicle being controlled &mdash; A</li>
<li>Trajectories of pedestrians &mdash; B</li>
<li>Trajectories of dogs and cats &mdash; B</li>
<li>Trajectories of wheeled vehicles &mdash; C</li>
<li>Trajectories of trains &mdash; C</li>
<li>Trajectories of balls &mdash; C</li>
<li>Trajectories of UTOs (unidentified terrestrial objects &mdash; D</li>
<li>Locations of stationary objects in the road &mdash; E</li>
<li>Locations of road endings &mdash; E</li>
<li>Locations of curbs &mdash; E</li>
<li>Locations of bridge abutments &mdash; E</li>
<li>Locations of rocks of size greater than 2 cm &mdash; E</li>
<li>Traffic signs &mdash; F</li>
<li>Traffic lights &mdash; F</li>
<li>Train signals &mdash; F</li>
<li>Locations of wire gates &mdash; G</li>
<li>States of horizontal raise-able gates &mdash; H</li>
<li>Indications of many pedestrians &mdash; I</li>
</ul>

<p>Is it best to train for trajectories of type B with the same system as training for signal recognition of type F?  Will the saving of additional components be more important than specialization?  Will such generalizations work well for the full array of surprises that may occur during real routes of vehicles?  Or is the diversity in system architecture a better a better choice for an AV embedded AI system?</p>

<p>In support of either position, why?</p>
"
2338,"<p>I'm new to AI and and would like to understand the difference between learning agents and other types of agents, in what ways learning agents can be applied, and if they differ from deep learning.</p>
"
2339,"<p>this is my first post here.</p>

<p><strong>Our problem setting:</strong>
We have to do a binary classification of data given a training-dataset D, where the majority of items belongs to class A and some items belong to class B. The classes are heavily imbalanced. </p>

<p><strong>Our Approach:</strong>
We wanted to use a GAN to produce more samples of class B so that our final classification model has a nearly balanced set to train.</p>

<p><strong>Our Problem:</strong>
Let‘s say that the data from both class A and class B are very similar. Given that we want to produce synthetic class-B-samples with the GAN, we feed real class-B-examples into the discriminator alongside with generated samples. But wait.. A and B are similar. It could happen that the generator produces an item x, that would naturally belong to class A. But since the discriminator has never seen a class-A-item before and both classes are very close, the discriminator could say that this item x is part of the original data that was fed into the discriminator. So the generator successfully fooled the discriminator in believing that an item x is part of the original data of class B, while x is actually part of class A. 
If the GAN keeps producing items like this, the produced data is useless since it would add heavy noise to the original data, if combined. </p>

<p>At the same time, let‘s say before we start training the generator, we show the discriminator our class A and class B samples while giving information, that the class-A-items are not part of class B (through backprop). The discriminator would learn to reject class-A-items that are fed to it. But wouldn‘t this mean that the discriminator has just become the classification model we wanted to build in the first place to distinguish between class A and class B?</p>

<p>Do you know any solution to the above stated problem or can you refer to some paper/other posts on this?</p>

<p>Thanks a lot! </p>
"
2340,"<p>I'm in high school but hoping to have a career in artificial intelligence.  What should I be pursuing educationally to get into this field?</p>
"
2341,"<p>I am interested in creating the neural network architecture described in this <a href=""https://arxiv.org/pdf/1805.01542.pdf"" rel=""nofollow noreferrer"">recent paper</a>. However, I am not sure how to get started. Does TFLearn or Keras allow us to wire the LSTM in such ways?
<a href=""https://i.stack.imgur.com/84Nzi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/84Nzi.png"" alt=""NLU""></a></p>
"
2342,"<p>In error based learning using Gradient descent, If I give you a training dataset then can you find the minimum error after training? and the minimum error should be true for all architectures of neural Network. Consider you will use MSE for calculating the error. and you can choose anything you want other than my specified condition. IT'S like no matter how you change your network you can never cross the limit.</p>
"
2343,"<p>What is main difference between goal-based agent and utility-based agent? </p>

<p>Please, give a real world example.</p>
"
2344,"<p>The <a href=""https://www.notebookcheck.net/Asus-s-GeForce-RTX-2080-Ti-and-2080-portfolio-is-now-official.323914.0.html"" rel=""nofollow noreferrer"">Nvidia 2080 GPUs</a> command a $500 premium for 3GB of incremental RAM (8GB -> 11GB).</p>

<p>What are the relevant questions and thought process to determine the incremental improvement by moving from the 8GB to the 11GB GPU?  Assume these contexts for image segmentation (people counting)</p>

<ol>
<li>MATLAB </li>
<li>Tensorflow</li>
</ol>

<p>I am inclined to think that the platform affects the RAM evaluation, however, I can not be certain.</p>

<h1>UPDATE</h1>

<p>Thanks for the good responses that sharpen the question.  Updates to questions are provided below.</p>

<p>The plan is to train an image segmenter with recorded images and try different algorithms (YOLO R-CNN etc.) .  The end goal is to develop a model that will process a live stream.  I am in the planning stage and expect to build (if not buy) a GPU DL Server.  </p>
"
2345,"<p>In <a href=""https://www.youtube.com/watch?v=PTbxa6GsTWc&amp;index=3&amp;list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3"" rel=""nofollow noreferrer"">one of his lectures</a> Levine describes the objective of reinforcement learning as: <span class=""math-container"">$$J(\tau) = E_{\tau\sim p_\theta(\tau)}[r(\tau)]$$</span>
where <span class=""math-container"">$\tau$</span> refers to a single trajectory and <span class=""math-container"">$p_\theta(\tau)$</span> is the probability of having taken that trajectory so that <span class=""math-container"">$p_\theta(\tau) = p(s_1)\prod_{t = I}^T \pi_{\theta}(a_t, s_t)p(s_{t+1}|s_t, a_t))$</span>.</p>

<p>Starting from this definition, he writes the objective as <span class=""math-container"">$J(\tau) =\sum_{t=1}^T E_{(s_t, a_t)\sim p_\theta(\tau)}[r(s_t, a_t)]$</span> and argues that this sum can be decomposed by using conditional expectations, so that it becomes:</p>

<p><span class=""math-container"">$$J(\tau) = E_{s_1 \sim p(s_1)}[E_{a_1 \sim \pi(a_1|s_1)}[r(s_1, a_1) + E_{s_2 \sim p(s_2|s_1, a_1)}[E_{a_2 \sim \pi(a_2|s_2)}[r(s_2, a_2)] + ...|s_2]|s_1,a_1]|s_1]]$$</span></p>

<p>Can anyone explain this last step? I guess the <a href=""https://en.wikipedia.org/wiki/Law_of_total_expectation"" rel=""nofollow noreferrer"">law of total expectation</a> is involved, but I can not figure out how exactly.</p>
"
2346,"<p><em>I'm not sure this question is in the right Stack Exchange website, but I thought this place is a good starting point, because of the experience of users.</em></p>

<p><em>This question is not about artificial intelligence, but about time-management.</em></p>

<hr>

<p>I am a new machine learning engineer, and I am working with Neural Networks. </p>

<p>However, I have difficulties to manage my time : the training time of my neurals networks are long (an hour for the simplest model, up to more than 24 hours).</p>

<p>I find it difficult to find good hyperparameters in these conditions. Another problem is that often, I find myself having nothing to do while training time.</p>

<p><strong>I am looking for methods/advices to optimize my working time considering the training time for my network can be very long.</strong></p>

<hr>

<p><strong>What I have tried :</strong></p>

<ul>
<li>Organizing my work so I have to do all the little tasks (code cleaning, documentation, etc...) while training.</li>
<li>Use this free-time to learn more.</li>
<li>Meanwhile, implement other models, to use as a baseline.  </li>
</ul>
"
2347,"<p>I am working on a RL project,but got stuck at one point: The task is continuous (Non-episodic). Following some suggestion from Sutton's <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">RL book</a>, I am using a value function approximation method with average reward (differential return instead of discount return). For some state (represented by some features), only one action is legal. I am not sure how to design reward for such action. Is it ok to just assign the reward in the previous step? Could anyone tell me the best way to decide the reward for the only legal action. Thank you! </p>

<p>UPDATE:
To give more details, I added one simplified example:
Let me explain this by a simplified example: the state space consists of a job queue with fix size and a single server. The queue state is represented by the duration of jobs and the server state is represented by the time left to finish the current running job.  When the queue is not full and the server is idle, the agent can SCHEDULE a job to server for execution and see a state transition(taking next job into queue) or the agent can TAKE NEXT JOB into queue. But when the job queue is full and server is still running a job, the agent can do nothing except take a BLOCKING action and witness a state transit (time left to finish running job gets decreased by one unit time). The BLOCKING action is the only action that the agent can take in that state.</p>
"
2348,"<p>I'm a bit confused about this. Assume I have a CNN network with two branches:</p>

<ol>
<li>Top</li>
<li>Bottom</li>
</ol>

<p>The top branch outputs a feature vector of shape 1x1x1x10 (batch, h, w, c)
The bottom branch outputs a feature vector of shape (1, 10, 10, 10).</p>

<p>I want to use the top feature vector as a convolutional filter, and convolve it with the bottom feature vector. I can do this in pytorch with the ""functional.Conv2D"" function, the problem is, I don't know how back-prop works in this case (will it be unstable?) since the output feature is a now a parameter as well, do I need to stop gradients or do something else in this case to backprop correctly?</p>
"
2349,"<p>How do I define a reward function for my POMDP model?</p>

<p>In literature it is common to use one simple number as a reward, but I am not sure if this is really how you define a function. Because this way you have do define a reward for every possible Action-State combination. I think that the examples in literature might be not practical in reality, but only for purpose of explanation.</p>
"
2350,"<p>I need to be able to detect and track humans from all angles, especially above.</p>

<p>There are, obviously, quite a few well-studied models for human detection and tracking, usually as part of general-purpose object detection, but I haven't been able to find any information that explicitly works for tracking humans from above.</p>
"
2351,"<p>I just wanted to confirm that my understanding of the different Markov Decision Processes are correct, because they are the fundamentals of reinforcement learning. Also, I read a few literature sources, and some are not consistent with each other.  What makes most sense to me is listed below:</p>

<p><strong>Markov Decision Process</strong></p>

<p>All the states of the environment is known so the agent has all the information required to make the optimal decision in every state.  We can basically assume that the current state has all information about the current state and all the previous states (i.e., the Markov property)</p>

<p><strong>Semi Markov Decision Process</strong></p>

<p>The agent has enough information to make decisions based on the current state.  However, the actions of the agent may take a long time to complete, and may not be completed in the next time step. Therefore, the feedback and learning portion should wait until the the action is completed before being evaluated.  Because the action takes many time steps, ""mini rewards"" obtained from those time steps should also be summed up.</p>

<p>Example: Boiling water.</p>

<p>State 1: Water is at 23 C</p>

<p>Action 1: Agent sets stove top at 200 C</p>

<p>Reward (30 seconds after, when water started to boil): +1 for the fact that water boiled in the end, but -0.1 reward for each second it took for the water to start boiling.  So total reward was -1.9 (-2.9 because the water did not boil for 29 seconds, then +1 for water boiling on 30th second)</p>

<p><strong>Partially Observable Markov Decision Process</strong></p>

<p>The agent does not have all information regarding the current state, and has only an observation, which is a subset of all the information in a given state.  Therefore, it is impossible for the agent to truly behave optimally because of lack of information.  One way to solve this is to use belief states, or to use RNNs to try to remember previous states to make better judgements on future actions (i.e., we may need states from the previous 10 time steps to know exactly what's going on currently).</p>

<p>Example: We are in a room that is pitch black.  At any time instant, we do not know exactly where we are, and if we only take our current state, we would have no idea.  However, if we remember that we took 3 steps forward already, we have a much better idea of where we are.</p>

<p>Are my above explanations correct?  And if so, isn't it possible to also have Partially Observable Semi Markov Decision Processes?</p>
"
2352,"<p><strong>I would like to create an NPC engine for games which are in a fantasy context</strong> like Lord of the Rings, Warcraft, Skyrim, Dragon Age etc.</p>

<p>This engine should be able to <strong>determine the polarity of the given sentence</strong> (positive/negative/neutral) and should also be able to tag the words (part-of-speech tagging, but that's not relevant now).
<strong>The problem is that I cannot find a dataset which</strong> I can teach my AI with, so that <strong>fits into the fantasy context</strong>.
As an example for Lord of the Rings, there are characters like Sauron which are negative, but there's a ton of expressions too in a fantasy world which cannot be found in a general dataset.</p>

<p>Therefore, <strong>I would like to ask if you could suggest to take dataset(s)</strong> which contain expressions, even better names in a fantasy context.
It doesn't matter which fantasy universe does it take.</p>

<p>Thank you!</p>
"
2353,"<p>Greedy best first search Algorithm take the history and then check value ans then reach to goal.</p>

<p>In A* search Algorithm take history and also cost then calculate value then reach to goal. </p>

<p>When we use greedy best first search Algorithm then start from initial then check value that taken by history then go to next node and so on.
When using A* search Algorithm then start from initial then calculate value by adding cost and history value then check value and no next. In this we go back also if we see that in first level there is a small value then all node in first level or sec or so on.</p>

<p><strong>MY QUESTION</strong>:
Am confuse that we call A* search is best then greedy best first search when we use A* search then if we go back then we stuck in loop so how we say its best then greedy </p>
"
2354,"<p>I was using PCA in my whole dataset (and after split to training, validation and test), but after some researchs I found out that is wrong way to do. Then I have few questions:</p>

<p><strong>-Are there some articles/references explain why is the wrong way?</strong> </p>

<p><strong>-How can I transform the validation/test set?</strong> </p>

<p>Steps to do PCA (from <a href=""https://www.sciencedirect.com/science/article/pii/S0022460X0093390X"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S0022460X0093390X</a>):</p>

<ol>
<li>zero mean</li>
</ol>

<p><span class=""math-container"">$$\mu = \frac{1}{M}\sum_{i=1}^{M} x_{i}$$</span></p>

<p>where <strong>x</strong> is my training set</p>

<ol start=""2"">
<li>centering (variance)</li>
</ol>

<p><span class=""math-container"">$$S^{2} = \frac{1}{M}\sum_{i=1}^{M} (x_{i}-\mu)^{T}(x_{i}-\mu)$$</span></p>

<ol start=""3"">
<li>use (1) and (2) to transform my original training dataset</li>
</ol>

<p><span class=""math-container"">$$x_{new} = \frac{1}{\sqrt{M}} \frac{(x_{i} - \mu)}{S}$$</span></p>

<ol start=""4"">
<li>calculate covariance matrix (actually correlation matrix)</li>
</ol>

<p><span class=""math-container"">$$C= x_{new}^T x_{new}$$</span></p>

<ol start=""5"">
<li>take the <em>k</em>-eigenvectors (/phi) from covariance matrix and defined the new space for my new dimension training set (where k are the principal components that I choose acording my variance)</li>
</ol>

<p><span class=""math-container"">$$ x_{new dim} = x_{new}\phi$$</span></p>

<p>Ok, then I have my new dimensional training dataset after PCA (till here its right according to other papers that I have read). The question is: <strong>What I have to do now for my validation/testing set?</strong></p>

<p>just the equation below?</p>

<p><span class=""math-container"">$$y_{new dim} = y\phi $$</span></p>

<p>where <strong>y</strong> is my (for exemple) validation original dataset?</p>

<p>Can someone explain the right thing to do?</p>

<p>Thanks!!! :)</p>
"
2355,"<p>I'm having trouble implementing AC for continuous action space. As far as I can tell, my code doesn't seem to have any bugs! The agent is learning ""something"" as its behaviour seems to vary dramatically after several episodes, but it never seems to ever approach a type of behaviour which I'd think is reasonable. </p>

<p>I've used very similar code and things have gone smoothly in discrete space and little has changed other than changes to the output (mean and variance). </p>

<p>Below is the relevant code:</p>

<pre><code>class Actor(object):
def __init__(self, sess, s_size, h_size, a_size, env, lr=1e-3):


    mu = tf.layers.dense(self.hidden_1, 
                                    self.a_size,
                                    activation=tf.nn.tanh,
                                    bias_initializer=None)

    sigma = tf.layers.dense(self.hidden_1, 
                                    self.a_size,
                                    activation=tf.nn.softplus,
                                    bias_initializer=None)
    sigma = sigma + 1e-10

    self.normal_dist = tf.contrib.distributions.Normal(mu, sigma)
    self.action = tf.clip_by_value(self.normal_dist.sample(1), env.action_space.low[0], env.action_space.high[0])

    self.adv = tf.placeholder(dtype=tf.float32)    # get log prob of the actions taken in _samples
    self.acts = tf.placeholder(shape=[None, a_size], dtype=tf.float32)

    self.log_prob = self.normal_dist.log_prob(self.acts)

    self.loss = -self.log_prob * self.adv 
</code></pre>

<p>The environment I'm using is the LunarLanderContinuous-v2. I've tested DDPG in this same environment and the agent learns incredibly quickly in comparison with the same learning rate and model size which is making me very confused. If anyone has any input it would be very much appreciated. Thanks</p>

<pre><code>class Critic(object):
def __init__(self, sess, s_size, h_size, env, gamma=0.99, lr=1e-3):
    self.gamma = gamma
    self.replay_buffer = []
    self.input = tf.placeholder(shape=[None, s_size], dtype=tf.float32)

    self.hidden_1 = tf.layers.dense(self.input, 
                                    h_size,
                                    activation=tf.nn.relu,
                                    bias_initializer=None)

    self.hidden_2 = tf.layers.dense(self.hidden_1, 
                                    h_size,
                                    activation=tf.nn.relu,
                                    bias_initializer=None)

    self.value = tf.layers.dense(self.hidden_2, 
                                    1,
                                    activation=None,
                                    bias_initializer=None)

    self.q_value = tf.placeholder(shape=[None,], dtype=tf.float32)
    self.advantage = self.q_value - self.value

    self.loss = tf.reduce_mean(tf.square(self.advantage))

    self.lr = lr
    optimizer = tf.train.AdamOptimizer(self.lr)
    self.update = optimizer.minimize(self.loss)
</code></pre>
"
2356,"<p>In language models, CNNs can extract different n-gram features from the input. From my current understanding, these models are called ""multi-channel CNNs"". I'm referring to these materials:</p>

<ul>
<li><a href=""https://medium.com/@sharaf/a-paper-a-day-19-a-multichannel-convolutional-neural-network-for-cross-language-dialog-state-bb00f5328163"" rel=""nofollow noreferrer"">https://medium.com/@sharaf/a-paper-a-day-19-a-multichannel-convolutional-neural-network-for-cross-language-dialog-state-bb00f5328163</a></li>
<li><a href=""https://arxiv.org/pdf/1701.06247.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1701.06247.pdf</a></li>
</ul>

<p>What is the difference between channels in convolutional networks and the ""multi-channel"" model?</p>

<p>Are these two types of channels similar or do they have different functions?</p>
"
2357,"<p>Let's say there's an optimization problem of 10 elements.</p>

<p>If I try to optimize it, I'll get to some result. (***)</p>

<p>My question is, if somehow I start optimizing while having 3 elements already solved with the almost the ground truth (AKA almost best) solution, will the optimization of the other 7 elements reach better or the same results, compared to (***) ?</p>
"
2358,"<p>I am building 2 models using XGboost, one with x number of parameters and the other with y number of parameters of the data set.</p>

<p>It is a classification problem.
A yes-yes, no-no case is easy, but what should I do when one model predicts a yes and the other model predicts a no ?</p>

<p>Model A with x parameters has an accuracy of 82% and model B with y parameters has accuracy of 79%.  </p>
"
2359,"<p>I have a dataset of millions of chat messages from different discussions.</p>

<p>Some of the messages are written by people who lack understanding or relevant language skills. These messages almost always contain little to no thought and are almost always irrelevant to the main topic.</p>

<p>Some of the messages are written by highly educated people. The lexical structure of these messages is more complex.</p>

<p><strong>What popular algorithm can yield results of good precision for this task - measuring or classifying human intelligence ?</strong></p>

<p>Let's say the goal is classifying the data in these 3 categories: ""dumb"", ""average"", ""intelligent"".</p>

<p>The exact measuring of people's Intelligent Quotient  by the way they write is a bit more interesting topic but I expect this would require far more research and fiddling.</p>

<p>I found almost no resources on prior research on this topic.</p>

<p><strong>Has anyone come across a labeled dataset that might be useful for training a classifier on this task ?</strong></p>
"
2360,"<p>In a CNN, the <em>receptive field</em> is the portion of the image used to compute the filter's output. But one filter's output (which is also called a ""feature map"") is the next filter's input.</p>

<p>What's the difference between a receptive field and a feature map?</p>
"
2361,"<p>I'm doing my thesis on <code>Reinforcement Learning</code>. My focus on Partially Observable Environments like 3D Games. I want to choose a 3D platform for testing and doing research. </p>

<p>I know some of them. <code>DeepMind Lab</code> and <code>OpenAi Universe</code>. But my question is that which of these environments is good for me? Is there any environment for this purpose that is benchmark and reliable?</p>

<p>I want a platform that accepted in Academia and reliable. For example DeepMind is not a standard or Open Source friendly, Is it rational to use their platform for research in academia?</p>

<p>What i have to do?</p>
"
2362,"<p>What is a learning agent, and how does it work? What are examples of learning agents (e.g., in the field of robotics)?</p>
"
2363,"<p>What's the difference between simple reflex and model bases reflex agent?</p>
"
2364,"<p>I have trained a convolutional neural network on images to detect emotions. Now I need to use the same network to extract features from the images and use the features to train an LSTM. The problem is: the dimensions of the top layers are: <code>[None, 4, 4, 512]</code> or <code>[None, 4, 4, 1024]</code>. Therefore, extracting features from this layer will result in a <code>4 x 4 x 512 = 8192</code> or <code>4 x 4 x 1024 = 16384</code> dimensional vector for each image. Clearly, this is not what I want. </p>

<p>Therefore, I would like to know what to do in this case and how to extract features that are of reasonable size. Should I apply gloabl average pooling to the activation or what?</p>

<p>Any help is much appreciated!!</p>
"
2365,"<p>It is an easy matter to make paper look old, for example, using any of the techniques explained on this page of wikiHow: <a href=""https://www.wikihow.com/Make-Paper-Look-Old"" rel=""nofollow noreferrer"">https://www.wikihow.com/Make-Paper-Look-Old</a></p>

<p>This raises the obvious and interesting question:
Is present-level AI sufficient to distinguish fake old paper from real?</p>
"
2366,"<p>In a <code>gym</code> environment, the action space is often a discrete space, where each action is labeled by an integer. I cannot find a way to figure out the correspondence between action and number. For example, in frozen lake, the agent can move Up, Down, Left or Right. The code:</p>

<pre><code>import gym
env = gym.make(""FrozenLake-v0"")
env.action_space
</code></pre>

<p>returns <code>Discrete(4)</code>, showing that four actions are available. If I <code>env.step(0)</code>, which direction is my agent moving?</p>
"
2367,"<p>When should the iterative deepening search (IDS), also called  iterative deepening depth-first search (IDDFS), and the depth-limited search be used?</p>
"
2368,"<p>From the paper <a href=""https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"" rel=""nofollow noreferrer"">Human level Control through DeepRL</a>,  the correlation in the data causes <code>instability</code> in the network and may causes the network to <code>diverge</code>. I wanted to understand what does this <code>instability</code> and <code>divergence</code> mean ? And why correlated data causes this instability.</p>
"
2369,"<p>Assume one is using transfer learning via a model which was trained on imagenet.  </p>

<ol>
<li><p>Assume that the pre-processing which was used to achieve the pretrained model contained z-score standardization using some mean and std which was calculated on the training data.<br>
Should one apply the same transformation on their new data?<br>
Should they apply z-score standardization using a mean and std of their own training data?</p></li>
<li><p>Assume that the pre-processing now did not contain any standardization.<br>
Should one apply no standardization on their new data as well?<br>
Or should one apply z-score standardization, using the mean and std of their new data, and expect better results?</p></li>
</ol>

<p>For example i've seen that the Inception V3 model which was trained by Keras did not use any standardization, and i'm wondering if using z-score standardization on my new data could yield better results.</p>
"
2370,"<p>This is a very basic question. I'm running a faster rcnn trainer on a dataset for object recognition. My images range from 200x200 to 7360x4912 in resolution. There are only 2 classes being trained (both are very similar, but slightly different). 2291 total images, 727 labels for one class and 917 labels for the second. After about 400k steps this is what my loss curve looks like. It hasn't completely plateaued just yet, but I'm not convinced it will get to where I need it. My question is about the jaggedness of the results. I have smoothing set to 0.99 to get an overall view of the progress, but the loss actually fluctuates very wildly. Is this to be expected or normal? If not then what should I be looking for to improve the results? Is this mostly a dataset cleanup issue or could the network settings use some fine tuning? Thanks for any help.</p>

<p><a href=""https://i.stack.imgur.com/rheoB.png"" rel=""nofollow noreferrer"">Loss Graph</a></p>
"
2371,"<p>I am working on a project for price movement forecasting and I am stuck with poor quality predictions.</p>

<p>At every time-step I am using an LSTM to predict the next 10 time-steps. The input is the sequence of the last 45-60 observations. I tested several different ideas, but they all seems to give similar results. The model is trained to minimize MSE.</p>

<p>For each idea I tried a model predicting 1 step at a time where each prediction is fed back as an input for the next prediction, and a model directly predicting the next 10 steps(multiple outputs). For each idea I also tried using as input just the moving average of the previous prices, and extending the input to input the order book at those time-steps.
Each time-step corresponds to a second.</p>

<p>These are the results so far:</p>

<p>1- The first attempt was using as input the moving average of the last N steps, and predict the moving average of the next 10. 
At time t, I use the ground truth value of the price and use the model to predict t+1....t+10</p>

<p>This is the result</p>

<p><a href=""https://i.stack.imgur.com/YOXxu.png"" rel=""nofollow noreferrer"">Predicting moving average</a></p>

<p>On closer inspection we can see what's going wrong:</p>

<p><a href=""https://i.stack.imgur.com/m4Xb5.png"" rel=""nofollow noreferrer"">Prediction seems to be a flat line. Does not care much about the input data.</a></p>

<p>2) The second attempt was trying to predict differences, instead of simply the price movement. The input this time instead of simply being X[t] (where X is my input matrix) would be X[t]-X[t-1].
This did not really help.
The plot this time looks like this:</p>

<p><a href=""https://i.stack.imgur.com/toFGz.png"" rel=""nofollow noreferrer"">Predicting differences</a></p>

<p>But on close inspection, when plotting the differences, the predictions are always basically 0.</p>

<p><a href=""https://i.stack.imgur.com/eSqGU.png"" rel=""nofollow noreferrer"">Plot of differences</a></p>

<p>At this point, I am stuck here and running our of ideas to try. I was hoping someone with more experience in this type of data could point me in the right direction.</p>

<p>Am I using the right objective to train the model? Are there any details when dealing with this type of data that I am missing?
Are there any ""tricks"" to prevent your model from always predicting similar values to what it last saw? (They do incur in low error, but they become meaningless at that point).</p>

<p>At least just a hint on where to dig for further info would be highly appreciated.</p>

<p>Thanks!</p>

<p><strong>UPDATE</strong></p>

<p>Here is my config</p>

<pre><code>{
    ""data"": {
        ""sequence_length"":30,
        ""train_test_split"": 0.85,
        ""normalise"": false,
        ""num_steps"": 5
    },
    ""training"": {
        ""epochs"":200,
        ""batch_size"": 64
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""input_timesteps"": 30,
                ""input_dim"": 101,
                ""return_seq"": true,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""activation"": ""relu"",
                ""return_seq"": false
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""linear""
            }
        ]
    }
}
</code></pre>

<p>Notice the last layer with 101 neurons. It is not an error. We just want to predict the features as well as the price. In other words, we want to predict the price for time t+1 and use the features predicted to predict the price and new features at time t+2, ...</p>
"
2372,"<blockquote>
  <p>“Funding artificial intelligence is real stupidity.”
  -- <a href=""https://en.wikipedia.org/wiki/John_R._Pierce"" rel=""nofollow noreferrer"">John R. Pierce</a></p>
</blockquote>

<p>Was this computer pioneer way off the mark? – or was there important <em>sub-text</em> there?</p>

<p>Pierce was an expert for machine translation in the 1960s. He coauthored the following paper Pierce, John R., and John B. Carroll. ""Language and machines: Computers in translation and linguistics."" (1966). That means, he worked on the domain of AI but explained his subject as useless. Perhaps Marvin Minsky, Rodney Brooks and Sebastian Thrun would agree to him?</p>
"
2373,"<p>I use PyTorch, bauces AllenNLP is built on it and good libraries are for it. But can AutoKeras be used for PyTorch based ML pipelines, or am I required to switch to Keras? Google is quite silent when asked for tutorials for this combination. Maybe PyTorch is lacking automated ML framework?</p>
"
2374,"<p>Until now, I always thought that Genetic Algorithm can be used for problems of which the solution space can be encoded (modeled) as a chromosome of a specific length. However, some people claim that they used GA for <a href=""https://www.codingame.com/multiplayer/optimization/code-vs-zombies"" rel=""nofollow noreferrer"">this game</a> and <a href=""https://www.codingame.com/multiplayer/bot-programming/coders-strike-back"" rel=""nofollow noreferrer"">this game</a>. They are basically games in which we control an agent on a 2-dimensional area. </p>

<p>Obviously, the length of the genome sequence depends on how fast the game is finished. So, how is GA used for such games?</p>

<p>If you think GA is not the most suitable method for this kind of problems can you explain why and give better alternatives?</p>
"
2375,"<p>What is the uniform-cost search algorithm? How does it work? I would appreciate to see a graphical execution of the algorithm. How does the ""frontier"" evolve in the case of UCS?</p>
"
2376,"<p>I am implementing an actor-critic reinforcement learning algorithm for winning a two player tic-tac-toe like game. The agent is trained against a min-max player and after a number of episodes is able to learn a set of rules which lead it to winning a good majority of games.</p>

<p>However, as soon as I play against the trained agent by using even a slightly different playing style, it looses miserably. In other words, it is evident the agent overfitted with respect to the deterministic behaviour of the min-max player. It is clear to me what are <a href=""https://ai.stackexchange.com/questions/7963/how-can-a-reinforcement-learning-agent-generalize-if-it-is-trained-against-only"">the roots of the problem</a>, but I would like to get an overview of the different methodologies which can be applied to overcome (or mitigate) this issue. </p>

<p>The two solutions I would like to try are the following:<br>
1. Training the agent with different opponents for fixed amounts of episodes (or time) each. So that for example I train the agent by using a depth 2 min-max player for the first 10000 episodes, then I use a random playing agent for the next 10000 episodes, then I use a depth 4 min-max player for other 10000 episodes and repeat the process.<br>
2. Starting episodes from different initial configurations. In this way the agents will play a much wider set of sampled games and will be more difficult for the agent to overfit.  </p>

<p>Are these two reasonable approaches? Are there other tricks/good practices to try out?</p>
"
2377,"<p>I am a student and I study Algorithms like </p>

<ul>
<li><p>tree search algorithm</p></li>
<li><p>graph search algorithm</p></li>
</ul>

<p>Then apply </p>

<ul>
<li>BFS</li>
<li>DFS</li>
<li>UCS</li>
<li>IDS</li>
<li>DLS </li>
</ul>

<p>Where are all these algorithms are used? 
I know there is a long list of algorithms, but I don't know about all those algorithms.
They all have little difference, I just want to know which algorithm is mostly used in Robots.
Explain with example.</p>
"
2378,"<p>Which algorithm is used in robot Sophia to understand and answer the questions ?</p>
"
2379,"<p>Ben Goertzel, the developer of Sophia the robot, is not only interested in the research of Artificial General Intelligence (AGI), but is discussing ethical aspects of Artificial Intelligence too. Law firms like <a href=""https://en.wikipedia.org/wiki/Gowling_WLG"" rel=""nofollow noreferrer"">Gowling WLG</a> advise the public and other companies in the risks of artificial Intelligence. The question is:</p>

<p>Why robot sophia,s developer not use wig and is that true she has ability of thinking?</p>
"
2380,"<p>With a team, we are studying how it is possible to predict the price movement with high-frequency. Instead of predicting the price directly, we have decided to try predicting price difference as well as the features. In other words, at time <code>t+1</code>, we predict the price difference and the features for time <code>t+2</code>. We use the predicted features from time <code>t+1</code> to predict the price at time <code>t+2</code>. </p>

<p>We got very excited, because we thought getting good results with the following graph</p>

<p><a href=""https://i.stack.imgur.com/xSVd8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xSVd8.png"" alt=""enter image description here""></a></p>

<p>We got problems in production and we wasn't known the problem till we plot the price difference.</p>

<p><a href=""https://i.stack.imgur.com/PZu4F.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PZu4F.png"" alt=""enter image description here""></a></p>

<p>Here is the content of the config file</p>

<pre><code>{
    ""data"": {
        ""sequence_length"":30,
        ""train_test_split"": 0.85,
        ""normalise"": false,
        ""num_steps"": 5
    },
    ""training"": {
        ""epochs"":200,
        ""batch_size"": 64
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""input_timesteps"": 30,
                ""input_dim"": 101,
                ""return_seq"": true,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 51,
                ""activation"": ""relu"",
                ""return_seq"": false
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dense"",
                ""neurons"": 101,
                ""activation"": ""linear""
            }
        ]
    }
}
</code></pre>

<p>Prices don't change very fast. Therefore, the next price is almost always very close to the last price. In other words, <code>P_{t+1} - P_{t</code>} is very often close to zero or zero directly. If there is too many zeros then the network will only recognize the zeros. The model has picked up on that.</p>

<p>I guess the model learned almost nothing except the very simple relationship that the next price is close to the last price. There is not necessarily anything wrong with the model. Predicting stock prices should be a very hard problem. </p>

<p>So a straightforward improvement should be of taking the features as a whole instead of their difference. </p>

<p>I want to keep working with price difference instead of the price in itself because we are making the series potential more stationary.</p>

<p>What might be a good solution to deal with the repetitive zeros related to our ""price difference"" problem? Does applying the log-return is a better idea than applying price differences?</p>

<p>Does a zero inflated estimators is a good idea? First predict whether it's gonna be a zero. If not predict the value. <a href=""https://gist.github.com/fonnesbeck/874808"" rel=""nofollow noreferrer"">https://gist.github.com/fonnesbeck/874808</a> ?</p>
"
2381,"<p>I was lately curious about a reinforcement learning approach that would solve maths equations.</p>

<p>For example, if I have the following equation:</p>

<p><span class=""math-container"">$$ f(g(h(w))) = 0 , with \ w = \begin{matrix}
a_{11} &amp;  0  &amp; \ldots &amp; a_{1n}\\
0  &amp;  a_{22} &amp; \ldots &amp; a_{2n}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0  &amp;   0       &amp;\ldots &amp; a_{nn}
\end{matrix}
$$</span></p>

<p>Along additional constraints on the 3 functions like 
<span class=""math-container"">$ f &lt; g $</span>;
<span class=""math-container"">$ h &gt; 2 * g $</span>; and
<span class=""math-container"">$ f, g,h $</span> not constant</p>

<p><strong>The goal is to find the 3 functions expressions given a specific matrix <span class=""math-container"">$w$</span> and the constraints.</strong></p>

<p>Can I use reinforcement learning to find a solution or solution<strong>s</strong> to this problem ?</p>

<p>Thank you</p>
"
2382,"<p>I'm interested in different approaches to study artificial intelligence that would open up career opportunities.  What resources are available?  What disciplines are required?</p>
"
2383,"<p>I'd like to explore the possibilities of applying artificial intelligence to ocr reading.
Basic ocr invoices processing  let me convert 30% of them only. 
Main pourpose is defining invoices areas by training an ai , then process those areas with ocr.
So I am looking into ai to define and recognize a document topology first, then apply ocr locally.
From a brief search it is classified as zonal or template ocr.
Any chance of a premade open source library?</p>
"
2384,"<p>In examples and tutorial about DQN, I've often noticed that during the experience replay (training) phase people tend to use stochastic gradient descent / online learning. (e.g. <a href=""https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762"" rel=""nofollow noreferrer"">link1</a>, <a href=""https://keon.io/deep-q-learning/"" rel=""nofollow noreferrer"">link2</a>) </p>

<pre><code># Sample minibatch from the memory
minibatch = random.sample(self.memory, batch_size)
# Extract informations from each memory
for state, action, reward, next_state, done in minibatch:
    # if done, make our target reward
    target = reward
    if not done:
      # predict the future discounted reward
      target = reward + self.gamma * \
               np.amax(self.model.predict(next_state)[0])
    # make the agent to approximately map
    # the current state to future discounted reward
    # We'll call that target_f
    target_f = self.model.predict(state)
    target_f[0][action] = target
</code></pre>

<p>Why can't they use mini batches instead?
I'm new to RL, but in deep learning people tends to use mini-batches as they would result in a more stable gradient. Doesn't the same principle apply to RL problems? Is the randomness/noise introduced actually beneficial to the learning process? Am I missing something, or are these sources all wrong?</p>

<hr>

<p>Note:</p>

<p>Not all the sources rely on stochastic gradient descent: e.g. keras-rl seems to rely on minibatches (<a href=""https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py"" rel=""nofollow noreferrer"">https://github.com/keras-rl/keras-rl/blob/master/rl/agents/dqn.py</a>)</p>
"
2385,"<p>I am trying to understand the proof of theorem 2.1 from this paper:</p>

<blockquote>
  <p>Ross, Stéphane, and Drew Bagnell. ""Efficient reductions for imitation learning."" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.</p>
</blockquote>

<p>The cost-to-go is given as </p>

<p><span class=""math-container"">$$J(\pi) = \sum_{t=1}^{T}\mathbb{E}_{s\,\sim\, d^t_{\pi}(s)}\left[C_\pi(s)\right].$$</span></p>

<p>In the paper they use <span class=""math-container"">$\hat{\pi}=\pi$</span> for the learned policy and <span class=""math-container"">$\pi^*$</span> for the expert policy.</p>

<p>In the derivation they write</p>

<p><span class=""math-container"">$$J(\pi)\leq \sum_{t=1}^{T}\{ p_{t-1}\mathbb{E}_{s\, \sim \, d_t(s)}\left[C_\pi(s) \right]+(1-p_{t-1})\}$$</span>
<span class=""math-container"">$$\leq \sum_{t=1}^{T}\{ p_{t-1}\mathbb{E}_{s\, \sim \, d_t(s)}\left[C_{\pi^*}(s) \right]+p_{t-1}{\ell_t(s,\pi)}+(1-p_{t-1})\},$$</span></p>

<p>in which <span class=""math-container"">$p_{t-1}$</span> is the probability of not not making an error with policy <span class=""math-container"">$\pi$</span> up to the time <span class=""math-container"">$t-1$</span>. And <span class=""math-container"">$\ell$</span> is the surrogate 0-1 loss. </p>

<p>The following steps are easy to follow, but how did they come up with these steps? </p>
"
2386,"<p>I'm interested in the differences in scope between statistical AI and Classical AI (<a href=""https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence"" rel=""nofollow noreferrer"">gofai</a>).  </p>

<ul>
<li>What are the different qualities, and the differences in capability and application?</li>
</ul>

<p>Real world examples would be appreciated.</p>
"
2387,"<p>I am interested in making a simple chess engine using neural networks. I already have a fairly good value network but I can't figure out how to train a policy network. I know that Leela chess zero outputs the probability of any of the about 1800 possible moves. But how do you train such a network? How do you calculate the loss when you  only have the 1 move that was played in the game 
to work with?</p>
"
2388,"<p>For example Haar Cascade can be trained using only positive and negative examples, you don't need any bounding box annotations. But it not a deep learning approach.</p>

<p>Another example can be the most straight forward Image recognition model + sliding window. But it is very slow</p>
"
2389,"<p>We all know <strong><em>Information Filter</em></strong> is a dual representation of Kalman Filter. The main difference between Information Filter and Kalman Filter is the way the Gaussian belief is represented. In Kalman Filter, the Gaussian belief is represented by their moments, whereas in Information Filter, the Gaussian belief is represented by the canonical form. Canonical form is comprised of an information matrix<code>(<span class=""math-container"">$\Omega$</span>)</code> and an information vector<code>(<span class=""math-container"">$\xi$</span>)</code>.</p>

<p>As we all know, the name Kalman Filter is after Rudolf E. Kálmán, one of the primary developers of this theory.</p>

<p>Now I want to know where the name information filter came from? Why it is called information filter? What type of information is attached to this filter? Is there any significance behind the nomenclature?</p>

<p>I have the same question about information matrix and information vector. Is there any significance behind the nomenclature?</p>

<p>I already read Probabilistic Robotics by Sebestian Thrun. Chapter 3 Gaussian filter, in the subsection 3.4 The Information Filter. There are many equations and theories but that does not tell us about the nomenclature.</p>

<p><a href=""https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf"" rel=""nofollow noreferrer"">Probabilistic Robotics</a></p>
"
2390,"<p>I created a very simple bot to learn how to use chatterbot. This library already comes with a training, but I wanted extra training with an import of a corpus in Portuguese that I found in github.</p>

<pre><code>from chatterbot import ChatBot

bot = Futaba(
""Terminal"",
storage_adapter=""chatterbot.storage.SQLStorageAdapter"",
logic_adapters=[
""chatterbot.logic.MathematicalEvaluation"",
""chatterbot.logic.TimeLogicAdapter"",
""chatterbot.logic.BestMatch""
],

input_adapter=""chatterbot.input.TerminalAdapter"",
output_adapter=""chatterbot.output.TerminalAdapter"",
database_uri=""../database.db""
)

print(""Type something to begin..."")

while True:
    try:
        bot_input = bot.get_response(None)
    except (KeyboardInterrupt, EOFError, SystemExit):
        break
</code></pre>

<p>That's all I have.</p>

<p>How can I import this corpus into my chatbot?</p>
"
2391,"<p>I'm building a weather station, where I'm sensing <em>temperature, humidity, air pressure, brightness, <span class=""math-container"">$CO_2$</span></em>, but I don't have a raindrop sensor. </p>

<p>Is it possible to create an AI which can say if it's raining or not, with the help of the given data above and maybe analyzing the slope from the last hour or something? Which specific technology should I use and how can I train it?</p>
"
2392,"<p>Let me compare two textbooks: </p>

<p>(1) ""Artificial Intelligence: A Modern Approach"" by Stuart J. Russell and Peter Norvig and </p>

<p>(2) ""Artificial Intelligence: Structures and Strategies for Complex Problem Solving"" by George F. Luger (sixth edition).</p>

<p>I have an impression that the former (1) is biased towards symbolic AI (especially logic-based) and the latter (2) is biased towards statistical methods. Do you think the same? Or, in wider sense, is (1) more rational and (2) more empirical? Do you know other AI books with a strong emphasis on statistical methods?</p>
"
2393,"<p>Currently, I found the right recipe for a time series regression problem to finally get acceptable to good results. </p>

<p>Here is the config file </p>

<pre><code>{
    ""data"": {
        ""sequence_length"":45,
        ""train_test_split"": 0.85,
        ""normalise"": false,
        ""num_steps"": 10
    },
    ""training"": {
        ""epochs"":30,
        ""batch_size"": 32
    },
    ""model"": {
        ""loss"": ""mse"",
        ""optimizer"": ""adam"",
        ""layers"": [
            {
                ""type"": ""lstm"",
                ""neurons"": 161,
                ""input_timesteps"": 45,
                ""input_dim"": 161,
                ""return_seq"": true,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dropout"",
                ""rate"": 0.1
            },
            {
                ""type"": ""lstm"",
                ""neurons"": 161,
                ""activation"": ""relu"",
                ""return_seq"": false
            },
            {
                ""type"": ""dense"",
                ""neurons"": 128,
                ""activation"": ""relu""
            },
            {
                ""type"": ""dense"",
                ""neurons"": 1,
                ""activation"": ""linear""
            }
        ]
    }
}
</code></pre>

<p>Here is the results I got</p>

<p><a href=""https://i.stack.imgur.com/K19Ki.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K19Ki.png"" alt=""enter image description here""></a></p>

<p>What can be good improvements I can bring to my model so that I can get better results? There are a lot of small spikes and other places where the curve is rather constant that I should get a rise or fall of the curve.</p>
"
2394,"<p>The iterative deepening A* search is an algorithm that can find the shortest path between a designated start node and any member of a set of goals. </p>

<p>The A* algorithm evaluates nodes by combining the cost to reach the node and the cost to get from the node to the goal. My question is: how is iterative deepening A* better than the A* algorithm.</p>
"
2395,"<p>My understanding is that the conjugate gradient method is faster than gradient descent because it does less zig zags while descending. How come the state of the art papers I see all use gradient descent for backpropagation and not conjugate gradient descent?</p>
"
2396,"<p>Is there a good reference / tutorial for using RNN/LSTM to determine lag interval for 2 time series? E.g. I have {x_n}, {y_n} and I want to figure out by how much does {x_n} typically lags behind {y_n}?</p>
"
2397,"<p>It seems to me that the way neural networks are trained is similar to the way we ""train"" or educate a child (or a person, in general).</p>

<p>Can an AI <em>eventually</em> think like a human?</p>
"
2398,"<p>I am trying to read <a href=""https://arxiv.org/abs/1701.01727"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1701.01727</a> about generalisation of Hopfield neural networks and I like the clear ideas that physics and Hamiltoanian framework can be used for modeling such networks and for deducing lot of properties. My question is - how Hopfield networks are connected to the standard networks?</p>

<p>I see at least 3 differences:</p>

<ul>
<li>Hopfield neurons has binary values on/off (+1, -1) but machine learning neurons have real (are at least approximately (within machine limits) real) values.</li>
<li>threshold function is the only activation function used in Hopfield neurons, machine learning has lot of  more complex, real-valued functions</li>
<li>Connectivity patterns in machine learning networks (LSTM, GRU cells) are far more richer.</li>
</ul>

<p>So - maybe Hopfield neural networks can be generalized up to the level of machine learning networks and at the same time preserving the use of Hamiltonian framework. Is it possible, is there any work in this direction? </p>
"
2399,"<p>I am a CS student, our professor gave an assignment that each group of our class should watch at least 5 movies based on artificial intelligence, then discuss and share with the whole class. Can someone help me with searching of the movies about AI?</p>
"
2400,"<p>There are some fields of Computer Vision that are similar to Artificial Intelligence. For example, pattern recognition and path tracking. Based on these similarities, can we say that the Computer Vision is a part of Artificial Intelligence?</p>
"
2401,"<p>I'm aware of those AI programmes which can play games and neural networks which can identify pictures. 
But are they really thinking. Do they think like humans? Do they have consciousness? Or are they just obeying a bunch of codes?
For example, when an AI learns to play pacman, is it really learning that it should not touch the ghosts or are they just following a mechanical path which will make them win the game?</p>
"
2402,"<p>The default number of trees to be generated is 10. But I thought it should be a very large number and I put 500 trees. However it performed better when the number of trees are 10 than 500.
My question is 
How to determine the number of trees to be generated in Random Forest algorithm?</p>
"
2403,"<p>There are two models for the same task:</p>

<p><strong>model_1:</strong> 98% accuracy on training set, 54% accuracy on test set.<br/>
<strong>model_2:</strong> 48% accuracy on training set, 47% accuracy on test set.<br/></p>

<p>From the statistics above we can say that <strong>model_1</strong> overfits training set.<br/>
<strong>Q1:</strong> Can we say that <strong>model_2</strong> underfits?<br/>
<strong>Q2:</strong> Why <strong>model_1</strong> is bad choice if it performs better than <strong>model_2</strong> on test set?</p>
"
2404,"<p>One of the somewhat subliminal and entirely unsubstantiated assumptions we hear is that more computing power will allow us to approximate human intelligence &mdash; that quantitative augmentation will lead to qualitative improvements in automation.  No new techniques are proposed &mdash; Just the scaling of existing ones.</p>

<p>When we study functional MRIs, we see that much of the brain is at rest at any given time.  We read pop psychology articles about awakening the untapped intelligence, as if the brain volume at rest is a flaw.  We notice what is lit up, but we tend to ignore the possibility of purpose behind inactivity.  Here's the question these assumptions trigger.</p>

<blockquote>
  <p>Can not thinking be a form of intelligence?</p>
</blockquote>

<p>Many can see the intuitive attractiveness of the Buddhist idea that clearing the mind can lead to enlightenment.  We wish we could be more logical and more intuitive.  It's clear that there is no consensus about how to balance these intellectual goals.  If we admit this intellectual agnosticism, we are more likely to move toward the greater understanding of what intelligence can be at its best.</p>

<p>Is it possible that more computing power is necessary only for a small segment of problem types when approached under the assumption that power is necessary?  Will these current approaches be later seen as primitive and limiting?  Might a more enlightened approach we haven't yet considered lead to excellent performance on low end mobile phones for entire classes of problems?</p>

<blockquote>
  <p>What new ideas show promise along these lines?</p>
</blockquote>
"
2405,"<p>Set aside networks, image classification, gradients, and the strength of intelligence for a moment and consider the world before people lit fires.</p>

<p>Fires were started periodically just as they are now, when lightning struck dry deciduous matter.  People probably ran for water.  Perhaps some smart people learned that fire created warmth at night after a blaze and discovered how to preserve it.  They couldn't let it go out.   If they did, they'd have to wait through many cold nights until the next forest fire, which could take decades, and then try again.</p>

<p>How did someone invent how to start one without lightning?</p>

<p>The first plow, the first written law, the first bow, the first coin, the first water wheel, the first mechanical clock, the first circuit, the first logo, the first transistor, the first web site.</p>

<p>Now try to imagine it from the other end.  There is something important.  It has no name.  It has no design.  It has no method of procurement.  If you create one, no one will know what it does or why it is there.  But you do.  You're its inventor.  It began in your brain.  You saw it before it existed.  It was vision without the involvement of your eyes.  You then must bring it into being with your speech or make it with your hands.</p>

<p>This is the rarest, most human, and most precious form of intelligence.</p>

<p>What kind of algorithm can invent?  Genetic algorithms have not produced new designs of things that meant nothing to anyone when they appeared but mean something to the computer running the algorithm.  Something is missing in the way we perceive intelligence.  Maybe it is a kind of negative model, where the empty space represents the thing that needs inventing.  How would one create such a model?</p>

<p>Is there something like a virtual die, where the empty space in the universe of utility is highlighted?  What kind of algorithm can detect the absence of something without it ever having first existed?</p>

<p>Once that ability is understood, we can then approach the problem of running through a wide array of approaches to create this previously un-invented thing and check each for feasibility, but first we must learn how to artificially envision nameless things that fill previously unimagined niches.</p>

<blockquote>
  <p>What kind of algorithm can invent?</p>
</blockquote>

<p><strong>Addenda in Response to Excellent Comments</strong></p>

<p>This question is dear to me because I've invented things, some for corporations, some for my own laboratory, and some that I failed to push hard enough and someone else invented something very close and developed it first.  This last case is interesting, and I've seen it happen many times.  It's also common in scientific history, where two people who don't communicate directly simultaneously come up with some scientific or industrial invention.</p>

<p>Reading Alonso Church's interview we find that Alan Turing didn't study under him as most historical accounts state. According to Church, he developed his lambda calculus and Turing developed his machine in parallel and without direct consultation. There is an environmental aspect to invention, as if the world around the inventors are subconsciously searching without a clear objective.</p>

<p>There is an accidental appearance to invention, but everyone I know or read about who invented something was poking around in the area of the invention in their mind, and not just casually. We obsessed over some imaginary search space, hunting for something novel and purposeful.</p>

<p>It is like a rat in a maze that smells cheese but does not know the path.  We can't just try all the passageways marking each path to avoid duplicate trials to get to the cheese. We don't know it yet but there is a hole in the ceiling covered with a thin veil. Until we realize there is another level to the maze, we cannot find the cheese. The veil represents the discovery. The passage into the second level where the cheese resides is the novelty.</p>

<p>We could find the hole by accidentally hitting the veil when arbitrarily jumping up and down or with a stray ball when playing a game of toss with another rat, but it would sure be faster if we realized something. We smell cheese but failed to find it. When we doubt our method, we start looking for fissures in the surfaces we haven't yet stepped on. Doubt has something to do with it.</p>

<p>Yes, there is a requirement of some kind of understanding or model of the world that can be altered and tested in imaginary space.  This is most obvious in the writings containing the thought experiments of Archemedes (buoyancy principle which led to the relative incompressability of liquids, the first conception of a screw, ...), the thought experiments of Isaac Newton (the two prisms in series, the cannon ball blasted into orbit which led to the entire field of Newtonian physics, ...), and the thought experiments of Turing (the imitation game, computability and the punch tape machine).</p>
"
2406,"<p>How we will explain the best usage and need of artificial intelligence although now its has brought revolution in technology and other fields?</p>
"
2407,"<p>Mode collapse is a common problem faced by GANs. I am curious why doesn't VAE suffer Mode collapse?</p>

<p><a href=""http://aiden.nibali.org/blog/2017-01-18-mode-collapse-gans/"" rel=""nofollow noreferrer"">Some source</a> </p>
"
2408,"<p>If an antecedent in a rule involves <span class=""math-container"">$m$</span> two-state features and results in consequences from a set of <span class=""math-container"">$n$</span> possible ones, we have <span class=""math-container"">$2^{m+n}$</span> permutations, which are, in a sense, be categories.  If features and actions are identified in advance, along with a set of labelled example data for which the rules of labelling are unknown, can a rule could be created by an artificial network with <span class=""math-container"">$m+n$</span> binary outputs?</p>

<p>If so, can the rule created be excluded by the loss function in an independent application of the same or a similar artificial network, and can this process be recursed until a rule set for labelling examples has been created?</p>
"
2409,"<p>I observed in several papers that the Variational autoencoder's output is blurred while GANs output is crisp and has sharp edges. </p>

<p>Can someone please give some intuition why that is the case? I did think a lot but couldn't find any logic. </p>
"
2410,"<p>My guess is that they come under supervised learning, as we have labelled dataset of images, but I am not sure as there maybe other aspects in GANs which might come into play in the determination of the class of algorithms GAN falls under.</p>
"
2411,"<p>The problem statement: Mapping from a vector space representation onto a tree structure.</p>

<p>Possible solution: Given a word vector as input, produce a path in the tree from the root down to the node that most closely matches that vector.</p>

<p>The path would be a variable length string composed from a finite set of symbols.  The variable length output leads me towards long short-term memory (LSTM) models.  But I've never built a complete LSTM model before.</p>

<p>The understanding gap: My vector inputs are already dense representations.  Specifically, I'm working with GloVe right now.  But the output path symbols would require a different encoding, correct?  How can I structure/train the necessary encoder-decoder pair so that it can handle both word vectors and these path symbols as input while still being able to produce path symbols as output?</p>

<p><strong>EDIT</strong>: After more research, I think the correct term would be a ""one to many"" model using an RNN architecture (of which LSTM is one type).  So I wouldn't need an encoder for the one input vector because it's already a dense vector.  But I would need to train an autoencoder for decoding the multiple output vectors that describe the tree path.</p>
"
2412,"<p>What are the differences between the A* algorithm and the greedy best-first search algorithm? Which one should I use? Which algorithm is the better one, and why?</p>
"
2413,"<p>I understand A* and Dijkstra for avoiding obstacles, they require that points are traversable there are points that are not traversable thus the algorithms wont bump into the obstacles because the obstacles cant be traversed or maybe if cost is a factor the relationships between the point is of such a high weight that the algos wont take that path. ive been using graphs this way with good results so A* and Dijkstra are great candidates and I have them working sweet, but they always require me to have points in place to traverse, im the one who puts the points in and creates the relationships between the points, its not ai or any type of learning, its just an algo traversing points on a map. </p>

<p>Lets say I have a white image, a green blob in the middle and points at either side of the blob, I need to get from a -> b I don't have points to traverse I just have this image, is it a case of machine learning to learn an agent to get from point a -> b then apply that learning to more complex maps? if so what what should I be looking at? or is that the wrong route to take pardon the pun. If any of my google queries contain ""ai"" all I get back is deep mind this and deep mind that and a lot of game developer answers that include sending rays out in front etc, but again that's not ai or learning.</p>

<hr>

<h3>Edit after answers were posted:</h3>

<p>Ok thanks for the responses, I don't have 50 reputation so I cant answer to your posts Both answers seem to come back to graphs though as well as the paper referenced in the first answer which is what im using just now. Ok so Images is definitely out as your right its a super amount of work and would be very complicated. Ill try and explain it in a different way, take this image of a route created by my graph, the route is fine, it adheres to directional traffic separation schemes and is perfectly usable, zoomed out it looks a bit jaggy but zoomed in the route is fine
<a href=""https://i.stack.imgur.com/chQTt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/chQTt.png"" alt=""enter image description here""></a> </p>

<p>Are you saying there is no way an agent could be trained to navigate around a map going from a -> b without the use of graphs? In the image above the underlying dataset takes into account ocean points (low resolution), areas where there are many island (high resolution), canals, tss, port approaches and harbour navigation etc so there is a lot of under lying data and your route is only as good as the data you put in, theres also a lot of other concerns especially in the ocean portions where you don't wants to just connect to your neighbour, you want your route to take longer jumps.</p>

<p>If you had an array 640 millions point for latitude longitude 3 decimal precision with high values for land and low values for water, could an agent be trained to go from a -> b? by keeping the agent on water and if it crashes into land then do the simulation again but learn from its mistakes?</p>

<p>I know its a long shot but im just trying to get a handle on whats being done or if there is anything out there to look at. </p>

<p>Great responses thanks.</p>
"
2414,"<p>When and where the basic concept of artificial intelligence begin ? The first impact of artificial intelligence on human society?</p>
"
2415,"<p><strong>Problem</strong></p>

<p>My problem is the following: Given 1000 wins, losses, and ties from a chess simulation I am using, what shape should each game be (I.e., sequence of moves leading to win/loss/tie) in order to build an deep neural network on it?</p>

<p><strong>Literature Review</strong></p>

<ol>
<li><p><a href=""https://ai.stackexchange.com/questions/4672/can-an-ai-learn-how-to-play-chess-without-instructions"">Kind of got me in the right direction.</a></p></li>
<li><p><a href=""https://ai.stackexchange.com/questions/4910/how-would-you-encode-your-input-vector-matrix-from-a-sequence-of-moves-in-game-l"">Great theoretical way of thinking about it, but left me wanting detail.</a></p></li>
<li><p><a href=""https://erikbern.com/2014/11/29/deep-learning-for-chess.html"" rel=""nofollow noreferrer"">Gave a good high level tutorial, but not enough detail.</a></p></li>
<li><p><a href=""https://github.com/lamesjim/Chess-AI"" rel=""nofollow noreferrer"">Alpha beta pruning python example, but no neural network.</a></p></li>
<li><p><a href=""https://github.com/erikbern/deep-pink/blob/master/reinforcement.py"" rel=""nofollow noreferrer"">Neural network using reinforcement learning, but I cant figure out what the (X,y) shapes are and their meaning from just digging into the source code.</a></p></li>
</ol>

<p><strong>Current Situation</strong></p>

<p>I currently have a sample of 1000 wins, 1000 losses, and 1000 ties from the <code>python-chess</code> api, so if <code>i</code> is the index of a game, then this is the structure of the current dataset I am working with:</p>

<pre><code>game_i -&gt; (num_moves_i,8,8,16)
</code></pre>

<p>So each <code>game_i</code> where <code>i in {1..3000}</code> and <code>num_moves_i is variable</code> depending on the game (E.g., 14 for a good winning game, or 765 moves for a tie game). The 16 represents a <code>one_hot_encoding</code> for one of the 16 unique board pieces. The data set is also an alternating board state, so:</p>

<pre><code>game_i[0] == board state of whites first move
game_i[1] == baord state of blacks first move
</code></pre>

<p>Furthermore, I also have alpha-beta pruning and maximin working, so for each move I have an intrinsic value associated with it using recursion three levels deep. Leading my current approach to a regression of a given move, essentially leading me to believe the AI would simply learn the heuristic and predict a value the heuristic would give.</p>

<p><strong>Summary</strong></p>

<p>Clearly my proof of concept 1000 winning games is not enough to make a meaningful AI, but that isn't my goal. I want to learn the techniques, not produce an enterprise scale chess AI.</p>

<ol>
<li>Does the tensor shape make sense?</li>
<li>Is this a reinforcement learning problem? If so, how can I shape my current framing into that type of thinking? Theory in this area would be greatly appreciated as I am less familiar with it.</li>
<li>Is this a RNN/LSTM problem? (E.g., predict the next board state).</li>
<li>Is this a regression problem?</li>
<li>Is this a sequence mining problem?</li>
</ol>

<p>What is the standard approach to framing this problem, once you have data falling through the pipeline. </p>

<p>Your support is more than appreciated.</p>

<p><strong>* UPDATE (STILL IN RESEARCH) *</strong></p>

<p>With further research, a candidate label for the training data is the tensor and the item from the state-space that was selected with that board state. Then only keep games containing sequences of moves which obtain a cumulative value >= <code>epsilon</code>. This would require a <code>one_hot_encoding</code> of all moves played in all games we wish to train on, as labels. E.g., <code>(game_i,board_ij,e2e6)</code></p>
"
2416,"<p>There's the need to design a horizontal plane cleaning system that is controlled by positioning servos.  Two in two of three floor rollers and three in the x, y, and z positioning of a wiping device.  Cleaning fluid may be dispensed or vacuumed away, and the rollers can be concurrently locked so that the position of the cleaning system relative to the floor can be held steady or unlocked.</p>

<p>To keep device cost low, primitive lidar is used for ranging at each extremity most exposed by positioning extremes.  For instance, ranging is detected pointing upward so that the position in z is sensitive to headroom, and ranging is detected on the front, left, and right sides of the wiping device to ensure it doesn't hit walls at full speed or knock over items on the horizontal plane surface.</p>

<p>There is no pixel based input to the AI.  Ranges are reported as two 32 bit integers approximately representing two measurements in millimeters.  One is the distance to the closest point of reflection and the other is the mean distance of reflection.  Only by comparing them can walls can be distinguished from objects placed on the surface.</p>

<p>Positioning targets for the entire system rolling on the floor and for the wiping device relative to the entire system are provided as 32 bit integers to the driving systems of each servo.</p>

<p>What the system does to clean is roll over to a location, given a programmed of approach and line of movement in front of the horizontal surface, clean the surface area, constrained by the fixed parameters below, and roll back to the original system position.</p>

<p>Objects on the surface may change from cleaning to cleaning, but not walls.  There may be a need to lock the rollers, wipe in several motions, unlock the rollers, move over, lock again, wipe some more, and do this a few times.</p>

<p>There are three things we need to minimize that could be used in a loss function.</p>

<ul>
<li>Time to complete the cleaning of a horizontal surface </li>
<li>Cleaning fluid consumption</li>
<li>Variance in cleaning fluid distribution on the surface</li>
</ul>

<p>What kind of network could learn to send the right positions to the servos to perform the cleaning?  Is Q-learning best?  How is the example data to be collected?  Is there a way to learn without training, allowing performance to be poor at first but still get the job done and then improve each time afterward?  How can AI be used to make this a practical cleaning system?</p>

<p><strong>Technical Details</strong></p>

<p>There are eight numbers programmed into the firmware.</p>

<ul>
<li>The overlap of wiping in mm</li>
<li>The minimum time cleaning fluid remains on surface before vacuuming it up</li>
<li>The rate of cleaning fluid use in microns  (One milliliter is one cubic centimeter, so one milliliter of fluid per square meter horizontal surface has the unit cm cubed over meters square, which is one cm over 10,000 square cm per square meter, which is one micron.)</li>
<li>The maximum speed and acceleration of wiping motion changes relative to the horizontal surface</li>
<li>The maximum speed and acceleration of movement of the device center relative to the floor</li>
<li>The maximum permissible speed when the wiping device reaches a wall</li>
<li>The minimum clearance to maintain between the wiping device from items on the horizontal surface to avoid knocking one over</li>
</ul>
"
2417,"<p>How is a feed-forward neural network with few hidden layers and lots of nodes in those hidden layers different from a network with a lot of hidden layers but relatively lesser nodes in those hidden layers?</p>
"
2418,"<p>One disadvantage or weakness of Artificial Intelligence today the slow nature of learning or training success. For instance, an AI agent might require a 100,000 samples or more to reach an appreciable level of performance with a specific task. But this is unlike humans who are able to learn very quickly with a minimum number of samples. Humans are also able to teach one another, or in other words, transfer knowledge acquired.</p>

<p>My question is this: are Artificial Intelligence learnings or trainings transferable from one agent to the other? If yes, how? If no, why?</p>
"
2419,"<p>Let's say we have a convolutional layer which outputs <em>C</em> number of channels with width <em>W</em> and height <em>H</em> and we train it with a batch size of <em>B</em>. We feed that output to batch norm layer. So I have a couple of questions:</p>

<ol>
<li>How many gammas and betas will be in BN layer? </li>
<li>How do we compute mean and var? Do we compute them for every ""pixel"" in a feature map across batch or we firstly compute mean and var of the feature map and then we compute mean and var across batch? </li>
</ol>
"
2420,"<p>I have been given this question that how neural networks are different from normal computers? But after searching alot i still couldnt get the best reason on it which makes my point clear, so how authentically we can describe this difference?</p>
"
2421,"<p><strong>How does Recurrent Neural Network updates its weights and bias through backpropagation?</strong></p>

<p>Is time taken into account while updating the weights of a RNN using Backpropagation through time(BPTT)?""</p>
"
2422,"<p>I have a question about convolutional neural newtork.</p>

<p>Consider this image: <a href=""https://i.stack.imgur.com/ltOKu.png"" rel=""nofollow noreferrer"">conv example</a></p>

<p>We have a part of an input matrix and a filter. Ok, now we can do the convolution and the result is a scalar, if it is a large number the future was found otherwise no. So, the features map is a matrix where each number indicates the points where a feature was found. I understood this.
The output of this convolution is a features map (after activation function).
My misunderstanding start here.</p>

<p>The next convolution, will find another feature. I don't understand how this filter, can find a new feature from a representation of where the previous feature was found. The feature maps, output of the first convolution is a rappresentation <strong>of indicates only where the features were found</strong>.</p>

<p>Shouldn't be the features found instead of a number that indicates how much was found this feature??
How exactly does this work?</p>
"
2423,"<p>My AI (for the card game schnapsen) currently calculates every possible way the game could end and then evaluates the percentage of winning for every playable card / move. The calculation is done recursively using a tree. If a game could move on in three different ways the percentage of winning on this node would be <code>mean * (1 - (standardDeviation * f)) * 100</code> where f is between 0 and 2. When the game can't move on and the AI wins the percentage is 100, when lost 0. I'm including the standard deviation in this formula to prevent the AI from risking too much. In other words: I'm using a MCTS that uses percentages.</p>

<p>Is there a better formula or way of calculating the next move to maximize the chance of winning? Does including the standard deviation make sense?</p>
"
2424,"<p>Every neural network updates its weights through back-propagation.</p>

<p><strong>How is back-propagation used for updating weights in a combination of 2 or more neural networks (e.g.:CNN-LSTM, GAN-CNN, etc.)</strong>.</p>

<p>For instance a CNN-LSTM model is a CNN model stacked on top of an LSTM model. When CNN model is stacked on top of an LSTM model, do we consider hidden layer of both model or hidden layer of outer model(LSTM)?</p>
"
2425,"<p>Cisco and other companies are using Tara AI—a matching tool that connects IT projects with freelancers who have the exact skills required to complete them. </p>

<p>Looking for an explanation of how Tara AI works.  (<a href=""https://www.forbes.com/sites/abdullahimuhammed/2018/06/10/how-ai-is-making-an-entry-into-the-freelance-economy/#3a21634c6d0e"" rel=""nofollow noreferrer"">I read about it on Forbes</a> but cannot find info about the AI methods utilized via Google search.)</p>
"
2426,"<p>What are the differences between the uniform-cost search (UCS) and greedy best-first search (GBFS) algorithms? How would you convert a UCS into a GBFS?</p>
"
2427,"<p>Grammatical induction is the art of learning a grammar from training data. We have a domain-specific-language for example a Python dialect and should create for that dialect a grammar which is able to parse the language. Parsing means, to identify for new sourcecode samples if they are valid and which tokens has which meaning. To make things more complicated the language is not a programming language but the input in a textadvanture. That means, the grammar should learn what the internal parser is doing. Additionally, the language has a meaning. That means, the user types in a sentence and at as result the avatar on the screen moves in a direction.</p>

<p>In the literature such task is called Grammar induction, because the grammar is not there and must be find from scratch. Additional the link between the language and the actions on the screen are needed. I know this is a very complicated task and it is not possible to explain this in detail. So my question goes only into the direction of a submodul of the overall system. Before it is possible to iterate a grammar some datastructure is needed to store the grammar. How can i do this? Do I need the ANTLR syntax for a grammar, can i store a grammar as a graph, as a Lisp list tree or in a SQL database? In every case, the idea is not to program the grammar by hand in software, but to iterate with a solver over possible grammars according to the input stream which is an unknown language.</p>
"
2428,"<p>What is the difference between IDS* and A* Algorithm. And  how IDS* is better than A* Algorithm. Please explain your answer with pros and cons.
Thanks</p>
"
2429,"<p>I found the below image of how a CNN works but I don't really understand it. I do understand CNNs (I think) but I find this diagram very confusing. </p>

<p>My summarized, simplified understanding : </p>

<p>-Features are selected
-Convulution is carried out so see where these features fit (repeated with every feature, in every position)
-Pooling is used to shrink large images down (select the best fit feature).
-RLU, remove your negatives
-Fully-connected layers contribute weighted votes towards deciding what class the image should be in.
-These are added together, and you have your % chance of what class the image is.</p>

<p>Confusing points of this image to me :</p>

<ul>
<li>Why are we going from one image of <code>224 x * 224 * 3</code> to two images of <code>224 * 224 * 64</code>? Why does this halving continue? What is this meant to represent? </li>
<li>It continues on to 56 * 56 * 256, why does this number continue to halve, and the number at the end, the 256, continues to double?</li>
<li>I need more clarification.</li>
</ul>

<p><a href=""https://i.stack.imgur.com/I5nj1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/I5nj1.png"" alt=""Image""></a></p>
"
2430,"<h3>(Un-original) idea:</h3>

<p>Wouldn't it be cool if we could fact-check using an algorithm that could understand a whole bunch of documents (e.g. scientific papers) as higher-order logic?</p>

<h3>Question:</h3>

<p>What work has been done on this to date?</p>

<h3>What I've got so far:</h3>

<p>(1) I seem to recall there being prior work to create a subset of English (I think intended for use in scientific writing) that could be easily interpreted by an algorithm. This doesn't quite get us to the algorithm described above (as it's restricted to a subset of English) - but seems pertinent.</p>

<p>(2) Once parsed, I guess a resolution algorithm like that in <a href=""https://en.wikipedia.org/wiki/Prolog"" rel=""nofollow noreferrer"">Prolog</a> could be used to check wether a fact (presumably also inputted as a logical statement) contradicts the logic of the documents?</p>
"
2431,"<p>As the title says, I want to train a Jordan network (i.e. a particular kind of <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow noreferrer"">recurrent neural network</a>) using a certain number of time series.</p>

<p>Let's say that <span class=""math-container"">$x_1, x_2, \ldots x_N$</span> are <span class=""math-container"">$N$</span> input time series (i.e. <span class=""math-container"">$x_i = [x_{i,1}, x_{i,2}, \ldots, x_{i,T}]$</span>, where <span class=""math-container"">$T$</span> is the length of the time series) and <span class=""math-container"">$y_1, y_2, \ldots y_N$</span> (i.e. <span class=""math-container"">$x_i = [y_{i,1}, y_{i,2}, \ldots, y_{i,T}]$</span>) are the corresponding target time series. </p>

<p>More specifically, the target time series are just sequences of ""<span class=""math-container"">$0$</span>s"", which may end with sequences of ""<span class=""math-container"">$1$</span>""s. Here I show you some example:</p>

<p><span class=""math-container"">$$y_i = [0 ~ 0 ~ 0 \ldots 0 ~ 0 ~ 1 ~ 1 ~ 1 \ldots 1 ~1 ], $$</span>
<span class=""math-container"">$$y_i = [0 ~ 0 ~ 0 \ldots 0 ~ 0]. $$</span></p>

<p>This means that I want that my machine ""learn to raise"" under some situations related to the corresponding inputs <span class=""math-container"">$x_i$</span>. Indeed, the objective of my network is to ""raise"" an alarm if ""something"" happens.</p>

<p>At the moment, my training strategy is the following. I create a new time series which corresponds the <strong>concatenation</strong> of all the available <span class=""math-container"">$x_i$</span> and <span class=""math-container"">$y_i$</span>. Let's call the concatenated series <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span>. Then I use <span class=""math-container"">$X$</span> and <span class=""math-container"">$Y$</span> to train a network.</p>

<p>Here is my problem. If I concatenate, then I also teach to my machine to ""drop"", since I can have situation like this:</p>

<p><span class=""math-container"">$$Y = [ \ldots 1 ~ 1 ~ 0 ~ 0 \ldots].$$</span></p>

<p>Is this really a problem? Are there other ""training strategies"" to be employed so that I avoid this kind of unwanted behaviors?</p>
"
2432,"<p>What is the actual learning algorithm: back-propagation or gradient descent (or, in general, the optimization algorithm)?</p>

<p>I am reading through chapter 8 of Parallel Distributed Processing hand book and the title of the chapter is ""Learning internal representation by error propagation"" by PDP research Group. 
<a href=""https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf"" rel=""nofollow noreferrer"">https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf</a></p>

<p>If there are no hidden units, no learning happens.
If there are hidden units, they learn internal representation by propagating error back.
Does this mean back propagation[delta rule] is the learning rule and gradient descent is an optimization algorithm used to optimize cost function?</p>
"
2433,"<p>Is there any good langauge rather than python on artificiall intelligence so that there can be even more scope in AI.</p>
"
2434,"<p>I am confused by how HER learns from unsuccessful trajectories. I understand that from failed trajectories it creates 'fake' goals that it can learn from. </p>

<p>Ignoring HER for now, if in the case where the robotic arm reaches the goal correctly, then the value functions (V) and action-value functions (Q) that correspond to the trajectories that get to the goal quicker will increase. These high Q and V values are ultimately important for getting the optimal policy.</p>

<p>However if you create 'fake' goals from unsuccessful trajectories - that would increase the Q and Vs of the environment that lead to getting the 'fake' goal. Those new Q and Vs would be unhelpful and possibly detrimental for the robotic arm to reach the real goal.</p>

<p>What am I misunderstanding?</p>
"
2435,"<p>I am working on the blockchain technology and I am not very familiar with the AI concept. </p>

<p>The proposal of this web page: (<a href=""http://www.euraxess.lu/jobs/349354"" rel=""nofollow noreferrer"">http://www.euraxess.lu/jobs/349354</a>) opens a discussion about use of <strong>nature inspired artificial intelligent methods for traceability chain decision in blockchain</strong> technology as an <strong>alternative to current consensus</strong> mechanisms. It continues as follows:</p>

<blockquote>
  <p><em>""<strong>It has been demonstrated</strong> that using <strong>traceability chain</strong> is a more
  effective method. In traceability chain, since the mechanism has to
  trace related information among participant’s nodes across the entire
  chain, the extraction and recognition of the data features plays a
  crucial role in improving the efficiency of the process.""</em></p>
</blockquote>

<p>However, it does not give any example to demonstrate <strong>an instance</strong> of this approach. So, I searched in google.scholar and any ordinary web pages to find only an instance similar to this approach since it has mentioned: ""<strong><em>It has been demonstrated</strong> that using <strong>traceability chain</strong> is a more effective method.</em>"" (Please read the web page)   </p>

<p>Is someone here familiar with this approach? And if yes, Is there any article/example to explain a more about this approach of consensus in blockchain? And what does exactly mean ""traceability chain""? And also ingeneral, can we call this approach as a consensus? The text is not really clear to me.</p>

<p><strong>Please not that I have no idea about this proposal and just I'd like to know if it's practicable? or it's buzzwords?</strong></p>

<p><strong>Also, may this approach related to the approach that has been mentioned in this answer (Swarm Intelligence):</strong> <a href=""https://ai.stackexchange.com/a/1315/19910"">https://ai.stackexchange.com/a/1315/19910</a> <strong>Or it is a different concept?</strong></p>

<p>Thanks for your help</p>
"
2436,"<p>Question: Express each of the following tasks in the framework of learning from data by specifying the input space X, output space Y, target function f:X->Y and the specifics of the data set that we will learn from.</p>

<p>a)Medical diagnosis: A patient walks in with a medical history and some symptoms , and you want to identify the problem.</p>

<p><strong>*My answer: Input space: Medical history. Output space:Symptoms. target function: Identify problem: Medical history -> Symptoms *</strong></p>

<p>b)Handwritten digit recognition(for example postal zip code recognition for mail sorting)</p>

<p><strong><em>My answer: Input space: postal zip code. Output space: Handwritten digit recognition. Target function: Mail sorting</em></strong></p>

<p>c) Determining if an email is spam or not.</p>

<p><strong><em>My answer Input space: email, Output space: spam or not. Target function determining</em></strong></p>

<p>d)Predicting how an electric load varies with price,temperature,and day of the week.</p>

<p><strong><em>My answer: Input space:Price,temperature and day of the week. Output space: Electric load. Target function: prediction.</em></strong></p>

<p>e)A problem of interest to you for which there is no analytic solution, but you have data from which to construct an empirical solution.</p>

<p><strong><em>No answer</em></strong></p>

<p>This is my question and I provide answer of those question except the last one. </p>

<p>My question is that my answers are correct or not?</p>
"
2437,"<p>I have three equations that relates five variables <em>{a, b, c, r, s}</em> with a sum and two ratios.</p>

<pre>
Eq. 1: a = b + c;
Eq. 2: s = b / a;
Eq. 3: r = b / c.
</pre>

<p>Given two values for any of the five variables I get a solution. But, this is not the automation problem I want to solve.</p>

<p>I can have the solution of variable <em>r</em> by simply knowing <em>s</em>. This is solved by a ""human algorithm"" as follows.</p>

<ol>
<li>Substitute a of Eq. 1 in Eq. 2.
<li>Divide the second term of the new Eq. 2 by the variable c.
<li>Replace b/c by the expression of Eq. 3.
</ol>

<p>That means s = r / (r+1).</p>

<p>The questions is -How can an AI algorithm solve this?, i.e. the machine should recognize that given the variable <em>r</em> she can obtain directly the variable <em>s</em> and do no require another variable.</p>
"
2438,"<p>I have something I am not entirely clear on. If the number of input neurons and output neurons doesn't change, what will change if I have one hidden layer, but first with 1 neuron, then with 4 neurons? </p>

<p>Taking into consideration the fact that each perceptron is able to linearly separate points on an unknown/unwritten linear function, would this then be able to, theoretically, instead of simply linearly separate points, separate points into those that occur inside a square, and those that occur outside?</p>

<p>This is, of course, without a bias neuron present.</p>

<p>Edited: Formatting for better understanding and answers. Removed unnecessary elements.</p>
"
2439,"<p>In hill climbing methods, at each step, the current solution is replaced with the best neighbour (that is, the neighbour with highest/smallest value). In simulated annealing, ""downhills"" moves are allowed.</p>

<p>What are the advantages of simulated annealing with respect to hill climbing approaches? How is simulated annealing better than hill climbing methods?</p>
"
2440,"<p>What are the limitations of the <em>hill climbing algorithm</em>? How can we overcome these limitations?</p>
"
2441,"<p>I am new in artificial intelligence and want to getting start with deep learning in Python. I know about the basics of Python. What should i have to learn more?</p>
"
2442,"<p>I am not sure the name of this kind of problem, but anyway, the situation is as below.</p>

<p>Assign teachers into Groups and consider on each of their workload, availability etc.
There are some other soft/hard constraint (equality/inequality) like</p>

<ul>
<li>Each group should have at least 2 teachers</li>
<li>Everyone in the group have similar workload</li>
<li>Total workload in the group is below a certain value</li>
<li>All are in different expertise</li>
</ul>

<p>and more...</p>

<p>I am trying to build a sub-optimal solution to solve this problem. Linear/non-linear programming seems not working for grouping problems. I am thinking of genetic algorithm or reinforcement learning.</p>

<p>Can this problem solve by using RL or DRL?
I am trying to define the groups as state, and actions include ""assignToGroup"" and ""removeFromGroup"".
And any kind of idea or suggestion of how to solve this problem?</p>

<p>Many thanks</p>
"
2443,"<p>Is there neural machine translation methods, that for one input sentence outputs multiple alternative output sentences in that target language. It is quite possible, that sentence in source language have multiple meanings and it is not desirable that neural network discards some of the meanings if there is no context for disambiguation provided. How multiple outputs can be acommodated into encode-decoder architecture, or different architecture is required?</p>

<p>I am aware of only one work <a href=""https://arxiv.org/abs/1805.10844"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1805.10844</a> (and one referen herein) but I am still digesting whether their network outputs multiple sentences or whether it just acommodates variations during training phase.</p>
"
2444,"<p>There are 4 kinds of adverbs :</p>

<ul>
<li>Adverbs of Manner. For example, slowly, quietly</li>
<li>Adverbs of Place. For example, there, far</li>
<li>Adverbs of Frequency. For example, everyday, often</li>
<li>Adverbs of Time. For example, now, first, early</li>
</ul>

<p>nltk, spacy and textblob only tag a token as an adverb without specifying which kind it is.</p>

<p>Are there any libraries which tag including the type of adverb?</p>
"
2445,"<p>I am reading the book titled ""<a href=""https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view"" rel=""nofollow noreferrer"">Reinforcement Learning: An Introduction</a>"" (by Sutton and Barto). I am at chapter 5, which is about Monte Carlo methods, but now I am quite confused.</p>

<p>There is one thing I don't particularly understand. Why do we need the state-transition probability function when calculating the importance sampling ratio for off-policy prediction?</p>

<p>I understood that one of the main benefits of MC over Dynamic Programming (DP) is that one does not need to have a model of the state-transition probability for a system. Or is this only the case for on-policy MC?</p>
"
2446,"<p>Local search algorithms are useful for solving pure optimization problems, in which the aim is to find the best state according to an objective
function. My question is what is the objective function ?
<img src=""https://i.stack.imgur.com/y459H.jpg"" alt=""Hill Climbing""></p>
"
2447,"<p>I am building a generative model chatbot as a research and learning project. One of the most important parts of my project is to research ways in which I can make this chatbot work in a consistently ethical fashion. </p>

<p>This chatbot is simply a single Seq2Seq network running on my local machine. It can't be interacted with over the internet (yet), although I may end up creating a way to do that. It has no feedback loops of any kind as of right now, though reinforcement learning with a loop might be helpful.</p>

<p>The idea is that there would be some sort of unchanging knowledgebase for the chatbot to use that has hard-coded ethical statements and values that the bot has no ability to change. Whenever a question is asked of the chatbot,before being inputted to the network, the knowledgebase is searched and relevant facts will be appended to the input (separated from regular inptu by  tokens. </p>

<p>My question is, will this even be effective at allowing it to generate its own responses yet still be confined to the ethical standards given it?</p>

<p>My main concern is that it may begin to ignore these ""facts"" over time, and they may become irrelevant. </p>

<p>Another (possibly much better) approach might be to use deep reinforcement learning. However I may find it difficult to implement with my existing Sequence-to-Sequence network. </p>

<p>So which would likely be better? Or perhaps I should try a combination of the two?</p>
"
2448,"<p>Let's say there are two types of cancer(Type 1 and Type 2). Say we want to see if one of pour friends has cancer Type 1 or 2. We can treat this as a classification problem. But what if we use unsupervised learning (clustering) to separate the data into to 2 different groups and see each whether each item in group 1 belongs to a person with cancer Type 1 or 2. We will then  see whether our friend belongs to group 1 or 2. I know it is stupid to do this and we have to do extra work but can we even do this?</p>

<p>Let's say that the features are only the age and the height (I know it's really dumb but just bear with me). The data associated with people with cancer Type 1 is <code>[10, 150], [12, 153], [9, 143], [13, 160]</code> and for people with Type 2 cancer : <code>[20, 175], [23, 180], [19, 174]</code>. Let's say we plot the data on a graph (without labelling the data) and the unsupervised program (Clustering) just separates the two groups (Say group 1 for Type 1). We then can see that to whom each data in group 1 belongs. We see those people have cancer Type 1. So given new data, we see what group our friend  belongs to. If she/he belonged to group 1, he's got cancer Type 1 and if not, she/he has cancer Type 2.</p>
"
2449,"<p>With the advancement of deep learning and a few others automated features learning techniques, manual feature engineering started becoming obsolete. </p>

<p>Any suggestion on when to use manual feature engineering, feature learning or a combination of the two?</p>
"
2450,"<p>I'm trying to implement a Deep Q-network in Keras/TF that learns to play Minesweeper (our stochastic environment). I have noticed that the agent learns to play the game pretty well with both small and large board sizes. However, it only converges/learns when the layout of the mines is the same for each game. That is, if I randomize the mine distribution from game to game, the agent learns nothing - or near to it. I tried using various network architectures and hyperparameters but to no avail.</p>

<p>I tried a lot of network architectures including:</p>

<ol>
<li>The input to the network is the entire board matrix, with the individual cells having values of -1 if unrevealed, or 0 to 8 if revealed.</li>
<li>The output of the network is also the entire board representing the desirability of clicking each cell.</li>
<li>Tried Fully connected hidden layers (both wide and deep)</li>
<li>Tried Convolutional hidden layers (tried stacked them, using different kernel sizes, padding..)</li>
<li>Tried adding Dropout after hidden layers too</li>
</ol>

<p><strong>Is DQN applicable for environments that change every episode or have I approached this from the wrong side?</strong></p>

<p>It seems no matter the network architecture, the agent won't learn. Any input is greatly appreciated. Please let me know if you require any code or further explanations. Thank you.</p>
"
2451,"<p>The domain of emergency call for clogged pipelines has to do with taking a call and managing the reaction of plumber departments. It is mostly a group oriented communication situation between the caller, the first level call taker, the second level dispatcher and external stations in the back office. </p>

<p>From a linguistic point of view, there are different kind of speech acts available. For example paraphrasing which is the repetition of previous speech with own words, or counter-speech which is criticizing something said before. Modeling all the different social roles, their usage of speech acts and make the overall decision process transparent is a difficult task. </p>

<p>I've searched a bit for existing papers about the subject, but it seems that the domain wasn't explored yet. My question is: Is it possible to create some kind of chatbot population which talks to each other back and forth and is able to simulate an emergency dispatching task which includes conflicts between the operators and contrasting point of views about how to handle a certain situation under resource limitations?</p>
"
2452,"<p>What is the difference between breadth first search and recursive best first search ? How can I describe the key difference between them?</p>
"
2453,"<p>Recently, I have come across the information (lecture 8 and 9 about MDPs of <a href=""http://ai.berkeley.edu/lecture_slides.html"" rel=""nofollow noreferrer"">this UC Berkeley AI course</a>) that the time complexity for each iteration of the value iteration algorithm is <span class=""math-container"">$\mathcal{O}(|S|^{2}|A|)$</span>, where <span class=""math-container"">$|S|$</span> is the number of states and <span class=""math-container"">$|A|$</span> the number of actions.</p>

<p>Here is the equation for each iteration:</p>

<p><span class=""math-container"">$$
V_{k+1}(s) \gets \max_a \sum_{s'} T(s, a, s') [R(s, a, s') + \gamma V_k(s')]
$$</span></p>

<p>I could't understand why the time complexity is <span class=""math-container"">$\mathcal{O}(|S|^{2}|A|)$</span>. I searched the internet, but I didn't find any good explanation. </p>
"
2454,"<p>I'm new to machine learning and I was watching a video about gradient descent.It said that we want our cost function(Mean squared error) to have the minimum value but that minimum value shown in the graph wasn't 0;it was a negative number!How can our cost function which is mean squared error have a value under 0?The square of a real number is always positive.Even if it is possible, don't we want our error to be 0?
Can someone explain it please?
P.S: The video was Prof.Ng's machine learning course.
Thank you in advance for any help!</p>
"
2455,"<p>Apparently, in the Q-learning algorithm, the Q values are not updated according to the ""current policy"", but according to a ""greedy policy"". Why is that the case? I think this is related to the fact that Q-learning is off-policy, but I am also not familiar with this concept. </p>
"
2456,"<p>I have done a lot of research on the internet about Reinforcement Learning and I found encountered methods of Reinforcement Learning: Q-Learning and Deep Q-Learning. And I have developed a vague idea of how these two work.</p>

<p>Before I knew anything about Reinforcement Learning this is how I thought it would work:</p>

<p>Suppose I have 2 virtual players in a game who can shoot each other, one of them is a decent playing hard-coded/pre-coded AI, and the other one is the player I want to train (to shoot the other player and dodge his bullets), the aim of the game would be to get the greatest net score (shots you hit minus shots you took) within 1 minute (a session), and you only have 20 bullets.</p>

<p><a href=""https://i.stack.imgur.com/Y3Fba.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y3Fba.png"" alt=""A basic idea of the the game I mean""></a></p>

<p>You have 3 actions, move left-right <code>(0 = maxleftspeed, 1 = maxrightspeed )</code>, jump <code>(0=don't jump, 1=jump)</code>, shoot <code>(0 = don't shoot, 1 = shoot)</code>.</p>

<p>What I thought was, you could create a basic Feed-Forward Neural Network use the enemy's position and his bullet(s)'s position(s) and bullet direction(s) for the input layer and the action being taken will be given by the (3 nodes in the) output layer.</p>

<p>The untrained player starts off with a randomized algorithm, then (for back-propagation) at the end of each session it modifies one of the parameters by a bit in the neural network, and a new session is started with the slightly modified NN. If this session ends with more points than the previous session, it keeps the changes and makes more changes towards that direction, otherwise, the changes are redone, or possibly reversed. I would visualise this as gradient descent similar to that of Supervised learning.</p>

<p>So my questions are:</p>

<ul>
<li>Is something like this already out there? What is it called?</li>
<li>If nothing like this is out there, could you give me any tips to optimize this method or point out any key points I should keep in minds while carrying this out?</li>
<li>Since I have written this game, I have control over the speed of the actions, but if I did not, I know this AI would take ages to learn, so is there any way to make the learning faster while still keeping the basic idea in mind?</li>
<li>How exactly is this different from deep Q-learning (if it is)?</li>
</ul>

<p>Thanks in advance!</p>
"
2457,"<p>First of all i'm very new to the field. maybe my question is a bit too naive of even trivial..</p>

<p>I'm currently trying to understand how can i go about recognizing different faces.
Here is what i tried so far and the main issues with each approach:</p>

<p>1) Haar Cascade -> HOG -> SVM:
  The main issue is that the algorithm becomes very indecisive when more than 4 people are trained.. the same occurs when we change Haar Cascade for a pre-trained CNN to detect faces..  </p>

<p>2) dlib facial landmarks -> distance between points -> SVM or Simple Neural Network Classification:
  This is the current approach and it behaves very well when when 4 people are trained.. when more people are trained it becomes very messy, jumping from decision to decision and never resolves to a choice.</p>

<p>I've read online that Triplet loss is the way to go.. but I very confused as to how id go about implementing it.. can i use the current distance vectors found using Dlib or should i scrap everything and train my own CNN?</p>

<p>If i can use the distance vectors how would i pass the data to the algorithm? is Triplet loss a trivial neural network only with it's loss function altered? </p>

<p>I've took the liberty to show exactly how the distance vectors are being calculated:</p>

<p><a href=""https://i.stack.imgur.com/s7zGU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s7zGU.png"" alt=""The green lines represent the distances being calculated""></a></p>

<p>The green lines represent the distances being calculated
A 33 float list is returned which is then fed to the classifier</p>

<p>Here is the relevant code for the classifier (Keras):</p>

<pre><code>def fit_classifier(self):
    x_train, y_train = self._get_data(self.train_data_path)
    x_test, y_test = self._get_data(self.test_data_path)
    encoding_train_y = np_utils.to_categorical(y_train)
    encoding_test_y = np_utils.to_categorical(y_test)
    model = Sequential()
    model.add(Dense(10, input_dim=33, activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(40, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(max(y_train)+1, activation='softmax'))
    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])
    model.fit(x_train, encoding_train_y, epochs=100, batch_size=10)
</code></pre>

<p>I think this is a more theoretical question than anything else.. if someone with good experience in the field could help me out i'd be very happy! </p>
"
2458,"<p>Lets say we have a data-set of all cats and we have to identify the cat breed based on given test image. As, the two different cat breeds have visual similarity can we use existing networks (VGG, ImageNet, GoogleNet) to solve this problem?</p>

<ol>
<li>Should faceNet be applied here? As, the problem is similar to face detection where face characteristics of two different people are same yet it can correctly recognize a person.</li>
<li>What if with visual similarity in data-set we have only few example of each class? Like for a problem (random) we have good amount of data but for each class we have only few examples.</li>
</ol>

<p>Is there any model that can be applied here?</p>
"
2459,"<p>This question isn´t about a problem, so i apologize for that. I want to learn everything related to AI but i dont know where to start. I am studying computer science, first year of the career. Any advice, or a good book, or something that can help to start from 0? Thanks for your time.</p>
"
2460,"<p>In Q-learning, during training, it doesn’t matter how I select actions. The algorithm always converges to optimal optimal policy. Why does this happen?</p>
"
2461,"<p>Are there examples of applications in blockchain consensus using swarm intelligence, as opposed to classical consensus mechanisms like PoW or PBFT?</p>

<p><strong>Please note</strong> that recent classical consensuses, including <em>lottery-based</em> such as <strong>PoW</strong> in which the winner of lottery creates the new block, or <em>voting-based</em> such as <strong>PBFT</strong> or <strong>Paxos</strong> in which the entities achieve a consensus through a voting process; both approaches have problems of efficiency, latency, scalability, performance etc. And emerging a new alternative approach seems necessary. In this way, can <strong>nature-inspired</strong> algorithms (such as <strong>evolutionary algorithms</strong> (EA), <strong>particle swarm optimization</strong> (PSO), <strong>ant colony optimizatio</strong>n (ACO) etc) be employed as an alternative to classical consensus algorithms? </p>
"
2462,"<p>I am reading the paper on <span class=""math-container"">$\ell_0$</span> regularization of DNNs by Louizos, Welling and Kingma (2017) (<a href=""https://arxiv.org/abs/1712.01312"" rel=""nofollow noreferrer"">Link</a> to arxiv).</p>

<p>In Section 2.1 the authors define the cost function as follows: <span class=""math-container"">$$ \mathcal{R}\left( \tilde{\theta}, \pi \right) = \mathbb{E}_{q(z|\pi)}\left[ \frac{1}{N} \left(\sum_{i=1}^N \mathcal{L}\left(h\left( x_i, \tilde{\theta}\circ Z\right), y_i \right) \right)\right] + \lambda\sum_{i=1}^{|\tilde{\theta}|}\pi_i. $$</span></p>

<p>In the above display, <span class=""math-container"">$\tilde{\theta}$</span> are the weights, <span class=""math-container"">$Z$</span> is a random vector of the same dimension as <span class=""math-container"">$\tilde{\theta}$</span> consisting of independent Bernoulli components <span class=""math-container"">$q(Z_i|\pi) \sim Bernoulli (\pi_i)$</span>, and <span class=""math-container"">$\circ$</span> is the element-wise product. </p>

<p>The authors then state the following:</p>

<blockquote>
  <p>the first term is problematic for <span class=""math-container"">$\pi$</span> due to the discrete nature of <span class=""math-container"">$Z$</span>, which does not allow for efficient gradient based optimization. </p>
</blockquote>

<p>I am not sure I understand this. Denoting the first term by <span class=""math-container"">$\mathcal{R}_1 = \sum_{i=1}^N \frac{1}{N}R_i$</span> (<span class=""math-container"">$R_i$</span> defined below), and using the notation <span class=""math-container"">$\pi_z = \prod \pi_i^{z_i} (1-\pi_i)^{1-z_i}$</span> and <span class=""math-container"">$\mathcal{Z}$</span> for the set of all possible values of <span class=""math-container"">$Z$</span>, we should have 
<span class=""math-container"">$$
R_i := \mathbb{E}_{q(z|\pi)}\left[ \mathcal{L}\left( h\left(x_i, \tilde{\theta}\circ z\right), y_i \right) \right] = \sum_{z \in \mathcal{Z}}\pi_z\mathcal{L}\left( h\left(x_i, \tilde{\theta}\circ z\right), y_i \right)
$$</span></p>

<p>So, it seems to me that the gradient of <span class=""math-container"">$R_i$</span> with respect to <span class=""math-container"">$\pi_j$</span> can be obtained as 
<span class=""math-container"">$$
\frac{d R_i}{d\pi_j} = \sum_{z \in \mathcal{Z}}\mathcal{L}\left( h\left(x_i, \tilde{\theta}\circ z\right), y_i \right) \frac{d\pi_z}{d\pi_j}
$$</span>
and 
<span class=""math-container"">$\frac{d\pi_z}{d\pi_j} = \frac{\pi_z}{\pi_j}$</span> if <span class=""math-container"">$z_j=1$</span> and <span class=""math-container"">$-\frac{\pi_z}{1-\pi_j}$</span> if <span class=""math-container"">$z_j=0$</span>. So, it appears that we can obtain the derivative of the first term with respect to <span class=""math-container"">$\pi_j$</span> as well. </p>

<p>My question is the following:</p>

<blockquote>
  <p>If my above calculation is correct, then the derivatives <span class=""math-container"">$\frac{d\mathcal{R}_1}{d\pi_j}$</span> can be computed, and we can perform SGD on the cost function <span class=""math-container"">$\mathcal{R}(\tilde{\theta}, \pi)$</span>. But the authors claim that it cannot be obtained and hence they introduce the `hard concrete' distribution etc. to construct a differentiable cost function. </p>
</blockquote>
"
2463,"<p>Apparently, one can buy a special-purpose integrated circuit (an IC like <a href=""https://software.intel.com/en-us/neural-compute-stick"" rel=""nofollow noreferrer"">this one,</a> for instance) to host a convolutional neural network.</p>

<p><strong>QUESTION</strong></p>

<p>Is such a circuit digital? Except for digital random-number generation, is the circuit's behavior deterministic?</p>

<p>What <em>kind</em> of hardware is this?</p>

<p><strong>REFERENCE</strong></p>

<p>Already asked and answered: <a href=""https://ai.stackexchange.com/q/7328/20018"">""If digital values are mere estimates, why not return to analog for AI?""</a></p>

<p><strong>FURTHER DETAILS</strong></p>

<p>What I mean is this: suppose that you and I each acquired a special-purpose neural IC, same make, same model. Suppose that you and I configured and trained our circuits alike. Except for digital random-number generation, would our neural nets behave identically? Or would analog effects cause our nets to turn oppositely on some closely balanced edge case?</p>

<p>If it is plain to you that a misconception has caused me to pose the question improperly, then a correction to the question along with an answer would be appreciated.</p>

<p>If you wish to know my background, I am an electrical power engineer in my 50s.</p>
"
2464,"<p>I did some self-study to learn Neural network, object detection, and deep learning. Now I started implementing YOLOv3. I am looking for some website that I can communicate with people and make friends who are learning and implementing deep learning algorithms like me. 
It will speed up my learning since self-studying gets boring sometimes. </p>

<p>I was wondering if you could introduce some website that I can find peoples like myself.</p>
"
2465,"<p>Is AI limited by the fact that it requires us to give it a task or goal to achieve? It has all the capability to get to that goal in ways we might not think of but it still only gets to a goal we can imagine, how do we get AI to think of goals or tasks that go beyond us? Do we create a sense of 'Passion' in the AI to drive it to better than the goal it is given, if so how do we quantify a goal we see as 100% of our need when in fact it could be only a fraction of that?</p>
"
2466,"<p><a href=""https://en.wikipedia.org/wiki/Hebbian_theory"" rel=""nofollow noreferrer"">Hebb's postulate</a> attempts to explain associative learning via the processes of sampling (using sensors), emitting responses and receiving feedback. This is a form of <a href=""https://en.wikipedia.org/wiki/Control_system"" rel=""nofollow noreferrer"">control orientated architecture</a> using an environment and biological systems. Is this the progenitor of AI?</p>
"
2467,"<p>I read about the hill climbing algorithms, the simulating annealing algorithm, but I am confused. What is the basic purpose of local search methods?</p>
"
2468,"<p>I was watching a video which tells a bit about reinforcement learning, and I learnt that If the robot makes wrong movement then they train the network with negative learning rate. From this method, something came to my mind.</p>

<p>My question is ""Can I use a wrong data to train a neural network?"".</p>

<p>To illustrate the method, I'll be using the eye tracker project that I'm working on right now. In my project There are photos and the points that corresponds the locations that I m looking to at that photo. Its like grid (9, 16). If I look to the middle of the screen, it means the output is (4, 7.5). if I look left up side of the screen it means (0,0). Normally for a photo that I'm looking into the middle, we use that photo as input and (4, 7.5) as output to train network using positive learning rate. Now let me rephrase the question. Can I train a model giving a photo that I'm looking into the middle as input and (0,0) as output(label) using negative learning rate?</p>

<p>Thank you, If I made a mistake against the rules of stackoverflow, I'm so sorry. I'll be waiting your valuable answers. </p>

<p><strong>Edit: this is a conversation between me and someone from stackoverflow, I'll let you read, hope you get a point.</strong></p>

<p>-> Yes, you can. But, what would be the reason of passing a wrong ground truth to your training process? – Neb 14 hours ago </p>

<p>-> If I have no various data to train, I can create more data via this method to increase the certainty when I use squared error loss. But I have doubts about this method. for example lets assume we have a photo named 'X' and its label is (5,5). at first epoch, Let the model gives (2,2) for photo 'X'. if I try to train network with a photo X and label -> (4,4) using negative learning rate, it might send away the point from (2,2) to (1,1) whereas we expect it to send the point (2,2) to (5,5). Did you get what I meant? – Faruk Nane 14 hours ago   </p>

<p>-> You are right. Using a negative learning rate and a wrong ground truth will not necessarly make the learning process converge to the optimal value for your net's parameters – Neb 13 hours ago </p>

<p>-> So can I say that ""when I'm sure that the absolute error for each case is less than 2, I can use this method using points away 2 units."" So It'll make the outputs closer to the target point. I don't really know if we can easily say that. because we consider this method as if there are only 2 parameters which is the output point. However a model has many parameters so It might affect so differently. My brain is so confused. I think this might be an academic work, right? – Faruk Nane 13 hours ago   </p>

<p>-> Well, it is difficult to suggests you the path to follow without knowing the exact specifics of your problem. In any case, if you're trying to solve this problem for fun or self-improvement, I'd suggest you to experiment with the solutions you came up with and see if they works. – Neb 13 hours ago </p>

<p>//EDIT: UP UP</p>
"
2469,"<p>In the context of <em>evolutionary computation</em>, in particular <em>genetic algorithms</em>, there are two stochastic operations ""mutation"" and ""crossover"". What are the differences between them?</p>
"
2470,"<p>In genetic algorithm, there are different steps. One of those steps is selection of chromosomes for reproduction (Evolution). In this step there are different methods are used for selection of chromosomes, one of them is ""roll-it selection "". What are the other methods ?</p>
"
2471,"<p>I'm working in a company that opens restaurants in enterprises.
Every day at lunch, we want our clients to be able to scan their trays, sothat the food is detected automatically thanks to AI / image recognition.</p>

<p>Technically speaking, we have a number of food items that grows over time in our database, but everyday there are about 30 items available at the same time in the restaurant. About 5 items are changing each day (for example, the main dish changes, but the bottle of water is always the same).</p>

<p>This means, when the client go to the till to pay, the client will place the tray by himself, the camera will take photos of the tray and will try to identify different items separetely among the 30 items available this day.
Clients pay per food article, which means we don't need to track the weight or the quantity in the main dish.</p>

<p>I have absolutely no experience in AI/ML and don't know how to start for my need, I'm a web developer.</p>

<ul>
<li>Which tool should I look at first? </li>
<li>Which skill do I need to acquire? I mean, are there easy to use high level libraries , or do I need to learn ML from scratch?</li>
</ul>

<p>First I was thinking of Amazon Recognition or Google Vision but It seems to be made for recognizing ANY food item among their own database. My need seems easier since I just need to recognize several items on a tray among 30 known items.</p>

<p>Thanks a lot for your help.</p>
"
2472,"<p>What are the many ways that artificial intelligence robots protect their existence?</p>

<p>Isaac Asimov's ""Three Laws of Robotics""</p>

<p>A robot may not injure a human being or, through inaction, allow a human being to come to harm.</p>

<p>A robot must obey orders given it by human beings except where such orders would conflict with the First Law.</p>

<p>A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.</p>
"
2473,"<p>I am using PPO with an LSTM agent. My agent is performing 10 actions for each episode, one action is corresponding to one LSTM timestep and the action space is discrete. I have only one reward per episode which I can compute after the last action of the episode.</p>

<p>For each timestep (~ action) my agent has 20 choices. The following plot shows the reward (y-axis) versus the current episode (x-axis).
The plot shows a decreasing reward because I want to mimize this reward so I use: minus of the true reward.</p>

<p>At the beginning of the process, the agent seems to learn very well and the reward is decreasing but then it's converging to a value which is not the best. When I look at the results of my experiment it appears that the index of all actions are same (for example the agent is always choosing the second value of my discrete action space).</p>

<p>Does anyone have an idea about what is happening here ?</p>

<p><a href=""https://i.stack.imgur.com/1wdcR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1wdcR.png"" alt=""reward versus episode""></a></p>
"
2474,"<p>In a paper published 6 years ago a novel domain specific language (DSL) for character animation was presented. High-level action commands are described in the XML format for authoring a virtual human. The problem is, that such kind of framework is to complex for the reality. The predecessor, “Confusius” which was used for animation of a shadow play, was not very well documented and this new software XSAMPL3D has the same bottleneck. The XML schema for describing possible actions doesn't make much sense, because it's not possible to combine the motion primitives to a longer sequence. And the gantt chart in the middle of the paper looks a bit outdated. The idea is perhaps, to arrange the tasks on a timeline, but how can this be realized in XML? If we are looking how often the paper was cited, we will recognize that it doesn't attracted a large audience. A possible reason is that the concept of using a DSL can't be transferred to in-game AI problems, for example to play Atari game with the OpenAIgym.</p>

<p>Am i right?</p>

<ul>
<li>Vitzthum, Arnd, et al. ""Xsampl3d: An action description language for the animation of virtual characters."" JVRB-Journal of Virtual Reality and Broadcasting 9.1 (2012).</li>
</ul>
"
2475,"<p>Describe the basic algorithm for performing local search. Give an example of a search problem for which it is an appropriate solution. When would an algorithm such as A* search be preferred? </p>
"
2476,"<p>When feature scaling for a <a href=""http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex3/ex3.html"" rel=""nofollow noreferrer"">Multivariate Linear Regression</a> housing predictor (housing area and # bedrooms), the exercise suggested ""scale both types of inputs by their standard deviations and set their means to zero"" </p>

<p><a href=""https://i.stack.imgur.com/FOuls.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FOuls.png"" alt=""enter image description here""></a></p>

<hr>

<h1>MATLAB METHOD 1:</h1>

<pre><code>x = [ones(m, 1), x];
sigma = std(x);
mu = mean(x);
x(:,2) = (x(:,2) - mu(2))./ sigma(2);
x(:,3) = (x(:,3) - mu(3))./ sigma(3);
</code></pre>

<h1>MATLAB METHOD 2:</h1>

<p>Why not simply scale inputs between zero and one?  i.e. divide by the maximum:</p>

<pre><code>x_range=max(x)
x(:,2) = (x(:,2)/x_range(2));
x(:,3) = (x(:,3)/x_range(3));
</code></pre>

<h1>METHOD 2 FEATURE PLOTS:</h1>

<p>The exercise can be reproduced on MATLAB or OCTAVE:</p>

<p><a href=""https://i.stack.imgur.com/fdhSG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fdhSG.png"" alt=""enter image description here""></a></p>

<h1>QUESTION</h1>

<p>Is there a computational advantage with the first method over the second method?</p>
"
2477,"<p>I'm trying to train a DQN, so I'm using OpenAI gym and Breakout (<a href=""https://gym.openai.com/envs/Breakout-v0/"" rel=""nofollow noreferrer"">Breakout-v0</a>).</p>

<p>I have altered the reward supplied by the environment: If the episode is not completed fully, the agent gets a -10 reward. Could be this counterproductive for learning?</p>
"
2478,"<p>If we take the vanilla variational auto-encoder (VAE), we <span class=""math-container"">$p(z)$</span> is a Gaussian distribution with zero mean and unit variance and we approximate <span class=""math-container"">$p(z|x) \approx q(z|x)$</span> to be a Gaussian distribution as well, for each latent variable <span class=""math-container"">$z$</span>.</p>

<p>But what if <span class=""math-container"">$z$</span> is a discrete variable? What kind of distributions can be used to model discrete latent variables? For example, what kind of distribution can be used to model <span class=""math-container"">$p(z)$</span> and <span class=""math-container"">$p(z|x) \approx q(z|x)$</span>?</p>
"
2479,"<p>I've just started studying genetic algorithms and I'm not able to understand why a genetic algorithm can improve if, at each learning, the 'world' that the population encounters change. For example, in this demo (<a href=""http://math.hws.edu/eck/js/genetic-algorithm/GA.html"" rel=""nofollow noreferrer"">http://math.hws.edu/eck/js/genetic-algorithm/GA.html</a>), it's pretty clear to me that the eating statistics will improve every year if bunches of grass grow exactly in the same place, but instead they always grow in different positions and I can't figure out how it  can be useful to evaluate (through the fitness function) the obtained eating stats given that the next environment will be different.</p>
"
2480,"<p>Say I have a batch of examples, each examples represent a state:</p>

<pre><code>[0.1, 0.2, 0.5] #1st example
[0.4, 0.0, 0.3] #2nd example 
..........
[0.1, 0.1, 0.1] #16th example
</code></pre>

<p>I feed through the NN, and then the NN predict the following class:</p>

<pre><code>[move up]   #1st example
[move down] #2nd example
........
[move left] #16th example
</code></pre>

<p>And then I take the square loss (which calculated to be 0.1 after taking average over 16 examples), and do backward propagation. </p>

<p>So, can I assume that each of these examples will assign (or contribute) to a 0.1 loss?</p>
"
2481,"<p>This question assumes a definition of AI based on machine learning, and was inspired by this fun Technology Review post:</p>

<p><a href=""https://i.stack.imgur.com/pRyOW.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pRyOW.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://www.technologyreview.com/s/612404/is-this-ai-we-drew-you-a-flowchart-to-work-it-out/?utm_campaign=owned_social&amp;utm_source=twitter.com&amp;utm_medium=social"" rel=""nofollow noreferrer""><sub>SOURCE: Is this AI? We drew you a flowchart to work it out (Karen Hao, MIT Technology Review)</sub></a></p>

<p>As the definition of artificial intelligence has been a continual subject of discussion on this stack, I wanted to bring it to the community for perspectives.</p>

<p>The formal question here is:</p>

<ul>
<li>Is an algorithm that is no longer actively learning an AI?</li>
</ul>

<p>Specifically, can applied algorithms that are not actively learning be said to reason?  </p>
"
2482,"<p>I don't know much about AI or chess engines, but what is the fundamental difference between AlphaZero and Stockfish or Rybka?</p>
"
2483,"<p>In genetic algorithms, a function called ""fitness"" (or ""evaluation"") function is used to determine the ""fitness"" of the chromosomes. Creating a good fitness function is one of the challenging tasks in genetic algorithms. How would you create a good fitness function?</p>
"
2484,"<p>In evolutionary computation and in particular in the context of genetic algorithms, there is a stochastic operation called <a href=""https://en.wikipedia.org/wiki/Fitness_function"" rel=""nofollow noreferrer"">""fitness function""</a>. The better a state, the greater the value of the fitness function for that state. </p>

<ul>
<li>What would be a good fitness function for the <a href=""https://en.wikipedia.org/wiki/Eight_queens_puzzle"" rel=""nofollow noreferrer"">8-queens problem</a>?</li>
</ul>
"
2485,"<p>I'm new to ML / Reinforcement Learning.</p>

<p>I'm looking at source from
<a href=""https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py"" rel=""nofollow noreferrer"">https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py</a></p>

<p>where reward is calculated with</p>

<pre><code>reward = cmp(score(self.player), score(self.dealer))
</code></pre>

<p>I'm just curious why its ok to calculate reward based on hidden state?
A player only sees the dealers first card.</p>

<pre><code>self.dealer[0]
</code></pre>
"
2486,"<p>Suppose, for simplicity sake, to be in a discrete time domain with the action set being the same for all states <span class=""math-container"">$S \in \mathcal{S}$</span>. Thus, in a finite Markov Decision Process, the sets <span class=""math-container"">$\mathcal{A}$</span>, <span class=""math-container"">$\mathcal{S}$</span>, and <span class=""math-container"">$\mathcal{R}$</span> have a finite number of elements. We could then say the following</p>

<p><span class=""math-container"">$$p(s',r | s,a) = P\{S_t=s',R_t=r | S_{t-1}=s,A_t=a\} ~~~ \forall s',s \in \mathcal{S}, r \in \mathcal{R} \subset \mathbb{R}, a \in \mathcal{A}$$</span></p>

<p>where the function <span class=""math-container"">$p$</span> defines the dynamics of the finite MDP and <span class=""math-container"">$P$</span> defines the probability.</p>

<hr>

<p>How could I extend this to a general MDP? That is, an MDP where the sets <span class=""math-container"">$\mathcal{A}$</span>, <span class=""math-container"">$\mathcal{S}$</span>, and <span class=""math-container"">$\mathcal{R}$</span> haven't a finite number of elements? To be more precise, in my case <span class=""math-container"">$\mathcal{A} \subset \mathbb{R}^n$</span>, <span class=""math-container"">$\mathcal{S} \subset \mathbb{R}^m$</span>, and <span class=""math-container"">$\mathcal{R} \subset \mathbb{R}$</span>. My thought is that the equation above is still true, however, the probability is zero for each tuple <span class=""math-container"">$s',r,s,a$</span>. </p>

<p>Is it sufficient to say that for finite MDP we have</p>

<p><span class=""math-container"">$$\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1 ~~~ \forall s\in\mathcal{S},a\in\mathcal{A}$$</span></p>

<p>while in non-finite MDP (supposing that the sets <span class=""math-container"">$\mathcal{s}$</span> and <span class=""math-container"">$\mathcal{A}$</span> are continuous) we have</p>

<p><span class=""math-container"">$$\int_{s'\in\mathcal{S}}\int_{r\in\mathcal{R}}p(s',r|s,a)=1 ~~~ \forall s\in\mathcal{S},a\in\mathcal{A}$$</span></p>

<p>or is it more complex than this?</p>
"
2487,"<p><a href=""https://arxiv.org/pdf/1712.01815.pdf"" rel=""nofollow noreferrer"">The Alpha zero paper</a> says that the The first set of features are repeated for each position in a T = 8-step history. So what happens before the first 8 moves? Do they just repeat the starting position?</p>
"
2488,"<p>In CSP (Constraint Satisfaction Problem) state is a ""black box""- any data structure that supports a  successor function, heuristic function, and goal test. What is the successor function ?</p>
"
2489,"<p>Backtracking search is the basic uninformed algorithm for CSPs (constraint satisfaction problems). What are the directions along which backtracking efficiency can be improved ?</p>
"
2490,"<p>I am trying to build a Q learning-based bot for board games, specifically monopoly. I am fairly new to Q-learning and currently, I have only implemented some bots that can play simple games like Tic-Tac-Toe, and Frozen Lake. While implementing monopoly game, the problem is related to the total number of states and I see there are some research papers which talk about representing these states using Markovian Decision Process.</p>

<p><strong>My question is</strong> there any library or tutorial that I refer to build such a state representation? </p>

<p>While implementing Frozen lake, I came across OpenAI gym library which gives a default implementation of the environment. If I can get some pointers to something similar which to it for board games that would be really helpful.</p>
"
2491,"<p>Suppose if I am building a Linear Regression model with one fully connected layer and a sigmoid with minimizing mean squared error as objective. Why would the error surface be convex?</p>

<p>Does finding the optimal parameters for this network mean we cannot do better than this? Under what assumptions, this solution would be optimal? If we relax the linearity assumption and add some non-linearity to the network can we do better than this? Why so?</p>
"
2492,"<h1>If Human Intelligence and Creativity are Spawned from Struggle and Adversity would an Adverse-Free World of Artificial Intelligence Abduct Human Intelligence?</h1>

<p><a href=""https://i.stack.imgur.com/EsgdD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EsgdD.jpg"" alt=""DeepDream""></a>
(Google DeepDream)</p>

<blockquote>
  <p><em>Simplicity is the ultimate sophistication</em> - Leonardo da Vinci</p>
</blockquote>

<h1>NEAT Neural Networks and Unsupervised Learning</h1>

<p>Recently I have created artificial intelligence that plays the classic game <em>Space Shooter</em>. By doing so it really made me think about the future of unsupervised learning and its consequences. When I was younger I was always told that humans were special and that we can do what machines cannot. The main human aspect involved would be creativity. I never truly denied nor accepted this idea until recently. After creating the artificial intelligence I got a strong feeling that I only imagine parents get when their child takes their first step or the feeling Geppetto had when Pinnochio came to life before he was a ""real boy"". I would not call it gratification, I would describe it more as a shock, like how a child might be shocked that the seed they planted at the age of 5 turned into a full grown tree by 18. Although creating the neural network was exciting afterward I thought deeply about how it may - depending on how you look at it - exorcize or abduct human creativity and intelligence.</p>

<p>Applied computational neural networks in the past few decades in which we take a back-seat from problem-solving / creativity and solely rely on artificial evolution (like that of planting a tree, were we watch its growth) have been rapidly increasing. This, of course, raises many questions about the impacts of artificial intelligence on humanity. By creating artificial general intelligence or applied artificial intelligence are we migrating our intelligence into a machine intelligence? Are we trading intelligence for pleasure and simplicity? By this I mean with all our problems taken care of (the ultimate form of simplicity), is there even a point for creativity or innovation? We often hear about the singularity in which robots take over the planet; however, if we strictly focus on the applications of artificial intelligence and just make life simpler - like how the invention of the spear made acquiring food easier - the real singularity to me would be when we begin to lose creativity. </p>

<h1>The Flynn Effect as a Bell Curve</h1>

<p><a href=""https://i.stack.imgur.com/9mavJ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9mavJ.jpg"" alt=""Flynn""></a></p>

<p>(The Flynn Effect)</p>

<p>The Flynn Effect makes it seem as though our pure intelligence is on the rise. But with the increase in applications of neural networks and general unsupervised learning will the graph of the Flynn effect resemble a bell curve rather than O(n) or O(log n)? The Flynn effect seems to be a response to globalization, a severe increase in the distribution of knowledge (i.e. the internet), and the general way we see the world. But if we outsource our intelligence to an artificial network or artificial intelligence in general, would it not take a hit?</p>

<p>Could this explain why we have not - to my knowledge - been visited by other ""intelligent"" lifeforms. If humans have only been around for a couple hundred thousand years, and as of late have increased technological advances like wildfire, then if my hypothesis is correct (that we will lose intelligence) intelligence must have a short fuse relative to the age of the universe. This would mean meeting other intelligent lifeforms would be like trying to hop off a car onto a bus 10 kilometers ahead on a highway. A dormant highway.</p>

<p>My point is summarized by the question: Does intelligence leave a species once their domain is mastered?</p>

<h1>Da Vinci and Anthropocentrism</h1>

<p><a href=""https://i.stack.imgur.com/PurP9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PurP9.jpg"" alt=""Vinci""></a></p>

<p>(Vitruvian man)</p>

<blockquote>
  <p><em>The human foot is a masterpiece of engineering and a work of art</em> - Leonardo da Vinci</p>
</blockquote>

<p>As a species, it is easy to see ourselves as superior to the others that roam our planet, but if we take Da Vinci's quote about simplicity at the top of this post as a truth then are we really better than others. For example, would a squirrel need to evolve intelligence if it reached peak evolution, where it could propagate with little adversity because it mastered its domain? Would an artificial intelligence help us in the same way master our domain, and as a product lose intelligence? What is it good for at the singularity? In engineering, we seem to have adopted the mindset of Da Vinci with regards to looking inwards to solve our problems (not metaphysically, literally look inwards). By this I mean replicate evolution and biology (i.e. neural networks). To me, it seems that we are giving our humanity away or transferring it to machines.</p>

<p><strong>In short</strong>: Are we giving away our intelligence by creating artificial intelligence?</p>
"
2493,"<p>AI is used to develop bots If a person uses proper ontology then can we add emotions in agents and Bots?</p>
"
2494,"<p>Which search algorithm will use a limited amount of memory in online search mention its name?</p>
"
2495,"<p>Recently I worked on a paper by Hao Wang- Collaborative Deep learning for Recommender Systems which uses a two way tightly coupled method, Collaborative filtering for Item correlation and Stacked Denoising Autoencoders for the Optimization of the problem.</p>

<p>I want to know the limitations of using stacked Autoencoders and Hierarchical Bayesian methods to Recommender systems.</p>

<p>Graph of the model enter image description here</p>

<p>Here is a link to the paper:<a href=""http://www.wanghao.in/paper/KDD15_CDL.pdf"" rel=""nofollow noreferrer"">http://www.wanghao.in/paper/KDD15_CDL.pdf</a></p>
"
2496,"<p>I am a new learner in NLP. I am interested in the sentence generating task. As far as I am concerned, one state-of-the-art method is the <a href=""https://github.com/karpathy/char-rnn"" rel=""nofollow noreferrer"">CharRNN</a>, which uses RNN to generate a sequence of words.</p>

<p>However, <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a> has come out several weeks ago and is very powerful. Therefore, I am wondering whether this task can also be done with the help of BERT? I am a new learner in this field, and thank you for any advice!</p>
"
2497,"<p>A paper from machinelearningmastery.com on human activity recognition states that 1D convolutional neural networks work the best on classification of human activities using data from accelometer. But, according to me, human activities like swinging the arm are sequential actions and they require LSTMs. So, they one should be more efficient, CNNs or LSTMs. Or, in other words, is spatial learning required or sequence learning?</p>
"
2498,"<p>I think I missunderstood something when it comes to the SSE (= sum of squared errors) as a loss function. So: the formula for the SSE is: </p>

<p><a href=""https://i.stack.imgur.com/ITXVf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ITXVf.png"" alt=""SSE formula""></a></p>

<p>But, if ti - oi is negative, doessn't the power of 2 eliminate any negative result? How can then exist any negative gradient at the output? What if the output is too high and so the gradient should be negative, meaning that the weights (on average) have to be decreased? How can it be differentiated between 'Output too high' and 'Output too low'?</p>

<p>Sorry if my Question is absurd, but I can't wrap my head around this.
Thanks in advance</p>
"
2499,"<p>I've been thinking if machine learning can be used to play the game Scrabble. My knowledge is limited in the ML field, thus I've seeking some pointers :) </p>

<p>I want to know how could I possibly build a model that picks a move from all the given valid moves of the current game state, and then plays the move and wait for the delayed reward. The actions here aren't static actions, they are basically selecting move to maximize the final score.</p>

<p>Is there any way to encode the valid moves and then use a model to pick those moves?</p>

<p>I've also considered the genetic approach, but I think if I can represent my move with a set of features (score, consonantVowels ration, rack leave score, #blank tiles after the move, ...etc), training a neural network like this could take a long time.</p>

<p>Another training related question, is it feasible to run the training on a GPU given that I will be waiting for a response (the new game state) from the opponent (e.g. Quackle) after every action?</p>

<p>Thank you :)</p>
"
2500,"<p>How does <a href=""https://plato.stanford.edu/entries/formal-belief/#DemShaThe"" rel=""nofollow noreferrer"">Dempster-Shafer theory</a> work in representing ignorance in AI field? </p>
"
2501,"<p>On of the solutions to scale Blockchain is using off-chain channels. You can find its definition here: (<a href=""https://en.bitcoin.it/wiki/Off-Chain_Transactions"" rel=""nofollow noreferrer"">https://en.bitcoin.it/wiki/Off-Chain_Transactions</a>).</p>

<p>However, one of the problem of off-chain channels is finding a suitable <strong>decentralized routing</strong> mechanism.</p>

<p>Since in Bitcoin, there is no routing table and transactions are only broadcast and also in general, we need to <strong>avoid centralized</strong> approaches for routing, <strong>is it practical to use **Swarm Intelligence algorithms</strong> such as ACO (Ant colony optimization) for <strong>off-chain channels</strong> ?**  </p>

<p><strong>Update:</strong> I refer you to a proposed <strong>Ant routing algorithm</strong> in the following paper as an instance of employing Swarm Intelligence for routing in <strong>Lightning Network</strong>. However, the paper has not been evaluated to demonstrate its performance. </p>

<p>You can find the proposed idea here: <a href=""https://arxiv.org/pdf/1807.00151.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1807.00151.pdf</a></p>
"
2502,"<p>How do we determine the cost of the parent path to its child in <a href=""https://en.wikipedia.org/wiki/A*_search_algorithm"" rel=""nofollow noreferrer"">A* (""A star"") search</a>? </p>
"
2503,"<p>for a school project I have been given a dataset containing images of plants and weeds. The goal is to detect when there is a weed in the pictures. The training and validation sets have already been created by our teachers, however they probably didn't have enough images for both so they ""photoshopped"" some weeds in some of the training pictures.</p>

<p>Here are examples of images with the weed label in the training set:</p>

<p><a href=""https://i.stack.imgur.com/PSlqn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PSlqn.png"" alt=""weed label in the training set""></a></p>

<p>In some cases, the ""photoshopped"" weed is hard to detect, and no shape resembling a weed is clearly visible like in this picture (weed at the very bottom, near the middle):</p>

<p><a href=""https://i.stack.imgur.com/7GwrR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7GwrR.png"" alt=""enter image description here""></a></p>

<p>And here is an example of an image with the weed label in the validation set:</p>

<p><a href=""https://i.stack.imgur.com/SSFkA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SSFkA.png"" alt=""weed label in the validation set""></a></p>

<p>How would I go about preprocessing the training set so that a CNN trained on it would perform well on the validation set? I was thinking of applying a low-pass filter to the rough edges of the photoshopped images so that the network doesn't act as an edge detector, but it doesn't seem very robust. Should I manually select the best images from the training set? Thank you!</p>
"
2504,"<p>From DeepMind's <a href=""https://arxiv.org/pdf/1712.01815.pdf"" rel=""noreferrer"">research paper</a> on arxiv.org:</p>

<blockquote>
  <p>In this paper, we apply a similar but fully generic algorithm, which
  we call <em>AlphaZero</em>, to the games of chess and shogi as well as Go,
  without any additional domain knowledge except the rules of the game,
  demonstrating that a general-purpose reinforcement learning algorithm
  can achieve, tabula rasa, superhuman performance across many
  challenging domains.</p>
</blockquote>

<p>Does this mean AlphaZero is an example of AGI (Artificial General Intelligence)?</p>
"
2505,"<p>When someone wants to compare 2 inputs, the most widespread idea is to use a Siamese architecture. Siamese architecture is a very high level idea, and can be customized based on the problem we are required to solve.</p>

<p><strong>Is there any other architecture type to compare 2 inputs ?</strong> </p>

<hr>

<h3>Background</h3>

<p>I want to use a neural network for comparing 2 documents (semantic textual similarity). Siamese network is one approach, I was wondering if there is more.</p>
"
2506,"<p>I'm trying to build a DQN to replicate the DeepMind results. I'm doing with a simple DQN for the moment, but it isn't learning properly: after +5000 episodes, it couldn't get more than 9-10 points. Each episode has a limit of 5000 steps but it couldn't reach more than 500-700. I think the problem is in the replay function, which is:</p>

<pre><code>def replay(self, replay_batch_size, replay_batcher):
    j = 0
    k = 0
    replay_action = []
    replay_state = []
    replay_next_state = []
    replay_reward= []
    replay_superbatch = []

    if len(memory) &lt; replay_batch_size:
        replay_batch = random.sample(memory, len(memory))
        replay_batch = np.asarray(replay_batch)
        replay_state_batch, replay_next_state_batch, reward_batch, replay_action_batch = replay_batcher(replay_batch)
    else:
        replay_batch = random.sample(memory, replay_batch_size)
        replay_batch = np.asarray(replay_batch)
        replay_state_batch, replay_next_state_batch, reward_batch, replay_action_batch = replay_batcher(replay_batch)

    for j in range ((len(replay_batch)-len(replay_batch)%4)):

        if k &lt;= 4:
            k = k + 1              
            replay_state.append(replay_state_batch[j])
            replay_next_state.append(replay_next_state_batch[j])
            replay_reward.append(reward_batch[j])
            replay_action.append(replay_action_batch[j])

        if k &gt;=4:                
            k = 0
            replay_state = np.asarray(replay_state)
            replay_state.shape = shape
            replay_next_state = np.asarray(replay_next_state)
            replay_next_state.shape = shape
            replay_superbatch.append((replay_state, replay_next_state,replay_reward,replay_action))

            replay_state = []
            replay_next_state = []
            replay_reward = []
            replay_action = []

    states, target_future, targets_future, fit_batch = [], [], [], []

    for state_replay, next_state_replay, reward_replay, action_replay in replay_superbatch:

        target = reward_replay
        if not done:
            target = (reward_replay + self.gamma * np.amax(self.model.predict(next_state_replay)[0]))

        target_future = self.model.predict(state_replay)

        target_future[0][action_replay] = target
        states.append(state_replay[0])
        targets_future.append(target_future[0])
        fit_batch.append((states, targets_future))

    history = self.model.fit(np.asarray(states), np.array(targets_future), epochs=1, verbose=0)

    loss = history.history['loss'][0]

    if self.exploration_rate &gt; self.exploration_rate_min:

        self.exploration_rate -= (self.exploration_rate_decay/1000000)
    return loss
</code></pre>

<p>What I'm doing is to get 4 experiences (states), concatenate and introduce them in the CNN in shape (1, 210, 160, 4). Am I doing something wrong? If I implement the DDQN (Double Deep Q Net), should I obtain similar results as in the <a href=""https://www.youtube.com/watch?v=V1eYniJ0Rnk"" rel=""nofollow noreferrer"">DeepMind Breakout video</a>? Also, I'm using the Breakout-v0 enviroment from OpenAI gym.</p>

<p><strong>Edit:</strong>
Am I doing this properly? I implemented an identical CNN; then I update the target each 100 steps and copy the weights from <code>model</code> CNN  to <code>target_model</code> CNN. Should it improve the learning? Anyway I'm getting low loss.</p>

<pre><code>for state_replay, next_state_replay, reward_replay, action_replay in replay_superbatch:

            target = reward_replay
            if not done:

                target = (reward_replay + self.gamma * np.amax(self.model.predict(next_state_replay)[0]))
            if steps % 100 == 0:

                target_future = self.target_model.predict(state_replay)

                target_future[0][action_replay] = target
                states.append(state_replay[0])
                targets_future.append(target_future[0])
                fit_batch.append((states, targets_future))
                agent.update_net()

        history = self.model.fit(np.asarray(states), np.array(targets_future), epochs=1, verbose=0)

        loss = history.history['loss'][0]
</code></pre>

<p><strong>Edit 2:</strong>
So as far I understand, this code should work am I right?</p>

<pre><code>if not done:

            target = (reward_replay + self.gamma * np.amax(self.target_model.predict(next_state_replay)[0]))
            target.shape = (1,4)

            target[0][action_replay] = target
            target_future = target
            states.append(state_replay[0])
            targets_future.append(target_future[0])
            fit_batch.append((states, targets_future))

        if step_counter % 1000 == 0:

            target_future = self.target_model.predict(state_replay)

            target_future[0][action_replay] = target
            states.append(state_replay[0])
            targets_future.append(target_future[0])
            fit_batch.append((states, targets_future))
            agent.update_net()

    history = self.model.fit(np.asarray(states), np.array(targets_future), epochs=1, verbose=0)
</code></pre>
"
2507,"<p>Are all <strong>Ant routing algorithms</strong> the same? And if No, what is the <strong>common properties</strong> of all of them? In other words, how we can <strong>detect</strong> a routing algorithm is an Ant routing algorithm?</p>
"
2508,"<p>I am learning to create a dialogue system. The various parts of such a system are Intent classifier, slot filling, Dialogue state tracking (DST), dialogue policy optimization and NLG.</p>

<p>While reading <a href=""https://storage.googleapis.com/pub-tools-public-publication-data/pdf/44018.pdf"" rel=""nofollow noreferrer"">this</a> paper on DST, I found out that a discriminative  sequence model of DST can identify goal constraints, fill slots and maintain state of the conversation. </p>

<p>Does this mean that now I dont need to create an intent classifier and slot filling models separately as the tasks are already being done by this DST? Or I am misunderstanding both the things and they are separate?</p>
"
2509,"<p>I am working on an image data-set. As you may have guessed it is imbalanced data. I have 'Class A, 19,000 images' and 'Class B, 2,876 images'.</p>

<p>So I did an undersampling by removing randomly from the majority class till it becomes equal to the minority class.</p>

<p>On doing this I am loosing lot of information from those 19000 images which I could get. 
So I do an oversampling of minority class, by simply copying the 2,876 images again and again.</p>

<p>Is this undersampling method correct, will it effect my accuracy? I trained an Inceptionv4 model using this oversampled data and it is not at all stable and I am getting poor accuracy.</p>

<p><strong><em>What should be my strategy ?</em></strong></p>
"
2510,"<p>How do I show that uniform-cost search is a special case of A*? How do I prove this?</p>
"
2511,"<p>I understand both terms, linear regression and maximum likelihood, but, when it comes to the math, I am totally lost. So I am reading this article <a href=""http://complx.me/2017-01-22-mle-linear-regression/"" rel=""nofollow noreferrer""><em>The Principle of Maximum Likelihood</em></a> (by Suriyadeepan Ramamoorthy). It is really well written, but, as mentioned in the previous sentence, I don't get the math. </p>

<p>The joint probability distribution of <span class=""math-container"">$y,\theta, \sigma$</span> is given by (assuming <span class=""math-container"">$y$</span> is normally distributed): </p>

<p><a href=""https://i.stack.imgur.com/fReBa.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fReBa.gif"" alt=""joint probability distribution of y,θ,σ""></a></p>

<p>This equivalent to maximizing the log likelihood:
<a href=""https://i.stack.imgur.com/Jpron.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Jpron.gif"" alt=""enter image description here""></a></p>

<p>The maxima can be then equating through  the derivative of l(θ) to zero:
<a href=""https://i.stack.imgur.com/gXRPo.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gXRPo.gif"" alt=""enter image description here""></a></p>

<p>I get everything until this point, but don't understand how this function is equivalent to the previous one :
<a href=""https://i.stack.imgur.com/90mST.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/90mST.gif"" alt=""enter image description here""></a></p>
"
2512,"<p>As, we use FIFO(First In First Out) queue in breadth search algorithm but in depth first search we use LIFO.Why we use LIFO instead of FIFO in depth first search?</p>
"
2513,"<p>I'm doing an EPQ (Extended-Project Qualification) on Artificial Intelligence Bias, and would like to gather some primary data for analysis. </p>

<p>Do driver-less cars lead into controversies that are to do with ethics? For example: if the AI had to choose to minimize damage and could not avoid casualties.</p>
"
2514,"<p>I'm wondering if I can visualize the backprop process as follows (please excuse me if I have written something terrible wrong). If the loss function <span class=""math-container"">$L$</span> on a neural network represents the function has the form 
<span class=""math-container"">$$L = f(g(h(\dots u(v(\dots))))$$</span>
then we can visualize the derivative of <span class=""math-container"">$L$</span> wrt the <span class=""math-container"">$i$</span>th function <span class=""math-container"">$v$</span> as 
<span class=""math-container"">$$\frac{\partial L}{\partial v} = \frac{\partial f}{\partial g}\frac{\partial g}{\partial h}\dots\frac{\partial u}{\partial v}.$$</span></p>

<p>Am I able to view all neural networks as having a loss function of the form of <span class=""math-container"">$L$</span> given above? That is, am I correct in saying that any neural network is just a function composition and that I can write the partial derivative wrt any parameter as written above (I know I took the partial with respect to the function <span class=""math-container"">$v$</span>). </p>

<p>Thanks</p>
"
2515,"<p>Let's say we have 2 blocks of text, where there are some similarities as in 2 interpretations of the same story. Is there any solution that would recognize these consistencies and ultimately be able to recreate a story with all the details of each?</p>
"
2516,"<p>I have been reading introduction to statistical learning and I was going through Multiple Linear Regression. This is the topic that Im reading
<a href=""https://i.stack.imgur.com/1q6uR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1q6uR.jpg"" alt=""enter image description here""></a></p>

<p>As i was reading further I encountered an equation that Im not able to understand. Below is that equation
<a href=""https://i.stack.imgur.com/sKshT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sKshT.jpg"" alt=""enter image description here""></a></p>

<p>It further says that</p>

<p><a href=""https://i.stack.imgur.com/Drexg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Drexg.jpg"" alt=""enter image description here""></a></p>

<p>Please try to explain the above mentioned problem as simply as possible</p>
"
2517,"<p>Advantage Actor-Critic algorithm may use the following expression to get 1-step estimate of the advantage:</p>

<p><span class=""math-container"">$ A(s_t,a_t) = r(s_t, a_t) + \gamma V(s_{t+1}) (1 - done_{t+1})  - V(s_t) $</span></p>

<p>where <span class=""math-container"">$done_{t+1}=1$</span> if <span class=""math-container"">$s_{t+1}$</span> is a terminal state (end of the episode) and <span class=""math-container"">$0$</span> otherwise.</p>

<p>Suppose our learning environment has a goal, collecting the goal gives reward <span class=""math-container"">$r=1$</span> and terminates the episode. Agent also receives <span class=""math-container"">$r=-0.1$</span> for every step, encouraging it to collect the goal faster. We're learning with <span class=""math-container"">$\gamma=0.99$</span> and we terminate the episode after <span class=""math-container"">$T$</span> timesteps if the goal wasn't collected.</p>

<p>For the state before collecting a goal we have the following advantage, which seems very reasonable: <span class=""math-container"">$A(s_t,a_t) = 1 - V(s_t)$</span>.</p>

<p>For the timestep <span class=""math-container"">$T-1$</span>, regardless of the state, we have: <span class=""math-container"">$ A(s_{T-1},a_{T-1}) = r(s_{T-1}, a_{T-1}) - V(s_{T-1}) \approx -0.1 -\frac{-0.1}{1-\gamma} = -0.1 + 10 = 9.9 $</span> 
(this is true under the assumption that we're not yet able to collect the goal reliably often, therefore the value function  converges to something close to <span class=""math-container"">$\frac{r_{avg}}{1-\gamma} \approx -10 $</span> ).</p>

<p>Usually, <span class=""math-container"">$T$</span> is not a part of the state, so the value function has no way to anticipate the sudden change in reward-to-go. So, all of a sudden, we got a (relatively) big advantage for the arbitrary action that we took at the timestep <span class=""math-container"">$T-1$</span>.  Following the policy gradient rule, we will significantly increase the probability of an arbitrary action that we took at the end of the episode, even if we didn't achieve anything. This can quickly destroy the learning process.</p>

<p>How do people deal with this problem in practice? My ideas:</p>

<ol>
<li>Differentiate between actual episode terminations and ones caused by the time limit, e.g. for them we will not replace next step value estimate with <span class=""math-container"">$0$</span>.</li>
<li>Somehow add <span class=""math-container"">$t$</span> to the state such that the value function can learn to anticipate the termination of the episode.</li>
</ol>

<p>As I noticed, the A2C implementation in OpenAI baselines does not seem to bother with any of that:</p>

<ul>
<li><p><a href=""https://github.com/openai/baselines/blob/f3a5abaeeb1c1c9136a01c9dbfebc173dc311fef/baselines/a2c/runner.py#L64"" rel=""nofollow noreferrer"">A2C implementation - calculation of discounted rewards</a></p></li>
<li><p><a href=""https://github.com/openai/baselines/blob/8c2aea2addc9f3ba36d4a0c937e6a2d09830afc7/baselines/a2c/utils.py#L147"" rel=""nofollow noreferrer""><code>discount_with_dones</code> function</a></p></li>
</ul>
"
2518,"<p>I just started using IBM Cloud Private for Data this week and I wasn't sure if I can use other public clouds to connect with my ICP for data account. So I spoke with an IBM representative and I wanted to share their responses...</p>
"
2519,"<p>Apparently, the hill climbing algorithm just produces a local maximum, and not necessarily a global optimum. It's stuck on a local maximum. Why does hill climbing algorithm only produce a local maximum?</p>
"
2520,"<p>Is the research field ""Artificial intelligence"" a science or is it engineering? or neither or both ?</p>
"
2521,"<p>I am working on an autoregression problem where I use sequential LSTM. My target is well defined, but I think I am facing a problem with the features. As the features were non-stationary, then I decided to apply the log-returns to each of them. In other words, if <span class=""math-container"">$F_t$</span> is a feature at a certain time <span class=""math-container"">$t$</span>, then I apply <span class=""math-container"">$$\log(\frac{F_t}{F_{t-1}}).$$</span> </p>

<p><span class=""math-container"">$\Rightarrow$</span> <a href=""https://quant.stackexchange.com/questions/8875/why-non-stationary-data-cannot-be-analyzed?noredirect=1&amp;lq=1"">Why non-stationary data is hard to analyse?</a></p>

<p>The property makes it easier to analyse, but produce a lot of zeros when <span class=""math-container"">$F_t = F_{t-1}$</span>. </p>

<p>As there is lots of zeros, then the predictions tend to be closer to 0. How can I reduce that effect mathematically?</p>
"
2522,"<p>In <a href=""https://arxiv.org/pdf/1502.05477.pdf"" rel=""nofollow noreferrer"">the <em>Trust Region Policy Optimization</em> paper</a>, in Lemma 2 of Appendix A, I did not quite understand deriving inequality (31) from (30), which is:</p>

<p><span class=""math-container"">$$\bar{A}(s) = P(a \neq \tilde{a} | s) \mathbb{E}_{(a, \tilde{a}) \sim (\pi, \tilde{\pi})|a \neq  \tilde{a}} \left[ A_{\pi}(s, \tilde{a}) - A_{\pi}(s,a) \right]$$</span>
<span class=""math-container"">$$|\bar{A}(s)| \le \alpha. 2 \max_{s,a} |A_{\pi}(s,a)|$$</span></p>

<p>Would you mind let me know how the inequality is derived? </p>
"
2523,"<p>I'm trying to gain some insight into acoustic voice data composed of 19 features. I want to understand what features contribute most for classification.</p>

<p>ADDED: Most features are related with the fundamental frequency stability. In particular I'm using voice shimmer and jitter and some related calculations.</p>

<p>I'm trying to use MRMR (max relevance min redundancy), but I would like to compare with some other options. 
ADDED: I have tried to use FeatureMiner (<a href=""http://featureselection.asu.edu/index.php"" rel=""nofollow noreferrer"">http://featureselection.asu.edu/index.php</a>) which provides some interesting algorithms implementations. However many of them use deprecated Python functions and require some effort to work properly. </p>

<p>Are there any popular tools for these purposes?</p>
"
2524,"<p>I am new for machine learning and I am tried to understand basic steps to get final modal of Logistic Regression. </p>

<p>I know Logistic Regression is supervisory learning technique. Therefore we want to give training data to training the modal. According to my understanding, I can take steps to get the final modal as follows. </p>

<p><strong>Step 1</strong> - Make an algorithm. For Logistic Regression is y = sigmoid(W x + B) function. Get zero or some value for W and B.  </p>

<p><strong>Step 2</strong> - Give sample known training data and find W and B values. x(1), x(2)...x(m) inputs and get y(1), y(2)...y(m) outputs. Gradient Descent use for find W and B that minimizes the cost function. </p>

<p><strong>Step 3</strong> - Then apply W and B values which I found.y^ = sigmoid(W x + B). And then again apply sample known training data to get the predicting of y^.  </p>

<p><strong>Step 4</strong> - Get the final modal to test unknown data.</p>

<p>are these steps right? Please give me basic fundamental steps to understand supervisory learning technique. I would like to know un-supervisory learning technique steps also. </p>
"
2525,"<p>Specifically, I am looking for a dataset that is concerned with pattern recognition. I could find no such dataset on Kaggle, and Google has been of little help.</p>
"
2526,"<p>I'm trying to replicate the results of the DeepMind's paper with Breakout included in OpenAI Gym. I wonder how much frames should I keep until I reach the fixed exploration rate. Actually it reaches the minimum value at about 1,5M frames (about 5000 episodes). It is much? If it is, does it impact negatively on training? My minimum rate is fixed at 0.01.</p>
"
2527,"<p>I'm working on stock price prediction and automatic or semi-automatic control of trading.  The price trends of these stocks exhibit recurring patterns that may be exploited.  My dataset is currently small, only in the thousands of points.  There are no images or very high dimensional inputs at all.  The system must select from among the usual trading actions.</p>

<ul>
<li>Buy <span class=""math-container"">$n$</span> shares at the current bid price</li>
<li>Hold at the current position</li>
<li>Sell <span class=""math-container"">$n$</span> shares at what the current market will bear</li>
</ul>

<p>I'm not sure if reinforcement learning is the best choice, deep learning is the best choice, something else, or some combination of AI components.</p>

<p>It doesn't seem to me to be a classification problem, with hard to discern features.  It seems to be an action-space problem, where the current state is a main input.  Because of the recurring patterns, the history that demonstrates the observable patterns is definitely pertinent.</p>

<p>I've tried some code examples, most of which employ some form of artificial nets, but I've been wondering if I even need deep learning for this, having seen the question, <a href=""https://ai.stackexchange.com/questions/3002/when-is-deep-learning-overkill""><em>When is deep-learning overkill?</em></a> on this site.</p>

<p>Since I have very limited training data, I'm not sure what AI design makes most sense to develop and test first.</p>
"
2528,"<p>If my algorithm detects the type of object, how should I know if that object is moving or not? Suppose a person carrying an umbrella. How to know that the umbrella is moving?</p>

<p>I am working on a project where I want to know whether that particular object belongs to the person entering inside the store.
I was thinking about the bounding boxes(bb) approach where if the person's bb overlaps with the object's bb. But the problem arises when there are multiple objects with a person. </p>

<p>I would appreciate your help. Thanks. </p>
"
2529,"<p>When we get the offspring and fitness so why we prefer to apply mutation in genetic algorithm?</p>
"
2530,"<p>I want to build a semi autonomous robot/machine that will clean up trash in cities. For this to be possible it needs to recognize 'trash'. As trash can be all sorts of things (think ciggaret buts, plastic, bottles or anything that humans leave behind) I was curious if it would be possible for a AI to learn this broad concept with current technologies? And if so what would I need to build it? 
I was thinking Tensorflow and a dataset with different kinds of trash. </p>

<p>Thank you for taking the time to help!</p>
"
2531,"<p>We want to figure out the connection between people, based on their speech.
Assume, that a conversation is a poetry with lines belongs to characters. There are a lot of poetries and the lines are mixed. Now we want to define a conversation to which each line belongs to. 
We assume, that people in a conversation use similar words (their dictionaries should be similar). It means that there is a correlation between words of person A and words belongs to person B, and we could detect a connection between the peoples which had a conversation. What are the next steps for content understanding after NLP?
Can some of you advise us about the field of study and tools/libraries, which are dealing with content processing? Maybe, some of you know good articles or online resources, which can help us to dive into this field.</p>
"
2532,"<p>Most implementations I'm seeing for playing games like Atari (usually similar to DeepMind's work using DQN) have 4 graphical frames of input fed into 3 convolutional layers which are then fed into a single fully connected layer. <strong>The explanation of no pooling layer is due to positioning of features/objects being very critical to most games.</strong></p>

<p>My concern with this is that it may be weighing visual features based on position without regard for feature->feature proximity. By this, I mean to question if learning to avoid bullets in the bottom left of the screen is knowledge also used in the bottom right of the screen in a game like Space Invaders.</p>

<p>So, question 1: Is my concern with only using 3 conv layers into a fc layer legitimate regarding spatially localized learning?</p>

<p>Question 2: If my concern is legitimate, how might the network be modified to still treat feature position as significant, but to also take note of feature to feature proximity?</p>

<p>(I'm still quite the novice if that isn't extremely obvious, so if my questions aren't completely ridiculous on their own, please try to keep responses relatively high level if you would.)</p>
"
2533,"<p>I am having a video feed with multiple faces in it. I need to detect each face and the gender as well and assign the gender against each person. I am not sure how to uniquely identify a face as Face1 and Face2 etc. I do not need to know their names, just need to track a person in the entire video. I thought of tracking methods but people are constantly moving and changing so a box location can be occupied by another person after some frames. </p>

<p>I am interested in a way where I can assign an id to a face but I am not sure how to do it. I can use Facial Recognition Based embedding on each face and track that. But that seems to be an overkill for the job. Is there any other method available or Facial Recognition/Embedding is the only method to uniquely identify people in a video?</p>
"
2534,"<p>Can someone suggest an AI approach to moving blocks, one at a time, assuming control of an robotic arm, to get from the initial state on the left to the final state on right, preferably using goal stack planning.</p>

<p><strong>actions</strong></p>

<ul>
<li><p>Pickup() &mdash; to pick up a block from table only</p></li>
<li><p>Putdown &mdash; to putdown a block on table only</p></li>
<li><p>Unstack &mdash; unstack a block from another block</p></li>
<li><p>Stack &mdash; stack a block on another clear block only</p></li>
</ul>

<p><strong>property functions</strong></p>

<ul>
<li><p>On(x,y)</p></li>
<li><p>Above(x,y)</p></li>
<li><p>Table(x)</p></li>
<li><p>Clear(x)</p></li>
</ul>

<p><a href=""https://i.stack.imgur.com/8MRUp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8MRUp.png"" alt=""enter image description here""></a></p>
"
2535,"<p>I have to build a KI for a made-up game similar to chess. As I did research for a proper solution, I came upon the MinMax algorithm, but I'm not sure it will work with the given game dynamics.</p>

<p>The challenge is that we have far more permutations per turn than in chess because of these game rules.</p>

<ul>
<li>Six pieces on the board, with different ranges.</li>
<li>In average, there are 8 possible moves for a piece per turn.</li>
<li>The player can choose as many pieces to move as he likes. For example none, all of them, or some number in between (whereas in chess you can only move one.)</li>
</ul>

<p>Actual questions:</p>

<ul>
<li>Is it feasible to implement MinMax for the described game?</li>
<li>Can alpha-beta-pruning and a refined evaluation function help (despite of the large number of possible moves)?</li>
<li>If no, is there a proper alternative?</li>
</ul>
"
2536,"<p>Which one gives better optimization results? Genetic Algorithm or Particle Swarm Optimization? Can I use them for online tuning problems? Thanks in advance!</p>
"
2537,"<p>Biology is used in AI terminology. What are the reasons? What does biology have to do with AI? For instance, why is the genetic algorithm used in AI? Does it fully belong to biology?</p>
"
2538,"<p><a href=""https://stackoverflow.com/questions/36162180/gradient-descent-vs-adagrad-vs-momentum-in-tensorflow"">https://stackoverflow.com/questions/36162180/gradient-descent-vs-adagrad-vs-momentum-in-tensorflow</a></p>

<p>Here, the nice gifs explain how different algorithms approach towards the root. Unfortunately, the environment in the gif is way too simple and real cases have much more complex environments. Also, in reinforcement learning, the solutions should change each moment in a difficult enough environment since things are dynamic.</p>

<p>My question is which optimizer is best for reinforcement learning in such dynamically changing environment? Adadelta should not move beyond local minima so do we have to use SGD or Adadelta with an exploration heuristic? Please let me know in detail your thoughts.</p>
"
2539,"<p>AI cant handle all the problem.In robot intelligence they cant understand the human brain process they cant fell happiness or sadness.What are our misconception about robots intelligence system?</p>
"
2540,"<p>What problems of AI are not machine learning?<br>
What is difference between AI and machine learning?<br>
Which problems handle both AI and machine learning?</p>
"
2541,"<p>This might be a stupid question, but bear with me i'm only a beginner at this. Recently i started to look at policy gradient methods and policies are represented as functions with features for larger problems with many states. Many articles and pseudocodes of algorithms mention sampling an action from the policy but it is unclear to me how. Actions are something we do in the environment like going left, right, etc... and functions take some feature values and parameters, make calculations and 'spit' some number. So how do we actually map that number to certain action, how do we know what action to take?</p>
"
2542,"<p>Which is the most straight forward approach for planning algorithm?can we use random algorithm like BFS,DFS,RBFS etc?</p>
"
2543,"<p>Can the same input for a plain neural network be used for CNNs? Or does the input matrix need to be structured in a different way for CNNs compared to regular NNs? </p>
"
2544,"<p>Modular/Multiple Neural networks (MNNs) revolve around training smaller, independent networks that can feed into each other or another higher network.</p>

<p>In principle, the hierarchical organization could allow us to make sense of more complex problem spaces and reach a higher functionality, but it seems difficult to find examples of concrete research done in the past regarding this. I've found a few sources:</p>

<p><a href=""https://en.wikipedia.org/wiki/Modular_neural_network"" rel=""noreferrer"">https://en.wikipedia.org/wiki/Modular_neural_network</a></p>

<p><a href=""https://www.teco.edu/~albrecht/neuro/html/node32.html"" rel=""noreferrer"">https://www.teco.edu/~albrecht/neuro/html/node32.html</a></p>

<p><a href=""https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isAllowed=y"" rel=""noreferrer"">https://vtechworks.lib.vt.edu/bitstream/handle/10919/27998/etd.pdf?sequence=1&amp;isAllowed=y</a> </p>

<p>A few concrete questions I have:</p>

<ul>
<li><p><em>Has there been any recent research into the use of MNNs?</em></p></li>
<li><p><em>Are there any tasks where MNNs have shown better performance than large single nets?</em></p></li>
<li><p><em>Could MNNs be used for multimodal classification, i.e. train each net on a fundamentally different type of data, (text vs image) and feed forward to a higher level intermediary that operates on all the outputs?</em></p></li>
<li><p><em>From a software engineering perspective, aren't these more fault tolerant and easily isolatable on a distributed system?</em></p></li>
<li><p><em>Has there been any work into dynamically adapting the topologies of subnetworks using a process like Neural Architecture Search?</em></p></li>
<li><p><em>Generally, are MNNs practical in any way?</em></p></li>
</ul>

<p>Apologies if these questions seem naive, I've just come into ML and more broadly CS from a biology/neuroscience background and am captivated by the potential interplay. </p>

<p>I really appreciate you taking the time and lending your insight!</p>
"
2545,"<p>So, in situations where some output-data-chunks could change without changing the input data from time to time, it could be challenging to automate the testing process.</p>

<p>My question is, are there any ways by using AI in general,
in which those output data analyzed(after the training)
and as a result of the actual and mutual data (that could be tested) return?</p>
"
2546,"<p>Motion capture (Mocap) is about recording human motions in 3d space. In most cases, a marker based tracking is used, but some newer development goes into markerless capture. The result of the mocap step is the recorded trajectory which is stored in a file. It contains of 3d position information which are connected to a timeframe:</p>

<pre><code>time 0, (0,0,0)
time 1, (4,3,0)
time 2, (5,2,0)
time 3, (1,4,3)
</code></pre>

<p>This file has to be parsed on a semantic level. According to the literature, a parser works with a domain specific language (DSL) which is about biped walking, human motions, grasping tasks and so on. The advantage of a DSL for a mocap parser is, that it is equal to a heuristic. If the trajectories are mapped to textual commands it's possible to reproduce a task on a robot.</p>

<p>My question is: Which domain specific languages for human motion capture are available? </p>
"
2547,"<p>We have convolutional neural networks and recurrent neural networks for analysing images and sequential data, respectively.</p>

<p>What is the main architecture used for function approximation? (e.g. a neural network learning the function <span class=""math-container"">$f(x,y) = \sin(2\pi x)\sin(2\pi y)$</span> on <span class=""math-container"">$\Omega = [0,1]\times [0,1]$</span>)</p>

<p>Also, what kind of activation function is generally used for those kinds of problems?</p>
"
2548,"<p>I just wanted to gather some perspective on why this is a great opportunity to be able to study machine learning today? </p>

<p>With all the online resources (online courses like Andrew Ng's, availability of datasets such as Kaggle, etc), learning machine learning has become possible. </p>

<p>I understood that you can have high paid jobs; but you also need a lot of work dedication to be good at it, which makes your salary not so attractive! (in comparison to the number of hours you spend to keep up with this fast moving field) </p>

<p>Why it is so desirable to take this opportunity and start learning machine learning today? (community, ability to start a business, etc.)</p>
"
2549,"<p>Why not just have single RNN/LSTM that given input x outputs some value y(t) and later uses that value and the RNN hidden state to produce the next output?
The architecture shall consists of only one network instead of two separate ones.
Example:</p>

<p>first step:
input x produces y(t) and hidden state h(t)</p>

<p>Next,</p>

<p>Given h(t) and y(t) the next output y(t+1) should be produced</p>

<p>Next,</p>

<p>given h(t+1) and y(t+1) produces y(t+2) and so on...</p>

<p>All done just by using one network.</p>
"
2550,"<p>so, I am currently Looking at different documents to understand backpropagation, mainly at <a href=""https://www.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf"" rel=""nofollow noreferrer"">this document</a>. Now, at page 3, there is the Epsilon Symbol involved:</p>

<p><a href=""https://i.stack.imgur.com/QKDHI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QKDHI.png"" alt=""Gradient equation""></a></p>

<p>While I understand the main part of the equation, I don't understand the epsilion factor. Searching for the meaning of the Epsilon in math, it means (for example) a error value to be minimized, but why should I multiply with the error (it is denoted as E anyways).</p>

<p>Shouldn't the Epsilon be the learning rate in this equation? I think that would be what makes sense, because we want to calculate by how much we want to adjust the weight, and since we calculate the gradient, I think the only thing that's missing is the multiplication with the learning rate. The thing is, isn't the learning rate usually denoted with the Alpha?</p>

<p>Please put me in the picture, thanks</p>
"
2551,"<p>We have a series of data which want to label part of each series. As we do not have any training data, we try using Active Learning as a solution. But, the problem is our classifier is something like RNN which needs a lot of data to be trained. Hence, we have a problem in converging fast to just label proportional small parts of unlabeled data. The question is ""is there any article about this problem (Active Learning and some complex classifiers like RNN) or not?"". </p>

<p>Or is there any alternative to approach this problem or not? (as data is a series of actions)</p>
"
2552,"<p>When will it be possible to give a computer program a bunch of assumptions and ask it if a certain statement is true or false, giving a proof or a counterexample respectively?</p>
"
2553,"<p>I'm working on a problem and need to use Karaboga's code of the ABC algorithm but I have some questions...</p>

<p>Does this formula for calculating a parameter have to be changed:</p>

<pre><code>{/v_{ij}=x_{ij}+\phi_{ij}(x_{kj}-x_{ij}) */}
</code></pre>

<p>A standard one or this what Karaboga see is better for algorithm.</p>

<p>The second is the same question for this formula of calculating fitness:</p>

<pre><code>{fFitness(ind)=1./(fObjV(ind)+1)}
</code></pre>

<p><a href=""https://sci2s.ugr.es/sites/default/files/files/TematicWebSites/EAMHCO/Software/ABC.C"" rel=""nofollow noreferrer"">Link to ABC algorithm coded using C programming language</a></p>
"
2554,"<p>My task involves a large grid-world type of environment (grid size may be <span class=""math-container"">$30\times30$</span>, <span class=""math-container"">$50\times50$</span>, <span class=""math-container"">$100\times100$</span>, at the largest <span class=""math-container"">$200\times200$</span>). Each element in this grid either contains a 0 or a 1, which are randomly initialized in each episode.  My goal is to train an agent, which starts in a random position on the grid, and navigate to every cell with the value 1, and set it to 0. (Note that in general, the grid is mostly 0s, with sparse 1s). </p>

<p>I am trying to train a DQN model with 5 actions to accomplish this task:</p>

<ol>
<li><p>Move up</p></li>
<li><p>Move right</p></li>
<li><p>Move down</p></li>
<li><p>Move left</p></li>
<li><p>Clear (sets current element to 0)</p></li>
</ol>

<p>The ""state"" that I give the model is the current grid (<span class=""math-container"">$N\times N$</span> tensor). I provide the agent's current location through the concatenation of a flattened one-hot (<span class=""math-container"">$1\times(N^2)$</span>) tensor to the output of my convolutional feature vector (before the FC layers).</p>

<p>However, I find that the epsilon-greedy exploration policy does not lead to sufficient exploration. Also, early in the training (when the model is essentially choosing random actions anyway), the pseudo-random action combinations end up ""canceling out"", and my agent does not move far enough away from the starting location to discover that there is a cell with value 1 in a different quadrant of the grid, for example. I am getting a converging policy on a <span class=""math-container"">$5\times5$</span> grid w/ a non-convolutional MLP model, so I think that my implementation is sound.</p>

<ol>
<li><p><strong>How I might encourage exploration that will not always ""cancel out"" to only explore a very local region to my starting location?</strong></p></li>
<li><p><strong>Is this approach a good way to accomplish this task (assuming I want to use RL)?</strong></p></li>
<li><p><strong>I would think that attempting to work with a ""continuous"" action space (model outputs 2 values: vertical and horizontal indices of grid cells that contain 1s) would be more difficult to achieve convergence. Is it wise to always try to use discrete action spaces?</strong></p></li>
</ol>
"
2555,"<p>I am wondering how the output of randomly initialized MLPs and ConvNets behave with respect to their inputs.  Can anyone point to some analysis or explanation of this?</p>

<p>I am curious about this because in the <a href=""https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/"" rel=""nofollow noreferrer"">Random Network Distillation work from OpenAI</a>, they use the output of randomly initialized network to generate intrinsic reward for exploration.  It seems that this assumes that similar states will produce similar outputs of the random network.  Is this generally the case?  </p>

<p>Do small changes in input yield small changes in output, or is it more <a href=""https://en.wikipedia.org/wiki/Chaos_theory"" rel=""nofollow noreferrer"">chaotic</a>?  Do they have other interesting properties?</p>
"
2556,"<p>I am preparing to perform research comparing the performance of two different systems that probabilistically generate the next word of an input sentence.</p>

<p>For example, given the word 'the', a system might output 'car', or any other word. Given the input 'the round yellow', a system might output 'sun', or it might output something that doesn't make sense.</p>

<p>My question is, how can I quantitatively evaluate the performance of the two different systems performing this task? Of course if I tested each system manually I could qualitatively determine how often each system responded in a way that makes sense, and compare how often each system responds correctly, but I'd really like a meaningful quantitative method of evaluation that I could preferably automate.</p>

<p>Precision and recall don't seem like they would work here, seeing as for each given input there are many potentially acceptable outputs. Any suggestions?</p>
"
2557,"<p>I use a modified training script for modeling images with <strong>Tensorflow/Keras/Mobilenet_V2</strong>. After a few errors that I could solve I now get the following error:</p>

<pre><code>File ""/home/Machine_Learning_Verstening/scripts/object_detection/builders/model_builder.py"", line 234, in _build_ssd_model
    freeze_batchnorm=ssd_config.freeze_batchnorm,
AttributeError: 'Ssd' object has no attribute 'freeze_batchnorm'
</code></pre>

<p>Within this script I see the following pieces of code:</p>

<pre><code>freeze_batchnorm=ssd_config.freeze_batchnorm                     
freeze_batchnorm=ssd_config.freeze_batchnorm  
freeze_batchnorm=ssd_config.freeze_batchnorm
</code></pre>

<p>On google I cannot find the solution, I guess it is very specific for using the <strong>SSD</strong> in <strong>Tensorflow</strong>. Anyone has experience with this error?</p>
"
2558,"<p>The field of artificial intelligence is so vast. So many methodologies of handling continuous  data and just now I have read about Hybrid Bayesian network. I just want to know  that what a hybrid Bayesian network contains?</p>
"
2559,"<p>Sorry if this is basic or covered elsewhere, I am just starting here and I wasn't able to find an answer, but I might have not been searching for the right thing. So:</p>

<p>I am training a neural network to predict current draw in a system. There are a number of obvious numerical inputs, like temperature, counting rate, voltage, etc. </p>

<p>The most predictive thing, however, is what operation the system is doing. So like, if it's doing a 'calibration' then the current profile is much different than if it's in 'standby'. I know that I can just use a different network for each operation, but in this case I have a couple hundred different macros defined and I don't want to have 200+ neural networks retrain all the time. </p>

<p>I also know that I can have a digital value as an input, but my understanding is that it has to be either 0/1. Also, the relationship to operation is not at all correlated - so operation 100 is not necessarily more current draw than 99 or less than 101. </p>

<p>So, is there a way to have an operation ID or something factor in, but not have it be in the linear combination mathematically? So, basically, tell the system to do a different training based on ID or something? I'll be using python and scikit-learn.</p>

<p>Thanks!</p>
"
2560,"<p>I am trying to generate 90 and 270 degrees rotated versions of my sample images on the fly during training. I found an example and modifying it. But I am confused about what should be the order? For instance in one batch I have 32 images and my image generator should return total 64 images. Let's say upper case letters are 90 degree and lower case letters are 270 degree rotated images. Should the order be AaBbCc or ABCabc? I apply the same to the validation set. Here is the related code fragment:</p>

<p><strong>Edit:</strong> Code fragment added.</p>

<pre><code>    def _get_batches_of_transformed_samples(self, index_array):
    # create list to hold the images
    batch_x = []
    # create list to hold the labels      
    batch_y = []
    # rotation angles
    target_angles = [0, 90, 180, 270]
    angle_categories = list(range(0, len(target_angles)))
    self.classes = target_angles
    self.class_indices = angle_categories
    # generate rotated images and corresponding labels
    for i, j in enumerate(index_array):           
        is_color = int(self.color_mode == 'rgb')
        image = cv2.imread(self.filenames[j], is_color)
        if is_color:
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                               

        for rotation_angle, cat_angle in zip(target_angles, angle_categories):
            rotated_im = rotate(image, rotation_angle, self.target_size[:2])
            if self.preprocess_func: rotated_im = self.preprocess_func(rotated_im)
            # add dimension to account for the channels if the image is greyscale
            if rotated_im.ndim == 2: rotated_im = np.expand_dims(rotated_im, axis=2)                                
            batch_x.append(rotated_im)
            batch_y.append(cat_angle)

    # convert lists to numpy arrays
    batch_x=np.asarray(batch_x)
    batch_y=np.asarray(batch_y)

    batch_y = to_categorical(batch_y, len(target_angles))            
    return batch_x, batch_y
</code></pre>

<p>I actually rotate them as 0, 90, 180 and 270 degrees. As seen in the code, for each batch I return all the rotated versions of all the images in the batch. But is this correct or should I return first 0 degree rotated versions, second 90 degree rotated versions so on?</p>

<p><strong>Edit2:</strong> I checked my previous work which I use the built in Keras <code>ImageDataGenerator</code>. <code>generator.classes</code> returns [zeros(100,1); ones(100,1)]. In that study I only have two classes. I understand that Keras indexes the images as [class1, class2, ...]. I think I have to do the same.</p>
"
2561,"<p>Can Machine Learning be applied to decipher the script of <strong>lost</strong> ancient languages? (Languages that many years ago were being used, but currently are not used in the human societies and have been forgotten e.g. <a href=""https://en.wikipedia.org/wiki/Avestan"" rel=""nofollow noreferrer"">Avestan language</a>.)</p>

<p>And if yes, is there already any successful experiment to decipher the script of unknown ancient languages Machine Learning?   </p>
"
2562,"<p>Question to NN practicioners. I'd like to encode Azul board game state as an input to NN, let's focus on 2-player variant for a while.</p>

<p><a href=""https://i.stack.imgur.com/PR1vo.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PR1vo.jpg"" alt=""3-player Azul pictured""></a></p>

<p>There are 5 round ""Factories"" on the table (7 on picture, ignore it). Each one can keep 4 tiles of 5 colors. There is also center of the board which can keep up to 15 tiles. What are the advantages and disadvantages of different encodings? Here are my ideas:</p>

<ol>
<li><p>Every Factory has five integer counters, one for each tile color. Sample encoding of single Factory: <code>[3,1,0,0,0]</code></p></li>
<li><p>Every Factory has 20 binary flags, four for each color. Single flag encodes presence of tile of given color. Sample encoding of single factory: <code>[1,1,1,0, 1,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0]</code></p></li>
<li><p>Every Factory has 20 binary flags, four for each color, but only one flag can be set for given color and position of raised flag encodes number of tiles of given color. Sample encoding of single factory: <code>[0,0,1,0, 1,0,0,0, 0,0,0,0, 0,0,0,0, 0,0,0,0]</code></p></li>
<li><p>Every Factory has 4 enum fields, with 6 possible values each (5 colors + empty). Sample encoding of single factory: <code>[red, red, red, blue]</code> or <code>[Empty,Empty,Empty,Empty]</code></p></li>
</ol>

<p>(note: encoding schema would also cover center of the board, up to 15 tiles, as said earlier. Of course player's board would also be encoded, but I don't want to ask too broad question)</p>

<p>I'd like to train NN to play Azul, which means it needs to properly process number of tiles taken in given round (up to 15 in theory, 2-4 in practice) because it would also need to indicate where to put all those tiles to player board positions.</p>

<p>Based on your experience, which encoding is most promising? Or maybe there is some better method I didn't think of? Or it is not possible to tell or it doesn't matter?</p>
"
2563,"<p>when we perform tik tok toe game using adversarial search i know how make tree
My Question is :
is there any way to find the depth of tree which level is the last level.</p>
"
2564,"<p>I need a phone or device I can talk to as if I had no hands or sight. I need to be able to tell my phone to read my text, ""tell me who called me in the last hour (or whatever)"" then it tells me who the number is registered to using the internet. Most important I want the AI to understand me the first time I say it. There are more facets to the AI I would need to be able to do without ever touching the phone.</p>

<p>Is there anything out there like that I can carry with me?  </p>
"
2565,"<p>I am working on character recognition using convolutional neural networks. I have 9 layer model and 19990 training data and 4470 test data. But when I am using keras with Tensorflow backend. When I try to train the model, it runs extremely slow, like 100-200 samples per minute. I tried adding batch normalization layer after flattening, using regularization, adding dropout layers, using fit_generator to load data from disk batch wise so that ram remain free(that did the worse performance) using different batch sizes, but nothing is working. So, I tried reducing network size to 4 layers and added more channels to initial layers to increase parallel computing but now I started getting memory allocation errors. It says allocation of some address exceeds 10% and than my entire system freezes. I have to restart my laptop every time. I tried going back to the earlier version with 9 layers but that is giving me same error as well now, even though it worked earlier(not really worked, but atleast started training). So, what is the solution for this problem? Is it the problem of hardware being less capable or something else? I have 8gb ram and 2 gb gpu, but i dont use gpu for training. I have intel i5 7gen processor.</p>
"
2566,"<p>In section 7.1 (about the n-step bootstrapping) of the book <em>Reinforcement Learning: An Introduction</em> (2nd edition), by Andrew Barto and Richard S. Sutton, the authors write about what they call the ""n-step return error reduction property"":</p>

<p><a href=""https://i.stack.imgur.com/BUSZM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BUSZM.png"" alt=""enter image description here""></a></p>

<p>But they don't prove it. I was thinking it should not be too hard but how can we show this? I was thinking of using the definition of n-step return (eq. 7.1 on previous page):</p>

<p><span class=""math-container"">$$G_{t:t+n} = R_{t+1} + \gamma*R_{t+2} + ... + \gamma^{n-1}*R_{t+n} + \gamma^{n}*V_{t+n-1}(S_{t+n})$$</span></p>

<p>Because then this has the <span class=""math-container"">$V_{t+n-1}$</span> in it already. But in the definition above of the n-step return it uses <span class=""math-container"">$V_{t+n-1}(S_{t+n})$</span> but on the right side of the inequality (7.3) that we want to prove it is just little s <span class=""math-container"">$V_{t+n-1}(s)$</span> ? So kind of confused here which state s it is using? And then I guess after this probably pull out a <span class=""math-container"">$\gamma^{n}$</span> term or something, how should we go from here?</p>

<p>This is the newest Sutton Barto book (book page 144, equation 7.3):
<a href=""https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view</a></p>
"
2567,"<p>Speech is a major primary mechanism of communication between humans. With respect to artificial intelligence, which signal is used to identify the sequence of words in speech?</p>
"
2568,"<p>I was wondering whether a LFSR could be approximated by a NN (output or current state). We know that a LFSR is called linear in some sort of mathematical sense, but is that true? Considering it follows Galois field mathematics. So can a Neural Network approximate a LFSR?</p>

<p>Answers with mathematical proof or actual experience is preferred.</p>
"
2569,"<p>According to the chaos report of the Standish group most IT projects fail. A famous example is the OS/2 operating system, but large scale database projects from the government have also a high probability of wasting time and money. It's important to focus on such software projects, especially if they went wrong because this helps to not repeat the same error twice. Unfortunately, the examples in the chaos report are from classical computing and not from Artificial Intelligence and robotics.</p>

<p>In AI history, the so called AI Winter was a direct result of failed expectations and not working software, for example the “General problem solver” wasn't able to solve practical problems. A failed robotics project from the past was the automation of an assembly line at the Volkswagen car manufacturer, called “Halle 54”. General Motors had made in the 1980s a similar failed robotics project which was “GM Hamtramck”. Are more projects from that category available, and why did they fail? Was it a software problem, or was the communication in the team not very well?</p>
"
2570,"<p>Around 2000, existed popular fights videogames. There, if you made the same kick movement for several running times, the CPU normally defended and caught your strategy. This is the case in Tekken 3 ie.</p>

<p>How did the work? Is that considered intelligence? Did videogames in that time worked based on graphs?</p>
"
2571,"<h2>Background info</h2>

<p>In Python, I've implemented a rudimentary engine to play ""Cheat"", supporting both bots and a human or only bots. When only bots are playing, the game is simulated.</p>

<p>When placing cards, input is represented by an array of integers corresponding to the indices of the cards. When bots play, they are presented with all valid combinations of cards to place (currently, the choice made is random). For example, if the bot has two cards, their options are:</p>

<p><code>[[0], [1], [0, 1]]</code></p>

<p>After a player places cards, the other players get a chance to call cheat (<code>true</code> to accuse, <code>false</code> not to).</p>

<p>When a player depletes their cards, they are appended to the <code>winners</code> list. The goal of the game is to have the lowest index possible in the <code>winners</code> list.</p>

<h2>Summary of game data and end goal</h2>

<p>In summary, here is the data which I believe would be useful for the bots to play:</p>

<ul>
<li>the current number of cards that have been placed</li>
<li>the current type (e.g. Ace) to play</li>
<li>the type of and suit of each card of the bot's hand</li>
<li>the number of cards that were just placed by a player</li>
<li>the possible inputs to play during a bot's turn</li>
<li>the options for calling cheat (<code>true</code>, <code>false</code>)</li>
</ul>

<p>with the goal of ending up with the lowest index in the <code>winners</code> list.</p>

<h2>Help wanted</h2>

<p>I'm very new to machine learning, so I apologize for a such a high level question, but how might I go about using a Python module to implement a system for bots to learn to play intelligently as they play? Are there any modules which you think would be ideal for this situation?</p>

<p>Thank you!</p>
"
2572,"<p>Consider a perceptron where <span class=""math-container"">$w_0=1$</span> and <span class=""math-container"">$w_1=1$</span>:
<a href=""https://i.stack.imgur.com/eBSki.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eBSki.jpg"" alt=""Perceptron""></a>
Now, say we use an activation function </p>

<p><span class=""math-container"">$f(x)=1,~for~x=1$</span><p><span class=""math-container"">$~~~~~~~~~~~~~0, otherwise$</span></p>

<p>The output is then summarised as:<p>
<span class=""math-container"">$x_0~~~~~x_1~~~~~w_0*x_0 + w_1*x1~~~~~f(.)$</span><p>
<span class=""math-container"">$0~~~~~~~0~~~~~~~~~~~~~~0~~~~~~~~~~~~~~~~~~~~~~~~~~~~0$</span><p>
<span class=""math-container"">$0~~~~~~~1~~~~~~~~~~~~~~1~~~~~~~~~~~~~~~~~~~~~~~~~~~~1$</span><p>
<span class=""math-container"">$1~~~~~~~0~~~~~~~~~~~~~~1~~~~~~~~~~~~~~~~~~~~~~~~~~~~1$</span><p>
<span class=""math-container"">$1~~~~~~~1~~~~~~~~~~~~~~2~~~~~~~~~~~~~~~~~~~~~~~~~~~~0$</span></p>

<p>Someone tell Rosenblatt I solved his problem ... </p>

<p>...or have I?</p>

<p>Is there something wrong with the way I've defined the activation function?</p>
"
2573,"<p>I'm trying to train a Siamese network to check if two images are similar. My implementation is based on <a href=""https://leimao.github.io/article/Siamese-Network-MNIST/"" rel=""nofollow noreferrer"">this</a>. I find the Euclidian distance of the feature vectors(the final flattened layer of my CNN) of my two images and train the model using the contrastive loss function. </p>

<p>My question is, how do I get a binary output from the Siamese network for testing (1 if it two images are similar, 0 otherwise). Is it just by thresholding the Euclidian distance to check how similar the images are? If so, how do I go about selecting the threshold? If I wanted to measure the training and validation accuracies, the threshold would have to be increased as the network learns better. Is there a way to learn this threshold for a given dataset? </p>

<p>I would appreciate any leads, thank you.</p>
"
2574,"<p>What is the difference between additive and discounted rewards?</p>
"
2575,"<p>Consider an extremely complicated feed-forward neural network training example but with no need of computational efficiency or limiting of processing time. </p>

<p>What is the maximum number of hidden neurons <strong><em>h</em></strong> that a hidden layer should possess to detect all unique features/ correlations between input data from the previous layer which has <strong><em>n</em></strong> nodes?</p>

<p>In other words if we wanted to create a neural network with a large number of neurons in a hidden layer, what is the maximum neuron count possible that helps the network train (give <strong><em>n</em></strong> neurons are in the previous layer)?</p>
"
2576,"<p>In some tweets about NeurIPS 2018, <a href=""https://news.developer.nvidia.com/nvidia-opening-core-ai-ml-research-lab-in-santa-clara/?_lrsc=45894c3e-3518-40ee-9b28-2be5f79597ef&amp;ncid=so-twi-lt-799"" rel=""nofollow noreferrer"">this video</a> from NVIDIA appeared. At around 0.37, she says:</p>

<blockquote>
  <p>... If you think about the current computations in our deep learning systems, they are all based on Linear Algebra. Can we come up with better paradigms to do multi-dimensional processing. Can we do truly tensor algebraic techniques in our tensor cores...</p>
</blockquote>

<p>I was wondering what she is talking about. I'm not an expert so I'd like to understand better this specific point.</p>
"
2577,"<p>What is the hidden message inside human DNA seequences? More specificly, the relationship between the braiding of the double helix to a person's emotions?</p>
"
2578,"<p>To be honest, I had no idea where to put this question, but it's sure that it's related to AI. I want to build an application which uses camera, and by the movement it can calculate the
-camera's position compared to the objects
-the objects creator and edge points by the movement.</p>

<p>What it means that if the camera is in a static position, it's just a picture. A set of coloured pixels. If we move the camera, we calculate the time, the gyroscope's values, but most importantly, we can have a comparison of two images taken by the same objects. This way:
-we can detect the edges
-from the edges, we can detect which is closer than the others</p>

<p>Today's phone camera's are accurate enough to create ~60 crystal clear images per second, and it should be enough resource to accurately create high res models from just moving the camera according to some instructions (that's why I'm surprised why it isn't existing in just a phone app). Here comes the problem. I think the idea is worth for the try, but I'm just a JavaScript developer. The browser can have access to the camera, with TensorFlow I can use machine learning to detect edges, but if I want to be honest, I have no idea where to start, and how to continue step by step. Can you please provide me some guidelines how it would be ideal to create the idea?</p>
"
2579,"<p>Which do you think would be more helpful to humanity, AI or Automation, and why? how can we differentiate between them?</p>
"
2580,"<p><a href=""https://en.wikipedia.org/wiki/Karel_(programming_language)"" rel=""nofollow noreferrer"">Karel the robot</a> is an education software comparable to turtle graphics to teach programming for beginners. It's a virtual stackbased interpreter to run a domain specific language for moving a robot in a maze. In it's vanilla version, the user authors the script manually. That means, he writes down a computer program like “1. moveforward 2. if reachedobstacle==true then stop 3. moveleft”. This program gets started in the virtual machine.</p>

<p>In contrast, Genetic programming has the aim to produce computercode without human intervention. So called permutations are tested if they are fulfill the constraints and after a while the sourcecode is generated. In most publications, the concept is explained on a machine level. That means, assembly instructions are generated with the aim to replace normal computercode. In “Karel the robot” a high-level language for controlling a robot is presented which has a stack but has a higher abstraction. The advantage is, that the state space is smaller. </p>

<p>My question is: is it possible to generate “Karel the robot” programs with genetic programming?</p>
"
2581,"<blockquote>
  <p>I am searching for an Interpretation of LSTMs and recurrent neural
  Networks within Cognitive Neuroscience.</p>
</blockquote>

<ul>
<li>Is there a mechanism in the human brain, that works analog to LSTMs? </li>
<li>How does Long-term and short-term Memory work in the brain on a Neuron Level?</li>
</ul>

<p>I am looking Forward to hints, explanations and links to in depth literature.</p>

<p>Thank you.</p>
"
2582,"<p>I want like to be able to draw a shape outline e.g. (pen and paper) triangle, square, circle.. then label the vertices and sides And have ML identify the shape and the symbol associates with each vertex / side.</p>

<p>For example a triangle and I label it with the adjacent, opposite and hypotenuse. Or draw parallel and perpendicular lines and label angles and such.</p>

<p>I would prefer it make one myself rather than use a pre built one. However if you know of a simple pre built one then I will ould love to pick it apart.</p>

<p>Can you please share some guidance on how to solve the above problem, namely shape identification / vertex and side labelling.</p>

<p>Any language</p>
"
2583,"<p>In AIMA, 3rd Edition on Page 125, Simulated Annealing is described as:</p>

<blockquote>
  <p>Hill-climbing algorithm that <em>never</em> makes “downhill” moves toward states with lower value (or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maximum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly at random from the set of successors—is complete but extremely inefficient. Therefore, it seems reasonable to try to combine hill climbing with a random walk in some way that yields both efficiency and completeness. Simulated annealing is such an algorithm. In metallurgy, <strong>annealing</strong> is the process used to temper or harden metals and glass by heating them to a high temperature and then gradually cooling them, thus allowing the material to reach a lowenergy crystalline state. To explain simulated annealing, we switch our point of view from hill climbing to <strong>gradient descent</strong> (i.e., minimizing cost) and imagine the task of getting a
  ping-pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will come to rest at a local minimum. If we shake the surface, we can bounce the ball out of the local minimum. The trick is to shake just hard enough to bounce the ball out of local minima but not hard enough to dislodge it from the global minimum. The simulated-annealing solution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the intensity of the shaking (i.e., lower the temperature)</p>
</blockquote>

<p>I know its about its example, but I just want more examples where Stimulated Annealing used in daily life </p>
"
2584,"<p>Some early AI research, inspired by Claude Shannon's maze learning mouse, <em>Theseus</em>, sought to discover resolutions to conflict. In the case of Theseus, the goal was to resolve the conflict between the simulated hunger and the walls of the maze separating Theseus from the cheese. Researchers of early AI theorem proving software in LISP sought ways out of mathematical mazes. Finding the cheese, for those theorem provers, was to find a logical proof.</p>

<p><strong>Framing Intelligence as an Adaptive Response to Opposition</strong></p>

<p>When Morgenstern and von Neumann's game theory was applied, it was decided that the games would be games of opposition rather than games collaboration, possibly a consequence of the source of funding for much of the research. The software was designed such that the only other intelligence encountered in game play was an opponent and the goal was to annihilate it.</p>

<p>Dialog created by Ted Chiang and Eric Heisserer in Denis Villeneuve's <em>Arrival</em>, 2016, starring Amy Adams, Jeremy Renner, and Forest Whitaker, exposes the folly of approaching new minds with the assumption of adversity.</p>

<blockquote>
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LOUISE <br>
  Following suit. Suits. Suits, honor, flowers. Colonel, those are all tile sets in mah-jongg. God, are they... Are the Chinese using a game to converse with their heptapods?</p>
  
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WEBER <br>
  Maybe. Why? </p>
  
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LOUISE <br>
  Well, let's say that I taught them chess instead of English. Every conversation would be a game. Every idea expressed through opposition, victory, defeat. You see the problem? If all I ever gave you was a hammer ...</p>
  
  <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WEBER <br>
  Everything's a nail.</p>
</blockquote>

<p>In the third act, the viewer discovers that the objective of game play was more like Shannon's maze and that projected adversarialism was the only wall in the maze.</p>

<p><strong>Consciously Directing Technological Advancement</strong></p>

<p>This leads to some questions underlying the main question.</p>

<ul>
<li>Do we want AI developed in a laboratory setting to be developmentally equivalent to a human child growing up in a war zone?</li>
<li>Is elimination of the enemy a proven successful strategy in human geopolitical conflict?</li>
<li>What is the trend of blow-back from annihilation demonstrated throughout history?</li>
<li>Wouldn't an AI system that seeks to discover win-win scenarios where players work together to overcome shared obstacles be better?</li>
<li>Should AI research return to its Claude Shannon roots, where the conflict is between obstacles of the physical universe and objectives shared by living things?</li>
<li>Do we want to imbue into intelligent robots and disembodied intelligence systems an obsession with winning or a more balanced set of objectives that includes collaborating?</li>
<li>How can AI be developed to win over things like poverty, crime, disease, ignorance, addiction, and economic instability?</li>
<li>As automated decision making develops, is it time to think about good AI citizenship?</li>
</ul>

<p>With the power and complexity of AI systems increasing, when we researchers and engineers create loss, error, value, and reward functions, should we develop the discipline of always considering whether we are creating learning incentives that point the AI in the direction of becoming good contributors rather than narrow minded sociopaths?</p>

<p>Must adaptation in artificial mental capacities be neither adversarial (as in a chess or go player) nor codependent (as in Asimov's second law) but rather compassionate, loving, transparent, and interested in growing authentic relationships of mutual benefit?</p>

<p><strong>Central Question and Specifics</strong></p>

<blockquote>
  <p>Would AI systems not obsessed with winning become better citizens of the world?</p>
</blockquote>

<p>What work is being done along these lines and how can best practices be developed to intentionally and responsibly steer AI development?</p>

<p>Although these questions of the direction of technology were topics of science fiction and philosophy in the twentieth century, in the twenty first century, they are necessities of responsible research. It is wise to consider them legal and social questions that deserve the rigor of mathematical formalization and long term risk management, just as should be done with nuclear and genetic technologies.</p>

<hr>

<p><strong>Addendum Response to Comments</strong></p>

<blockquote>
  <p>At some point doesn't it boil down to winning against the enemy, shared obstacles?</p>
</blockquote>

<p>Whether the assignment of enemy status to members of the same species is of value to the species is questionable. Evidence indicates human excellence to be primarily the result of collaboration, win-win scenarios, and symbiotic relationships. It is possible that further work on modelling civilization may someday reveal that framing all activity as a competition of some type may be a disease of the collective the primary consequence of which is the erosion of excellence.</p>
"
2585,"<p>Which is best approach to choose for game playing problem?</p>
"
2586,"<p>I want to learn a lot about the AI of CCG, such as Hearthstone. And now I have known one of the main algorithms that used in this kind of games, MCTS. It analyses the most promising moves, and expands the search tree based on random sampling of the search space. But there are too many random events in this game that can cause different results to one battle. For example, a card can randomly deal X damage to a hero or other follower, and X is a random number from 0 to 30. The number of X is important for the next decision, but there will be a low accuracy by only using MCTS.</p>

<p>So what does the AI do to deal with these random events?</p>
"
2587,"<p>I am trying to understand how to train a neural network to win a Pong game using reinforcement learning, by following the blog post <a href=""https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/"" rel=""nofollow noreferrer"">
Spinning up a Pong AI with deep reinforcement learning</a>.</p>

<p>The environment is provided by <a href=""http://gym.openai.com/envs/Pong-v0/"" rel=""nofollow noreferrer"">Gym AI</a>. It gives the AI a reward of 1 if the opponent misses the ball, and a reward of -1 if it misses the ball.</p>

<p>I am confused about how reward discounting works in this context. This is the function that <a href=""https://github.com/mtrazzi/spinning-up-a-Pong-AI-with-deep-RL/blob/master/karpathy.py"" rel=""nofollow noreferrer"">the blog post used</a>:</p>

<pre><code>def discount_rewards(r, gamma):
  """""" take 1D float array of rewards and compute discounted reward """"""
  r = np.array(r)
  discounted_r = np.zeros_like(r)
  running_add = 0
  # we go from last reward to first one so we don't have to do exponentiations
  for t in reversed(range(0, r.size)):
    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum
    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently
    discounted_r[t] = running_add
  discounted_r -= np.mean(discounted_r) #normalizing the result
  discounted_r /= np.std(discounted_r) #idem
  return discounted_r
</code></pre>

<p>Basically, the list of rewards is mostly filled with zeros, because usually nothing happens. When something happens, e.g. the reward is 1, this is not only due to the action taken in that step. Therefore, we need to smoothen the list of rewards so that some of that reward also belongs to previous actions. So far so good. However, it seems to me that if the opponent misses the ball and the reward is 1, then this will be smeared such that it will emphasize the actions taken right before the opponent missed the ball. This seems wrong to me, the actions taken by the AI right before the opponent missed the ball are irrelevant. They don't affect the ball in any way.</p>

<p>I only think reward discounting makes sense when you have lost a point, then the actions just preceding the loss are surely very important and should be emphasized. However, the function takes into account both wins and losses.</p>

<p>How should reward discounting be understood in the context of this Pong game?</p>
"
2588,"<p>I want to implement YOLO V3. I want to know which framework will give me a faster result.</p>

<p>what are the advantages of implementing YOLO V3 on darknet framework vs Keras framework?</p>
"
2589,"<p>A* algorithm is based on which search methods?Explain me with example and is it optimal or complete if yes then how explain briefly?</p>
"
2590,"<p>What kind of problems is simulated annealing better suited for compared to genetic algorithms?</p>

<p>From my experience, genetic algorithms seem to perform better than simulated annealing for most problems.</p>
"
2591,"<p>I am classifying about 9 books from the image of their cover pages. I am using a TensorFlow Keras CNN model for this. But, the model predicts a book even when a picture of a book is not taken, like a wall, sofa, house etc. I want to avoid this. I want the model to <strong>first classify whether there is a book in the image</strong> and then <strong>classify the book in 9 classes</strong>. How could I achieve this?</p>
"
2592,"<p>This question is regarding <strong>Reinforcement Learning</strong> and <strong>different/inconsistent action space for every/some states</strong>.</p>

<h2>What do I mean by <i>inconsistent action space</i>?</h2>

<p>Let say you have an MDP where the number of actions varies between states (for example like in Figure 1 or Figure 2). We can express ""inconsistent action space"" formally as <span class=""math-container"">$$\forall s \in S: \exists s' \in S: A(s) \neq A(s') \wedge s \neq s'$$</span></p>

<p>That is, for every state, there exists some other state which do not have the same action set. In the figures (1, 2) there's a relatively small amount of actions per state. Instead imagine states <span class=""math-container"">$s \in S$</span> with <span class=""math-container"">$m_s$</span> number of actions, where <span class=""math-container"">$1 \leq m_s \leq n$</span> and <span class=""math-container"">$n$</span> is a really large integer.  </p>

<p><a href=""https://i.stack.imgur.com/q2Huu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q2Huu.png"" alt=""Some MDP&#39;s""></a></p>

<h2>Environment</h2>

<p>To get a better grasp of the question, here's an environment example. Take Figure 1 and let it explode into a really large directed acyclic graph with a source node, huge action space and a target node. The goal is to traverse a path, starting at any start node, such that we'll maximize the reward which we'll only receive at the target node. At every state, we can call a function <span class=""math-container"">$M : s \rightarrow A'$</span> that takes a state as input and returns a valid number of actions. </p>

<h2>Approches</h2>

<p>(1) A naive approach to this problem (discussed <a href=""https://ai.stackexchange.com/questions/7755/how-to-implement-a-constrained-action-space-in-reinforcement-learning"">here</a> and <a href=""https://ai.stackexchange.com/questions/2219/board-card-game-ai-questions-concerning-state-action-space-deep-reinforcemen"">here</a>) is to define the action set equally for every state, return a negative reward whenever the performed action <span class=""math-container"">$a \notin A(s)$</span> and move the agent into the same state, thus letting the agent ""learn"" what actions are valid in each state. This approach has two obvious drawbacks:</p>

<ul>
<li>Learning <span class=""math-container"">$A$</span> takes time, especially when the Q-values are not updated until either termination or some statement is fulfilled (like in <a href=""https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits"">experience replay</a>)</li>
<li>We know <span class=""math-container"">$A$</span>, why learn it?   </li>
</ul>

<p>(2) Another approach (first answer <a href=""https://ai.stackexchange.com/questions/7755/how-to-implement-a-constrained-action-space-in-reinforcement-learning"">here</a>, also very much alike proposals from papers such as <a href=""https://arxiv.org/pdf/1512.07679"" rel=""nofollow noreferrer"">Deep Reinforcement Learning in Large Discrete Action Spaces</a> and <a href=""https://arxiv.org/pdf/1705.05035"" rel=""nofollow noreferrer"">Discrete Sequential Prediction of continuous
action for Deep RL</a>) is to instead predict some scalar in continuous space and by some method map it into a valid action. The papers are discussing how to deal with large discrete action spaces and the proposed models seam to be a somewhat solution for this problem as well.   </p>

<p>(3) Another approach that came across was to, assuming the number of different action set <span class=""math-container"">$n$</span> is quite small, have functions <span class=""math-container"">$f_{\theta_1}$</span>, <span class=""math-container"">$f_{\theta_2}$</span>, ..., <span class=""math-container"">$f_{\theta_n}$</span> that returns the action regarding that perticular state with <span class=""math-container"">$n$</span> valid actions. E.i, the performed action of a state <span class=""math-container"">$s$</span> with 3 number of actions will be predicted by <span class=""math-container"">$\underset{a}{\text{argmax}} \ f_{\theta_3}(s, a)$</span>.  </p>

<p>None of the approaches (1, 2 or 3) are found in papers, just pure speculations. I've searched a lot but cannot find papers directly regarding this matter. My questions are therefore</p>

<ol>
<li>Does anyone know any paper regarding this subject?</li>
<li>Is the terminology wrong? ""Inconsistant"", ""Irregular"", ""Different""... ?</li>
<li>Anyone having another approach worth digging into?</li>
</ol>
"
2593,"<p>I'm working on a home tool that will help create a shopping list from a list of recipes chosen for a coming week.</p>

<p>This boils down to:</p>

<ol>
<li>Extracting ingredients and their quantities from recipes.</li>
<li>Grouping similar ingredients together.</li>
<li>Summing up quantities for similar ingredients.</li>
<li>Naming groups of similar products in a shopping list.</li>
</ol>

<p>The tasks seem non-trivial for a few reasons.</p>

<ul>
<li><p><strong>Similar ingredients are described differently</strong>, depending on the recipe book/portal, e.g.:</p>

<blockquote>
  <ul>
  <li>5 lemons</li>
  <li>5 lemons (to be squeezed)</li>
  <li>5 fresh lemons</li>
  <li>5 big yellow lemons</li>
  </ul>
</blockquote></li>
<li><p><strong>Recipes lists alternatives for ingredients</strong> (e.g., <em>""3 lemons or 5 limes""</em>), leaving decision up to a user. </p></li>
<li><p><strong>Recipes involve some information about product-preprocessing</strong>. For instance, one has to buy lemons instead of lemon juice when the recipe says:</p>

<blockquote>
  <ul>
  <li>100ml lemon juice</li>
  <li>100ml freshly squeezed lemon juice</li>
  </ul>
</blockquote></li>
<li><p><strong>My language has a complex inflection</strong>. For instance, there can be multiple plural forms of a noun and the form of an adjective must be agreed with a form of noun. Adapting NLP algorithms designed for English language might be not straightforward and require some lemmatizing/stemming but not for single words, but whole phrases.</p></li>
<li><p><strong>Naming products group is hard</strong>. Once fresh lemons and big yellow lemons are group together and their quantities summed up, one need to decide how to name this group in a shopping list, e.g.: <em>""10 lemons""</em> or <em>""10 fresh lemons""</em>.</p></li>
</ul>

<p>Is there any research paper that would cover those challenges?</p>

<p>Especially applied in the same domain? </p>
"
2594,"<p>I wonder if it's possible to 'clean up' an audio recording of a lecture from a smartphone, using some type of AI system?</p>
"
2595,"<p>How to select an action in a state if the action does not necesarily cause the environment to change state? </p>

<p>Given 10 states (<span class=""math-container"">$S_0$</span> to <span class=""math-container"">$S_9$</span>) and in each state <span class=""math-container"">$i$</span> there are two actions defined <span class=""math-container"">$(1,-1)$</span>. <span class=""math-container"">$1$</span> increases a parameter of the environment and <span class=""math-container"">$-1$</span> decreases it.</p>

<p>For example, if the parameter is speed and it is currently 1 rad/s, corresponding to <span class=""math-container"">${S_i}$</span>, an action can be either increase or decrease this and therefore transition to the next state, ideally <span class=""math-container"">$S_{i-1}$</span> or <span class=""math-container"">$S_{i+1}$</span>.</p>

<p>It is unclear how to formulate a reinforcement learning problem in this case. The problem is the same magnitude of the parameter change through an action. </p>

<p>The range of the parameter is (3, 25) and step size is 1. The problem is thath the responce of the environment is not the same in every satate. In some states a parameter change with magnitude 1 results in a state transition in some state the magintude proves to be to small to provoke a transition change. </p>

<p>For example, if the envitonment is instate <span class=""math-container"">$S_1$</span> and action 1 is applied, how can the magintude of the paremter be adopted in a way which assures that the environment will transition to state <span class=""math-container"">$S_2$</span>?  How can the step in the paremeter change be made adaptive? Actually my environment is uncertain that is why I don't know exactly whether this action will take me to next state or not.</p>

<p>For your information, I am using Q learning off policy algorithm. Suppose state 9 is the goal state and  my Q table is <span class=""math-container"">$10 \times 2$</span>.</p>
"
2596,"<p>I've been working on a game-playing engine for about half a year now, and it uses the well known algorithms. These include minimax+alpha-beta pruning, iterative deepening, transposition tables, etc.</p>

<p>I'm now looking for a way to include Monte Carlo search, which is something I've wanted to do for a long time. I was thinking of just making a new engine from scratch, but if possible I'd like to somehow import MC search into the engine I've already built.</p>

<p><strong>Question</strong>: Are there any interesting strategies to import MC search into a standard game-playing AI?</p>
"
2597,"<p>I am learning about probabilistic graphical models and I was wondering if there is an example explaining the math behind Conditional random Fields. Looking solely on the formula I have no Idea what we actually do. I found a lot of examples for Hidden Markov Model. There is a part speech-tag task where we have to find the tags for the sentence ""flies like a flower"". On these slides (slide 8) <a href=""http://www.cse.aucegypt.edu/~rafea/CSCE569/slides/ch7.pdf"" rel=""nofollow noreferrer"">Ambiguity Resolution: Statistical Method-Prof. Ahmed Rafea</a> a HMM is used to find the correct tags. How would I transform this model to a CRF and how would I apply the math?</p>
"
2598,"<p>According to this website: <a href=""http://yarpiz.com/67/ypea104-acor"" rel=""nofollow noreferrer"">http://yarpiz.com/67/ypea104-acor</a> (in the website it is mentioned that it is a project aiming to be a resource of academic and professional scientific source codes and tutorials.):</p>

<blockquote>
  <p>""Originally, the Ant Algorithms are used to solve <strong>discrete and
  combinatorial</strong> optimization problems. Various extensions of Ant Colony
  Optimization (ACO) are proposed to deal with optimization problems,
  defined in <strong>continuous domains</strong>. One of the most useful algorithms of
  this type, is ACOR, the Ant Colony Optimization for Continuous
  Domains, proposed by Socha and Dorigo, in 2008 (<a href=""http://www.sciencedirect.com/science/article/pii/S0377221706006333"" rel=""nofollow noreferrer"">here</a>).""</p>
</blockquote>

<p>My question is that what is difference between <strong>Continuous Domains</strong> vs. <strong>Discrete and  combinatorial problems</strong> ? I appreciate if you could also mention some examples for each type.</p>
"
2599,"<p>Is it possible to mention the drawbacks/advantages of <strong>Swarm routing</strong> (such as Ant routing etc) in comparison with <strong>classical routing</strong> algorithms in communication networks in a general view?</p>

<p>In other words, what will we gain if we replace a classical routing algorithm with a swarm routing based algorithm? </p>

<p>Can we compare these two type of routing algorithms in a general view to mention their to count the drawbacks/advantages?</p>

<p>The main purpose of this question is to define applications of each of those routing approaches. Which one is more decentralized? And which one has more efficient performance?</p>

<p>Here is my personal opinion (I am not sure about it) :</p>

<p><strong>classical</strong> internet routing in <strong>more centralized</strong> than <strong>swarm</strong> routing (such as <strong>ACO</strong> based routing) which does <strong>not</strong> use any <strong>routing table and router</strong> to avoid moving towards <strong>centralization</strong> where those routing tables and routers <strong>can be manipulated</strong> (as a <strong>point of failure</strong>). Instead, <strong>classical</strong> internet routing may be <strong>faster</strong> than <strong>swarm</strong> based routing. Briefly, <strong>classical</strong> may be <strong>more centralized</strong>, but <strong>faster</strong> and on the other side, <strong>swarm</strong> based is <strong>more decentralized</strong> but may be <strong>slower</strong>. Am I wrong?</p>

<p><strong>Please note</strong> that when I say ""<strong>decentralized</strong>"", I mean a network like ""<strong>ad hoc networks</strong>"" that do <strong>not</strong> rely on <strong>routers or access points</strong> (as a <strong>point of failure</strong>) to avoid moving towards <strong>centralization</strong>. In case of using routers or access point, some kind of <strong>centralization</strong> is inherent. With this definition of <strong>decentralization</strong>, it seems <strong>Swarm</strong> based routing such as <strong>Ant routing</strong> would be <strong>more decentralized</strong>. <strong>However, I am not sure about it and that's my main question.</strong></p>
"
2600,"<p>[English is not my native language and I find it hard to express something that I want to search for in Artificial Intelligence papers.]</p>

<p>People could be sad, happy, depressive, angry, nervous, calm, relaxed, bored, etc. I don't know how to express all of these feelings and emotions in English terms that enable me to search papers related to them.</p>

<p>I want to know if there is some research about how to identify these feelings with artificial intelligence.</p>

<p>How can I search for the above in search engines like IEEE Xplore or Scopus, ScienceDirect, etc.?</p>

<p><strong>UDPATE</strong><br/>
I want to identify a person's emotions using facial expression, heartbeat, body temperature, sweating or nervous behaviour (using one or all of them).</p>
"
2601,"<p>When adding dropout to a neural network, we are randomly removing a fraction of the connections (setting those weights to zero for that specific weight update iteration). If the dropout probability is p, then we are effectively training with a neural network of size 1−p*(Number of original units).</p>

<p>Using this logic, there is not limit how big I can make a network, as long as I proportionately increase dropout, I can always effectively train with the same sized network, and thereby just increasing the number of ""independent"" models working together, making a larger ensemble model. Thereby <strong>improving generalization</strong> of the model.</p>

<p>For example, if a network with 2 units already achieves good results in the training set (but not in unseen data -i.e validation or test sets-), also a network with 4 units + dropout 0.5 (ensemble of 2 models), and also a network with 8 units + dropout 0.75 (ensemble of 4 models)... and also a network with 1000 units with a dropout of 0.998 (ensemble of 500 models)!</p>

<p>In practice it is recommended to keep dropout at 0.5, which advises against the approach mentioned above. So there seem to be reasons for this.</p>

<p>What speaks against blowing up a model together with an adjusted dropout parameter?</p>
"
2602,"<p>In reinforcement learning book from Sutton &amp; Barto (2018 edition), specifically in section 7.5 of the book, they present an n-step off-policy algorithm that doesn't require importance sampling called n-step tree backup algorithm. In other algorithms the return in the update was consisted of rewards along the way and the estimated value(s) of the node(s) at the bottom, but in tree backup update a return is consisted of things mentioned before plus the estimated values of the actions that weren't picked during these n steps, all weighted by the probability of taking the action from the previous step.<br>
I have few questions about things in this algorithm that are unclear to me.</p>

<ol>
<li><p>question: Why is this algorithm consider an off-policy algorithm? As far as I could notice, only a single target policy is mentioned and there is no talk about behaviour policy generating actions to take.</p></li>
<li><p>question: In control we want our target policy to be deterministic, greedy policy, so how do we exactly generate actions to take in this case since behaviour policy isn't used? If we generate actions from greedy policy we wont explore so we won't learn the optimal policy. What am I missing here?</p></li>
<li><p>question: If I understood something wrong and we are actually using behaviour policy, I don't understand how would the update work in the case where our target policy is greedy. The return is consisted of estimates taken from actions that werent picked, but because our policy is greedy the probabilities used in calculating those estimates would be 0 so the total estimate of those actions would be 0 as well. The only non 0 probability in our target policy is the one of the greedy action(which is probability of 1) so the entire return would fall down to n-step SARSA return. So basically my question here is how are we allowed to do this update in this case, why is this return allowed to replace the one with importance sampling?</p></li>
</ol>
"
2603,"<p>In this article <a href=""https://www.technologyreview.com/s/612561/a-radical-new-neural-network-design-could-overcome-big-challenges-in-ai/"" rel=""nofollow noreferrer"">here</a>, the writer claims that a new type of neural net is required to deal with data that is both continuous, and also sparsely sampled. </p>

<p>It was my understanding that this was the entire purpose of techniques that use neural nets, to make assumptions about a system with a non-continuous data set.</p>

<p>So why do we need to switch to a non-layered design to deal with these data sets better?</p>
"
2604,"<p>I am trying to use tensorflow / keras to play a text based game. The game opposes two players that play by answering questions by choosing an answer among the proposed ones.</p>

<p>Game resembles this:</p>

<ol>
<li>Questions asked from player 1, choose value {0, 1, 2}<br/></li>
<li>Player 1 chooses answer 1<br/></li>
<li>Questions asked from player 2, choose value {0, 1}<br/></li>
<li>Player 2 chooses answer 0<br/>
( and so on )<br/></li>
</ol>

<p>The issue is that I do not have any data to use for training the agents and it not possible to evaluate each actions of the agent individually.<br /></p>

<p>My idea is to get 2 agents to play against each other and evaluate them depending on who won / lost ( the games are very short with about 20 to 30 decisions made for each player ).</p>

<p>The issue I have is that I do not know where to start. </p>

<p>I normalized my input, but I do not know how to get the 2 agents to compete, as I do not have any training data as shown in the tutorials and the agents have to complete a full game in order to evaluate their performance.</p>
"
2605,"<p>I've been re-reading the <a href=""https://en.wikipedia.org/wiki/Chinese_room"" rel=""nofollow noreferrer"">Wikipedia article on the Chinese Room argument</a> and I'm... actually quite unimpressed by it. It seems to me to be largely a semantic issue involving the conflation of various meanings of the word ""understand"". Of course, since people have been arguing about it for 25 years, I doubt very much that I'm right. However... the argument can be thought of as consisting of several layers, each of which I can explain away (to myself, at least).</p>

<ul>
<li><strong>There is the assumption that being able to understand (interpret a sentence in) a language is a prerequisite to speaking in it.</strong><br>
Let's say that I don't speak a word of Chinese, but I have access to a big dictionary and a grammar table. I could work out what each sentence means, answer it, and then translate that answer back into Chinese, all without speaking Chinese myself. Therefore, being able to interpret (parse) a language is not a prerequisite to speaking it.<br>
(Of course, by <a href=""https://en.wikipedia.org/wiki/Extended_cognition"" rel=""nofollow noreferrer"">the theory of extended cognition</a> I <em>can</em> interpret the language, but we can all agree that the books and lookup tables are simply a source of information and not an algorithm; I'm still the one using them.)<br>
Nevertheless, this task can be removed by a dumb natural language parser and a dictionary, converting Chinese to the set of concepts and relationships encoded in it and vice versa. There is no understanding involved at this stage.</li>
<li><strong>There is the assumption that being able to understand (identify and maintain a train of thought about concepts in) a language is a prerequisite to speaking in it.</strong><br>
We've already optimised away the language, to a set of concepts and relationships between concepts. Now all we need is another lookup table: a sort of verbose dictionary that maps concepts to other concepts and relationships between them. For example, one entry for ""computer"" might be ""performs calculations"" and another might be ""allows people to play games"". An entry for ""person"" might be ""has opinions"", and another might be ""has possessions"". Then an algorithm (yes, I'm introducing one now!) would complete a simple optimisation problem to find a relevant set of concepts and relationships between them, and turn ""I like playing computer games"" into ""What is your favourite game to play on a computer?"" or, if it had some entries on ""computer games"", ""Which console do you own?"".<br>
The only ""understanding"" here, apart from the dumb optimisation algorithm, is the knowledge bank. This could conceivably be parsed from Wikipedia, but for a good result it would probably be at least somewhat hand-crafted. Following this would fall down, because this process wouldn't be able to talk about itself.</li>
<li><strong>There is the assumption that being able to understand (""know"" how information in affects one's self) a language is a prerequisite to speaking in it.</strong><br>
A set of ""opinions"" and such associated with the concept ""self"" could be implemented into the knowledge bank. All meta-cognition could be emulated by ensuring that the knowledge bank had information about cognition in it. However, this program would still just be a mapping from arbitrary inputs to outputs; even if the knowledge bank was mutable (so that it could retain the current topics from sentence to sentence and learn new information) it would still, for example, not react when a sentence is repeated to it verbatim 49 times.</li>
<li><strong>There is the assumption that being able to have effective meta-cognition is a prerequisite to speaking in a language.</strong><br>
Except... there's not. The program described would probably pass the Turing Test. It certainly fulfils the criteria of speaking Chinese. And yet it clearly doesn't think; it's a glorified search engine. It'd probably be able to solve maths problems, would be ignorant of algebra unless somebody taught it to it (in which case, with sufficient teaching, it'd be able to carry out algebraic formulae; Haskell's type system can do this without ever touching a numerical primitive!), and would probably be Turing-complete, and yet wouldn't think. And that's OK.</li>
</ul>

<p>So why is the Chinese Room argument such a big deal? What have I misinterpreted? This program understands the Chinese language as much as a Python interpreter understands Python, but there is no conscious being to ""understand"". I don't see the philosophical problem with that.</p>
"
2606,"<p>I recently encountered an interesting problem and was wondering how RL would solve it.  The objective of the problem is to maximize the coffee quality, given by box X.  The coffee quality objective function is defined by the company.  </p>

<p>To maximize the quality of the coffee, we can perform 2 actions:</p>

<ul>
<li>change stirring speed of the coffee machine</li>
<li>change the temperature of the coffee machine</li>
</ul>

<p>Now to the tricky part.  The coffee bean characteristic coming into the coffee machine is random.  We can measure its characteristics before sending them to the coffee machine, but we <strong>cannot</strong> change them.</p>

<p>I formulated this problem into a control problem, such that <span class=""math-container"">$X$</span> is a function of my previous states, <span class=""math-container"">$X(t - k)$</span>, the control input , <span class=""math-container"">$U$</span>, and the measured disturbance, <span class=""math-container"">$D$</span>. Given a constant <span class=""math-container"">$D$</span>, the problem is trival to solve because the disturbance is a constant part of the environment.  However, during times when <span class=""math-container"">$D$</span> changes rapidly, the policy is no longer optimal.</p>

<p>How do I inject the information of the measured disturbance into my RL agent?</p>

<p><a href=""https://i.stack.imgur.com/WTkQ9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WTkQ9.png"" alt=""enter image description here""></a></p>
"
2607,"<p>I'm currently implementing an Android app for street sign recognition. My solution works quite well for the GTSRB dataset, since it provides a labeled test set of centered images. However, it doesn't scale up to more realistic scenarios like for images in the GTSDB, where the signs only take up some pixels. Is it still recommended to downsample the image to 224x224?</p>
"
2608,"<p>I already post the question on Stackoverflow : <a href=""https://stackoverflow.com/questions/53785922/is-the-use-of-a-non-monotonic-activation-function-is-a-correct-and-viable-solut?noredirect=1#comment94423971_53785922"">https://stackoverflow.com/questions/53785922/is-the-use-of-a-non-monotonic-activation-function-is-a-correct-and-viable-solut?noredirect=1#comment94423971_53785922</a></p>

<p>I found several papers about how to build a single layer perceptron able to solve the XOR problem. The papers describe a solution where the heaviside step function is replaced by a non-monotonic activation function. Here is the papers :</p>

<ul>
<li><a href=""http://hsmazumdar.net/single_layer_neural_net.htm"" rel=""nofollow noreferrer"">http://hsmazumdar.net/single_layer_neural_net.htm</a></li>
<li><a href=""https://www.researchgate.net/publication/227309651_Solving_the_XOR_and_parity_Nproblems_using_a_single_univers"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/227309651_Solving_the_XOR_and_parity_Nproblems_using_a_single_univers</a></li>
</ul>

<p>I also found this topic on Stackoverflow : <a href=""https://stackoverflow.com/questions/30412427/solving-xor-with-single-layer-perceptron"">https://stackoverflow.com/questions/30412427/solving-xor-with-single-layer-perceptron</a></p>

<p>Can we really solve the XOR problem with a simple perceptron ? Because the use of a non-monotonic activation function is not really common on this topic. Papers about this idea are scarce. Generally, the main solution is to build a multilayer perceptron.</p>
"
2609,"<p>I'm completely new at ML, but really interested. To be honest, read many articles about it, but still don't understand the workings of it.</p>

<p>I just started to understand this example: <a href=""https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html"" rel=""nofollow noreferrer"">https://storage.googleapis.com/tfjs-examples/mnist/dist/index.html</a></p>

<p>My thinking about it is that TF has some resources, some examples of how numbers look like, and try to match them with the ones in the test. I saw that sometimes the test changes a right prediction to a wrong, but makes better and better predictions. But how? I think that the program doesn't know the right predictions (and this way it won't know the wrong ones). In the training how it makes better predictions? Test by test, from what exceptions it will change it's predictions? What happens in a new test?</p>
"
2610,"<p>I am a computer science student and my task is to develop a mobile app for Android and/ or IOS using Artificial Intelligence, designed to help people reduce/ combat depression.Regarding the programming language, I am thinking to go with JavaScript because this is the one I am more comfortable with. 
I mention that my knowledge on artificial intelligence is little and I haven ' t done anything in AI or of this kind before.
 I am planning to use AI to help people suffering from depression by creating a mobile app:</p>

<ul>
<li>user-friendly app in terms of design and sequence of actions ,easily customizable looks and functionality</li>
<li>to be able to import all the relevant information:key words, text, images, videos from Facebook, Instagram, Twitter to prevent or report the existence of depression and correctly interpret the nature, intensity and frequency of depression symptoms</li>
<li>the app includes: a patient profile,an achievement board and a dream chart page, to-do list page, led lights which react to emotions, facial recognition technology, Thinking Patterns page( the app uses the principals of cognitive- behavioral therapy) </li>
<li>incorporates sensors that can measure changes in mood, sleep patterns and increased isolation, measuring the device lock time, ambient lighting and audio to predict sleep time</li>
<li>utilizes GPS sensor, recent calls to monitor the wellbeing of the patients</li>
<li>the app provides users with self-help guidelines/techniques,questionnaire designed to track severity of symptoms over time, educational resources for treatment</li>
<li>the users are encouraged to write down daily thoughts in order to analyze and identify negative thinking patterns</li>
<li>the app sends notifications to remind the patients to take their medication, is offering activity suggestions</li>
<li><p>the app incorporates appointment scheduler tool for the counseling sessions including private messages, online meetings</p>

<ol>
<li><p>How do I get started? What platform would you recommend me to develop a mental health mobile app (for Android and/or iOS) using AI in order to help people suffering from depression to combat it and why? A platform based on cloud or better not?</p></li>
<li><p>Can a chat bot be integrated in a mental health mobile app?</p></li>
</ol>

<p>Please, suggest me some related literature, eBooks to set my research objectives because I'm having a hard time finding it.
Please, help me!Thank you very much in advance.</p></li>
</ul>
"
2611,"<p>I have multiple pictures that look exactly like the one below this text. I'm trying to train <code>CNN</code> to read the digits for me. Problem is isolating the digits. They could be written in any shape, way, and position that person who is writing them wanted to. I thought of maybe training another <code>CNN</code> to recognize the position/location of the digits, but I'm not sure how to approach the problem. But, I need to get rid of that string and underline. Any clue would be a great one. Btw. I would love to get the 28x28 format just like the one in <code>MNIST</code>.</p>

<p>Thanks up front.</p>

<p><a href=""https://i.stack.imgur.com/wOMma.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wOMma.jpg"" alt=""enter image description here""></a></p>
"
2612,"<p>I've been reading up on how NEAT (Neuro Evolution of Augmenting Topologies) works and i've got the main idea of it, but one thing that's been bothering me is how you split the different networks into species. I've gone through the algorithm but it doesn't make a lot of sense to me and the paper I read doesn't explain it very well either so if someone could give an explanation of what each component is and what it's doing then that would be great thanks.</p>

<p>The 2 equations are:</p>

<p><span class=""math-container"">$\delta = \frac{c_{1}E}{N} + \frac{c_{2}D}{N} + c_{3} .\overline{W}$</span></p>

<p><span class=""math-container"">$f_{i}^{'} = \frac{f_i}{\sum_{j=1}^{n}sh(\delta(i,j))}$</span></p>

<p>btw i can understand the greek symbols so you don't need to explain those to me</p>

<p><a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf"" rel=""nofollow noreferrer"">The original paper</a></p>
"
2613,"<p>What's the strategy if the resolution of an image is very low such as 28 x 28 or 100 x 100 or 150 x 150, for <strong>transfer learning</strong>?</p>

<p>Pre-trained models such as Inception, Xception, VGG-16 etc are required specific size images. In these case, how to solve this issue? In addition, what also about very high-resolution images?</p>

<p>Training such model and concern weights on such data set which is far more different than image-net data set, I know that we should take whole model structure but except that last softmax classifier by setting <code>include_top = false</code>. And manually setting this for the specific task and performing backpropagation for fine-tuning. </p>

<p>Can anyone please inform more info on this? I read <code>keras</code> doc but these few silly things stuck on my head. Thanks.</p>

<p>With Appreciation,
In.</p>
"
2614,"<p>I have heard about bidirectional RNN LSTM units (endcoders-decoders), but my question is - is there bidirectional neural machine translation, that uses A->B weights for the translation in the opposite direction B->A? If not, then what are the obstacles to such system?</p>
"
2615,"<p>I was reading this paper <a href=""https://arxiv.org/abs/1610.04325"" rel=""nofollow noreferrer"">Hadamard Product for Low-rank Bilinear Pooling</a> I understand what they are trying to say but I don't know why do we have to convert the element-wise multiplication into a scalar(using the dot product)
<a href=""https://i.stack.imgur.com/75PHK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/75PHK.png"" alt=""in equation 2""></a><br>
Why do we have to multiply the resulting vector by the one vector?<br>
I mean, we would still use the multiplicative interaction between elements if we did not consider multiplying by that one vector</p>
"
2616,"<p>If I got well the global idea of DropOut it allows to improve the sparsity of the information that come from one layer to another by setting some weights to zero.</p>

<p>In another hand, pooling, let's say max pooling, takes the maximum value in a neighborhood, reducing as well to zero, the influence of values apart from this maximum.</p>

<p>Without considering shape transformation due to pooling layer, can we say that pooling is a kind of DropOut step ? </p>

<p>Does adding DropOut or DropConnect layer after a pooling layer has a sense in CNN? And does it help further more the training process and generalization property ?</p>
"
2617,"<p>In the literature, many papers are written about Fuzzy control systems. Usually they are based on diagrams which shows the relationship between input signal and control signal. The interesting aspect of fuzzy controllers is, that this relationship can be nonlinear and become error tolerant. Unfortunately, it is hard to grasp to concept of a linguistic variable which is used in the diagrams.</p>

<p>Let us take a step back to a different kind of concept, described outside of Fuzzy control, namely Motion Primitives. A motion primitive is a subroutine which can have a name in English, for example “walk” or “standup”. Combining many motion primitives together results into a behavior tree and in a subsumption architecture (Rodney Brooks). Similar to linguistic variables in Fuzzy control theory, a motion primitive has a name in natural language. That means, that the function name is grounded in terms known outside of the software and by humans.</p>

<p>My question: Are Motion primitives and fuzzy control the same, because they are containing both linguistic variables like walk, stand-up, or “speed=middle”?</p>
"
2618,"<p>When reading Reinforcement Learning by Sutton and Barto, I came across the importance sampling ratio.</p>

<p>The first equation, I believe, describes the probability a particular sequence is obtained given the current state, and the policy.
<a href=""https://i.stack.imgur.com/gra9Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gra9Y.png"" alt=""Importance Sampling Ratio""></a></p>

<p>The next part takes takes the ratio between the probabilities of the two trajectories:
<a href=""https://i.stack.imgur.com/YSin0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YSin0.png"" alt=""Ratio""></a></p>

<p>I don't understand how this ratio could lead to this:</p>

<p><a href=""https://i.stack.imgur.com/g1AAg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g1AAg.png"" alt=""Expected Value""></a></p>

<p>The <span class=""math-container"">$G_t$</span> rewards are obtained through the <span class=""math-container"">$b$</span> policy, not the <span class=""math-container"">$\pi$</span> policy.</p>

<p>I think there is something to do with Bayes Rule, but I could not derive it. Could someone guide me through the derivation?</p>
"
2619,"<p>I came across this article today: <a href=""https://www.theverge.com/2018/12/17/18144356/ai-image-generation-fake-faces-people-nvidia-generative-adversarial-networks-gans"" rel=""nofollow noreferrer"">These faces show how far AI image generation has advanced in just four years</a>. I would never in a million years have guessed that the people on the right (in the first image in the article) were fakes! <img src=""https://i.stack.imgur.com/cgBjW.png"" alt=""[AI generated images (2014 on the left and 2018 on the right)""></p>

<p>Will it be possible to create videos of such AI generated images? What, then, will become of actors and actresses?</p>
"
2620,"<p>I don't know what to search for, so I'll try to describe what I'm trying to do : I want an AI that learns by showing it how to navigate on the Internet.</p>

<p>Let's say I want to save 50 images from a website.</p>

<p>1) what do you call this style of learning : I show the AI 1 to 5 saves, and it has to do the next</p>

<p>2) does it exist ? Is it possible to go ""this far"" ? In particular, the AI has to learn how to deal with connection errors (repeat a bit later for instance).</p>

<p>3) should it be based on the code of the page, or a screenshot, or something else ?</p>

<p>Thank you in advance, I need the english terms for further researchs, and leads for algorithms (I want it to work also in Windows, for renaming files) ^^</p>
"
2621,"<p>I'm learning AI, but this confuses me. The derivative function used in backpropagation is the <strong>derivative of activation function</strong> or the <strong>derivative of loss function</strong>?</p>

<p>These terms are confusing: derivative of act. function, partial derivative wrt. loss function??</p>

<p>I'm still not getting it correct.</p>
"
2622,"<p>How would one go about predicting which characters (actors) are going to die in the next Avengers movie.</p>

<p>To elaborate a bit, given all leaked scripts (fake or not), interviews of different actors and directors, contracts with different actors and directors. How would/should one go about predicting if an actor is going to die in the upcoming movie or not. </p>

<p>NOTE: I am not sure but i think a similar effort has already been made for Game of Thrones. </p>
"
2623,"<p>I am a computer science student and my task is to develop a mobile app for Android and/ or IOS using Artificial Intelligence, designed to help people reduce/ combat depression.Regarding the programming language, I am thinking to go with JavaScript because this is the one I am more comfortable with. I mention that my knowledge on artificial intelligence is little and I haven ' t done anything in AI or of this kind before. I am planning to use AI to help people suffering from depression by creating a mobile app:</p>

<ul>
<li>user-friendly app in terms of design and sequence of actions ,easily customizable looks and functionality</li>
<li>to be able to import all the relevant information:key words, text, images, videos from Facebook, Instagram, Twitter to prevent or report the existence of depression and correctly interpret the nature, intensity and frequency of depression symptoms</li>
<li>the app includes: a patient profile,an achievement board and a dream chart page, to-do list page, led lights which react to emotions, facial recognition technology, Thinking Patterns page( the app uses the principals of cognitive- behavioral therapy)</li>
<li>incorporates sensors that can measure changes in mood, sleep patterns and increased isolation, measuring the device lock time, ambient lighting and audio to predict sleep time</li>
<li>utilizes GPS sensor, recent calls to monitor the wellbeing of the patients</li>
<li>the app provides users with self-help guidelines/techniques,questionnaire designed to track severity of symptoms over time, educational resources for treatment</li>
<li>the users are encouraged to write down daily thoughts in order to analyze and identify negative thinking patterns</li>
<li>the app sends notifications to remind the patients to take their medication, is offering activity suggestions</li>
<li><p>the app incorporates appointment scheduler tool for the counseling sessions including private messages, online meetings</p>

<ol>
<li><p>How do I get started? What platform would you recommend me to develop a mental health mobile app (for Android and/or iOS) using AI in order to help people suffering from depression to combat it and why? A platform based on cloud or better not? I was reading about AppMachine, AppMakr, ShoutEm, but I still can not decide about the most appropriate platform I am going to use in order to develop my mental health mobile app. </p></li>
<li><p>Can a chat bot be integrated in a mental health mobile app?</p></li>
</ol></li>
</ul>

<p>Please, suggest me some related literature, eBooks to set my research objectives because I'm having a hard time finding it. Please, help me!Thank you very much in advance.</p>
"
2624,"<p>I'm using RL to train a Network on the game Connect4. It learns quickly that 4 connected pieces is good. It gets a reward of 1 for this. A zero is rewarded for all other moves. </p>

<p>It takes quite a time until the AI tries to stop the opponent from winning. Is there a way this could be further reinforced? I thought about giving a negativ reward for the move played before the winning move. Thinking about this I came to the conclusion that this is a bad idea. There'll be always a looser (except for ties), therefor there always be a last move from the loosing player. This one hasn't to be a bad one. Mistakes could have been made much earlier.</p>

<p>Is there a way to improve this awareness of opponents? Or does it just have to train more? I'm not perfectly sure if the rewards will propagate back in a way that encourages this behavior with my setup.</p>
"
2625,"<p>I've been given an assignment to create a neural network that will suggest a Croatian word for a word given in any other European language (out of those found <a href=""https://www.indifferentlanguages.com/words/beer"" rel=""nofollow noreferrer"">here</a>). The words are limited to drinks you can find on a bar menu.</p>

<p>I've looked at many NN examples, both simple and complex, but I'm having trouble with understanding how to normalize the input.</p>

<p>For example, words ""beer"", ""birra"" and ""cervexa"" should all translate to ""pivo"". If I include those 3 in the training set, and after the network has finished training I input the word ""bier"", the output should be ""pivo"" again.</p>

<p>I'm not looking for a working solution to this problem, I just need a nudge in the right direction regarding normalization.</p>
"
2626,"<p>I am learning Reinforcement Learning from the lectures from David Silver. I finished lecture 6 and went on to try SARSA with linear function approximator for MountainCar-v0 environment from OpenAI.</p>

<p>A brief explanation of the MountainCar-v0 environment. The state is denoted by two features, position, and velocity. There are three actions for each state, accelerate forwards, don't accelerate, accelerate backward. The goal of the agent is to learn how to climb a mountain. The engine of the car is not strong enough to power directly to the top. So speed has to be built up by oscillating in the cliff.</p>

<p>I have used a linear function approximator, written by myself. I am attaching my code here for reference :-</p>

<pre><code>    class LinearFunctionApproximator:
      ''' A function approximator must have the following methods:-
          constructor with num_states and num_actions
          get_q_value
          get_action
          fit '''

      def __init__(self, num_states, num_actions):
        self.weights = np.zeros((num_states, num_actions))
        self.num_states = num_states
        self.num_actions = num_actions

      def get_q_value(self, state, action):
        return np.dot( np.transpose(self.weights), np.asarray(state) )[action]

      def get_action(self, state, eps):
        return randint(0, self.num_actions-1) if uniform(0, 1) &lt; eps else np.argmax( np.dot(np.transpose(self.weights), np.asarray(state)) )

      def fit(self, transitions, eps, gamma, learning_rate):
        ''' Every transition in transitions should be of type (state, action, reward, next_state) '''
        gradient = np.zeros_like(self.weights)
        for (state, action, reward, next_state) in transitions:
          next_action = self.get_action(next_state, eps)
          g_target = reward + gamma * self.get_q_value(next_state, next_action)
          g_predicted = self.get_q_value(state, action)
          gradient[:, action] += learning_rate * (g_target - g_predicted) * np.asarray(state)

        gradient /= len(transitions)
        self.weights += gradient
</code></pre>

<p>I have tested the gradient descent, and it works as expected. After every epoch, the mean squared error between current estimate of Q and TD-target reduces as expected.</p>

<p>Here is my code for SARSA :-</p>

<pre><code>def SARSA(env, function_approximator, num_episodes=1000, eps=0.1, gamma=0.95, learning_rate=0.1, logging=False):

  for episode in range(num_episodes):
    transitions = []

    state = env.reset()
    done = False

    while not done:
      action = function_approximator.get_action(state, eps)
      next_state, reward, done, info = env.step(action)
      transitions.append( (state, action, reward, next_state) )
      state = next_state

    for i in range(10):
      function_approximator.fit(transitions[::-1], eps, gamma, learning_rate)

    if logging:
      print('Episode', episode, ':', end=' ')
      run_episode(env, function_approximator, eps, render=False, logging=True)
</code></pre>

<p>Basically, for every episode, I fit the linear function approximator to the current TD-target. I have also tried running fit just once per episode, but that also does not yield any winning episode. Fitting 10 times ensures that I am actually making some progress towards the TD-target, and also not overfitting.</p>

<p>However, after running over 5000 episodes, I do not get a single episode where the reward is greater than -200. Eventually, the algorithm choses one action, and somehow the Q-value of other actions is always lesser than this action.</p>

<pre><code># Now, let's see how the trained model does
env = gym.make('MountainCar-v0')
num_states = 2
num_actions = env.action_space.n

function_approximator = LinearFunctionApproximator(num_states, num_actions)

num_episodes = 2000
eps = 0
SARSA(env, function_approximator, num_episodes=num_episodes, eps=eps, logging=True)
</code></pre>

<p>I want to be more clear about this. Say action 2 is the one which is the action which gets selected always after say 1000 episodes. Action 0 and action 1 have somehow, for all states, have their Q-values reduced to a level which is never reached by action 2. So for a particular state, action 0 and action 1 may have Q-values of -69 and -69.2. The Q-value of action 2 will never drop below -65, even after running the 5000 episodes.</p>
"
2627,"<p>Once a book is published in a language, why can't the publishers use Google Translate AI or some similar software to immediately render the book in other languages? Likewise for Wikipedia: I'm not sure I understand why we need editors for each language. Can't the English Wikipedia be automatically translated into other languages?</p>
"
2628,"<p>In short, imitation learning means learning from the experts. Suppose I have a dataset with labels based on actions of experts. I use a simple binary classifier algorithm to assess whether it is good expert action or bad expert action. </p>

<p>How is this binary classifier different from imitation learning? Imitation learning is associated with reinforcement learning but in this case, it looks more like a basic classification problem to me.</p>

<p>What is the difference between imitation learning and classification done by experts? I am getting confused because imitation learning relates to reinforcement learning while classification relates to supervised learning.</p>
"
2629,"<p>In ""<code>4.1 Learning multi-layer deconvolutional filters</code>"" section, the last paragraph says that ""Since our model is generative, we can sample from it. In Fig. 3 we show samples from the two different models from each level projected down into pixel space. The samples were drawn using the relative firing frequencies of each feature from the training set.""</p>

<p><a href=""https://i.stack.imgur.com/owCeC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/owCeC.png"" alt=""Fig.3 and the explanition""></a></p>

<p>I don't know how the pictures in Fig.3 are generative. Since that the filters has been learned and feature maps in every layer can be infered, for example, in terms of fruit samples, in ""Layer 1"", is the the first layer's feature map ? i feel that not true...seems like the sample are low-level...paper says ""... from each level projected into pixel space"", these words are short and confuse me.</p>

<p>somebody could explain that for me? thank you very much!</p>
"
2630,"<p>So, I'm using a pretrained pnasnet5large model to do some image classification (<a href=""https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py"" rel=""nofollow noreferrer"">https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py</a>)</p>

<p>In the file, it says that the input range is in [0,1] (i'm assuming pixel values of input images). The images I have are already in this range.
The channel means and standard deviation for RGB channels are stated as [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] respectively.
Now when I use the torchvision.transforms.Normalize(<a href=""https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize</a>) to normalize the images using the stated means and standard deviations, the pixel values get to the range [-1,1].</p>

<p>The code I wrote for normalization:</p>

<p>transforms.Normalize([0.5, 0.5, 0.5],[0.5, 0.5, 0.5])</p>

<p>I believe I'm missing something fundamental. Should I normalize the images or should I not? Thanks!</p>
"
2631,"<p>Consider some image classification problem. Conceptually, we then have some high dimensional space where all the images can be represented as points, and having large enough labeled data set we can build a classifier. But how do we know that our data in this space has some structure? Like this one in two dimensional case:
<a href=""https://i.stack.imgur.com/jxpNO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jxpNO.png"" alt=""enter image description here""></a>
If we have a data set with images of, say, cats and dogs, why these two classes are not just uniformly mixed with each other but have some distribution or shape in appropriate space? Why it cannot be like this:</p>

<p><a href=""https://i.stack.imgur.com/kHA5W.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kHA5W.png"" alt=""enter image description here""></a></p>

<p>Thanks!</p>
"
2632,"<p>Can anyone suggest different methods with which to approach this problem?</p>
"
2633,"<p>I am interested in optimizing the memory capacity of an AGI. Given a specific complex input an AI can create a simplified model. This is a problem that can be solved using <em>sparse coding</em> <sup>[1]</sup>. However, this solves only the problem of encoding and not the maintenance of online representations—in cognitive terms: the state of mind.</p>

<p>A default cognitive model of short-term memory can be separated in three different stages:</p>

<blockquote>Encoding → Maintenance → Retrieval
</blockquote>

<p>One solution is to use specialized hardware <sup>[2][3]</sup>, but I am interested in software approaches to this problem and I would thus like to emphasize that it is the digital representation, which I am most interested in.</p>

<p>With the exception of qubits, the smallest possible representation are binary digits. However, additional architecture is required to represent phase spaces (i.e. floating point precision memory) and higher-order representations maybe include arrays and dictionaries. (Optimizing these are trivial... or simply to be postponed until needed, according to Knuth).</p>

<ul>
<li>How can a specific <em>connectivity pattern</em> be stored in an <em>optimally
compact</em> representation?</li>
<li>Is there an implementation with concrete example code?</li>
<li>What is the state-of-the-art?<sup>*</sup></li>
</ul>

<p><sub>* I will not specify ""real-time"" here, but the context is humanoid AGI. </sub></p>

<hr>

<p><sub> 
[1] Papyan, V., Romano, Y., &amp; Elad, M. (2017). Convolutional Neural Networks Analyzed via Convolutional Sparse Coding. <em>Journal on Machine Learning Research, 18</em>(83): 1–52. arXiv:1607.08194
<a href=""http://jmlr.org/papers/volume18/16-505/16-505.pdf"" rel=""nofollow noreferrer"">http://jmlr.org/papers/volume18/16-505/16-505.pdf</a>
</sub></p>

<p><sub>[2] LeGallo et al. (2018). Mixed-precision in-memory computing. <a href=""https://www.nature.com/articles/s41928-018-0054-8"" rel=""nofollow noreferrer"">https://www.nature.com/articles/s41928-018-0054-8</a>
</sub></p>

<p><sub>
[3] IBM. (2018). IBM Scientists Demonstrate Mixed-Precision In-Memory Computing for the First Time; Hybrid Design for AI Hardware. <a href=""https://www.ibm.com/blogs/research/2018/04/ibm-scientists-demonstrate-mixed-precision-in-memory-computing-for-the-first-time-hybrid-design-for-ai-hardware/"" rel=""nofollow noreferrer"">https://www.ibm.com/blogs/research/2018/04/ibm-scientists-demonstrate-mixed-precision-in-memory-computing-for-the-first-time-hybrid-design-for-ai-hardware/</a></sub></p>
"
2634,"<p>I'm wondering if there exists a network for simple image classification. What I mean by this is if I have two image datasets, one of horses and one of zebras, I want to train off the horses and classify an image as either a horse or not a horse, so if I test it on an image of a horse, it says it is a horse, but if I use a zebra, it says it is not a horse. Does any library/project for this exist?</p>
"
2635,"<p>In policy gradient method, there's a trick to reduce a variance of policy gradient. We use causality, and remove part of the sum over rewards so that only actions happened after the reward are taken into account (See here <a href=""http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf"" rel=""nofollow noreferrer"">http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf</a>, slide 18).</p>

<p>Why does it work? I understand the intuitive explanation, but what's the rigorous proof of it? Can you point me to some papers?</p>
"
2636,"<p>I have only a limited dataset (&lt;25) with large-sized images (>1500x2000) and their pixelwise labels. The aim is to find unusual patterns in this industry dataset and highlight them.</p>

<p>To generate training images I crop 256x256 grids out of every image and do some data augmentation and use these images to train my U-Net.</p>

<p>For my prediction I split my image with numpy again into 256x256px grids and predict every grid separately and put them together to an image. But this will take some time, like >10 Minutes. But it has a quite good accuracy. <strong>How can I optimize my prediction to be faster?</strong></p>

<p>Is it faster to create this with a Tensorflow Pipeline? When I want to predict the full image with giving shape(None,None,3), I get ""ConcatOp : Dimensions of inputs should match"" after some time. </p>
"
2637,"<p>Problem: Fraud detection
Task : classifying transactions as either fraud/ Non-fraud using GAN </p>
"
2638,"<p>In <a href=""https://en.wikipedia.org/wiki/Semi-supervised_learning"" rel=""nofollow noreferrer"">semi-supervised learning</a>, there are <em>hard labels</em> and <em>soft labels</em>. Could someone show me what's exactly the meaning of the two things?</p>
"
2639,"<p>I fine tuned <a href=""https://arxiv.org/pdf/1704.04861.pdf"" rel=""nofollow noreferrer"">MobileNetSSD</a> for object detection using a dataset with just one class (~4000 images). All the training images include at least one bounding box related to that class (no empty images). By following the <a href=""https://github.com/chuanqi305/MobileNet-SSD"" rel=""nofollow noreferrer"">example</a> with the VOC dataset, the labelmap includes two classes, the background and my custom class. However, as I mentioned, there are no annotations related to the background and I  am not sure if there should be any.</p>

<p>Now my fine tuned network performs very well when objects belonging to my class are present, however there are some false detections with very high confidence when the class is not present. Can this be related to the fact that I don't have empty images in my training set?</p>
"
2640,"<p>My dog goes bonkers every time the sound of a barking dog is heard on a television program.  I never noticed this before but literally every movie or show with an outdoors setting eventually includes the sound of a barking dog. Is it possible to develop a real-time filter that blocks or masks these sounds?</p>
"
2641,"<p>This question was asked in an AI exam. How would you answer such question?</p>
"
2642,"<p>I have a couple different segmentation tasks that I would like to perform on medical imaging data using CNN's. I'm currently trying to wrap my head around how well a 3D network might work, using a U-Net architecture, but I have some hesitation. </p>

<p>My specific questions are as follows:</p>

<p>Question 1 - Say we have medical imaging taken at different slices (different heights/depths) of the patient's body. In order for a 3D CNN to work properly on such data, do the images need to be taken at a heights that are close together, i.e. does the 3rd dimension need to be fairly continuous? For example, if you had a 3D stack/volume of 5 images that you wanted to feed to a 3D CNN like so,</p>

<ol>
<li>img_1_depth_0cm.png</li>
<li>img_2_depth_5cm.png</li>
<li>img_3_depth_10cm.png</li>
<li>img_4_depth_15cm.png</li>
<li>img_5_depth_20cm.png</li>
</ol>

<p>and the images were taken 5 cm apart from one and other, I would imagine that a 3D convolution might not perform very well because of the 5 cm of depth in between images? Is this an incorrect assumption?</p>

<p>(As a reference point, this nice repository on GitHub was designed for training on images of the brain, but the images do appear to be fairly continuous/close together, almost like a video: <a href=""https://github.com/ellisdg/3DUnetCNN"" rel=""nofollow noreferrer"">https://github.com/ellisdg/3DUnetCNN</a>)</p>

<p>Question 2 - For a 3D network to properly segment non-labelled volumes after training is finished, I know that the input data dimensions would have to match those of the training data. </p>

<p>But I would also think that the new images must have been taken in a similar fashion (taken at similar depths and in general a similar orientation to the training data) in order for the NN to be able to perform its task. So unless the medical imaging processes are always performed similarly across machines and hospitals, I'm guessing that the NN performance might vary pretty wildly when it tries to segment new data. Is this correct?</p>
"
2643,"<p>I was trying to implement <code>CapsuleNet</code> for classifying some of the Native digits. All the images are RGB images and resize to <strong><code>32 X 32</code></strong> and dataset has <strong>10</strong> classification output.</p>

<blockquote>
  <p><strong>X_train_all.shape</strong>: <code>(72045, 32, 32, 1)</code><br>
  <strong>y_train_all.shape</strong>: <code>(72045, 10)</code></p>
</blockquote>

<p>Here is the pre-processing I've done on the dataset.</p>

<pre><code>X = [] # initialize empty list for resized images
for i, path in enumerate(paths_img):
    img = cv2.imread(path,cv2.IMREAD_GRAYSCALE) # cv2.IMREAD_COLOR : Loads a color image. Any transparency of image will be neglected. It is the default flag.

    if resize_dim is not None:
        img = cv2.resize(img, (resize_dim, resize_dim), interpolation=cv2.INTER_AREA) # resize image to 28x28

    #X.append(np.expand_dims(img,axis=2)) # expand image to 28x28x1 and append to the list.
    X.append(img) # expand image to 28x28x1 and append to the list

    # display progress
    if i == len(paths_img) - 1:
        end='\n'
    else: end='\r'
    print('processed {}/{}'.format(i+1, len(paths_img)), end=end)

X = np.array(X) # tranform list to numpy array

if path_label is None:
    return X
else:
    # Concatenate all data into one DataFrame
    df = pd.DataFrame()
    l = []
    for file_ in path_label:
        df_x = pd.read_csv(file_, index_col=None, header=0)
        l.append(df_x)

    df = pd.concat(l)
    df = df.set_index('filename') 
    y_label = [df.loc[get_key(path)]['digit'] for path in  paths_img] # get the labels corresponding to the images
    y = to_categorical(y_label, 10) # transfrom integer value to categorical variable

    return X, y 
</code></pre>

<p>First, define the <strong>CapsNet</strong> model which takes the following parameters.I used the Capsule Net (<strong>CapsNet</strong>, <strong>CapsLayer</strong>) implemented code from <a href=""https://github.com/XifengGuo/CapsNet-Keras"" rel=""nofollow noreferrer"">here</a>. </p>

<pre><code># define model
model = CapsNet(input_shape=[32, 32, 1],
                n_class=10,
                num_routing=3)
</code></pre>

<p>Next, Another function naming <code>train</code> has been defined for actual training. It defines as follows:</p>

<pre><code>def train(model, data, epoch_size_frac=1.0):
    """"""
    Training a CapsuleNet
    :param model: the CapsuleNet model
    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`
    :param args: arguments
    :return: The trained model
    """"""
    # unpacking the data
    (x_train, y_train), (x_val, y_val) = data

    # callbacks
    log = callbacks.CSVLogger('CapsuleNet/log.csv')
    checkpoint = callbacks.ModelCheckpoint('CapsuleNet/CapsuleNet Weights Each Epochs/weights-{epoch:02d}.h5',
                                           save_best_only=True, save_weights_only=True, verbose=1)
    lr_decay = callbacks.LearningRateScheduler(schedule = lambda epoch: 0.001 * np.exp(-epoch / 10.))

    # compile the model
    model.compile(optimizer='adam',
                  loss=[margin_loss, 'mse'],
                  loss_weights=[1., 0.0005],
                  metrics={'out_caps': 'accuracy'})

    # Using the Tensorboard callback of Keras. 
    tbCallBack = keras.callbacks.TensorBoard(log_dir='CapsuleNet/CapGraph', histogram_freq=0, 
                                         write_graph=True, write_images=True)


    # -----------------------------------Begin: Training with data augmentation -----------------------------------#
    def train_generator(x, y, batch_size, shift_fraction=0.):
        train_datagen = ImageDataGenerator(width_shift_range = shift_fraction,
                                           height_shift_range = shift_fraction)  # shift up to 2 pixel for MNIST
        generator = train_datagen.flow(x, y, batch_size=batch_size)
        while 1:
            x_batch, y_batch = generator.next()
            yield ([x_batch, y_batch], [y_batch, x_batch])

    # Training with data augmentation. 
    model.fit_generator(generator=train_generator(x_train, y_train, 64, 0.1),
                        steps_per_epoch=int(epoch_size_frac*y_train.shape[0] / 64),
                        epochs = 1,
                        validation_data = [[x_val, y_val], [y_val, x_val]],
                        callbacks = [log, checkpoint, lr_decay, tbCallBack])
    # -----------------------------------End: Training with data augmentation -----------------------------------#


    return model
</code></pre>

<p>Ok, I think this chunk of information only enough here. However, then I define a function for data augmentation and both training the model. This function will iteratively train on the shuffling fold of the data set, defined below.</p>

<pre><code>kfold = KFold(n_splits=10, shuffle=True, random_state=42)
cvscores = []
Fold = 1
for train, val in kfold.split(X_train_all, y_train_all):
    gc.collect()
    K.clear_session()
    print ('Fold: ', Fold)

    X_train = X_train_all[train]
    X_val = X_train_all[val]

    X_train = X_train.astype('float32')
    X_val = X_val.astype('float32')

    y_train = y_train_all[train]
    y_val = y_train_all[val]

    # train the model with data augmentation    
    train(model = model, data = ((X_train, y_train), (X_val, y_val)), epoch_size_frac = 0.5)


    # evaluate the model
    scores = model.evaluate(X_val, y_val, verbose = 0)
    print(""%s: %.2f%%"" % (model.metrics_names[1], scores[1]*100))
    cvscores.append(scores[1] * 100)

    Fold = Fold + 1
</code></pre>

<p>Now, when I start training, I get following error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-67-a8ca211d12a2&gt; in &lt;module&gt;()
     19 
     20 
---&gt; 21     train(model = model, data = ((X_train, y_train), (X_val, y_val)), epoch_size_frac = 0.5)
     22 
     23 

TypeError: 'numpy.ndarray' object is not callable
</code></pre>

<p>I think I need to look over the dimension of the image. While I visualize the image I got the following:</p>

<pre><code>import matplotlib.pyplot as plt 
print(X_train_all.shape)
plt.imshow(X_train_all[1])
</code></pre>

<p><strong>(72045, 32, 32, 1)</strong></p>

<p><a href=""https://i.stack.imgur.com/JGAO6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JGAO6.png"" alt=""enter image description here""></a></p>

<p>Any informative help will be highly appreciated.</p>
"
2644,"<p>According to WIkipedia, a <a href=""https://en.wikipedia.org/wiki/Notation"" rel=""nofollow noreferrer"">notation</a> is a semiotics term to describe artistic disciplines. Famous examples are: chess notation, Siteswap notation for juggling, Labanotation for dancing, basketball play diagrams and the Aresti Catalog (flight maneuvers). In some papers about advanced robotics, also a notation was used to parse motion capture data, for example in the EU Poeticon project lead by Aloimonos, Yiannis. The interesting fact is, that a notation is usually discussed outside of core Artificial Intelligence. It has nothing to do with programming computers itself nor with deeplearning, instead notations are researched by linguists. </p>

<p>They are interesting because they reduce the state space. A notation is some kind of structure to summarize millions of potential states of a system into a handful, which is expressed in a handy grammar. It's not direct an Artificial Intelligence, but it allows to build such a software more easily. In all cases, notations are grounded in natural language, which means that the communication about the topic is done between humans. For example, a basketball team is using a notation for discussing the next move they want to make and they are making signs during the game as a reference.</p>

<p>So my question is: How important are notations for Artificial Intelligence? Can they be used to build robot-control-systems?</p>
"
2645,"<p>I'm a relative newbie to fuzzie logic systems but I have some knowledge in mathematics. I have the following problem:</p>

<p>I want to fuzzify certain values. Some are in the range [-<span class=""math-container"">$\inf$</span>,<span class=""math-container"">$\inf$</span>] and some are in the range [<span class=""math-container"">$0$</span>,<span class=""math-container"">$\inf$</span>]. For the first range I have chosen the sigmoid function:</p>

<p><span class=""math-container"">$f(x) = \frac{1}{1+e^{-x}}$</span></p>

<p>The question is, which fuzzification process I should use for the second range. Since the function <span class=""math-container"">$f(x) = \ln(x)$</span> transforms [<span class=""math-container"">$0$</span>,<span class=""math-container"">$\inf$</span>] to [-<span class=""math-container"">$\inf$</span>,<span class=""math-container"">$\inf$</span>] a natural choice could be:</p>

<p><span class=""math-container"">$f(x) = \frac{1}{1+e^{-\ln(x)}} = \frac{x}{x+1}$</span></p>

<p>A different function could also be:</p>

<p><span class=""math-container"">$f(x) = 1 - 2^{-x}$</span></p>

<p>Which one would be more suitable? Particularly when considering that I may want to compare values from both ranges. </p>
"
2646,"<p>I tried to build a neural network for working on IRIS dataset using only numpy after reading an article (link: <a href=""https://iamtrask.github.io/2015/07/12/basic-python-network/"" rel=""nofollow noreferrer"">https://iamtrask.github.io/2015/07/12/basic-python-network/</a>).</p>

<p>I tried to search the internet but everyone was using ml libraries and found no solution using just numpy. I tried to add different hidden layers to my feed forward neural network still it wasn't converging. I tried to use backpropagation. I used sigmoid and also relu neither of which was successful. </p>

<p>Can someone please give me the code which will work on IRIS dataset and built only using feed forward neural networks and numpy as the only library or if it is not possible to built such a thing with these constraints then please let me know what goes wrong with these constraints.</p>

<p>Also tell me will it be possible to create a neural network to predict values of a matrix multiplication i.e. if we have A * B = C with matrix A as input and C as output, can we acheive substantial amount of accuracy with feed forward neural networks here?.  </p>
"
2647,"<p>Problem: ""For a given news article, generate another title for the article if the article is to be published under a different Publication.""</p>

<p>Which algorithm will be well suited for this? Should I use naive Bayesian or any NLP algorithm? </p>
"
2648,"<p>I have a steady hex-map and turn-based wargame featuring WWII carrier battles
I would like to improve the fixed policy for the AI using reinforcement learning and have a bunch of noob questions which I will try to spread them on several posts.</p>

<ol>
<li><p>Software &mdash;  Is Python mandatory?  The goal is to learn through self-play and the program is written in Objective-C with the aim to migrate it to Unity/C#.</p></li>
<li><p>Hardware  &mdash;    Is it hopeless to achieve anything with a single computer?</p></li>
</ol>
"
2649,"<p>I have a steady hex-map and turn-based wargame featuring WWII carrier battles</p>

<p>On a given turn, a player may choose different and independent actions  (moving one, two naval unit, assigning a mission to an air unit, changing some battle parameters, reorganizing naval task forces etc…).  </p>

<p> Usually, boardgames deals mainly with one action (Go, chess) or very few (Backgammon).  </p>

<p>Here the player may select
- Several actions 
- The actions are of different nature
- Each action getting some variants (strength, payload, destination)</p>

<p>How to approach this problem ?</p>

<p><a href=""https://i.stack.imgur.com/21wjd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/21wjd.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/vgUiV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vgUiV.png"" alt=""enter image description here""></a>
Found this article of interest
<a href=""https://project.dke.maastrichtuniversity.nl/games/files/msc/Roelofs_thesis.pdf"" rel=""nofollow noreferrer"">https://project.dke.maastrichtuniversity.nl/games/files/msc/Roelofs_thesis.pdf</a></p>
"
2650,"<p>I have a steady hex-map and turn-based war game featuring WWII carrier battles. I would like to improve the fixed policy for the AI using reinforcement learning. I have some beginner's questions, which I will try to spread across several posts.</p>

<p>The game use fog of war. </p>

<ul>
<li>Information about the game is zero at the beginning</li>
<li>and is slowly disclosed (approximate position and composition of naval task forces)</li>
</ul>

<p>What is the best approach to deal with that?</p>

<p><a href=""https://i.stack.imgur.com/99we1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/99we1.png"" alt=""Partial info on blue naval forces. Others are not visible on map""></a></p>

<p>Partial info on blue naval forces. Others are not visible on map</p>
"
2651,"<p>I'm trying to implement the NEAT Algorithm using c#, based off of Kenneth O. Stanley's paper. On page 109 (12 in the pdf) it states ""Matching genes are inherited randomly, whereas disjoint genes (those that do not match in the middle) and excess genes (those that do not match in the end) are inherited from the more fit parent.""<br>
Does this mean that the child will always have the exact structure that the more fit parent has? It seems like the only way the structure could differ from crossover was if the two parents were equally fit.</p>
"
2652,"<p>I am starting to learn LSTM by understanding how it is used for creating a char-RNN and had a fundamental question. </p>

<p>Does the number of nodes in the hidden layer need to be the same as that of the input sequence as depicted in the figure
<a href=""https://i.stack.imgur.com/Cc9z6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cc9z6.png"" alt=""enter image description here""></a></p>

<p>Or can we have less than the number of input sequences. If less than number of input sequence can be used how do we interpret the LSTM structure</p>
"
2653,"<p>For <a href=""http://neuralnetworksanddeeplearning.com/chap5.html"" rel=""nofollow noreferrer"">http://neuralnetworksanddeeplearning.com/chap5.html</a> , could anyone suggest:</p>

<p>1) how to approach the derivation of expression (123) ?</p>

<p>2) what constitutes value ~ 0.45 ?</p>

<p>3) why the need of taylor series when we can observe the identity property without any maths proof (input == output) ?</p>

<p><a href=""https://i.stack.imgur.com/hh3uI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hh3uI.png"" alt=""exploding gradient""></a></p>
"
2654,"<p>I am trying to use a artifical neural network to produce a single output, which in my mind should be an index into a list of data (or close to it). All of the results I get are 0.9999+ and very close to each other. I don't know if my whole way of thinking here is off, or if I am just missing an approach, or if perhaps my network code is just broken.</p>

<p>I am trying to make use of the simple neural network from Microsoft here:
<a href=""https://social.technet.microsoft.com/wiki/contents/articles/36428.basis-of-neural-networks-in-c.aspx"" rel=""nofollow noreferrer"">https://social.technet.microsoft.com/wiki/contents/articles/36428.basis-of-neural-networks-in-c.aspx</a></p>

<p>I have tried this with a significantly more complex data set, but I've also tried using a very simple data set. </p>

<p>Here is the simple training data I'm trying to use:</p>

<pre><code>eat poo bad
eat dirt bad
eat cookies okay
eat fruit good
study poo okay
study dirt okay
study cookies okay
study fruit okay
dispose poo good
dispose dirt okay
dispose cookies bad
dispose fruit bad
</code></pre>

<p>The basic idea is that the network has two input neurons and a single output neuron. I assigned a unique number to each distinct word such that I can train the network with two inputs (verb and object), and expect a single output (good, bad, or okay).</p>

<p>Example training:</p>

<pre><code>input: 1 (for eat) 10 (for dirt) output: 15 (for bad)
input: 1 (for eat) 11 (for cookies) output: 16 (for good)
</code></pre>

<p>I would expect that after training, I would see the output numbers close to 15, 16, etc, but all I get are numbers like 0.999997333313168, etc.</p>

<p>Example run:</p>

<pre><code>input: 1 (for eat) 10 (for dirt), output is 0.999997333313168 (instead of ~15 expected)
</code></pre>

<p>What do these outputs mean, or what am I missing in how I should be thinking about making a basic classification system (given inputs, get a meaningful output)?</p>

<p>The C# code I am using, if it is helpful:</p>

<pre><code>using NeuralNet.NeuralNet;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text;

namespace NeuralNet
{
    internal class TestSmallSampleNetwork
    {
        internal static void Run()
        {
            var data = @""eat poo bad
                        eat dirt bad
                        eat cookies okay
                        eat fruit good
                        sell poo bad
                        sell dirt okay
                        sell cookies okay
                        sell fruit okay
                        study poo okay
                        study dirt okay
                        study cookies okay
                        study fruit okay
                        dispose poo good
                        dispose dirt okay
                        dispose cookies bad
                        dispose fruit bad"";
            var simples = data.Split(new[] { ""\r\n"" }, StringSplitOptions.None ).Select(_ =&gt; new Simple(_)).ToList();

            var verbs = simples.Select(_ =&gt; _.Verb).Distinct().Select(_ =&gt; new NetworkValue(_)).ToList();
            var objects = simples.Select(_ =&gt; _.Object).Distinct().Select(_ =&gt; new NetworkValue(_)).ToList();
            var judgments = simples.Select(_ =&gt; _.Good).Distinct().Select(_ =&gt; new NetworkValue(_)).ToList();
            var values = verbs.Concat(objects).Concat(judgments).ToDictionary(_ =&gt; _.Term, _ =&gt; _);

            // Create a network with 2 inputs, 2 neurons on a single hidden layer, and 1 neuron output.
            var net = new NeuralNetwork(0.9, new int[] { 2, 2, 1 });

            for (int iTrain = 0; iTrain &lt; 1000; iTrain++)
            {
                for (int iSimple = 0; iSimple &lt; simples.Count; iSimple++)
                {
                    net.Train(MakeInputs(simples[iSimple], values), MakeOutputs(simples[iSimple], values));
                }
            }

            foreach (var value in values.Values)
            {
                Console.WriteLine(value);
            }

            Console.WriteLine();

            // Run samples and get results back from the network
            Run(""study"", ""poo"", values, net);
            Run(""eat"", ""poo"", values, net);
            Run(""dispose"", ""fruit"", values, net);
            Run(""sell"", ""dirt"", values, net);
        }

        private static void Run(string verb, string obj, Dictionary&lt;string, NetworkValue&gt; values, NeuralNetwork net)
        {
            var result = net.Run(new List&lt;double&gt; {
                values[verb].Value,
                values[obj].Value,
            }).Single();

            var good = ""xxx"";

            Console.WriteLine($""{verb} {obj} {good} ({result})"");
        }

        private static List&lt;double&gt; MakeInputs(Simple simple, Dictionary&lt;string, NetworkValue&gt; values)
        {
            return new List&lt;double&gt;() {
                values[simple.Verb].Value,
                values[simple.Object].Value
            };
        }

        private static List&lt;double&gt; MakeOutputs(Simple simple, Dictionary&lt;string, NetworkValue&gt; values)
        {
            return new List&lt;double&gt; { values[simple.Good].Value };
        }

        public class Simple
        {
            public string Verb { get; set; }
            public string Object { get; set; }
            public string Good { get; set; }

            public Simple(string line)
            {
                var words = line.Trim().Split("" "".ToCharArray(), 3, StringSplitOptions.None);
                Verb = words[0];
                Object = words[1];
                Good = words[2];
            }

            public override string ToString()
            {
                return $""{Verb} {Object} {Good}"";
            }
        }

        public class NetworkValue
        {
            private static int Next = 1;

            public string Term { get; set; }
            public double Value { get; set; }

            public NetworkValue(string term)
            {
                Term = term;
                Value = Next++;
            }

            public override string ToString()
            {
                return $""{Value}. {Term}"";
            }
        }
    }
}
</code></pre>
"
2655,"<p>I am wondering is I can use a 2-dimensions features matrix rather than a feature vector as inout layer of a neural network</p>

<p>For a WWII naval wargame, I have sorted out the features of interest to approximate the game state S at a given time t</p>

<ul>
<li>they are maximum of 50 naval task forces and airbases on map</li>
<li>each one has a lot of features (hex, weather, number of ships, type, distance to other task forces, distance to objective, cargo, intelligence, combat value, speed, damage,  etc...)</li>
</ul>

<p>The output would be the probability of winning and the level of victory</p>

<p><a href=""https://i.stack.imgur.com/Gh2U5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gh2U5.png"" alt=""enter image description here""></a></p>
"
2656,"<p>Suppose I want to predict the position of a sensor based on its reading.</p>

<p>I can first predict the unit vector and predict the distance to be multiplied to this vector.
And I know that distance will never be negative because all the negative parts are inside unit vector already.</p>

<p><strong>Should I apply ReLU to the distance before multiplying it to the unit vector?</strong></p>

<p>I'm thinking that this can be helpful to eliminate the network from needing too much training data by restricting the output ranges the network could give. But I also think that it could make the learning slower when the ReLU unit dies (value=0) so the gradient doesn't flow properly somehow.</p>
"
2657,"<p>Imagine I wish to classify images of digits from 0-9.
Let's say I have trained the network to recognise '1'.
If I were to train the same network to recognise '2', wouldn't the backpropagation process mess up the weights and biases for '1'?</p>

<p>Or do programs like Tensorflow allocate a new layer of neural network for different object classification? Thanks.</p>
"
2658,"<p>I am a novice developer in AI. Any help appropriated. </p>

<p>I have a set of images and from that I want to predict position(x,y co-ordinates) of the Ball.<a href=""https://i.stack.imgur.com/bXjN9.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bXjN9.jpg"" alt=""enter image description here""></a></p>

<p>Thanks in advance.</p>
"
2659,"<p>Am working on credit card fraud detection problem using autoencoders. Regarding that I have some doubts given below :</p>

<ol>
<li><p>The dataset for the above problem has been downloaded from kaggle which is highly imbalanced. That is, only 494 frauds are there in the dataset comprising of 2,84,807 transactions. So my doubt is why the dataset is not balanced before applying autoencoder on it.</p></li>
<li><p><a href=""https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd"" rel=""nofollow noreferrer"">https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd</a>
I have read this blog post and my doubt is what is the threshold value for setting a boundary for anomaly detection.</p></li>
<li><p>Are autoencoders used for anomaly detection?</p></li>
</ol>
"
2660,"<p>In many papers about expert systems so called rules are introduced to the reader. In most papers, the rules are expressed in a certain syntax, because the terms are written in uppercase. For example a rule for an expert system could be: IF condition THEN result. The first assumption in interpreting these expert systems rules might be, that the syntax is equal to what is known from the C programming language. In C, it is possible to write the following statement: </p>

<pre><code>if (number==3) { printf(“number is three”); }
</code></pre>

<p>But, in the c syntax, the statements are usually written in small cases and no dedicated “then statement” is there. The only language which provides the correct syntax is the Forth programming language. According to the official tutorial, the syntax is </p>

<pre><code>RULE1 RULE2 AND IF ACTION THEN RESULT
</code></pre>

<p>What is interesting in Forth is, that the condition is written before the IF statement and that a stack is used. That means, a Forth if-then statement has a different kind of logic than a C style sourcecode. But does it make sense, that the papers about expert systems are using the Forth syntax? Or is it only a coincidence, and in reality the expert systems in the 1980s were realized with LISP?</p>
"
2661,"<p>Each chromosome contains an array of genes, each gene contains a letter and a number, both letter and number can only exist once in each chromosome.</p>

<pre><code>Parent A = {a,1}{c,2}{e,3}{g,4}
Parent B = {a,2}{b,1}{c,4}{d,3}
</code></pre>

<p>What would be the best crossover operator to create a child that doesn't break the rule described above?</p>
"
2662,"<p>I am trying to train a RNN with text from wikipedia but I having having trouble getting the RNN to converge. I have tried increasing the batch size but it doesn't seem to be helping. All data is one hot encoded before being used and I am using the Adam optimizer which is implemented like this.</p>

<pre><code>   for k in M.keys(): ##For k in weights
        M[k] = beta1 * M[k] + (1-beta1)*grad[k]
        R[k] = beta2 *R[k] + (1-beta2)*grad[k]**2
        m_k = M[k] / (1-beta1**n)
        r_k = R[k] / (1-beta2**n)
        model[k] = model[k] - alpha * m_k / np.sqrt(r_k + 1e-8)
</code></pre>

<p>Beta1 is set to 0.9, beta2 to 0.999 and alpha is set to 0.001. When I train it for 50,000 I get very high fluctuation of the cost and it never seems to significantly decrease (only sometimes due to the fluctuations (and I catch the weights with the lowest cost)).My hidden_size is 400 and the batch size is 200</p>

<p>After sketching the cost of iterations I get a graph like this:<a href=""https://i.stack.imgur.com/Amcc3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Amcc3.png"" alt=""]""></a></p>

<p>It seems to be increasing on average only seeming to decrease to the the large fluctuations. What can I change to have better success and have it converge?</p>

<p>Thanks for any help</p>

<p>Edit: I plotted the norm of the gradient using the slightly different cost function which @DennisSoemers suggested and the gradient does not reduce significantly.<a href=""https://i.stack.imgur.com/C64mf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C64mf.png"" alt=""""></a></p>

<p>Sorry the title should read ""Gradient over iterations""</p>
"
2663,"<p>I'm relatively new to this whole AI thing and have a question..</p>

<p>Let's say I have two different fully trained neural networks. The first one is trained for mathematical addition and the second one on mathematical multiplication and now I want to ""merge these to a cluster"" that knows about both operations.</p>

<p>Is there a representative name for this kind of technique?
I had read somthing about bilinear cnn models that sounds similar to what I'm looking for, right? </p>
"
2664,"<p>Is AI a set of programs or a hardware setup?
I was curious about AI, it may be obvious for many but I tried different searches, but was unable to decide.</p>
"
2665,"<p>how to choose the dimensions of the encoding layer in autoencoders?please explain what should be the dimensions of the encoding and decoding layers</p>
"
2666,"<p><strong>Description</strong></p>

<p>I have designed this robot in URDF format and its environment in pybullet. Each leg has a minimum and maximum value of movement. </p>

<p>What reinforcement algorithm will be best to create a walking policy in a simple environment in which a positive reward will be given if it walks in the positive X-axis direction?</p>

<p><strong>I am working in the following but I don´t know if it is the best way:</strong></p>

<p>The expected output from the policy is an array in the range of (-1, 1) for each joint. The input of the policy is the position of each joint from the past X frames in the environment(replay memory like DeepQ Net), the center of mass of the body, the difference in height between the floor and the body to see if it has fallen and the movement in the x-axis.</p>

<p><strong>Limitations</strong></p>

<p>left_front_joint      => lower=""-0.4"" upper=""2.5"" id=0</p>

<p>left_front_leg_joint  => lower=""-0.6"" upper=""0.7"" id=2</p>

<p>right_front_joint     => lower=""-2.5"" upper=""0.4"" id=3</p>

<p>right_front_leg_joint => lower=""-0.6"" upper=""0.7"" id=5</p>

<p>left_back_joint       => lower=""-2.5"" upper=""0.4"" id=6</p>

<p>left_back_leg_joint   => lower=""-0.6"" upper=""0.7"" id=8</p>

<p>right_back_joint      => lower=""-0.4"" upper=""2.5"" id=9</p>

<p>right_back_leg_joint  => lower=""-0.6"" upper=""0.7"" id=11</p>

<p>The code below is just a test of the environment with a set of movements hardcoded in the robot just to test how it could walk later. The environment is set to real time, but I assume it needs to be in a frame by frame lapse during the policy training. (<em>p.setRealTimeSimulation(1) #disable and p.stepSimulation() #enable</em>)</p>

<p><strong>A video of it can be seen in:</strong></p>

<p><a href=""https://youtu.be/j9sysG-EIkQ"" rel=""nofollow noreferrer"">https://youtu.be/j9sysG-EIkQ</a></p>

<p><strong>The complete code can be seen here:</strong></p>

<p><a href=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS"" rel=""nofollow noreferrer"">https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS</a></p>

<p><strong>CODE</strong></p>

<pre><code>import pybullet as p
import time
import pybullet_data

def moveLeg( robot=None, id=0, position=0, force=1.5  ):
    if(robot is None):
        return;
    p.setJointMotorControl2(
        robot,
        id,
        p.POSITION_CONTROL,
        targetPosition=position,
        force=force,
        #maxVelocity=5
    )

pixelWidth = 1000
pixelHeight = 1000
camTargetPos = [0,0,0]
camDistance = 0.5
pitch = -10.0
roll=0
upAxisIndex = 2
yaw = 0

physicsClient = p.connect(p.GUI)#or p.DIRECT for non-graphical version
p.setAdditionalSearchPath(pybullet_data.getDataPath()) #optionally
p.setGravity(0,0,-10)
viewMatrix = p.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)
planeId = p.loadURDF(""plane.urdf"")
cubeStartPos = [0,0,0.05]
cubeStartOrientation = p.getQuaternionFromEuler([0,0,0])
#boxId = p.loadURDF(""r2d2.urdf"",cubeStartPos, cubeStartOrientation)
boxId = p.loadURDF(""src/spider.xml"",cubeStartPos, cubeStartOrientation)
# boxId = p.loadURDF(""spider_simple.urdf"",cubeStartPos, cubeStartOrientation)



toggle = 1



p.setRealTimeSimulation(1)

for i in range (10000):
    #p.stepSimulation()


    moveLeg( robot=boxId, id=0,  position= toggle * -2 ) #LEFT_FRONT
    moveLeg( robot=boxId, id=2,  position= toggle * -2 ) #LEFT_FRONT

    moveLeg( robot=boxId, id=3,  position= toggle * -2 ) #RIGHT_FRONT
    moveLeg( robot=boxId, id=5,  position= toggle *  2 ) #RIGHT_FRONT

    moveLeg( robot=boxId, id=6,  position= toggle *  2 ) #LEFT_BACK
    moveLeg( robot=boxId, id=8,  position= toggle * -2 ) #LEFT_BACK

    moveLeg( robot=boxId, id=9,  position= toggle *  2 ) #RIGHT_BACK
    moveLeg( robot=boxId, id=11, position= toggle *  2 ) #RIGHT_BACK
    #time.sleep(1./140.)g
    #time.sleep(0.01)
    time.sleep(1)

    toggle = toggle * -1

    #viewMatrix        = p.computeViewMatrixFromYawPitchRoll(camTargetPos, camDistance, yaw, pitch, roll, upAxisIndex)
    #projectionMatrix  = [1.0825318098068237, 0.0, 0.0, 0.0, 0.0, 1.732050895690918, 0.0, 0.0, 0.0, 0.0, -1.0002000331878662, -1.0, 0.0, 0.0, -0.020002000033855438, 0.0]
    #img_arr = p.getCameraImage(pixelWidth, pixelHeight, viewMatrix=viewMatrix, projectionMatrix=projectionMatrix, shadow=1,lightDirection=[1,1,1])

cubePos, cubeOrn = p.getBasePositionAndOrientation(boxId)
print(cubePos,cubeOrn)
p.disconnect()
</code></pre>

<p><img src=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS/raw/master/images/PyBullet.png"" alt=""robot""></p>

<p><img src=""https://github.com/rubencg195/WalkingSpider_OpenAI_PyBullet_ROS/raw/master/images/spider(8).jpeg"" alt=""robot""></p>
"
2667,"<p>I've inherited a neural network project at the company I work for. The person who developed gave me some very basic training to get up and running. I've maintained it for a while. The current neural network is able to classify messages for telcos: it can send them to support people in different areas, like ""activation"", ""no signal"", ""internet"", etc. The network has been working flawlessly. The structure of this neural network is as follows:</p>

<pre><code> model = Sequential()
 model.add(Dense(500, input_shape=(len(train_x[0]),)))
 model.add(Activation('relu'))
 model.add(Dropout(0.6))
 model.add(Dense(250, input_shape=(500,)))
 model.add(Activation('relu'))
 model.add(Dropout(0.5))
 model.add(Dense(len(train_y[0])))
 model.add(Activation('softmax'))
 model.compile(loss='categorical_crossentropy',
                  optimizer='Adamax',
                  metrics=['accuracy'])
</code></pre>

<p>This uses a Word2Vec embedding, and has been trained with a ""clean"" file: all special characters and numbers are removed from both the training file and the input data.</p>

<p>Now I've been assigned to make a neural network to detect if a message will be catalog as ""moderated"" (meaning it's an insult, spam, or just people commenting on a facebook post), or ""operative"", meaning the message is actually a question for the company. </p>

<p>What I did was start from the current model and reduce the number of categories to two. It didn't go very well: the word embedding was in spanish from Argentina, and the training data was spanish from Peru. I made a new embedding and accuracy increased by a fair margin (we are looking for insults and other curse words. In spanish a curse word from a country can be a normal word for another: in Spain ""coger"" means ""to take"", and in Argentina it means ""to f__k"". ""concha"" means shell in most countries, but in Argentina it means ""c__t"". You get the idea).</p>

<p>I trained the network with 300.000 messages. Roughly 40% of these were classified as ""moderated"". I tried all sorts of combinations of cycles and epochs. The accuracy slowly increased to nearly 0.9, and loss stays around 0.5000. </p>

<p>But when testing the neural network, ""operative"" messages generally seem to be correctly classified, with accuracy around 0.9, but ""moderated"" messages aren't. They are classified around 0.6 or less. At some point I tried multiple insults in a message (even pasting sample data as input data), but it didn't seem to improve.</p>

<p>Word2Vec works fantastically. The words are correctly ""lumped"" together (learned a few insults in Peruvian spanish thanks to it). </p>

<p>I put the neural network in production for a week, to gather statistics. Basically 90% of the messages went unclassified, and 5% were correctly classified and 5% wrong. Since the network has two categories, this seems to mean the neural network is just giving random guesses.</p>

<p>So, the questions are:</p>

<ul>
<li>Is it possible to accomplish this task with a neural network?</li>
<li>Is the structure of this neural network correct for this task?</li>
<li>Are 300k messages enough to train the neural network?</li>
<li>Do I need to clean up the data from uppercase, special characters, numbers etc?</li>
</ul>
"
2668,"<p>For example, if I constructed a neural network and the computer running it where to be demolished, is the information/program of the neural network still an existent entity within or outside the remnants of the hardware?  </p>
"
2669,"<p>So i googled this but other than some papers i couldnt find any reverse engineering tool that was built using machine learning</p>

<p>I'm not an expert in machine learning and deep learning but it seems rational to think that considering we have billions of open source codes out there, we can use them so our ""machine"" can learn how the assembly and executable of these codes look like and just study on them, mastering the art of reversing, and therefore building a tool this way that can reverse any given program with a great accuracy </p>

<p>Now is this doable or am i missing something here? is there any tool built this way or on its way to coming out? what are your thoughts on this? is better reversing tool even needed or is there  already a great reversing tool that can do the job with the best accuracy possible? </p>
"
2670,"<p>One-shot learning seems to work really well in many application domains. Are there any major (or even minor) drawbacks of using one-shot learning? Does it have flaws that could prevent it from being used in certain image identification scenarios?</p>

<p>In this case, I'm specifically referring to the Siamese Neural Network and Memory Augmented Neural Network approaches to one-shot learning.</p>
"
2671,"<p>I have some problems with understanding of batch concept and batch size. I messed something up. First i start it consider based on convolutional neural network I heard two versions: 
1. When batch size is set on 50 - first network is fed with 50 images and then learned / recalculated (it doesn't make sense to me, because in this case the network learns one of 50 images). 
2. When batch size is set on 50, one of 50 neurons is recalculated in learning process on single image.</p>

<p>Both of this explanations seems to be wrong to me, so I assume, that i completely don't understand this. What is batch / batch size in RNN ?Could you show any example?</p>

<p>I can tell you how would i learn recurrent neural network. Let's say, that I would like to learn a neural network to predict weather next day. 
1. I would take a weather data from expected area from last 30.000 days. 
2. I would assume, that my prediction would be based on measurements from last 365 days. 
3. I would take data from day 1 to 365 - feed RNN with it and learn. 
4. Then i would take data from day 2 to 366 => feed + learn 
5. Then day 3 to 367 => feed + learn
6. And so on. </p>

<p>Is this 365 measurement concept a batch size?</p>
"
2672,"<p>Artificial intelligence seems like a modelling program for interfaces of possible processes forming into another possible or greater architectures of further or differing processes. In what capacity is it intelligence and not simply a continuous flux of ever-increasing builds of emergent information that ISN'T necessarily feasibility in function but rather feasibility of new emergent abstractions? More clearly,how does artificial intelligence create valid data we can deem as intelligence as oppose to simply new avenues of abstraction that are fixed to our criteria of validity?     </p>
"
2673,"<p>I've often heard MCTS grouped together with neural nets and machine learning. From what I gather, MCTS uses a refined intuition (from maching learning) to evaluate positions. This allows it to better guess which moves are worth playing out more.</p>

<p>But I've almost never heard of using machine learning for Minimax+alpha-beta engines. Couldn't machine learning be used for the Engine to better guess which move is best, and then look at that move's subtree first? A major optimization of the minimax algorithm is move-ordering, and this seems like a good way to accomplish that.</p>
"
2674,"<p>I have been reading a few papers (<a href=""https://www.sciencedirect.com/science/article/abs/pii/S003132030000114X"" rel=""nofollow noreferrer"">paper1</a>, <a href=""https://arxiv.org/abs/1410.2474"" rel=""nofollow noreferrer"">paper2</a>) on stereo matching using genetic algorithms. I understand how genetic algorithms work in general and how stereo matching works, but I do not understand how genetic algorithms are used in stereo matching. </p>

<p>The first paper by Han et al says that ""1) individual is a disparity set, 2) a chromosome has a 2D structure for handling image signals efficiently, and 3) a fitness function is composed of certain constraints which are commonly used in stereo matching"".</p>

<p>Does it mean that an individual is a disparity map with random numbers?
Then a chromosome is a block within the individual's disparity map. 
The constraint used for fitness function could be the famous epipolar line. </p>

<p>I dont seem to understand how this works and even <em>WHY</em> you should use genetic algorithm on an algorithm that at its simplest form uses 5 for loops, for example, like in <a href=""https://github.com/davechristian/Simple-SSD-Stereo/blob/master/stereomatch_SSD.py"" rel=""nofollow noreferrer"">here</a>.</p>
"
2675,"<p>I am trying to find a good approach to create a computer player for the game ""<a href=""https://gamious.com/lines/"" rel=""nofollow noreferrer"">Lines</a>"" from gamious on Android. The concept of the game is pretty straightforward :</p>

<blockquote>
  <p>Lines is an abstract ‘zen’ game experience where form is just as important as function. Place or remove Dots to initiate a colourful race that fills a drawing. The colour that dominates the race wins.</p>
</blockquote>

<p>The game starts with a drawing (that can be described as a set of ""blank"" lines, with connection to other lines). Dots of different colour are placed somewhat randomly on the lines. The player get a colour assigned. When the game start, paint start flowing from the dots and filling the (at first blank) lines of the drawing. You win if your colour dominates.</p>

<p>The game gives you different tools to win (the game starts when all of them have been used) :</p>

<ul>
<li>[0 to 2] scissor to cut lines</li>
<li>[0 to 5] additional dot of your own color to place on the drawing</li>
<li>[0 to 4] enemy dots eraser</li>
<li>[0 to 3] additional straight lines to connect different part of the drawing</li>
</ul>

<p>A quick example : the first image is the initial state of a round. ""My"" colour is the yellow (1 enemy = brown) and I have 4 tools (2 eraser and 2 lines). The second image shows the game running after I used the tools to put my colour in a winning position (yes, we can do better)
<a href=""https://i.stack.imgur.com/8Ptar.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Ptar.jpg"" alt=""starting the game""></a>
<a href=""https://i.stack.imgur.com/oVsG2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oVsG2.jpg"" alt=""go yellow !""></a></p>

<p>If I try to approach this as a <em>classical</em> optimization problem, things get messy pretty fast :</p>

<ul>
<li>highly non-linear</li>
<li>high number of dimensions</li>
</ul>

<p>AI seems to be the right way to go, but I would like your help to get in the right direction : what would be your approach to create an AI to play this game ?
To limit the scope of this question, you can consider that I already have a data structure to represent the game initial state, the use of different tools and the game ""physics"". <strong>What I really want to do is finding how to create an AI which can learn how to <em>efficiently</em> use the tools.</strong></p>

<p>Regarding my experience, I took 2 semesters of AI classes during the last year getting my engineering degree and have used non-linear optimization tools for a while : you can go technical... but not you-need-a-specialized-AI-degree-to-understand-the-answer technical</p>
"
2676,"<p>I am trying to understand what channels mean in convolutional neural networks. When working with grayscale and colored images, I understand that the number of channels is set to 1 and 3 (in the first conv layer), respectively, where 3 corresponds to red, green, and blue.</p>

<p>Say you have a colored image that is 200x200 pixels. The standard is such that the input matrix is a 200x200 matrix with 3 channels. The first convolutional layer would have a filter that is size <span class=""math-container"">$N \times M \times 3$</span>, where <span class=""math-container"">$N,M&lt;200$</span> (I think they're usually set to 3 or 5). </p>

<p>Would it be possible to structure the input data differently, such that the number of channels now becomes the width or height of the image? i.e., the number of channels would be 200, the input matrix would then be 200x3 or 3x200. What would be the advantage/disadvantage of this formulation versus the standard (# of channels = 3)? Obviously, this would limit your filter's spatial size, but dramatically increase it in the depth direction. </p>

<p>I am really posing this question because I don't quite understand the concept of channels in CNNs. </p>
"
2677,"<p>I am trying to use a Keras LSTM neural network for character level language modelling. As the input, I give it the last 50 characters and it has to output the next one. It has 3 layers of 400 neurons each. For the training data, I am using 'War of The Worlds' by H.G. Wells which adds up to 269639 training samples and 67410 validation samples.</p>

<p>After 7 epochs the validation accuracy has reached 35.1% and the validation loss has reached 2.31. However, after being fed the first sentence of war of the worlds to start it outputs:</p>

<blockquote>
  <p>the the the the the the the the the the the the the the the the...</p>
</blockquote>

<p>I'm not sure where I'm going wrong; I don't want it to overfit and output passages straight from the training data but I also don't want it to just output 'the' repeatedly. I'm really at a loss as to what I should do to improve it.</p>

<p>Any help would be greatly appreciated. Thanks!</p>
"
2678,"<p><strong>Related Questions</strong></p>

<p>Which transition will be most difficult for some who grew up in a culture where driving autonomy is a mechanism of personal expression?</p>

<p>Which transition will be most difficult for institutions designed around the existence of enforcement, justice, licensing, and litigation?</p>

<hr>

<p><strong>Possible Sequence of Transitions</strong></p>

<p>Reins (for horses)
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Steering wheels, accelerators, breaks, and signals
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Add cruise control and anti-skid
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Hybrid AI-manual driving
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
AV vehicle lanes on highways
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
AV safety stats prove manual driving dangerous
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Removal of steering wheel and pedals
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
AV the norm, with diminishing manual lanes and roads
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Elimination of driver's licenses
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Legislation to prohibit manual driving
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Swarm updated real time map maintenance
<br>&nbsp;&nbsp;&nbsp;&nbsp;⇓<br>
Emergence of community shared automated chauffeuring</p>

<hr>

<p><strong>Returning to the Main Question</strong></p>

<p>What socioeconomic points of resistance do autonomous auto designers and manufacturers face?</p>

<p>Does the resistance have any real basis, considering the dangers of manual driving, or just the typical yet baseless fear of change?</p>
"
2679,"<p>I was wondering whether there is mathematical evidence or proof of functions that are happening at the backend of deep learning. Particularly in training and testing operations. Secondly, deep learning is invented many years ago but still very minimal hardware systems to do something real.</p>
"
2680,"<p>I'm reading this really interesting article <a href=""https://arxiv.org/pdf/1712.02950.pdf"" rel=""nofollow noreferrer"">CycleGAN, a Master of Steganography</a> and I understand everything up until this paragraph:</p>

<blockquote>
  <p>we may view the CycleGAN
  training procedure as continually mounting an adversarial attack on
  G
  , by optimizing a generator
  F
  to generate adversarial maps that force
  G
  to produce a desired image. Since we have demonstrated
  that it is possible to generate these adversarial maps using gradient descent, it is nearly certain that the
  training procedure is also causing
  F
  to generate these adversarial maps. As
  G
  is also being optimized,
  however,
  G
  may actually be seen as cooperating in this attack by learning to become increasingly
  susceptible to attacks. We observe that the magnitude of the difference
  y
  ∗
  −
  y
  0
  necessary to generate
  a convincing adversarial example by Equation 3 decreases as the CycleGAN model trains, indicating
  cooperation of
  G
  to support adversarial maps</p>
</blockquote>

<p>How is the CycleGAN training procedure an adversarial attack? I don't really understand the quoted explanation.</p>
"
2681,"<p>I have the following binary classification problem, my labeled dataset contains images 96x96 px. Now in every image the interest area is of size 32x32 px in the center of the image, and the images are labeled based on that 32x32 px area. If whatever i am trying to detect is in the outer region of the 32x32 px area the label of that image is not affected.</p>

<p>The problem here is that if i use the whole image when traing, my model will not learn that the interest area is only in the center of the image but on the other hand if i crop the images to be of size 32x32 i am loosing a lot of information which can help the model to train on. I found out that i am getting the best results if i crop the images to be of size 64x64 (kind of a trade of).</p>

<p>Now going to the test set, for the test set it doesn't make sense to use this trade of because the model is not learning anything anyways so i would rather crop the test images to 32x32 but then the test set and train set sizes are not the same.</p>

<p>Has anyone came across this problem before? Can i just pad the test images to the size of the train images? is this a good way to go?</p>
"
2682,"<p>I want to create a NHL game predictor and have already trained one neural network on game data. </p>

<p>What I would like to do is train another model on player seasonal/game data and combine the two models to archive better accuracy. </p>

<p>Is this approach feasible? If it is, how do I go about doing it?</p>

<p>EDIT:</p>

<p>I have currently trained a neural network to classify the probability of the home team winning a game on a dataset that looks like this:</p>

<pre><code>h_Won/Lost  h_metric2 h_metric3 h_metric4 a_metric2 a_metric3 a_metric4 h_team1 h_team2 h_team3 h_team4 a_team1 a_team2 a_team3 a_team4
 1            10       10         10        10         10        10      1       0        0      0         0      1        0      0
 1            10       10         10        10         10        10      1       0        0      0         0      1        0      0
 1            10       10         10        10         10        10      1       0        0      0         0      1        0      0
</code></pre>

<p>and so on.</p>

<p>I am preparing a dataset of player-data for each game that will have the shape of this:</p>

<pre><code>Player     PlayerID    Won/Lost     team      opponent     metric1     metric2   
 Henke         1           1          NY          CAP         10          10
</code></pre>

<p>Hopefully, this new dataset will have some accuracy on if team is going to have some predictive features that are good and recognised. </p>

<p>Now, say I have these two trained Nural Networks and they both have an accuracy of 70% by them self. But I want to combine them both in the hopes to achieve better predictability. How is this archived? How will the test-dataset be structured?</p>
"
2683,"<p>I'm just curious as to the state of the field since I'm planning to have it as my A.I grad school specialization. From what I've observed it peaked in the 90s to the 2000s but is seen as more of a novelty within A.I/ML research today. I'm aware however that MIT Press Journals and IJALR are still publishing new research. Just that I have rarely if not ever heard these topics discussed in A.I/ML conferences. </p>
"
2684,"<p>I'm new to data science I'm currently working on regression problem and I have 10 inputs/attributes. My question is what to do if there are correlations among different features of the input data? Does correlation b/w inputs affect the performance of Model?</p>
"
2685,"<p>I’m a researcher and I’m currently conducting a research project. I will conduct a study where I would like to trigger different emotions using chatbots on a smartphone (e.g. on Facebook Messenger).</p>

<p>Are there any existing chatbots which are able to trigger different emotions intentionally (also negative ones)?</p>
"
2686,"<p>Hey I am training an initialized Neural Network with this Method</p>

<pre><code>public void rlearn(ArrayList&lt;Tuple&gt; tupels, double learningrate, double discountfactor) {

    MLDataSet set = new BasicMLDataSet();
    MLDataSet input = new BasicMLDataSet();
    MLDataSet ideal = new BasicMLDataSet();
    for(int i = 0; i &gt; tupels.size()-1; i++) {
        MLData datain = new BasicMLData(45);
        MLData dataout = new BasicMLData(4);
        int index = 0;
        for(double w : tupels.get(i).statefirst.elements) {
            datain.add(index++,w);
        } //added State
        //Add new Q Values
        index = 0;
        for(int k = 0; k &lt; tupels.get(i).qactions.elements.length;k++) {

            if(k == tupels.get(i).actionTaken) {
                //New Q - Value
                double currentQValue = tupels.get(i).qactions.getElement(k);
                double reward = tupels.get(i).rewardafter;
                //Calculate maximal Q Value of next State
                double max = Double.MIN_VALUE;
                for(double w : tupels.get(i+1).qactions.elements) {
                    if(w &gt; max) {
                        max = w;
                    }
                }
                dataout.add(index++,currentQValue + learningrate*(reward + discountfactor*max - currentQValue));
            } else {
                dataout.add(index++, tupels.get(i).qactions.getElement(k));
            }
        }
        set.add(datain,dataout);

    }
    System.out.println(""Training Data: "" + set.size());
    if(set.size() != 0) {
    Backpropagation prop = new Backpropagation(nn, set);
    prop.setLearningRate(0.1);
    prop.iteration(10);


    System.out.println(""Training Done: "" + prop.getError());
    }
}
</code></pre>

<p>Unfortunately this does not work pretty well. The Error is converging to Zero (pretty fast from 10000), but the Neural Net does not seem to have learned something (it is big enough)</p>

<p>The actual goal is to create a NN which can play something similar like astroids. Therefore the goal is to survive as long as possible. After every frame the NN gets +1 Point, if it dies -100;</p>

<p>Edit: The game looks like <a href=""https://youtu.be/qxGR2bgj8VY"" rel=""nofollow noreferrer"">https://youtu.be/qxGR2bgj8VY</a> (I coded it) but the AI can only go up,down,left and right.
Furthermore, the AI can only steer the Player every 1/3 second...</p>
"
2687,"<p>There are some happenings in Human Swarm Artificial Intelligence and some schools K-12 running without teachers and the kids studying anything on their own.
What would be a possible approach for Human Swarm AI toward classroom management?</p>
"
2688,"<p>I understand the minimax algorithm, but I am unable to understand deeply the minimax algorithm with alpha-beta pruning, even after having looked up several sources (on the web) and having tried to read the algorithm and understand how it works.</p>

<p>Do you have a good source that explains alpha-beta pruning clearly, or can you help me to understand the alpha-beta pruning (with a simple explanation)?</p>
"
2689,"<p>What are the steps involved to create an AI agent which can do the following</p>

<ol>
<li>can learn from text in digital format from a book</li>
<li>Gain knowledge from the digital text input</li>
<li>Can answer question from the book fed to it</li>
</ol>
"
2690,"<p>Going through the DQN paper, it said the state-space is high dimensional. I am a little bit confused here. Suppose my state is a high dimensional vector of <code>N</code> length where N is a huge number. Let's say I solve this task using Q-learning and I fix my state space to 10 vectors each of <code>N</code> dimensions. Q-learning can easily work with these settings as we need only a table of dimensions 10 x number of actions. </p>

<p>Let's say my state space can have an infinite number of vectors each of <code>N</code> dimensions. In these settings Q-learning would fail as we cannot store Q-values in a table for each of these infinite vectors. Meanwhile on the other hand DQN would easily work as neural networks can generalize for any vector in the state-space. </p>

<p>Let's also say I have a state space of infinite vectors but each vector is now of length 2 i.e. very small dimensional vectors. Would it make sense to use DQN in these settings ? Should this state-space be called high dimensional or low dimensional ? </p>
"
2691,"<p>This is a question related to <a href=""https://ai.stackexchange.com/questions/9725/neural-network-to-detect-spam"">Neural network to detect &quot;spam&quot;?</a>. 
I'm wondering how it would be possible to handle the emotion conveyed in text. In informal writing, especially among a juvenile audience, it's usual to find emotion expressed as repetition of characters. For example, ""Hi"" doesn't mean the same as ""Hiiiiiiiiiiiiiii"" but ""hiiiiii"", ""hiiiiiiiii"", and ""hiiiiiiiiii"" do. </p>

<p>A naive solution would be to preprocess the input and remove the repeating characters after a certain threshold, say, 4. This would probably reduce most long ""hiiiii"" to 4 ""hiiii"", giving a separate meaning (weight in a context?) to ""hi"" vs ""long hi"".</p>

<p>The naivete of this solution appears when there are combinations. For example,
haha vs hahahahaha or lol vs lololololol. Again, we could write a regex to reduce lolol[ol]+ to lolol. But then we run into the issue of hahahaahhaaha where a typo broke the sequence.</p>

<p>There is also the whole issue of Emoji. Emoji may seem daunting at first since they are special characters. But once understood, emoji may actually become helpful in this situation. For example,  may mean a very different thing than , but  may mean the same as  and . </p>

<p>The trick with emojis, to me, is that they might actually be easier to parse. Simply add spaces between  to convert  to     in the text analysis. I would guess that repetition would play a role in training, but unlike ""hi"", and ""hiiii"", Word2Vec won't try to categorize  and  as different words (as I've now forced to be separate words, relying in frequency to detect the emotion of the phrase). </p>

<p>Even more, this would help the detection of ""playful"" language such as , where the  emoji might imply there is anger, but alongside  and especially when repeating  multiple times, it would be easier for a neural network to understand that the person isn't really angry.</p>

<p>Does any of this make sense or I'm going in the wrong direction?</p>
"
2692,"<p>On Google Scholar the term fuzzy logic results into a list of 1 million papers, which is a lot. The subproblem of “Fuzzy control” has around 238000. If we are assuming, that the total number of papers in Academia is only 50 million, than the term fuzzy logic occupies 2% of the published information. But how important is the topic for Artificial Intelligence and Robotics? As far as i know, there is a hype around deeplearning. The topic is pushed forward and many lectures are held at universities about machine learning, neural network and stochastic automaton. In contrast, the topic fuzzy control is not covered very well. It sounds a bit esoteric. We have on the one hand a huge number of published papers about the subject, while on the other hand nobody is using fuzzy control in reality. Or at least, it seems so. A closer look into the papers shows, that fuzzy logic is everywhere: from simple line follower robots, over starcraft playing AI engines up to fuzzy nanoparticles which are cruising in the human body.</p>

<p>My question is: is fuzzy logic something which can be ignored because other topics like neural networks are more important or does it make sense to focus on fuzzy control and support the idea of describing a system with linguistic variables?</p>

<p>A similar question was asked by <a href=""https://ai.stackexchange.com/questions/8611/why-did-fuzzy-logic-fall-out-of-fashion"">Why did fuzzy logic fall out of fashion?</a> two months ago. But it doesn't contains numbers about published articles.</p>
"
2693,"<p>I am trying to implement a Deep Q Network to play <a href=""http://www.freeasteroids.org/welcome/"" rel=""nofollow noreferrer"">Asteroids</a>. Unfortunately, I am not sure how to calculate the Q value exactly, if I am exploring. For example, the agent is exploring for 1 second (otherwise makes no sense; I cannot let it just explore one step). Unfortunately, it makes a mistake at 0.99s, and the reward collapses. </p>

<p>At the moment, I am using the following formula to evaluate or update the Q value:</p>

<p><span class=""math-container"">$$Q_{new,t} = reward + \gamma Q_{max,t+1}$$</span></p>

<p>But how do I know the max Q value of the next step? I could consider the best Q value the network says, but this is not necessarily true.</p>

<p>You can see the current implementation at the following URL:
<a href=""https://github.com/SuchtyTV/RLearningBird/blob/master/src/main/java/rlgame/Brain.java"" rel=""nofollow noreferrer"">https://github.com/SuchtyTV/RLearningBird/blob/master/src/main/java/rlgame/Brain.java</a>.</p>
"
2694,"<p>Below is a taxonomy of neurons. Some of these types occur in different locations in the brain, but there are adjacent neurons of varying types with clearly functional type diversity in many parts of the brain, so the idea of having a layer of homogeneous cells in artificial networks may be limiting. Is there any reason why that is definitely the case or definitely not the case?</p>

<p><a href=""https://i.stack.imgur.com/hx5jY.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hx5jY.jpg"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Since there are different types of neurons in adjacent positions in the brain's arrays, should heterogeneous layers be developed, mathematically, experimentally, and in APIs designed to provide AI components for applications?</p>
</blockquote>

<p><strong>References</strong></p>

<p><a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4455486/"" rel=""nofollow noreferrer""><em>Brain-wide analysis of electrophysiological diversity yields novel categorization of mammalian neuron types</em></a>, 2015, Tripathy et. al.</p>
"
2695,"<h2>Background</h2>

<p>My understanding is the input neurons seem to seem to compute a weighted sum moving from one layer to another. </p>

<p><a href=""https://i.stack.imgur.com/9tqAt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9tqAt.png"" alt=""enter image description here""></a>
<span class=""math-container"">$$ \sum_i a_i w_i = a'_{k} $$</span></p>

<p>But to compute this weighted sum the sum must be discrete. Is there any known method to compute the sum when the activation is a continuous function? Is the below formula of an any problems in artificial intelligence? Can anyone give a specific problem where it might be useful?</p>

<h2>My Method</h2>

<p><span class=""math-container"">$$ \lim_{k \to \infty}  \lim_{n \to \infty}\ \sum_{r=1}^n d_r \left(  f(\frac{k}{n}r)\frac{k}{n} \right) =  \lim_{s \to 1} \! \underbrace{\frac{1}{\zeta(s)}   \sum_{r=1}^\infty  \frac{d_r}{r^s}}_{\text{removable singularity}} \int_0^\infty f(x) \, dx  $$</span></p>

<p>I will not go into the proof of this over but for those who are interested: <a href=""https://math.stackexchange.com/questions/2888976/a-rough-proof-for-infinitesimals"">https://math.stackexchange.com/questions/2888976/a-rough-proof-for-infinitesimals</a> I will merely state what the formula means:</p>

<p>Consider we have a curve <span class=""math-container"">$f(x)$</span> now if one wishes to perform a weighted sum in the limiting case of this function. </p>

<p><a href=""https://i.stack.imgur.com/liYJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/liYJS.png"" alt=""Area of the Curve""></a></p>

<p>Consider the curve <span class=""math-container"">$f(x)$</span>. Then splitting it to <span class=""math-container"">$k/n= h$</span> intervals then adding the first strip (<span class=""math-container"">$d_1$</span> times): <span class=""math-container"">$  f(h) \cdot d_1$</span>. Then the second strip (<span class=""math-container"">$d_2$</span> times) <span class=""math-container"">$ f(2h) \cdot d_2$</span> times ... And so on . Hence. <span class=""math-container"">$d_r$</span> can be thought of as the weight at <span class=""math-container"">$f(rh)$</span>.</p>

<p>A sample example of <span class=""math-container"">$f(x)$</span> that should work is <span class=""math-container"">$f(x) = e^{-x}$</span></p>

<h2>Disclaimer</h2>

<p>I am not familiar with this field and am merely a physicist in training. I recently watched 3 blue 1 browns video of artificial intelligence <a href=""https://www.youtube.com/watch?v=aircAruvnKk"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=aircAruvnKk</a> and realised a formula I had constructed for fun might be of relevance (?). </p>
"
2696,"<p>I am looking for some non-ML methods. I am looking for a way of two chat bots communicating to each other on specific topic. I could not find online any resources that do this without using ML. The reason for non-ML is that I want to have better overview/control of how it's done instead of using neural approaches that sometimes can be like black box. Later, I will possibly transition to ML methods. Thank you!</p>
"
2697,"<p>Hello I am new to neural network and I have a question about activation functions.
People usually use their activation function such as relu, sigmoid, tanh etc. But what happens when I mix activation functions?
I recently found that Google has developed Swish activation function which is (x*sigmoid). By altering activation function can it increase accuracy on small neural network problem such as xor problem?</p>
"
2698,"<p>I am not sure if I understood the q learning algorithms correctly. 
Therefore I would give a concrete example and ask if someone can tell me how to update the q value correctly. </p>

<p>First I initialized a Neural Network with random weights. It shall henceforth evaluate the Q Value for all possible actions(4) given a State S.</p>

<p>Then the following happens. The agent is playing and is exploring. 
For 3 steps the Q Values evaluated were:
<em>(0,-1,-5,0), (0,-1,0,0), (0,-.6,0,0)</em></p>

<p>The reward given was: <em>0,0,1</em>
The action took were: <em>(1.,1.,1.)</em>
In the random walk example (same reward given), it was: <em>(1.,2.,3.)</em></p>

<p>So what are the new Q - Values, assuming a discount factor of 0.99 and the learning rate 0.1?</p>

<p>The States for Simplicity are only one number: <em>1,1.3,2.4</em> Where 2.4 is the state who ends the game...</p>

<p>The same example holds for exploiting. Is the algorithm the same here?</p>

<p>Here you see my last implementation:</p>

<pre><code>    public void rlearn(ArrayList&lt;Tuple&gt; tupels, double learningrate, double discountfactor) {

    //newQ = sum of all rewards you have got through
    for(int i = tupels.size()-1; i &gt; 0; i--) {
        MLData in = new BasicMLData(45);
        MLData out = new BasicMLData(5);

        //Add State as in
        int index = 0;
        for(double w : tupels.get(i).statefirst.elements) {
            in.add(index++, w);
        }

        //Now start updating Q - Values 
        double qnew = 0;
        if(i &lt;= tupels.size()-2){
            qnew = tupels.get(i).rewardafter + discountfactor*qMax(tupels.get(i+1));
        } else {
            qnew = tupels.get(i).rewardafter;
        }

        tupels.get(i).qactions.elements[tupels.get(i).actionTaken] = qnew;
        //Add Q Values as out
        index = 0;
        for(double w : tupels.get(i).qactions.elements) {
            out.add(index++, w);
        }
         bigset.add(in, out);
    }
}
</code></pre>

<p>Edit: This is the qMax - function:</p>

<pre><code>    private double qMax(Tuple tuple) {
    double max = Double.MIN_VALUE;
    for(double w : tuple.qactions.elements) {
        if(w &gt; max) {
            max = w;
        }
    }
    return max;
}
</code></pre>
"
2699,"<p>What are the strengths of the Hierarchical Temporal Memory model compared to competing models such as 'traditional' Neural Networks as used in deep learning? And for those strengths are there other available models that aren't as bogged down by patents?</p>
"
2700,"<p>After reading an excellent BLOG post <a href=""http://karpathy.github.io/2016/05/31/rl/"" rel=""nofollow noreferrer""><strong>Deep Reinforcement Learning: Pong from Pixels</strong></a> and playing with the code a little, I've tried to do something simple: use the same code to train a logical XOR gate.</p>

<p>But no matter how I've tuned hyperparameters, the reinforced version does not converge (gets stuck around -10). What am I doing it wrong? Isn't it possible to use Policy Gradients, in this case, for some reason?</p>

<p>The setup is simple:</p>

<ul>
<li>3 inputs (1 for bias=1, x, and y), 3 neurons in the hidden layer and 1 output.</li>
<li>The <em>game</em> is passing all 4 combinations of x,y to the RNN step-by-step, and after 4 steps giving a reward of +1 if all 4 answers were correct, and -1 if at least one was wrong.</li>
<li>The <em>episode</em> is 20 <em>games</em></li>
</ul>

<p>The code (forked from original and with minimal modifications) is here: <a href=""https://gist.github.com/Dimagog/de9d2b2489f377eba6aa8da141f09bc2"" rel=""nofollow noreferrer"">https://gist.github.com/Dimagog/de9d2b2489f377eba6aa8da141f09bc2</a></p>

<p>P.S. Almost the same code trains XOR gate with <em>supervised</em> learning in no time (2 sec).</p>
"
2701,"<p>I'm learning machine learning by looking through other people's kernel on kaggle, specifically this <a href=""https://www.kaggle.com/raghuchaudhary/mushroom-classification/data"" rel=""nofollow noreferrer"">Mushroom Classification kernel</a>. The author first applyed PCA to the transformed indicator matrix. He only used 2 principal components for visualization later. Then I checked how much variance it has maintained, and found out that only 16% variance is maintained. </p>

<pre><code>in [18]: pca.explained_variance_ratio_.cumsum()
out[18]: array([0.09412961, 0.16600686])
</code></pre>

<p>But the test result with 90% accuracy  suggests it works well. My question is if variance stands for information, then how can ML model works well when so-much information has lost? </p>
"
2702,"<p>This might sound dumb but i kept scratching my head for long and couldn't understand the non linearity concept. Let's say i have a 2 x 2 pixel of grayscale picture where there is one edge such that the left pixel contains a value 30  and right pixels contain a value 0. And for edge detection i have padded the input image and then used the Sobel Vertical filter to find out the vertical edges and apply Relu to the output. The output is a 2 x 2 matrix with all pixel values 0. So that should mean there is no edge in the picture whereas in actual case it has one. Where am i going wrong?</p>

<p><a href=""https://i.stack.imgur.com/464WN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/464WN.png"" alt=""enter image description here""></a></p>
"
2703,"<p>Consider the following loss function</p>

<p><span class=""math-container"">$$
L(\mathbf{w}) = [(r + \gamma max_{a'} Q(s', a', \mathbf{w^-})) - Q(s, a, \mathbf{w})]^2
$$</span></p>

<p>where <span class=""math-container"">$Q(s, a, \mathbf{w^-})$</span> and <span class=""math-container"">$Q(s, a, \mathbf{w})$</span> are represented as neural networks, where <span class=""math-container"">$w^-$</span> and <span class=""math-container"">$w$</span> are the corresponding weights.</p>

<p>But how do you calculate <span class=""math-container"">$max_{a'} Q(s', a', \mathbf{w^-})$</span>? Do you really need to hold always an older version of the network? If yes, why and how old should it be?</p>
"
2704,"<p>Consider this game state:
<a href=""https://i.stack.imgur.com/wfOrF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wfOrF.png"" alt=""GameState""></a></p>

<p>d5 captures c6</p>

<p>Quiescence search returns about 8.0 as evaluation because after dxc6 and bxc6 Qxd6 would be played (then Qxd6 by black). A normal player would not play this move but quiescence search includes it in the evaluation and it would result in this end state:<a href=""https://i.stack.imgur.com/ACI6V.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ACI6V.png"" alt=""GameState after quiescence search end""></a>
which would result in a huge advantage for black.</p>

<p>Is my interpretation of quiescence search wrong?</p>
"
2705,"<p>Why is the actor-critic algorithm limited to using on-policy data? Or can we use the actor-critic algorithm with off-policy data?</p>
"
2706,"<p>when using Rectified Linear Unit after convolution layers we have to have twice as much filters to be able to detect features (eg both left and right edge detector). Why do we just throw out negative output of the unit?
As I understand ReLU should have two outputs - positive and negative</p>
"
2707,"<p>I'm trying to understand exactly what does a convnet do to what, and I have trouble finding the dimensions alongside the convolutions.</p>

<p>If we take VGG 16 architecture, how do I get from 224x224x3 to 112x112x64 ? (The 112 is understandable, it's the last part I don't get)</p>

<p>I thought the CNN was to apply filters/convolutions to layers (for instance, 10 different filters to channel red, 10 to green... : are they the same filters between channels ?), but obviously 64 is not divisible by 3.</p>

<p>And then, how do we get from 64 to 128 ? Do we apply new filters to outputs of previous filters ? (in this case we only have 2 filters applied to previous outputs) Or is it something different ?</p>

<p><a href=""https://i.stack.imgur.com/AJo0w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AJo0w.png"" alt=""vgg 16 archictecutre""></a></p>
"
2708,"<p>I have a time series data where i use a sliding window to detect anomalies in those windows. A sliding window is an interval of the dataset that steps one datapoint for each iteration. Datapoints are seen multiple times in this way equal to the size of the window.</p>

<p>In short the algorithm works like this:</p>

<ol>
<li>choose window length : <strong>wl</strong> </li>
<li>learn normal data with sliding window</li>
<li>try to detect anomalies on test data with sliding window</li>
</ol>

<p>I want to keep the sliding window method since it is nessecary for the performance of the algorithm. 
However one anomaly occurs multiple times in the sliding window. When the anomaly appears in the sliding window for the first time it's on the 'right' side of the window.  </p>

<p>How do we measure accuracy of anomaly detection in this case ?
We could say that detecting the anomaly once in the window is enough or detect it <strong>wl</strong> times. 
What's best practice ? </p>
"
2709,"<p>During reading some papers about fuzzy systems, I've recognized a subtopic called “fuzzy commands”. The idea is to provide a sentence in natural language, like “move ball to right”, and the fuzzy reinforcement learning algorithm will determine by it's own how to fulfill the task. That means the fuzzy rules are adapted, they are learned.</p>

<p>From other papers about reinforcement learning it is known, that such systems are able to adapt to new problems. That means, the same reinforcement learning algorithm can be used to solve the cart-balancing problem as well as the pong game or the inverted pendulum problem. This is usually done with policy iteration. The policy is stored in a q-table and describes state-action values or it is described by a markov chain which is probabilistic transition diagram. If this is combined with natural language what is the result? Is a fuzzy command reinforcement learning system equal to a general game playing agent? Or has such a system some kind of disadvantages, that means it fails to solve more complex problems?</p>
"
2710,"<p>I am a strategy consultant, deeply interested in edge computing and distributed and decentralized systems.</p>

<p>In performing some analysis on current edge offerings, I am curious as to how ML and AI tools (like TensorFlow and DeepLearning4J) will integrate with AWS GreenGrass and Azure Edge. Say I want to deploy my IIOT solution on Azure, can I use Azure ML solutions and sit TensorFlow on top of it to coordinate the logic between programs in my system? </p>

<p>Am I incorrect in my thought that TensorFlow, Pytorch, ONNX, DeepLearning4J, and some others sit on top of native ML functions? </p>

<p>Cheers, Jack!</p>
"
2711,"<p>For the <strong>shortest path</strong> problem in a graph, which type of algorithms have better average <strong>time complexity</strong>? <strong>Ant colony optimization</strong> algorithms vs. <strong>Classical routing</strong> algorithms?</p>

<p>In general, can we compare efficiency of these two types of algorithm for the <strong>shortest path</strong> problem in a graph?</p>
"
2712,"<p>AI should be perceiving -  I think. There are debates around the world about the intelligence of an Super-Intelligent AI. I'm curious whether AI can have sentience or not. </p>
"
2713,"<p>On recommendation of Kanak on stackoverflow I am posting this question here:</p>

<p>Currently I am experimenting with various loss functions and optimizers for my binary image segmentation problem. The loss functions that I use in my Unet however give different output segmentation maps.</p>

<p>I have a highly imbalanced dataset, thus I am trying dice loss for which the customized function is given below. </p>

<pre><code>    def dice_coef(y_true, y_pred, smooth=1):
        """"""
        Dice = (2*|X &amp; Y|)/ (|X|+ |Y|)
             =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))
        ref: https://arxiv.org/pdf/1606.04797v1.pdf
        """"""
        intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
        return (2. * intersection + smooth) / (K.sum(K.square(y_true), -1) + K.sum(K.square(y_pred), -1) + smooth)

    def dice_coef_loss(y_true, y_pred):
        return 1 - dice_coef(y_true, y_pred)
</code></pre>

<p>Binary cross entropy results in a probability output map, where each pixel has a color intensity that represents the chance of that pixel being the positive or negative class. However, when I use the dice loss function, the output is not a probability map but the pixels are classed as either 0 or 1.</p>

<p>My questions are: </p>

<p>1.How is it possible that these different loss functions have these vastly different results?</p>

<ol start=""2"">
<li>Is there a way to customize the dice loss function so that the output segmentation map is a probability map similar to the one of binary crossentropy loss.</li>
</ol>
"
2714,"<p>I have this question in my head: does the current level of AI development allow us to spot faked or photoshoped images? (i.e forged ID card or personal documents).  </p>

<p>If it is possible, what is such a process to follow in order to build an AI that achieves this task?  </p>
"
2715,"<p>Are there any projects where you can detect and measure the width of a crack? I am using tensorflow and labeling the data sets for now. </p>
"
2716,"<p>Is ""emotion"" ever used in AI?</p>

<p>Psychologists have a lot to say about emotion and it's functional utility for survival - but I've never seen any AI research that uses something resembling ""emotion"" inside an algorithm. (Yes, there's some work done on trying to classify human emotions, called ""emotional intelligence"", but that's extrememly different from /using/ emotions within an algorithm) For example, you could imagine that a robot might need fuel and be ""very thirsty"" - causing it to prioritize different tasks (seeking fuel). Emotions also sometimes don't just focus on objectives/priorities - but categorize how much certain classifications are ""projected"" into a particular emotions.<br>
For example, maybe a robot that needs fuel might be very ""afraid"" of going towards cars because it's been hit in the past - while it might be ""frustrated"" at a container that doesn't open properly. 
It seems very natural that these things are helpful for survival - and they are likely ""hardcoded"" in our genes (since some emotions - like sexual attraction - seem to be mostly unchangeable by ""nurture"") - so I would think they would have a lot of general utility in AI. </p>
"
2717,"<p>I'm new to machine learning, and AI in general (but with 20+ years for programming). I'm wondering if machine learning is a good general approach to find the seed of a random number generator. </p>

<p>Suppose I have a list of 2000 numbers. Is there a machine learning algorithm to correctly guess the next number? </p>

<p>Just to be clear, as there are many random number generator algorithms. I'm taking about rand and srand from the <a href=""http://www.cplusplus.com/reference/cstdlib/rand/"" rel=""nofollow noreferrer"">stdlib</a>. </p>

<p>Thanks,
Eden </p>
"
2718,"<p>I'm reading the book ""Reinforcement Learning: An Introduction"" (by Andrew Barto and Richard S. Sutton). </p>

<p>The authors provide the pseudocode of the <em>prioritized sweeping</em> algorithm, but I do not know what is the meaning of <code>Model(s, a)</code>. Does it mean that <code>Model(s, a)</code> is the history of rewards gained when we are in state <code>s</code> and the action <code>a</code> is taken? </p>

<p>Does <code>R, S_new = Model(s,a)</code> mean that we should take a random sample from rewards gained in state <code>s</code> and action <code>a</code> is taken?</p>
"
2719,"<p>In my view intelligence begins once the thoughts/actions are logical rather than purely randomn based. The learning environments can be random but the logic seems to obey some elusive rules. There is also the aspect of a parenting that guides through some really bad decisions by using the collective knowledge. All of this seems to hint that intelligence needs intelligence to coexist and a sharing communication network for validation/rejection.</p>

<p>Personally I believe that we must keep the human intelligence in a parental role for long enough time until at least the AI had fully assimilated our values. The actual danger is to leave the artificial intelligence parenting another AI and loose control of it. This step is not necessary from our perspective but can we resist the temptation and try it eventually, only time will tell.</p>

<p>Above all we must remember the purpose of AI. I think the purpose should always be to help humans achieve mastery of the environment while ensuring our collective preservation.</p>

<p>AI should not be left unsupervised as we would not give guns to kids, do we?</p>

<p>To resume it all AI needs an environment and supervision where to learn and grow. The environment can vary but the supervision must stay in place.</p>

<p>Are initiated thoughts/actions by the means of guidance and supervision considered random?</p>

<p>Lastly I believe that the sensible think to do is to only develop artificial intelligence that is limited by our own beliefs and values rather than searching for something greater than us.</p>

<p>It seems not possible to create greater than our intelligence without letting it go exploring!
Exploring has greater access to random actions and can go against the intended purpose.</p>
"
2720,"<p>When trying to map artificial neuronal models to biological facts it was not possible to find an answer regarding the <strong>biological justification</strong> of randomly initializing the weights.</p>

<p>Perhaps this is not yet known from our current understanding of biological neurons?</p>
"
2721,"<p>I'm doing a research on a finite-horizon Markov decision process with <span class=""math-container"">$t=1, \dots, 40$</span> periods. In every time step <span class=""math-container"">$t$</span>, the (only) agent has to chose an action <span class=""math-container"">$a(t) \in A(t)$</span>, while the agent is in state <span class=""math-container"">$s(t) \in S(t)$</span>. The chosen action <span class=""math-container"">$a(t)$</span> in state <span class=""math-container"">$s(t)$</span> affects the transition to the following state <span class=""math-container"">$s(t+1)$</span>.</p>

<p>In my case, the following holds true: <span class=""math-container"">$A(t)=A$</span> and <span class=""math-container"">$S(t)=S$</span>, while the size of <span class=""math-container"">$A$</span> is <span class=""math-container"">$6 000 000$</span> (6 million) and the size of <span class=""math-container"">$S$</span> is <span class=""math-container"">$10^8$</span>. Furthermore, the transition function is stochastic.</p>

<p>Would Monte Carlo Tree Search (MCTS) an appropriate method for my problem (in particular due to the large size of <span class=""math-container"">$A$</span> and <span class=""math-container"">$S$</span> and the stochastic transition function?)</p>

<p>I have already read a lot of papers about MCTS (e.g. progressive widening and double progressive widening, which sound quite promising), but maybe someone can tell me about his experiences applying MCTS to similar problems or about appropriate methods for this problem (with large state/action space and a stochastic transition function).</p>
"
2722,"<p>In the book ""Reinforcement Learning: An Introduction"" (2018) Sutton and Barto define at page 102 the importance-sampling-ration as follows:</p>

<p><span class=""math-container"">$$\rho _{t:T-1}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)}$$</span></p>

<p>for a target policy <span class=""math-container"">$\pi$</span> and a behaviour policy <span class=""math-container"">$b$</span>.</p>

<p>One page before however they state: ""The target policy <span class=""math-container"">$\pi$</span> [...] may be deterministic [...]"".</p>

<p>When <span class=""math-container"">$\pi$</span> is deterministic and greedy it gives 1 for the greedy action and 0 for all other possible actions.</p>

<p>So how can the above formular give something else than zero, except for the case where policy <span class=""math-container"">$b$</span> takes a path that <span class=""math-container"">$\pi$</span> would have taken as well? Because if any selected action of <span class=""math-container"">$b$</span> is different from <span class=""math-container"">$\pi$</span>'s choice than the whole numerator is zero and thus the whole result.</p>
"
2723,"<p>In Deep Learning by Goodfellow et al., I came across the following line on the chapter on Stochastic Gradient Descent (pg. 287):</p>

<blockquote>
  <p>The main question is how to set <span class=""math-container"">$\epsilon_0$</span>. If it is too large, the
  learning curve will show violent oscillations, with the cost function
  often increasing significantly.</p>
</blockquote>

<p>I'm slightly confused why the loss function would increase at all. My understanding of gradient descent is that given parameters <span class=""math-container"">$\theta$</span> and a loss function <span class=""math-container"">$\ell (\vec{\theta})$</span>, the gradient update is performed as follows:</p>

<p><span class=""math-container"">$$\vec{\theta}_{t+1} = \vec{\theta}_{t} - \epsilon \nabla_{\vec{\theta}}\ell (\vec{\theta})$$</span></p>

<p>The loss function is guaranteed to monotonically decrease because the parameters are updated in the negative direction of the gradient. I would assume the same holds for SGD, but clearly it doesn't. With a high learning rate <span class=""math-container"">$\epsilon$</span>, how would the loss function increase in its value? Is my interpretation incorrect or does SGD have different theoretical guarantees than vanilla gradient descent?</p>
"
2724,"<p>I have non-smooth loss function - e.g. loss(x)=min(x, 0.5). Can gradient descent be used for training neural networks with such functions. Can gradient descent be used for fairly general, mathematically not-nice functions? Pytorch or Tensorflow can calculate numerically gradients from almost any function, but it is acceptable practice to use general, not-nice loss functions?</p>
"
2725,"<p>In the paper <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.8293&amp;rep=rep1&amp;type=pdf"" rel=""nofollow noreferrer"">Markov games as a framework for multi-agent reinforcement learning </a> (which introduces the minimax Q Learning algorithm), at the bottom left of page 3, my understanding is that the author suggests, for a simultaneous 1v1 zero-sum game, to do Bellman iterations with <span class=""math-container"">$$V(s)=\min_{o}\sum_{a}\pi_{a}Q(s,a,o)$$</span> with <span class=""math-container"">$\pi_{a}$</span> the probability of playing action <span class=""math-container"">$a$</span> for the maximizing player in his best mixed strategy to play in state <span class=""math-container"">$s$</span>. </p>

<p>If my understanding is correct, why does the opponent in this equation play a pure strategy (<span class=""math-container"">$\min_{o}$</span>) rather than his best mixed strategy in state <span class=""math-container"">$s$</span>. This would instead give <span class=""math-container"">$$V(s)=\sum_{o}\sum_{a}\pi_{a}\pi_{o}Q(s,a,o)$$</span> with <span class=""math-container"">$\pi_{o}$</span> the opponent's best mixed strategy in state <span class=""math-container"">$s$</span>. Which of these two formulations is correct and why? Are they somehow equivalent?</p>

<p>The context of this question is that I am trying to use minimax Q learning with a Neural Network outputting the matrix <span class=""math-container"">$Q(s,a,o)$</span> for a simultaneous zero-sum game. I have tried both methods and so far have seen seemingly equally bad results, quite possibly due to bugs or other errors in my method.</p>
"
2726,"<p>The spectrum of human sensory inputs seems to fall within certain ranges suggesting normalization is built-in into biological NNs?</p>

<p>It also adapts to circumstantial conditions, e.g. people living in a city with certain factory smell eventually don't perceive the smell anymore, at least consciously (within working memory) / it adapts to a new baseline?</p>
"
2727,"<p>As according to the definition of AI something that can learn overtime, can imitate human behaviors, comes under Artificial Intelligence. If Expert system(eg. MYCIN) that only involves if then else statements qualifies to be an AI then every program we write in our daily life that involves some condition based question answering should be an AI. Right? If not then what should be an exact and universal definition for AI. How can a software qualify to be called AI.
Can someone please explain it to me ?
Thanks in Advance!!</p>
"
2728,"<p>Disclaimer: I'm not a student in computer science and most of my knowledge about ML/NN comes from YouTube, so please bear with me!</p>

<hr>

<p>Let's say we have a classification neural network, that takes some input data <span class=""math-container"">$w, x, y, z$</span>, and has some number of output neurons. I like to think about a classifier that decides how expensive a house would be, so its output neurons are bins of the approximate price of the house.</p>

<p>Determining house prices is something humans have done for a while, so let's say we know <em>a priori</em> that data <span class=""math-container"">$x, y, z$</span> are important to the price of the house (square footage, number of bedrooms, number of bathrooms, for example), and datum <span class=""math-container"">$w$</span> has no strong effect on the price of the house (color of the front door, for example). As an experimentalist, I might determine this by finding sets of houses with the same <span class=""math-container"">$x, y, z$</span> and varying <span class=""math-container"">$w$</span>, and show that the house prices do not differ significantly.</p>

<p>Now, let's say our neural network has been trained for a little while on some random houses. Later on in the data set. it will encounter sets of houses whose <span class=""math-container"">$x, y, z$</span> and price are all the same, but whose <span class=""math-container"">$w$</span> are different. I would naively expect that at the end of the training session, the weights from <span class=""math-container"">$w$</span> to the first layer of neurons would go to zero, effectively decoupling the input datum <span class=""math-container"">$w$</span> from the output neuron. I have two questions:</p>

<ol>
<li>Is it certain, or even likely, that <span class=""math-container"">$w$</span> will become decoupled from the layer of output neurons?</li>
<li>Where, mathematically, would this happen? What in the backpropagation step would govern this effect happening, and how quickly would it happen?</li>
</ol>

<p>For a classical neural network, the network has no ""memory,"" so it might be very difficult for the network to realize that <span class=""math-container"">$w$</span> is a worthless input parameter.</p>

<p>Any information is much appreciated, and if there are any papers that might give me insight into this topic, I'd be happy to read them.</p>
"
2729,"<p>I'm looking to build a PC for deep learning will tensoflow work on amd GPU with the same speed as on nvidia ones as amd doesn't have tensorcore or cuda cores but it will have 16gb of hbm vram his much do the tensor cores impact training </p>
"
2730,"<p>This inquiry appeared in the comments to <a href=""https://ai.stackexchange.com/questions/9897/is-emotion-ever-used-in-ai/9900#9900"">one of the answers</a> of <a href=""https://ai.stackexchange.com/questions/9897/is-emotion-ever-used-in-ai"">this question</a>, but is actulaly not related to emotional intelligence, so it is reproduced here as a separate question</p>

<blockquote>
  <p>I thought (maybe incorrectly) fuzzy logic was mostly used in sort of pre-programmed control systems, as opposed to AI or learning systems that regress on some datasets.</p>
</blockquote>
"
2731,"<p>I understand why deep generative models like  DBN ( deep belief nets ) or DBM ( deep boltzmann machines ) are able to capture underlying structures in data and use it for various tasks ( classification, regression, multimodal representations etc ...).</p>

<p>But for the classification tasks like in <a href=""http://www.cs.cmu.edu/~rsalakhu/papers/annrev.pdf"" rel=""nofollow noreferrer"">Learning deep generative models</a>, I was wondering why the network is fine-tuned on labeled-data like a feed-forward network and why only the last hidden layer is used for classification?</p>

<p>During the fine-tuning and since we are updating the weights for a classification task ( not the same goal as the generative task ), could the network lose some of its ability to regenerate proper data? ( and thus to be used for different classification tasks ? )</p>

<p>Instead of using only the last layer, could it be possible to use a partition of the hidden units of different layers to perform the classifications task and without modifying the weights? For example, by taking a subset of hidden units of the last two layers ( sub-set of abstract representations ) and using a simple classifier like an SVM?</p>

<p>Thank you in advance!</p>
"
2732,"<p>I have checked out many methods and paper like yolo, ssd, etc with very promising result in detecting a rectangular box around object, But could not find any paper, which shows an learning a rotated bounding box. Is it difficult to learn the rotated bounding box for an object?</p>

<p><a href=""https://i.stack.imgur.com/Ljimz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ljimz.png"" alt=""""></a></p>

<p>For example for this object <a href=""https://stackoverflow.com/q/40404031/9277245"">Src</a>, its bounding box should be of the same shape(the rotated rectangle shown in 2nd right image), But prediction result for the yolo will be Ist right.</p>

<p>Can somebody refer some paper, by which we can learn such box, or it is an expensive task to learn?
Thanks</p>
"
2733,"<p>I'm using an object detection neural network and I employ data augmentation to increase a little my small dataset. More specifically I do rotation, translation, mirroring and rescaling.</p>

<p>I notice that rotating an image (and thus it's bounding box) changes its shape. This implies an erroneous box for elongated boxes, for instance on the augmented image (right image below) the box is not tightly packed around the left player as it was on the original image. </p>

<p>The problem is that this kind of data augmentation seems (in theory) to hamper the network to gain precision on bounding boxes location as it loosens the frame.</p>

<p>Are there some studies dealing with the effect of data augmentation on the precision of detection networks? Are there systems that prevent this kind of thing?</p>

<p>Thank you in advance!</p>

<p>(Obviously, it seems advisable to use small rotation angles)</p>

<p><a href=""https://i.stack.imgur.com/ZylvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZylvJ.png"" alt=""enter image description here""></a></p>
"
2734,"<p>Let's consider a classic feedforward neural network <span class=""math-container"">$F$</span> with input dimension <span class=""math-container"">$d$</span>, output dimension <span class=""math-container"">$k$</span>, <span class=""math-container"">$L$</span> layers <span class=""math-container"">$l_i$</span> with <span class=""math-container"">$m$</span> neurons each. ReLu activation.</p>

<p>This means that, given a point <span class=""math-container"">$x \in R^d$</span> its image <span class=""math-container"">$F(x) \in R^k$</span>. Let's now assume i add some gaussian noise <span class=""math-container"">$\eta_i$</span> in EVERY hidden layer <span class=""math-container"">$l_i(x)$</span> at the same time, where the norm of this noise is 5% the norm of its layer computed on the point <span class=""math-container"">$x$</span>. Let's call this new neural network <span class=""math-container"">$F_*$</span></p>

<p>I know that, empirically, neural networks are resistant to this kind of noise, especially on the first layers. How can i show this theoretically?</p>

<p>The question i'm trying to answer is the following:</p>

<p>After having injected this noise <span class=""math-container"">$\eta_i$</span> in every layer <span class=""math-container"">$l_i(x)$</span>, how far the output <span class=""math-container"">$F_{*}(x)$</span> will be from the output of the original neural network <span class=""math-container"">$F(x)$</span>?</p>
"
2735,"<p>If I'm performing a text classification task using a model built in Keras, and for example, am attempting to predict the appropriate tag given a Stack Overflow question:</p>

<p>""how to subtract 1 from an int  how do i subtract 1 from an int?""</p>

<p>And the true/gold tag for this question is:</p>

<p>""objective-c""</p>

<p>But my model is predicting:</p>

<p>""c#""</p>

<p>If I were to retrain my model but this time add the above question and tag in both the training and testing data, would the model be guaranteed to predict the correct tag for this question in the test data?</p>

<p>I suppose the tl;dr is: Are neural networks deterministic if they encounter identical data during training and testing?</p>

<p>I'm aware it's not a good idea to use the same data in both training and testing, but I'm interested from a hypothetical perspective, and for gaining more insight into how neural networks actually learn. My intuition for this question is ""no"", but I'd really be interested in being pointed to some relevant literature that expands/explains that intuition.</p>
"
2736,"<p>The Markov property is the dependence of a system's future state probability distribution solely on the present state, excluding any dependence on past system history. </p>

<p>The presence of the Markov property saves computing resource requirements in terms of memory and processing in AI implementations, since no indexing, retrieval, or calculations involving past states is required.</p>

<p>However, the Markov property is often an unrealistic and too strong assumption.</p>

<p>Precisely, what limitations does the Markov property place on real-time learning?</p>
"
2737,"<p>As the question states, I am wondering how, if at all, a varying length of a trajectory (series of state,action pairs) will impact training/performance of policy gradient algorithms such as PPO, TRPO and VPG. </p>

<p>Let's say an agent runs in an environment where the length of each episode may not always be the same (this is true especially in games such as poker). The cumulative reward for longer trajectories will inevitably be larger than the rewards for smaller trajectories. To me, this seems to cause an imbalance favoring the actions of a policy executed for a longer period of time (non terminal state) even if that policy may be sub-optimal compared to the policy applied for a shorter trajectory. </p>

<p>Are my assumptions correct? How does trajectory size end up impacting the updates on a policy?</p>
"
2738,"<p>BDI agents are usually implemented using logical and symbolic methods, e.g. AgentSpeak, some robot control is being done by cognitive architectures, like OpenCog. But - if neural networks allows to do the generalization, the reasoning and the language processing - why neural networks are not used for goal-directed (e.g. utilitarian) autonomous agents, e.g. controlling service robots of providing intelligent services more than chatbots can do, e.g. doing automatic programming based on the dialog with the end users?</p>

<p>I asked a bit similar question about continuous learning BDI agents in particular <a href=""https://ai.stackexchange.com/questions/8027/neural-network-as-bdi-agent-running-in-continuous-mode-that-do-inference-in"">Neural network as (BDI) agent - running in continuous mode (that do inference in parallel with learning)?</a> and received good answer, but I wonder, why Google is not givien recent results for search ""neural networks autonomous agents"".</p>

<p>I have bad experience with Google, that is why I am asking here. E.g. Google gives very few good answer for general query ""grammar extraction from neural network"", but Google gives excellent answers for the very specific question ""learning context free and context sensitive grammars with neural networks"". I feel that there is similar problem with neural networks and autonomous agents - something should be going on, but it uses different, specific keywords, it takes some distinct angle and it is impossible to get good search results because of Google failure to make some semantic search.</p>

<p>This findings <a href=""http://www.cs.cmu.edu/~dchaplot/papers/arnold_aaai17.pdf"" rel=""nofollow noreferrer"">http://www.cs.cmu.edu/~dchaplot/papers/arnold_aaai17.pdf</a> and <a href=""https://github.com/glample/Arnold"" rel=""nofollow noreferrer"">https://github.com/glample/Arnold</a> most likely show that reinforcement learning game playing agents are the current state of the art and there are not much efforts to go beyond them. There are more or less autonomous agents in logical-symbolic worlds that can induce and reason about goal hierarchy (deriving goals from the very general goals, e.g. maximizing utility in some scheme of preferences, earning profit from provided services) and that can act following deduced goals, but apparently - neural networks are not ready for this. But maybe still there are some good research trends?</p>
"
2739,"<p>How does DARTS compare to ENAS? Which one is better or what advantages does they each have?</p>

<p>Links:</p>

<ul>
<li><a href=""https://arxiv.org/abs/1806.09055"" rel=""nofollow noreferrer"">DARTS: Differentiable Architecture Search</a></li>
<li><a href=""https://arxiv.org/abs/1802.03268"" rel=""nofollow noreferrer"">Efficient Neural Architecture Search via Parameter Sharing</a></li>
</ul>
"
2740,"<p>Let's say for:
1. Image tasks
2. Deep RL in high dimensional state space</p>
"
2741,"<p>Some examples of low-variance Machine Learning algorithms include Linear Regression, Linear Discriminant Analysis and Logistic Regression. </p>

<p>Examples of high-variance Machine Learning algorithms include Decision Trees, k-Nearest Neighbors and Support Vector Machines.</p>

<p><strong>Source:</strong>
<a href=""https://i.stack.imgur.com/s2m9z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/s2m9z.png"" alt=""""></a></p>

<p><strong>What makes a Machine Learning algorithm a low variance one or a high variance one?</strong></p>

<p><a href=""https://i.stack.imgur.com/J4V0w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/J4V0w.png"" alt=""KNN model""></a></p>

<p><a href=""https://i.stack.imgur.com/EK9d9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EK9d9.png"" alt=""linear model""></a></p>
"
2742,"<p>For a neural turing machine, there is an attention distribution over the memory cells. A read operation consists of multiplying the memory cell's value by its respective probability, and adding these results for all memory cells.</p>

<p>Suppose we only did the above operation for memory cells with a probability greater than 0.5, or suppose we concatenated the results instead of adding them. Can this be implemented/ trained with stochastic gradient descent? Or would it not be differentiable?</p>

<p>Thanks!</p>
"
2743,"<p>Why is the e-function used to decide whether to accept a worse solution or not? 
To be more specific: Why was <span class=""math-container"">$e$</span> chosen as basis?</p>

<p>The propability to accept a worse solution is described with:
<span class=""math-container"">$p=e^{-\frac{E(y)-E(x)}{kT}}$</span></p>

<p><span class=""math-container"">$E(y)$</span> is the energy from the old solution
<span class=""math-container"">$E(x)$</span> is the energy from new solution <span class=""math-container"">$T$</span> is a constant temprature decreasing with a constant factor k in every iteration.</p>
"
2744,"<p>I am currently working with classical roboticists who insist on inverse kinematics, and what I (perhaps mistakenly) call the old way of thinking about robots accomplishing tasks. 
Much of the relatively recent research focuses on Robots using Brain models such as   Multiple timescales (Artificial Intelligence models) that segment sequences and reproduce them, having learned them. The problem I face is this bunch of roboticists insist that a robot already knows the sequence, and training it to be reproduced is redundant, since a Robot can already reproduce the sequence anyway.
How accurate would you rate this assessment of using AI in robotics?
Are there any advantages of using AI to learn sequences for robot control?</p>
"
2745,"<p>I would appreciate your help with this (naive) question of mine.</p>

<p>Given the set of points located on a circle, <span class=""math-container"">$x_{i}, y_{i}$</span> as the input data, Can a deep/machine learning algorithm infer that radius of the circle is constant ? In other words, given the data <span class=""math-container"">$x_{i}, y_{i}$</span> is there way that algorithm discovers the constraint: <span class=""math-container"">$x_{i}^2 + y_{i}^2 = \text{constant}$</span> ? </p>

<p>I would also appreciate any related reference on the subject.    </p>
"
2746,"<p>I have data that are a result of <strong>rules</strong> that are <strong>exceptionless</strong>. I want to my program to <em>'look'</em> at my data and figure out those rules. However, the data might contain what might look like an exception (rule within a rule) but that is too, true for all occasions e.g.</p>

<p>All men of the dataset with x common characteristics go out for a beer on Thursday after work. That is true for all men with those characteristics. However, they will cancel their plans if their wife is sick. That last condition might initially look as an exception to the rule (go out for beer on Thursdays), but it is not as long as it is true for all men with those x characteristics. </p>

<p><strong>So the question is</strong>: <em>Which approach/method would be suitable for this?</em></p>
"
2747,"<p>I have a machine learning project in which I have to separate spam and ham emails from a given dataset which has many txt files. Some of them are spam emails and the other ham emails. I have to implement an ID3 algorithm for this case.</p>

<p>How do I begin with this implementation?</p>

<p>Should I use a hashmap to have the words I read from the txt files and the times I read every single word? Something like HashMap&lt;String, int> or should I use two arrays maybe?</p>
"
2748,"<p>I was wondering if machine learning algorithms (CNNs?) can be used/trained to differentiate between small differences in details between images (such as slight differences in shades of red or other colours, or the presence of small objects between otherwise very similar images?)? And then classify images based on these differences? If this is a difficult endeavour with our current machine learning algorithms, how can it be solved? Would using more data (more images) help?</p>

<p>I would also appreciate it if people could please provide references to research that has focused on this, if possible. </p>

<p>I've only just begun learning machine learning, and this is something that I've been wondering from my research.</p>

<p>Thank you.</p>
"
2749,"<p>I have come across the question simple model vs complex model. How to decide which one have to use? and one more question connect to this. How to decide which is a <code>simple model</code> and which is a <code>complex model</code>, with the help of which parameters we can differentiate this </p>

<p>I read about one of this but how we differentiate. is it really based on a number of inputs or number of layer of required to train that model?
This question is very generic I think but I except answer or some hint or direction.</p>

<p>Thanks in advance!!</p>
"
2750,"<p>I am reading about <a href=""http://www.scholarpedia.org/article/Continuous_attractor_network"" rel=""nofollow noreferrer"">CANN</a>, however, I do not seem to grasp what it is. Maybe someone who has worked with it can explain it? I found out about it while reading about RatSLAM. I understand that it helps to keep long/short term memory. </p>
"
2751,"<p>Are there any working AI system designs or theory to support a system where an artificial network is trained to to adjust fuzzy probabilities and modify the parameters of a genetic algorithm that mutates the fuzzy rules?</p>
"
2752,"<p>Is there software available which can extract metaphors from texts?</p>

<p>eg: <em>His words cut deeper than a knife.</em></p>

<p>Or a simpler form like :</p>

<p>eg: <em>Life is a journey that must be travelled no matter how bad the roads and accommodations.</em></p>
"
2753,"<p>I was thinking of something of the sort:</p>

<p>1) Build a program (call this one fake user) that generates lots and lots and lots of data based on the usage of another program (call this one target) using stimuli and response. For example, if the target is minesweeper, the fake user would play the game a carl sagan number of times, as well as try to click all buttons on all sorts of different situations, etc...</p>

<p>2) run a machine learning program (call this one the copier) designed to evolve a code that works as similar as possible to the target. </p>

<p>3) kablam, you have a ""sufficiently nice"" open source copy of the target.</p>

<p>is this possible?</p>

<p>Is something else possible to achieve the same result? Namely: to obtain a ""suffienciently nice"" open source copy of the original target program?</p>
"
2754,"<p>I want to start a project for my artificial intelligence class about speaker recognition. Basically, I want to train my AI to detect if it's me who's speaking or somebody else. I would like some suggestions or libraries to work with.</p>
"
2755,"<p>I was trying to understand the loss function of GANs, while I found a little mis-match between different papers. </p>

<p>This is the screen-shot from the original paper of Goodfellow at <a href=""https://arxiv.org/pdf/1406.2661.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1406.2661.pdf</a>: <a href=""https://i.stack.imgur.com/VhMD7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VhMD7.png"" alt=""Goodfellow""></a>,</p>

<p>And equation (1) in this version of pix2pix paper at <a href=""https://arxiv.org/pdf/1611.07004.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1611.07004.pdf</a>
<a href=""https://i.stack.imgur.com/MXlU3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MXlU3.png"" alt=""enter image description here""></a></p>

<p>Putting aside the fact that pix2pix is using conditional GAN, which introduces a second term <span class=""math-container"">$y$</span>, the 2 formulas are quite resemble, except that in the pix2pix paper, they try to get minimax of <span class=""math-container"">${\cal{L}}_{cGAN}(G, D)$</span>, which is defined to be <span class=""math-container"">$E_{x,y}[...] + E_{x,z}[...]$</span>, whereas in the original paper, they define <span class=""math-container"">$\min\max V(G, D) = E[...] + E[...]$</span>. </p>

<p>I am not coming from a good math background, so I am quite confused. I'm not sure where the mistake is, but assuming that <span class=""math-container"">$E$</span> is expectation (correct me if I'm wrong), the version in pix2pix makes more sense to me, although I think it's quite less likely that Goodfellow could make this mistake in his amazing paper. Maybe there's no mistake at all and it's me who do not understand them correctly.</p>
"
2756,"<p>According to the press information from Dwave, quantum annealing is up and running and offers a lot of opportunities for society. “Basically, a quantum computer is a non-deterministic turing machine which is exciting and cool” – this is at least the statement of AI influencer Siraj Raval who has researched the topic in detail. He has made a video about the subject and is trying to communicte the subject to larger audience. Dwave itself is also interested in promoting their product. They have a lot of information out there and some companies are using these machines, for example Google.</p>

<p>But what i didn't understand is, why exactly do society needs a quantum computer? I mean, image recognition and robotics control can be done with classical deeplearning on mainstream hardware, why do we need superposition and qubits. Or asking more directly, which challenges are out there, that only quantum computers are able to master it?</p>
"
2757,"<p>Looking primarily for research papers, but any info or insights on this subject are welcome.  </p>

<p>Strength does not have to exceed that of the best human player (such as Kasparov), just skilled human players. </p>

<p>Thoughts on the structure of the problems constituting the games, which might lend themselves, at least partially, to a symbolic approach, is also welcome.  Also thoughts on potential flaws or limitations, despite successes. </p>
"
2758,"<p>I am trying to generate a model that uses several physico-chemical properties of a molecule (incl. number of atoms, number of rings, volume, etc.) to predict a numeric value Y. I would like to use PLS Regression, and I understand that standardization is very important here. I am programming in Python, using scikit-learn.
The type and range for the features varies. Some are int64 while other are float. Some features generally have small (positive or negative) values, while other have very large value. I have tried using various scalers (e.g. standard scaler, normalize, minmax scaler, etc.). Yet, the R2/Q2 are still low. I have a few questions:</p>

<ul>
<li>(1) Is it possible that by scaling, some of the very important features lose their significance, and thus contribute less to explaining the variance of the response variable?</li>
<li>(2) If yes, if I identify some important features (by expert knowledge), is it OK to scale other features but those? Or scale the important features only?</li>
<li>(3) Some of the features, although not always correlated, have values that are in a similar range (e.g. 100-400), compared to others (e.g. -1 to 10). Is it possible to scale only a specific group of features that are within the same range?</li>
</ul>

<p>Thank you for your help.</p>

<p>Yannick</p>
"
2759,"<p>Is there any way to control the extraction of features?How to recognize what features are been learnt during training i.e relevant information is been learnt or not?</p>
"
2760,"<p>Let's say I want to model purchase data (i.e. purchase records of many households across time). For simplicity, let's assume each household only picks one alternative at the time. A simple starting point is a multinomial logit model. Then, more flexible network architectures could be used. People have applied NN to this, but kept the number of alternatives (K) constant. In reality, the number of available options changes over time. Also, it would be interesting to predict how choices change when the number of alternatives is changed.</p>

<p>in bullet points</p>

<ul>
<li>there are N households </li>
<li>t_n purchases for each household</li>
<li>There are K_t alternatives at time t</li>
<li>Dependent variable Y=k indicates that alternative k was bought</li>
<li>Each alternative is characterized by features, so x_kt is a vector of those features (including brand name, price, ...). The number of features is constant across time. </li>
</ul>

<p>Any guidance or ideas?</p>
"
2761,"<p><a href=""https://i.stack.imgur.com/jLKuO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jLKuO.png"" alt=""minimaxQuestion""></a></p>

<p>The image is one of many similar exam questions can anyone pelase help me understand it fully?</p>

<p>'Internal node': This is simply every node except A?</p>

<p>Move choices: His only options are B, C and D for this move?</p>

<p>Focusing on B: E=8 F=4 and G are all opponent responses, therefore they will pick the minimum value.</p>

<p>Now my confusion, are M N and P your known responses in the case the opponent picks G, so you should pick M=0 (the highest value), so then G gets passed 0 which the opponent should choose so B has a h-value of 0? </p>

<p>Are the correct value then B=0, C=1 and D=2 so pick D as next move?</p>
"
2762,"<p>Recurrent Neural Networks (RNN) With Attention Mechanism is generally used for Machine Translation and Natural Language Processing. In Python, implementation of RNN With Attention Mechanism is abundant in Machine Translation (For Eg. <a href=""https://talbaumel.github.io/blog/attention/"" rel=""nofollow noreferrer"">https://talbaumel.github.io/blog/attention/</a>, however what I would like to do is to use RNN With Attention Mechanism on a temporal data file (not any textual/sentence based data). I have a CSV file with of dimensions 21000 x 1936, which I have converted to a Dataframe using Pandas. The first column is of Datetime Format and last column consists of target classes like ""Class1"", ""Class2"", ""Class3"" etc. which I would like to identify. So in total, there are 21000 rows (instances of data in 10 minutes time-steps) and 1935 features. The last (1936th column) is the label column.</p>

<p>It is predominant from existing literature that an Attention Mechanism works quite well when coupled into the RNN. I am unable to locate any such implementation of RNN with Attention Mechanism, which can also provide a visualisation as well. Any help in this regard would be highly appreciated. Cheers! </p>
"
2763,"<p>What is ""bad local minima""? </p>

<p>The following papers all mention this expression.</p>

<ol>
<li><a href=""https://arxiv.org/abs/1901.03909"" rel=""nofollow noreferrer"">Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit</a></li>
<li><a href=""https://arxiv.org/abs/1901.00279"" rel=""nofollow noreferrer"">limination of All Bad Local Minima in Deep Learning</a></li>
<li><a href=""https://arxiv.org/abs/1805.08671"" rel=""nofollow noreferrer"">Adding One Neuron Can Eliminate All Bad Local Minima</a></li>
</ol>
"
2764,"<p>What exactly is meant by ""humanitarian AI""? What research areas does this cover? AI in healthcare? Algorithmic fairness? Applications of AI for economic development? Can anyone provide links to relevant papers?</p>
"
2765,"<p>I have this problem where I need to get information out of PDF document sent from a scanner. The program needs to be learnable in some way to recognize what different figures mean. Most of this should happen without human interference so it could just give a result after scanning the file.
Do anyone know if it's possible to do with a machine learning program or any alternative way?</p>
"
2766,"<p>For example, I have the following csv: <a href=""https://gist.github.com/JafferWilson/3ab8ee88f3fc32e78579a1054aac757d"" rel=""nofollow noreferrer"">training.csv</a><br>
I want to know how I can determine which column will be the best feature for getting the output prediction before I go for machine training.<br>
Please do share your responses</p>
"
2767,"<p>I have a Deep Feedforward Neural Network <span class=""math-container"">$F: W \times \mathbb{R}^d \rightarrow \mathbb{R}^k$</span> (where <span class=""math-container"">$W$</span> is the space of the weights) with <span class=""math-container"">$L$</span> hidden layers, <span class=""math-container"">$m$</span> neurones per layer and ReLu activation. The output layer has a softmax activation function.</p>

<p>I can consider two different loss functions: </p>

<p><span class=""math-container"">$L_1 = \frac{1}{2} \sum_i || F(W,x_i) - y||^2$</span>  <span class=""math-container"">$
 \ \ \ $</span> and   <span class=""math-container"">$\ \ \ L_2  = -\sum_i log(F(w,x_i)_{y_i})$</span></p>

<p>where the first one is the classic quadratic loss and the second one is cross entropy loss. </p>

<p>I'd like to study the norm of the derivative of the loss function and see how the two are related, which means: </p>

<p>1) Let's assume I know that <span class=""math-container"">$|| \frac{\partial L_2(W, x_i)}{\partial W}|| &gt; r$</span>, where <span class=""math-container"">$r$</span> is a small constant. What can I assume about <span class=""math-container"">$|| \frac{\partial L_1(W, x_i)}{\partial W}||$</span> ?</p>

<p>2) Are there any result which tell you that, under some hypothesis (even strict ones) such as a specific random initialisation,  <span class=""math-container"">$|| \frac{\partial L_1(W, x_i)}{\partial W}||$</span> doesn't go to zero   during training?</p>

<p>Thank you</p>
"
2768,"<p>There are many people trying to show how computer models are still very different from humans, but I fail to see in what way are people different from neural models in anything but complexity?</p>

<p>The way we learn is similar, the way we process information is similar, the ways we predict outcomes and generate outputs are similar. Give a model enough processing power, enough training samples and enough time and you can train a human.</p>

<p>How are we different?</p>
"
2769,"<p>I tried to build an Q-learning agent which you can play tic tac toe against after training.</p>

<p>Unfortunately the agent performs pretty poorly. He tries to win but does not try to make me 'not winning' which ends up in me beating up the agent no matter how many loops I gave him for training. I added a reward of 1 for winning the episode and it gets a reward of -0.1 when he tries to put his label on an non empty square (after the attempt we have s = s'). I also start with an epsilon=1 which decreases in every loop to add some more randomness at the beginning because I witnessed that some (important in my opinion) states did not get updated. Since I spend some hours of debugging without noticeable progress I'd like to know what you think.</p>

<p>Best Rewgards</p>

<p>PS: Don't care about some print statements and count variables. Those where for debugging.</p>

<p>Code here or on <a href=""https://github.com/DataHamster/q-learning-tic-tac-toe"" rel=""nofollow noreferrer"">Github</a></p>

<pre><code>import numpy as np
import collections
import time

Gamma = 0.9
Alpha = 0.2


class Environment:
    def __init__(self):
        self.board = np.zeros((3, 3))
        self.x = -1  # player with an x
        self.o = 1  # player with an o
        self.winner = None
        self.ended = False
        self.actions = {0: (0, 0), 1: (0, 1), 2: (0, 2), 3: (1, 0), 4: (1, 1),
                        5: (1, 2), 6: (2, 0), 7: (2, 1), 8: (2, 2)}

    def reset_env(self):
        self.board = np.zeros((3, 3))
        self.winner = None
        self.ended = False

    def reward(self, sym):
        if not self.game_over():
            return 0
        if self.winner == sym:
            return 10
        else:
            return 0

    def get_state(self,):
        k = 0
        h = 0
        for i in range(3):
            for j in range(3):
                if self.board[i, j] == 0:
                    v = 0
                elif self.board[i, j] == self.x:
                    v = 1
                elif self.board[i, j] == self.o:
                    v = 2
                h += (3**k) * v
                k += 1
        return h

        def random_action(self):
            return np.random.choice(self.actions.keys())

    def make_move(self, player, action):
        i, j = self.actions[action]
        if self.board[i, j] == 0:
            self.board[i, j] = player

    def game_over(self, force_recalculate=False):
        # returns true if game over (a player has won or it's a draw)
        # otherwise returns false
        # also sets 'winner' instance variable and 'ended' instance variable
        if not force_recalculate and self.ended:
            return self.ended

        # check rows
        for i in range(3):
            for player in (self.x, self.o):
                if self.board[i].sum() == player*3:
                    self.winner = player
                    self.ended = True
                    return True

        # check columns
        for j in range(3):
            for player in (self.x, self.o):
                if self.board[:, j].sum() == player*3:
                    self.winner = player
                    self.ended = True
                    return True

        # check diagonals
        for player in (self.x, self.o):
            # top-left -&gt; bottom-right diagonal
            if self.board.trace() == player*3:
                self.winner = player
                self.ended = True
                return True
            # top-right -&gt; bottom-left diagonal
            if np.fliplr(self.board).trace() == player*3:
                self.winner = player
                self.ended = True
                return True

        # check if draw
        if np.all((self.board == 0) == False):
            # winner stays None
            self.winner = None
            self.ended = True
            return True

        # game is not over
        self.winner = None
        return False

    def draw_board(self):
        for i in range(3):
            print(""-------------"")
            for j in range(3):
                print(""  "", end="""")
                if self.board[i, j] == self.x:
                    print(""x "", end="""")
                elif self.board[i, j] == self.o:
                    print(""o "", end="""")
                else:
                    print(""  "", end="""")
            print("""")
        print(""-------------"")




class Agent:
    def __init__(self, Environment, sym):
        self.q_table = collections.defaultdict(float)
        self.env = Environment
        self.epsylon = 1.0
        self.sym = sym
        self.ai = True

    def best_value_and_action(self, state):
        best_val, best_act = None, None
        for action in self.env.actions.keys():
            action_value = self.q_table[(state, action)]
            if best_val is None or best_val &lt; action_value:
                best_val = action_value
                best_act = action
        return best_val, best_act

    def value_update(self, s, a, r, next_s):
        best_v, _ = self.best_value_and_action(next_s)
        new_val = r + Gamma * best_v
        old_val = self.q_table[(s, a)]
        self.q_table[(s, a)] = old_val * (1-Alpha) + new_val * Alpha

    def play_step(self, state, random=True):
        if random == False:
            epsylon = 0
        cap = np.random.rand()
        if cap &gt; self.epsylon:
            _, action = self.best_value_and_action(state)
        else:
            action = np.random.choice(list(self.env.actions.keys()))
            self.epsylon *= 0.99998
        self.env.make_move(self.sym, action)
        new_state = self.env.get_state()
        if new_state == state and not self.env.ended:
            reward = -5
        else:
            reward = self.env.reward(self.sym)
        self.value_update(state, action, reward, new_state)


class Human:
    def __init__(self, env, sym):
        self.sym = sym
        self.env = env
        self.ai = False

    def play_step(self):
        while True:
            move = int(input('enter position like: \n0|1|2\n------\n3|4|5\n------\n6|7|8'))
            if move in list(self.env.actions.keys()):
                break
        self.env.make_move(self.sym, move)



def main():
    env = Environment()
    p1 = Agent(env, env.x)
    p2 = Agent(env, env.o)
    draw = 1
    for t in range(1000005):

        current_player = None
        episode_length = 0
        while not env.game_over():
            # alternate between players
            # p1 always starts first
            if current_player == p1:
                current_player = p2
            else:
                current_player = p1

            # current player makes a move
            current_player.play_step(env.get_state())

        env.reset_env()

        if t % 1000 == 0:
            print(t)
            print(p1.q_table[(0, 0)])
            print(p1.q_table[(0, 1)])
            print(p1.q_table[(0, 2)])
            print(p1.q_table[(0, 3)])
            print(p1.q_table[(0, 4)])
            print(p1.q_table[(0, 5)])
            print(p1.q_table[(0, 6)])
            print(p1.q_table[(0, 7)])
            print(p1.q_table[(0, 8)])
            print(p1.epsylon)

    env.reset_env()
    # p1.sym = env.x

    while True:
        while True:
            first_move = input(""Do you want to make the first move? y/n :"")
            if first_move.lower() == 'y':
                first_player = Human(env, env.x)
                second_player = p2
                break
            else:
                first_player = p1
                second_player = Human(env, env.o)
                break
        current_player = None

        while not env.game_over():
            # alternate between players
            # p1 always starts first
            if current_player == first_player:
                current_player = second_player
            else:
                current_player = first_player
            # draw the board before the user who wants to see it makes a move

            if current_player.ai == True:
                current_player.play_step(env.get_state(), random=False)
            if current_player.ai == False:
                current_player.play_step()
            env.draw_board()
        env.draw_board()
        play_again = input('Play again? y/n: ')
        env.reset_env()
        # if play_again.lower != 'y':
        #     break


if __name__ == ""__main__"":
    main()
</code></pre>
"
2770,"<p>I'm a programmer with a background in mathematics, but I have no experience whatsoever with artificial intelligence/neural networks. I'd like to study it as a hobby, and my goal for now is to solve the following simple poker game, by letting the program play against itself:</p>

<p>We have two players, each with a certain number of chips. At the start of the game, they are obligated to put a certain amount of chips in the pot. Then they each get a random real number between 0 and 10. They know their own number, but not the one of their opponent. Then we have one round of betting. The first player puts additional chips in the pot (some number between 0 and their stack size). The second player can either fold (put no additonal chips in the pot, 1st player gets the entire pot), call (put the same number of chips in the pot, player with highest number gets the pot) or raise (put even more chips in the pot, action back on 1st player). There is no limit to the amount of times  a player can raise, as long as he still has chips behind to raise.</p>

<p>I have several questions:
- Is this indeed a problem that can be solved with neural networks?
- What do you recommend me to study in order to solve this problem?
- Is it feasible to solve this game when allowing for continuous bet/raise sizes? Or should I limit it to a few options as a percentage of the pot?
- Do you expect it to be possible to get close to an equilibrium with one nightly run on an 'average' laptop?</p>
"
2771,"<p>I have source data that can be represented as a 2D image of many similar curves. They may oftentimes cross over one another, so regions of interest will overlap.</p>

<p>My goal is to implement a neural network solution to identify each instance or the curves and the pixels that are associated with each instance.</p>

<p>(Each image is simple in its representation of the data. A pixel in the image is either a point on one of these curves or it is empty. So the image is represented by one or zero at each pixel. For training purposes, I have labels for every pixel, and I have about 150,000 images. The information in the images can be noisy in that there may be omissions of points and point locations are quantized due to measurement limitations and preprocessing for the image preparation.)</p>

<p>I started looking into what semantic segmentation can do for me, but since all of the instances are of the same class, distinguished mainly by their location in the image, I don't think semantic segmentation is the type of processing I would want to perform. (Am I wrong?)</p>

<p>I am very interested in seeing how a neural network will work on this problem to separate each instance. </p>

<p>My question is this: what is the terminology that describes the process I'm looking for? (How can I effectively research for this problem?) Is this an extension of semantic segmentation or is it referred to some other way?</p>
"
2772,"<p><a href=""http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching_files/pg.pdf"" rel=""nofollow noreferrer"">At slide 17</a> of the David Silver's series, the soft-max policy is defined as follows</p>

<p><span class=""math-container"">$$
\pi_\theta(s, a) \propto e^{\phi(s, a)^T \theta}
$$</span></p>

<p>that is, the probability of an action <span class=""math-container"">$a$</span> (in state <span class=""math-container"">$s$</span>) is proportional to the ""exponentiated weight"".</p>

<p>The score function is then defined as follows</p>

<p><span class=""math-container"">$$
\nabla_\theta \log \pi_\theta (s, a) = \phi(s, a) - 
\mathbb{E}_{\pi_{\theta}}[\phi(s, \cdot)]
$$</span></p>

<p>Where does the expectation term <span class=""math-container"">$\mathbb{E}_{\pi_{\theta}}[\phi(s, \cdot)]$</span> come from?</p>
"
2773,"<p>I need to evaluate the monetary potential of AI at my company for the year 2025 to justify a long term investment in a dedicated AI center of competence. The departments in which AI would be used later on (Purchasing, R&amp;D, Manufacturing, …) are generally very interested and some of them have already built some prototype use-cases. However, they are unwilling to predict and quantify any future potential. The investments in AI can only be justified if we can state the monetary benefit.</p>

<p>Are there any creative approaches out there to estimate the monetary potential of AI?</p>

<p>Any help is greatly appreciated! </p>
"
2774,"<p>I am new to this community, I just have a question : is doing a M.S. in the combination of Natural Language Processing and Machine Learning worth it? What are the opportunities that can be opened by having such a Master's?</p>
"
2775,"<p>How do I check which algorithm solves my problem best?</p>

<p>Given a optimaization problem, I apply different well known optimization algorithms (genetic algorithm, simulated annealing, ant colony etc.) to solve my problem. However, how do I know if my implementation ( e.g. cost function) is working for every case? How can I compare the algorithms or their goodness in the context of my problem?</p>
"
2776,"<p>I am new to this community, I have simple question. Can inputs nodes or output nodes be connected to each other in Neat Algorithm?</p>

<p>Note: Inputs to --> Inputs or Output to --> Outputs</p>
"
2777,"<p>I've seen monte-carlo reward <span class=""math-container"">$G_{t}$</span> used in REINFORCE and TD(0) reward <span class=""math-container"">$r_t + \gamma Q(s', a')$</span> used in vanilla actor-critic. I've never seen someone use lambda reward <span class=""math-container"">$G^{\lambda}_{t}$</span> in these situations, nor in any other algos.</p>

<p>Is there a specific reason for this? Could there be performance improvements if we used <span class=""math-container"">$G^{\lambda}_{t}$</span>?</p>
"
2778,"<p>Suppose there is an evaluation policy called <span class=""math-container"">$\pi_{e}$</span> and there are two behavior policies <span class=""math-container"">$\pi_{b1}$</span> and <span class=""math-container"">$\pi_{b2}$</span>. I know that it is possible to estimate the return of policy <span class=""math-container"">$\pi_{e}$</span> through behavior policies via important sampling which are unbaised. But I do not know about the variance of return estimated through two behavior policies <span class=""math-container"">$\pi_{b1}$</span> and <span class=""math-container"">$\pi_{b2}$</span>. Does anybody know about the variance or any bound on the variance of estimated return?</p>

<p>Let <span class=""math-container"">$G_{0}^{b1}=\sum_{t=1}^{T}\gamma^{t-1}r_{t}^{b1}$</span> represents the total return for an episode through behavior policy <span class=""math-container"">$\pi_{b1}$</span> and <span class=""math-container"">$G_{0}^{b2}=\sum_{t=1}^{T}\gamma^{t-1}r_{t}^{b2}$</span> represents the total return for an episode through behavior policy <span class=""math-container"">$\pi_{b2}$</span>.
It is possible to estimate the return of policy <span class=""math-container"">$\pi_{e}$</span> as follows:</p>

<p><span class=""math-container"">$$G_{0}^{(e,b1)}=\prod_{t=1}^{T}\frac{\pi_{e}(a_{t}|s_{t})}{\pi_{b1}(a_{t}|s_{t})}*G_{0}^{b1}$$</span></p>

<p><span class=""math-container"">$$G_{0}^{(e,b2)}=\prod_{t=1}^{T}\frac{\pi_{e}(a_{t}|s_{t})}{\pi_{b2}(a_{t}|s_{t})}*G_{0}^{b2}$$</span></p>

<p>I want to compare the variance of <span class=""math-container"">$G_{0}^{(e,b1)}$</span> and <span class=""math-container"">$G_{0}^{(e,b2)}$</span>. Is there any formulation to compute the variance <span class=""math-container"">$G_{0}^{(e,b1)}$</span> and <span class=""math-container"">$G_{0}^{(e,b2)}$</span>?</p>

<p>Thanks</p>
"
2779,"<p>Attention has been used widely in recurrent networks to weight feature representations learned by the model. This is not a trivial task since recurrent networks have a <em>hidden state</em> that captures sequence information. The hidden state can be fed into a small MLP that produces a context vector summarizing the salient features of the hidden state.</p>

<p>In the context of NLP, convolutional networks are not as straightforward. They have the notion of <em>channels</em> that are different feature representations of the input, but are channels the equivalent to hidden states? Particularly, this raises two questions for me:</p>

<ul>
<li><p>Why use attention in convolutional networks at all? Convolutions have shown to be adept feature detectors––for example, it is known that higher layers learn small features such as edges while lower layers learn more abstract representations. Would attention be used to sort through and weight these features?</p></li>
<li><p>In practice, how would attention be applied to convolutional networks? The output of these networks is usually <code>(batch, channels, input_size)</code> (at least in PyTorch), so how would the attention operations in recurrent networks be applied to the output of convolutional networks?</p></li>
</ul>

<hr>

<p><strong>References</strong></p>

<p><a href=""https://arxiv.org/pdf/1705.03122.pdf"" rel=""nofollow noreferrer""><em>Convolutional Sequence to Sequence Learning</em></a>, Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, Yann N. Dauphin, 2017</p>
"
2780,"<p>For the purposes of this question I am asking about training the generator, assume that training the discriminator is another topic.</p>

<p>My understanding of generative adversarial networks is that you feed random input data to the generator and it generates images. Out of those images, the ones which the discriminator thinks are real are used to train the generator.</p>

<p>For example, I have the random inputs <span class=""math-container"">$i_1$</span>, <span class=""math-container"">$i_2$</span>, <span class=""math-container"">$i_3$</span>, <span class=""math-container"">$i_4$</span>... from which the generator produces <span class=""math-container"">$o_1$</span>, <span class=""math-container"">$o_2$</span>, <span class=""math-container"">$o_3$</span>, <span class=""math-container"">$o_4$</span>. Say for example, the discriminator thinks that <span class=""math-container"">$o_1$</span> and <span class=""math-container"">$o_2$</span> are real but <span class=""math-container"">$o_3$</span> and <span class=""math-container"">$o_4$</span> are fake, I then throw away input output pairs 3 and 4, but keep 1 and 2, and run back propagation on the generator to tell it that <span class=""math-container"">$i_1$</span> should produce <span class=""math-container"">$o_1$</span>, and <span class=""math-container"">$i_2$</span> should produce <span class=""math-container"">$o_2$</span> since these were ""correct"" according to the discriminator.</p>

<p>The contradiction seems to come from the fact that the generator <em>already</em> generates those outputs from those inputs, so nothing will be gained by running backprop on those input output pairs.</p>

<p>Where is the flaw in my logic here? I seem to have something wrong in my reasoning, or a misunderstanding of how the generator is trained.</p>
"
2781,"<p>Everyone in the field of AI is aware that some of the objectives of AI could pose risks. A few of the risks exceed just car accidents as automated vehicles are beta tested or the replacement of cubicle jobs by processes running in data centers. Various stories, such as <em>Frankenstein</em>, <em>2001: A Space Odyssey</em>, <em>Terminator</em>, and <em>Transcendence</em> have made readers and movie watchers aware of some of the sequences of technological events that could lead to risks of significant magnitude and permanence.</p>

<p>From a risk management point of view, should ethics classes be taught universally? Should legislative steps be taken to require ethical information to accompany all technical presentations of AI approaches, designs, and implementations? Should universities require ethics classes for all high powered technologies such as AI?</p>

<p>Are there classes in AI ethics taught in high schools and universities yet?</p>

<p>If so, where? If not, why not?</p>

<hr>

<p><strong>One Possible Syllabus</strong></p>

<p>In response to the query in the comment, here is one possible AI Ethics course syllabus. University deans could decide whether it should be an academic requirement for 2nd year students enrolled in their school.</p>

<ul>
<li>Brief history of ethics in economics, law, science, and geopolitics</li>
<li>Triumphs in ethics applied to technology in the past</li>
<li>Ways current society is benefiting from past ethical integrity</li>
<li>Past negative effects resulting from ethical negligence</li>
<li>Team project: A corporate AI policy per board of directors request</li>
<li>Distinguishing plausible futures from artifacts of sci fi creativity</li>
<li>Ways of evaluating outcomes</li>
<li>Relationship between creators of a system and the system created</li>
<li>Developing assessments of cost, loss, value, and benefit functions</li>
<li>Essay assignment: Does humanity have a manifest destiny?</li>
<li>Dealing with predictive uncertainty in technological ethics</li>
<li>Legislative and judicial considerations</li>
<li>Considering career options in an ethical context</li>
<li>Final exam and team project due</li>
</ul>
"
2782,"<p>I have a neural network <span class=""math-container"">$F(W,x): \mathbb{R}^d \rightarrow \mathbb{R}^k$</span> with <span class=""math-container"">$L$</span> layers, <span class=""math-container"">$m$</span> neurones per layer, ReLu activation, softmax on the last layer and <span class=""math-container"">$n$</span> datapoint.</p>

<p>My loss function is the classic <span class=""math-container"">$L(W) = \frac{1}{2}\sum_{i \in [n]} || F(W,x_i) - y_i||^2$</span>. </p>

<p>This means that every weight matrix <span class=""math-container"">$W \in \mathbb{R}^{m \times m}$</span>. These matrices are started with random initialisation <span class=""math-container"">$W_{i,j} = \mathcal{N}(0, \frac{2}{m})$</span>. First of all, what's the Frobenius norm of this matrix? </p>

<p>If it was symmetric, Wigner's theory would suggest us that</p>

<p><span class=""math-container"">$$\mathbb{E}||W||_F = 2\sqrt{m}*2/\sqrt{m} = 4$$</span></p>

<p>I'm not sure it is right, but it should. What I need is an answer to the following questions:</p>

<p>Once I start training, how do the weights changes affect the norm of the weight matrices?</p>

<p>What's the expected norm change after every iteration? </p>

<p>What's the expected norm of a well trained network?</p>

<p>(I have never specified if I need the <span class=""math-container"">$||*||_2$</span> norm or the Frobenius one because I actually need both, so whatever answer with one of them is valid :D )</p>

<p>Thank you!</p>
"
2783,"<p>I am working to build a reinforcement agent with DQN. The agent would be able to place buy and sell orders for a day trading purpose. I am facing a little problem with that project. The question is ""how to tell the agent to maximize the profit and avoid the transaction where the profit is less than 100$"".</p>

<p>I want to maximize the profit inside a trading day and avoid to place the pair (limit buy order, limit sell order) if the profit on that transaction is less than 100$. The idea here is to avoid the little noisy movements. Instead, I prefer long beautiful profitable movements. Be aware that I thought using the ""Profit &amp; Loss"" as the reward.</p>

<p>""I want the minimal profit per transaction to be 100$"" ==> It seems this is not something that is enforceable. I can train the agent to maximize profit per transaction, but how that profit is cannot be ensured.</p>

<p>At the beginning, I wanted to tell the agent, if the profit of a transaction is 50 dollars, I will remove 100 dollars, then It becomes a penalty of 50 dollars for the agent. I thought it was a great way to tell the agent to not place a limit buy order if you are not sure it will give us a minimal profit of 100$. It seems that all I would be doing there is simply shifting the value of the reward. The agent only cares about maximizing the sum of rewards and not taking care of individual transactions. </p>

<p>How to tell the agent to maximize the profit and avoid the transaction where the profit is less than 100$? With that strategy, what guarantee that the agent will never make a buy/sell decision that results in less than 100 dollars profit? Does the sum of reward - # transaction * 100 can be a solution?</p>
"
2784,"<p>I have seen a few articles about neural nets. Mostly they went along these lines: we tried these architectures, these meta parameters, we trained it for x hours on y CPUs and it gave us these results that are 0.1% better than state of the art. </p>

<p>What I am interested is whether there exists (at least as a work in progress) a framework, that gives some explanation why is some architecture better than other, what makes one activation function more suitable for image recognition than other, etc. Do you have some tips where to start? I would prefer something more systematic than google search (a book, list of key articles is ideal). </p>
"
2785,"<p>I have an electromagnetic sensor and electromagnetic field emitter.
The sensor will read power from the emitter. I want to predict the position of the sensor using the reading.</p>

<p>Let me simplify the problem, suppose the sensor and the emitter are in 1 dimension world where there are only position X (not X,Y,Z) and the emitter emits power as a function of distance squared.</p>

<p>From the painted image below, you will see that the emitter is drawn as a circle and the sensor is drawn as a cross.</p>

<p><a href=""https://i.stack.imgur.com/kuJR8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kuJR8.png"" alt=""one emitter""></a></p>

<p>E.g. if the sensor is 5 meter away from the emitter, the reading you get on the sensor will be 5^2 = 25. So the correct position will be either 0 or 10, because the emitter is at position 5.</p>

<p>So, with one emitter, I cannot know the exact position of the sensor. I only know that there are 50% chance it's at 0, and 50% chance it's at 10.</p>

<p>So if I have two emitters like the following image:</p>

<p><a href=""https://i.stack.imgur.com/piKWZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/piKWZ.png"" alt=""two emitters""></a></p>

<p>I will get two readings. And I can know exactly where the sensor is. If the reading is 25 and 16, I know the sensor is at 10. </p>

<p>So from this fact, I want to use 2 emitters to locate the sensor.</p>

<p>Now that I've explained you the situation, my problems are like this:</p>

<ol>
<li>The emitter has a more complicated function of the distance. It's
not just distance squared. And it also have noise. so I'm trying to
model it using machine learning.</li>
<li><p>Some of the areas, the emitter don't work so well. E.g. if you are
between 3 to 4 meters away, the emitter will always give you a fixed
reading of 9 instead of going from 9 to 16.</p></li>
<li><p>When I train the machine learning model with 2 inputs, the
prediction is very accurate. E.g. if the input is 25,36 and the
output will be position 0. But it means that after training, I
cannot move the emitters at all. If I move one of the emitters to be
further apart, the prediction will be broken immediately because the
reading will be something like 25,49 when the right emitter moves to
the right 1 meter. And the prediction can be anything because the
model has not seen this input pair before. And I cannot afford to
train the model on all possible distance of the 2 emitters.</p></li>
<li><p>The emitters can be slightly not identical. The difference will
be on the scale. E.g. one of the emitters can be giving 10% bigger
reading. But you can ignore this problem for now.</p></li>
</ol>

<p>My question is <strong>How do I make the model work when the emitters are allowed to move?</strong> Give me some ideas.</p>

<p>Some of my ideas:</p>

<ol>
<li>I think that I have to figure out the position of both
emitters relative to each other dynamically. But after knowing the
position of both emitters, how do I tell that to the model?</li>
<li>I have tried training each emitter separately instead of pairing
them as input. But that means there are many positions that cause
conflict like when you get reading=25, the model will predict the
average of 0 and 10 because both are valid position of reading=25.
You might suggest training to predict distance instead of position,
that's possible if there is no <strong>problem number 2</strong>. But because
there is problem number 2, the prediction between 3 to 4 meters away
will be wrong. The model will get input as 9, and the output will be
the average distance 3.5 meters or somewhere between 3 to 4 meters.</li>
<li>Use the model to predict position
probability density function instead of predicting the position.
E.g. when the reading is 9, the model should predict a uniform
density function from 3 to 4 meters. And then you can combine the 2
density functions from the 2 readings somehow. But I think it's not
going to be that accurate compared to modeling 2 emitters together
because the density function can be quite complicated. We cannot
assume normal distribution or even uniform distribution.</li>
<li>Use some kind of optimizer to predict the position separately for each 
emitter based on the assumption that both predictions must be the same. If 
the predictions are not the same, the optimizer must try to move the 
predictions so that they are exactly at the same point. Maybe reinforcement 
learning where the actions are ""move left"", ""move right"", etc.</li>
</ol>

<p>I told you my ideas so that it might evoke some ideas in you. Because this is already my best but it's not solving the issue elegantly yet.</p>

<p>So ideally, I would want the end-to-end model that are fed 2 readings, and give me position even when the emitters are moved. How would I go about that?</p>

<p>PS. The emitters are only allowed to move before usage. During usage or prediction, the model can assume that the emitter will not be moved anymore. This allows you to have time to run emitters position calibration algorithm before usage. Maybe this will be a helpful thing for you to know.</p>
"
2786,"<p>I have a solid understanding what the numbers are. If I want, I can see numbers in everything. Could an AI have the same ability for any incoming information to tag them by numbers same as I have? </p>
"
2787,"<p>I am working to build an deep reinforcement learning agent which can place orders (i.e. limit buy and limit sell orders). The actions are <code>{""Buy"": 0 , ""Do Nothing"": 1, ""Sell"": 2}</code>. </p>

<p><a href=""https://i.stack.imgur.com/1PlBP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1PlBP.png"" alt=""enter image description here""></a></p>

<p>Suppose that all the features are well suited for this task. I wanted to use just the standard ""Profit &amp; Loss"" as a reward, but I hardly thought to get something similar to the above image. The standard P&amp;L will simply place the pair (limit buy order, limit sell order) on every up movement. I don't want that because very often it won't cover the commission and it is not a good indicator to trade manually. I would be interested that the agent can maximize the profit and give me a minimum profit of $100 on every pair (limit buy order, limit sell order). On the picture, I would be interested in something similar to the above picture.</p>

<p>Is there a reward function that could allow me to get such a result? If so, what is it?</p>

<p><strong>UPDATE</strong></p>

<p>Is the following utility function can work with the purpose of that question?</p>

<p><span class=""math-container"">$$
U(x) = \max(\$100, x)
$$</span></p>

<p>That seems correct, but I don't know how the agent will be penalized if it covers a wrong transaction, i.e. the pair (limit buy order, limit sell order) creates a loss of money.</p>
"
2788,"<p>Are current AI models entirely empiricist?
Can they be rationalist?If so How?</p>
"
2789,"<p>How would I go about designing a (relatively) simple AI that discovers and invents random more complex concepts on its own?</p>

<p>For example, say I had a robot car. It doesn't know it's a car. It has several inputs and outputs, such as a light sensor and the drive motors. If it stays in the dark, it's score drops (bad), and if it moves into the light, it's score rises (good). It'd have to discover that it's motor outputs cause the light input to change (because it's moving closer or farther away from a light source), and that brighter light means higher score.</p>

<p>Of course it'd be easier to design an AI that does specifically that, but I want it's behaviour discovery system to be more generic, if that makes any sense. Like later on, it could discover a way to fight or cooperate with other robots to increase it's score (maybe other robots destroy light sources when they drive over them, and they can be disabled by driving into them), but it'd have to discover this without initially knowing that another robot could possibly exist, how to identify one, what they do, and how to interact with one.</p>

<p>Also, I want it to be creative instead of following a 'do whatever is best to increase your score' rule. Like maybe one day it could decide that cooperating with other robots is another way to increase it's score (it finds out what love is), but if it's unable to do that, it becomes depressed and stops trying to increase it's score and just sits there and dies. Or it could invent any other complete random and possibly useless behaviour.</p>

<p>How hard would it be to make something like this, that essentially builds itself up from a very basic system, provided I give it a lots of different kinds of inputs and outputs that it can discover how to use and apply to its own evolving behaviour?</p>
"
2790,"<p>I want to develop an AI based object (mainly toy) picker that can clean my kid's room and put toys in toy basket.
Can somebody help me how to acheive this?
I want to make a custom solution so that it would be a learning for me.</p>

<p>Thanks!</p>
"
2791,"<p>People often value their pets, livestock, plants, yachts, cars, and plants.</p>

<ul>
<li>Dogs are considered family members by many families.</li>
<li>Cats that might contribute little more than a purr or a look are also.</li>
<li>Horses and people develop a connection and the horse permits riding.</li>
<li>People have favorite trees and keep flower gardens and lawns.</li>
<li>Water vessels, stuffed animals, and favorite livestock are given names.</li>
</ul>

<p>The requirement in society for an organism or inanimate object to be lovable can be quite low. The need for creating artificial pets is high for regions or housing situations were pets can't be.</p>

<p>What makes a dog loving and lovable? What are the actual behavioral characteristics that make it work for so many people, some of whom consider their dog a better friend than their spouse, parents, or work associates?</p>

<ul>
<li>Warmth to the touch (being a mammal)</li>
<li>Fuzziness</li>
<li>Smoothness in contour</li>
<li>Aesthetic design (based mostly on millennia of breeding)</li>
<li>Attentiveness (based mostly on millennia of breeding)</li>
<li>Appreciative response to provision of the most basic things like trivially prepared food, a dish of water, or access to the yard to expel waste</li>
<li>Learned responses to positive and negative verbal expressions and body language</li>
<li>Appreciation of simple gestures, such as petting</li>
<li>Barking when someone is in the yard unexpectedly</li>
<li>Performance of the the most basic tasks, such as a golden retriever on a hunt or merely fetching a stick or a toy</li>
<li>Perhaps attacking an attacker for some of the more fiercely bred breeds</li>
</ul>

<p>What of these is outside the realm of existing AI technology? Perhaps only the last two items, and that limitation may be overcome within the next ten or twenty years.</p>

<p>Although it would be nice to have a robot that cleaned the kitchen, painted the house, drove the car, or mowed the lawn, perhaps love, even if simulated, may be less superficial and more valuable than what makes some pets, plants, water vessels, and stuffed animals so dear.</p>

<blockquote>
  <p>How can some artificial loving pets be designed?</p>
  
  <p>Is there a good architecture or algorithm for lovable artificial pets?</p>
</blockquote>
"
2792,"<p>I'm relatively new to neural networks, and I've been trying to program my own Hopfield network. I got it to the point where it can reliably reproduce a single pattern from a completely scrambled starting state, for up to 400 neurons (potentially more, my computer takes a while for anything bigger than that). When I try it with more than 1 stored pattern however, it appears to settle on a spurious state that looks like some combination (sum?) of them rather than any of them in particular.</p>

<p>My question is whether the method I'm using to generate my patterns, the size of the network, or any conditions in general can cause this to happen. At the moment, I'm using randomly-generated patterns. To test the network, I set it to be identical to one of the patterns, introduce some random noise by flipping a certain number of nodes, and then seeing if it can recreate the pattern. Could anything about that process be causing the network to settle on that combination of states?</p>

<p>Thanks in advance.</p>
"
2793,"<p>Does it help to ""pre-classify"" natural language inputs using labeled input fields? E.g., ""Who,"" ""What,"" ""Where,"" ""When,"" ""Why,"" ""How,"" and ""How much?"" Or is a single, monolithic, free-form, long-text input field equally effective and efficient for model training purposes?</p>

Scenario 1: Without input labels

<blockquote>
  <p>We are three research fellows, Alice, Bob and Charlie at the University of Copenhagen. We want to understand the development of the human visual system. This knowledge will help in the prevention and treatment of certain vision problems in children. Further, the rules that guide development in the visual system can be applied to other systems within the brain. Our work, therefore, has wide application to other developmental disorders affecting the nervous system. We will conduct this research in 2019 under a budget of $15,000.</p>
</blockquote>

Scenario 2: With input lables

<blockquote>
  <p><strong>Who:</strong> We are three research fellows, Alice, Bob and Charlie.</p>
  
  <p><strong>What:</strong> We want to understand the development of the human visual system.</p>
  
  <p><strong>Where:</strong> At the University of Copenhagen.</p>
  
  <p><strong>When:</strong> During the calendar year of 2019.</p>
  
  <p><strong>Why:</strong> This knowledge will help in the prevention and treatment of certain vision problems in children.</p>
  
  <p><strong>How:</strong> Further, the rules that guide development in the visual system can be applied to other systems within the brain.</p>
  
  <p><strong>How Much:</strong> The research will cost $15,000.</p>
</blockquote>

<p>Use Case:</p>

<p>I am building an AI/ML recommendation system. Users subscribe to the system to get recommendations of research projects they might like to participate in or fund. There will be many projects from all over the globe. Far too many for a human to sort through and filter. So AI will sort and filter automatically.</p>

<p>Will pre-classifying input fields using labels help the training algorithm be more efficient or effective? </p>
"
2794,"<p>Consider a SmielApp<sup>1</sup>, pronounced smile-app. It's a proposed app for Android, iOS, LINUX, and other phone, tablet, laptop, and desktop environments.<sup>2</sup> The system requirements are a microphone, a speaker, and a user-facing camera, devices to interact with them in the operating system, and application access to those devices. The simplified data flows are as follows:</p>

<blockquote>
  <p>Camera <span class=""math-container"">$\Rightarrow$</span> Smile detection <span class=""math-container"">$\Rightarrow$</span> NL generator <span class=""math-container"">$\Rightarrow$</span> Text to speech <span class=""math-container"">$\Rightarrow$</span> Speaker</p>
  
  <p>Microphone <span class=""math-container"">$\Rightarrow$</span> wave to spectrum (FFT) <span class=""math-container"">$\Rightarrow$</span> NL generator</p>
  
  <p>Learning updates from server <span class=""math-container"">$\Rightarrow$</span> Detector, generator, or speech</p>
</blockquote>

<p>The user facing camera acquires one frame per second. Smile detection is a CNN system designed and trained via a labelled data set to locate and determine the presence of a smile or scowl and its degree of pronouncement, leading to a number that is zero when the mouth is at rest and expressionless, more negative with greater scowl, and more positive with greater smile up through laughter. That's the ""Smile detection"" above. So far, this seems quite achievable with current technology, especially if interaction between client and server allows parameter updates from learning that occurs on the back end.</p>

<p>The NL generator on the server and accessible from the client produces strings containing natural language using a GAN topology and drawing from an encoding of a 100,000 most common word vocabulary. The first data set for GAN training of word sequence generation is originally a data set of funny and encouraging statements, but new word sequences can be generated by the GAN.</p>

<p>An additional two subsystems of the NL generator need to be a word ordering component that improves the order of words using linguistic heuristics or another deep network, and there must be a phrase mutator perhaps as simple as a regular expression engine so that the text to speech converters get the punctuation and doubling of letters they need to use the intonations needed for natural sounding speech.</p>

<p>The audio input, processed with an FFT and spectral transform and normalized to forms similar to the auditory features of language (pitch, tone, consonant, and volume envelopes) are mapped to the selection of funny or encouraging statements through a DQN designed and configured with a value function tied to the Smile detector. Its actions direct the selection.</p>

<p>The text is converted to speech using Tacotron 2 and presented to the user through the speaker. If the user smiles, the selection is reinforced. If the user scowls, the selection is dissuaded. All associations of text and user affect response is sent to the server to continue to further refine Smile detection, NL generation's GAN input and word ordering heuristics, and speaker selection for text to speech conversion.</p>

<p>That's SmielApp.</p>

<blockquote>
  <p>Is there a flaw in this AI design?</p>
  
  <p>Is there a flaw in the concept for the app?</p>
  
  <p>Is anyone aware of something that takes this non-traditional, non-chat-bot approach to natural language interaction?</p>
  
  <p>Are one of the components above relying on abilities that are beyond the current state of AI technology?</p>
  
  <p>Would an encouraging and possibly humorous app something that would be of benefit users?<sup>3</sup></p>
</blockquote>

<hr>

<p><strong>Footnotes</strong></p>

<p>[1] SmielApp is not a registered trademark and is just an example name for this functionality.</p>

<p>[2] Since content here is <a href=""https://creativecommons.org/licenses/by-sa/4.0/"" rel=""nofollow noreferrer"">Creative Commons Share Alike</a>, this means that the app can be developed for profit provided the attribution returns to this site and this post but no claim to authorship of design can be protected as someone else's intellectual property, which would obviously be inappropriate, unless it has already been independently invented and protected as intellectual property. I'm not aware of any similar app in existence. (If there are none, that means any reader can develop and monetize it.)</p>

<p>[3] This idea came from the idea of actor-critic and the fact that critics are more appreciated by casual movie watchers than actors, who would in most cases want either respectful direction from a qualified Director or encouragement from the fans and from movie popularity upon release. No one picks friends because they are good critics, even though critique may be part of what is said. What friends say is encouraging and perhaps funny, even when it has a corrective element in the intent of the words.</p>
"
2795,"<p>Let's say there's a ball with features position, velocity, acceleration.</p>

<p>These three are all concatenated as inputs to my neural network.</p>

<p>However, I have prior knowledge that position is way more predictive than the other features.</p>

<p>How do I weight the position feature much more strongly than the others? Would just applying a large scalar coefficient to it as preprocessing work? Seems unprincipled...</p>
"
2796,"<p>I was trying to write a simple CNN in keras during a course, and I wrote one that does not learn at all, but I don't understand why.</p>

<p>Don't bother about the coding, first I load two images of a dog and one of a cat :</p>

<pre><code>img1 = np.array(Image.open('dog1.jpg').convert('L'))
img2 = np.array(Image.open('dog2.jpg').convert('L'))
img3 = np.array(Image.open('dog3.jpg').convert('L'))
img4 = np.array(Image.open('cat1.png').convert('L').resize((225,225)))

X = np.array([img1.reshape(1,225,225),img2.reshape(1,225,225),img4.reshape(1,225,225)])
y = np.array([np.array([1,0]),np.array([1,0]),np.array([0,1])])
</code></pre>

<p>then I define the CNN model :</p>

<pre><code>model = Sequential()
model.add(Conv2D(96, (11, 11), input_shape=(1, 225, 225), activation='relu'))
model.add(MaxPooling2D(pool_size=(3, 3),strides=2))
model.add(Flatten())
model.add(Dense(2,activation='relu'))
</code></pre>

<p>and then I train the CNN model :</p>

<pre><code>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X,y,epochs=5)

model.predict(img3.reshape(1,1,225,225))
</code></pre>

<p>I always get the following for the training (even for epochs=100) :</p>

<pre><code>Epoch 1/5
3/3 [==============================] - 2s 778ms/step - loss: 5.3727 - acc: 0.6667
Epoch 2/5
3/3 [==============================] - 2s 645ms/step - loss: 5.3727 - acc: 0.6667
Epoch 3/5
3/3 [==============================] - 2s 649ms/step - loss: 5.3727 - acc: 0.6667
Epoch 4/5
3/3 [==============================] - 2s 640ms/step - loss: 5.3727 - acc: 0.6667
Epoch 5/5
3/3 [==============================] - 2s 777ms/step - loss: 5.3727 - acc: 0.6667
</code></pre>

<p>and for the predict, sometimes I have values (always before the training) but often I have (always after the training) :</p>

<pre><code>array([[nan, nan]], dtype=float32)
</code></pre>

<p>So my questions are : why does my CNN not learn at all ?? And why do I end up with ""nan"" after learning but have none before ? since it doesnt learn apparently, the predict shouldn't change ?</p>
"
2797,"<p>If ""image captioning"" is utilized to make a commercial product, what application fields will need this technique? And what is the level of required performance for this technique to be usable?</p>
"
2798,"<p>This corresponds to Exercise 1.1 of <a href=""http://incompleteideas.net/book/RLbook2018.pdf"" rel=""nofollow noreferrer"">RLBook</a>, and a discussion followed from <a href=""https://ai.stackexchange.com/questions/6669/why-does-self-playing-tictactoe-not-become-perfect/6675?noredirect=1#comment15233_6675"">here</a>. 
Considering two reward schemes- </p>

<ul>
<li>Win = +1, Draw = 0, Loss = -1</li>
<li>Win = +1, Draw or Loss = 0</li>
</ul>

<p>Can we say something about the optimal Q-values?</p>
"
2799,"<p>I am Sharan. I am currently pursuing a mechatronics degree. I am pretty much interested in Artificial intelligence. So please anyone tell me where I have to start and where I have to update. Which type of software is used in this field?</p>
"
2800,"<p>I've got a challenge that feels like it should be solvable using some kind of clustering algo, but I can't get my head around how I can change the perspective such that it is solvable for such an algo. Maybe some of you would like to share some pointers in the right direction. Here it is.
(Also, I've manually written an old-fashioned decision tree kind of thing, but it's horrible.)</p>

<h3>The story</h3>

<p>Take in mind a, say, cookie factories. The factories work at the disposal of stores, and can work harder if stores predicts peek demand.</p>

<p>Here's what the factory offers to the store (data example), and in hind sight, if the offer was picked to actually run:</p>



<pre><code>cost | picked | h0  | h1  | h2  | h3  | h4  | h5  | h6  | ... | h23
-----|--------|-----|-----|-----|-----|-----|-----|-----|-----|-----
0.10 | false  | 0   | 10  | 20  | 20  | 8   | 0   | 0   | ... | 0
0.11 | true   | 0   | 0   | 10  | 20  | 20  | 8   | 0   | ... | 0
0.09 | false  | 0   | 0   | 0   | 10  | 18  | 8   | 0   | ... | 0
0.11 | false  | 0   | 10  | 20  | 20  | 8   | 0   | 0   | ... | 0
0.12 | true   | 0   | 0   | 10  | 20  | 20  | 8   | 0   | ... | 0
0.10 | false  | 0   | 0   | 0   | 10  | 18  | 8   | 0   | ... | 0
0.11 | true   | 0   | 6   | 8   | 8   | 8   | 7   | 0   | ... | 0
0.13 | false  | 0   | 0   | 6   | 8   | 8   | 8   | 7   | ... | 0
0.14 | false  | 0   | 0   | 6   | 7   | 6   | 0   | 0   | ... | 0
0.09 | false  | 8   | 8   | 8   | 8   | 8   | 8   | 8   | ... | 8
</code></pre>

<h3>The challenge</h3>

<p>We want to find out what offers come from the same factory. E.g. given the sample above, records 1, 2, and 3 would come from the same factory, as do 4, 5, and 6; and 7, 8, 9, and 10.</p>

<h3>The rules</h3>

<ul>
<li>No two picked offers can be in the same group.</li>
<li>Groups cannot contain more then a certain known number of offers, say 4.</li>
<li>Duplicates based on volume belong to different groups.</li>
<li>Not every group has to have a picked offer.</li>
</ul>
"
2801,"<p>Actually, I want to make an AI model which tells the seller about the maintaining stock of food items as a parameter of time and eventually learns by itself with the customer buying data </p>

<p>(problem - food items gets spoiled very fast. If we can tell the seller what product and how much he has to keep in stock with respect to time using AI we can decrease the cost of maintenance of food )
what are the steps and model I can use for this problem?</p>
"
2802,"<p><a href=""https://i.stack.imgur.com/thmqC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/thmqC.png"" alt=""enter image description here""></a></p>

<p>They only reference in the paper that the position embeddings are learned, which is different from what was done in ELMo.</p>

<p>ELMo paper - <a href=""https://arxiv.org/pdf/1802.05365.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1802.05365.pdf</a></p>

<p>BERT paper - <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1810.04805.pdf</a></p>
"
2803,"<p>When dealing with continuous action spaces, a common choice when designing a policy in policy gradient methods is to learn mean and variance of actions for a specific state and then simply sample from the normal distribution defined by the learned mean and variance to get an action.</p>

<p>My first question is, is explicit exploration strategy even needed is such cases, because the dose of randomness in actions would come from the sampling itself, on the other hand there could probably be cases where we would be stuck in a local optimum just by sampling.<br>
My second question is, in case that explicit exploration is needed, how would one approach this problem of exploration for this specific setup.</p>
"
2804,"<p>Problem: Maintaining the products is a big task for a retailer. If we can      estimate using AI to predict which products will sell the most, we can maintain sufficient stocks of the product without stocking excess inventory. This should be tracked with respect to time because I want to estimate the purchasing levels with respect to seasons.</p>

<p>We are building a AI model which can self learn with respect to purchasing data and provide an estimate to of which products should be purchased and in what volumes.</p>

<p>What approach should we take? As now we are currently thinking of using LSTM networks. Please give a proper approach and algorithm.</p>
"
2805,"<p>I'm having a hard time understanding how does the size of the hidden state affects GRU. For example in a concrete example lets say I want to lean a GRU to count. I'm gonna feed it fx 3 timestamps the last 3 numbers and expect it to predict the fourth. How do I know which hidden size to chose? Can I see the hidden state size as the network capabilities to encode all the past information in a fix size vector?</p>
"
2806,"<p>The notion of algorithm creation is central in considering the complexity of the human brain as well as the AI.</p>

<p>The ability to create new algorithms can be qualified as algorithmic 
in the context of AI, which is algorithms creating algorithms. </p>

<p>But this is not necessarily the case in the context of human brain context.  (We don't know.)</p>

<ul>
<li>Would the difference in the processes of the creation of new algorithms be an element of proof that human intelligence could not be recreated by a computer?</li>
</ul>
"
2807,"<p>In model-based reinforcement learning algorithms, the model of the environment is constructed to efficiently use samples, models such as Dyna, and Prioritize Sweeping. Moreover, eligibility trace helps the model learns (action) value functions faster. </p>

<p>Can I know if it is possible to combine learning, planning, and eligibility traces in a model to increase its convergence rate? If yes, how it is possible to use eligibility traces in the planning part, like Prioritize Sweeping?</p>
"
2808,"<p>My question is whether <em>Meta-learning</em> and <em>Zero-shot learning</em> are synonymous?</p>

<p>I have seen articles where they seem to imply that they are at least very similar concepts.</p>
"
2809,"<p>In an attempt at designing a neural network more closely modeled by the human brain, I wrote code before doing the reading. The neuron I have modeled operates on the following method.</p>

<ul>
<li>Parameters: potential, threshold, activation.</li>
<li>[activation] = 0.0</li>
<li>Receive inputs, added to [potential].</li>
<li>If ([potential] >= [threshold])

<ul>
<li>[activation] = [potential]</li>
<li>[potential] = 0.0</li>
</ul></li>
<li>Else

<ul>
<li>[potential] *= 0.5</li>
</ul></li>
</ul>

<p>In short, the neuron receives inputs, and decides if it ""fires"" if the threshold is met. If not, the input sum, or potential, decreases. Inputs are applied by adding their values to the input potentials of the input neurons, and connections multiply neuron activation values by weights before applying them to their destination potentials. The only difference between this an a spiking network is the activation model.</p>

<p>I am, however, beginning to learn that Spiking Neural Networks (SNNs), the actual biologically-inspired model, operate quite differently. Forgive me if my understanding is terribly flawed. I seem to have the understanding that signals in these networks are sharp sinusoidal wave-forms with between 100 and 300 ""spikes"" in a subdivision of ""time,"" given for 1 ""second."" These signals are sampled for the ""1 second"" by the neuron, and processed by a differential equation that determines the activation state of the neuron. Synapses seem to function in the same manner -> multiplying the signal by a weight, but increasing or decreasing the period of the graph.</p>

<p>However, I wish to know what form of neuron activation model I created. I have been unable to find papers that describe a method like this.</p>

<p>EDIT. The ""learnable"" parameters of this model are [threshold] of the neuron and [weight] of the connections/synapses.</p>
"
2810,"<p>I have some documents containing some text (machine writing text) that I intend to apply OCR on them in order to extract the text. The problem is that these documents contain a lot of noise but in different ways (some documents have noise in the middle, others in the top...); which means that I can't apply simple thresholding in order to remove the noise (i.e applying simple threshold does not only remove the noise, but it removes some parts of the text). For these reasons, I thought about using AI to do de-noise the documents.<br>
Does anyone know if it is possible to do that with AI or any alternative way?</p>
"
2811,"<p>DeepMind's paper <a href=""https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ"" rel=""nofollow noreferrer"">""Mastering the game of Go without human knowledge""</a> states in its ""Methods"" section on its ""Neural network architecture"" that the output layer of AlphaGo Zero's policy head is ""A fully connected linear layer that outputs a vector of size 19^2+1=362, corresponding to the <em>logit probabilities</em> for all intersections and the pass move"" (emphasis mine).  I am self-trained regarding neural networks, and I have never heard of a ""logit probability"" before this paper.  I have not been able by searching and reading to figure out what it means.  In fact, the <a href=""https://en.wikipedia.org/wiki/Logit"" rel=""nofollow noreferrer"">Wikipedia page on logit</a> seems to make the term a contradiction.  A logit can be converted into a probability using the equation <span class=""math-container"">$p=\frac{e^l}{e^l+1}$</span>, and a probability can be converted into a logit using the equation <span class=""math-container"">$l=\ln{\frac{p}{1-p}}$</span>, so the two cannot be the same.  The <a href=""https://github.com/gcp/leela-zero/blob/master/training/caffe/zero.prototxt"" rel=""nofollow noreferrer"">neural network configuration for Leela Zero</a>, which is supposed to have a nearly identical architecture to that described in the paper, seems to indicate that the fully connected layer described in the above quote needs to be followed with a softmax layer to generate probabilities (though I am absolutely new to Caffe and might not be interpreting the definitions of ""p_ip1"" and ""loss_move"" correctly).  The <a href=""https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0"" rel=""nofollow noreferrer"">AlphaGo Zero cheat sheet</a>, which is otherwise very helpful, simply echoes the phrase ""logit probability"" as though this is a well-known concept.  I have seen several websites that refer to ""logits"" on their own (such as <a href=""https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/"" rel=""nofollow noreferrer"">this one</a>), but this is not enough to satisfy me that ""logit probability"" must mean ""a probability generated by passing a logit vector through the softmax function"".</p>

<p>What is a logit probability?  What sources can I read to help me understand this concept better?</p>
"
2812,"<p>There are many methods and algorithms dealing with planning problems.</p>

<p>If I understand correctly, according to Wikipedia, there are classical planning problems, with:</p>

<ul>
<li>a unique known initial state,</li>
<li>duration-less actions,</li>
<li>deterministic actions,</li>
<li>which can be taken only one at a time,</li>
<li>and a single agent.</li>
</ul>

<p>Classical planning problems can be solved using classical planning algorithms. The STRIPS framework for problem description and solution, using backward chaining) of the GraphPlan algorithm can be mentioned here.</p>

<p>If actions are non-deterministic, according to Wikipedia, we have a Markow Decision Process (MDP), with:</p>

<ul>
<li><p>duration-less actions,</p></li>
<li><p>nondeterministic actions with probabilities,</p></li>
<li><p>full observability, or partial observability for POMDP</p></li>
<li><p>maximization of a reward function,</p></li>
<li><p>and a single agent.</p></li>
</ul>

<p>MDPs are mostly solved by Reinforcement Learning.</p>

<p>Obviously, classical planning problems can also be formulated as MDPs (with state transition probabilities of 1, i.e. deterministic actions), and there are many examples (e.g. some OpenAI Gyms), where these are successfully solved by RL methods.</p>

<p>Two questions: </p>

<ol>
<li><p>Are there some characteristics of a classical planning problem, which makes MDP formulation and Reinforcement Learning a better suiting solution method? Better suiting in the sense that it finds a solution faster or it finds the (near)optimal solution faster.</p></li>
<li><p>How do graph search methods like A* perform with classical planning problems? Does STRIPS with backward chaining or GraphPlan always outperform A*? Outperform in the sense of finding the optimal solution faster.</p></li>
</ol>
"
2813,"<p>It is said, that the essence of <a href=""https://www.springer.com/us/book/9780817639495"" rel=""nofollow noreferrer"">https://www.springer.com/us/book/9780817639495</a> ""Neural Networks and Analog Computation. Beyond the Turing Limit"" is that the continuous/physical/real-valued weights for neural networks can induce super-Turing capabilities. Current digital processors can not implement real-valued neural networks, they can only approximate them. There are very little efforts to build analog classical computers. But it is quite possible that quantum computers will be analogue. So - is there research trend that investigates true real-valued neural networks on analog quantum computers?</p>

<p>Google is of no use for my efforts, because it does not understand the true meaning of ""true real-valued neural network"", it gives just real-value vs complex valued neural networks articles, which are not relevant to my question.</p>
"
2814,"<p>Sorry if my question is at the wrong place, I'm new in this community.</p>

<p>So, I have dataset with total of 1 million images (augmented) that separated in 28 classes. I followed this tutorial <a href=""https://www.tensorflow.org/hub/tutorials/image_retraining"" rel=""nofollow noreferrer"">https://www.tensorflow.org/hub/tutorials/image_retraining</a> to do transfer learning using Inception V3 in Tensorflow to create my own model. But I have no strong background in ML or DL so I'm not sure how to tune the parameter correctly for training step. </p>

<p>This is the training source code that I'm using : <a href=""https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py</a> <br>
Using the default setting I was able to get 80~84% accuracy with 16.000 steps. I've tried to change the training, validation, test ratio, training_steps, batch. But, still the accuracy is below 90%. So, Im seeking for advice which parameter that should I change to achieve good accuracy.</p>

<p>Parameter:</p>

<ul>
<li>training_steps</li>
<li>learning_rate</li>
<li>testing_percentage</li>
<li>validation_percentage</li>
<li>eval_step_interval</li>
<li>train_batch_size</li>
<li>test_batch_size</li>
<li>validation_batch_size</li>
</ul>

<p>Thank you</p>
"
2815,"<p>Imagine that I have an artificial neural network with a single hidden layer and that I am using ReLU as my activating function.
If by change I initialize my bias and my weights in such a form that:
<span class=""math-container"">$$
X * W + B &lt; 0
$$</span>
for every input <strong>x</strong> in <strong>X</strong> then the partial derivate of the Loss function with respect to W will always be 0! </p>

<p>In a setup like the above where the derivate is 0 is it true that an NN won´t learn anything? </p>

<p>If true (the NN won´t learn anything) can I also assume that once the gradient reaches the value 0 for a given weight, that weight won´t ever be updated?</p>
"
2816,"<p>Unfortunately there is no <code>speech-recognition</code> or <code>speech-to-text</code> tag yet so I go with the <code>voice-recognition</code>.</p>

<p>My question concerns various datasets for automated speech recognition and how training and test split should be generated. Even more specific, I'd like to know how important it is to separate speakers from the training, test and dev split.</p>

<p>In very imbalanced datasets one might have very few speaker which contribute a lot to the corpus and thus the model may be biased towards those speakers and have generalized less. Is this concern legitimate?</p>
"
2817,"<p>I am curious if it is possible to do so.
For example,</p>

<p>If I supply <code>[0,1,2,3,4,5]</code> the model should return <code>natural number sequence</code></p>

<p><code>[1,3,5,7,9,11]</code> should return <code>natural number with step of 2</code></p>

<p><code>[1,1,2,3,5]</code> should return <code>Fibonacci Number</code></p>

<p><code>[1,4,9,16,25]</code> should return <code>Square natural number</code></p>

<p>and so on.</p>
"
2818,"<p>I am trying to write an AI to a game, where there is no real adversary. This means, that only the AI player has choices in which move to perform, his opponent may or may not react to the move the AI player made, but when he reacts, he will always do the one and only single move that he is able to do. The goal of this AI would be, to find a solution to the situation, which results in the least amount of monster activations.</p>

<p>To explain this a bit further, I will describe the game in a few words: there is a 3x3 board, on which there are some monsters. These monsters has a prewritten AI, and activate based on prewritten rules, ie, they do not have to make any decision at all. This is done, by an enrage mechanic, meaning, that when a monster hits it's enrage limit, it activates, and performs his single move action.</p>

<p>The AI should control the other side of this board, the hero players. Each hero player has a different number of possible moves, each move dealing an amount of damage to the monsters, and increasing it's enrage value, thus getting him closer to his enrage limit.</p>

<p>What I want to achieve, is to write an AI, that will perform this fight in the least amount of monster activations as possible.</p>

<p>For now, I've written a minimax algorithm for this, without the <code>min</code> player. I've done this, by calculating the negative effect of the monsters move, in the maximizing and only players move.</p>

<p>The AI works in the following way: he draws the game tree for a set amount of depth of moves, calculates the bottom move with a heuristic function, selects the highest value from the given depth, and returns the value of this function up one level, then repeat. When he reaches the top of the tree, he performs the move, with the highest quantification value.</p>

<p>This works, somewhat, but I have a big problem: <strong>As there is no randomness in the game, I was expecting that the greater the depth that he can search forward, the better moves he will find, but this is not always the case, sometimes a greater depth, returns a worse solution then a smaller depth</strong></p>

<p>My questions are as follows:</p>

<ul>
<li>what could cause the above error? My quantification function? The weights that I use in the function? Or something else?</li>
<li>is <code>minimax</code> the correct algorithm to use, for a game where there is no real adversarry, or is there any algorithm that will perform better for a game like this?</li>
</ul>
"
2819,"<p>Typical Feed Forward Neural Networks require a fixed sized input and output. So when you have variable sized input, it seems to be common practice to pad the input with zero vectors.</p>

<p>Why does it not seem to be common practice to have a ""is_padding"" attribute?  That way the network can easily distinguish between padding and actual data?  Especially considering input is commonly centered around 0 by subtracting the mean and using unit variance.</p>
"
2820,"<p>I do understand that there are plenty of mobile apps available for body measurement (e.g. MTailor) or creating 3D model (3dlook).</p>

<p>What I would like to find out is how we can use deep learning to achieve the accurate body measurement/3D model with just smart phone camera?</p>

<p>For example, MTailor can predict one's body measurement quite accurately given the cameara angle/camera distance from the human and human height. Can we do the same using deep learning with some labeled images to achieve the same accurate body measurement prediction?</p>

<p>Thanks</p>

<p>Regards,
Han</p>
"
2821,"<p>I'm quite new to the field of computer vision and was wondering what are the purposes of having the boundary boxes in object detection.
Obviously, it shows where the detected object is, and using a classifier can only classify one object per image but my question is that </p>

<p>1) If I don't need to know 'where' an object is (or objects are) and just interested in the existence of them and how many there are, is it possible to just get rid of the boundary boxes?</p>

<p>2) If not, how does bounding boxes help detect objects? From what I have figured is that a network (if using neural network architectures) predicts the coordinates of the bounding boxes if there is something in the feature map. Doesn't this mean that the detector already knows where the object is (at least briefly)? So continuing from question 1, if I'm not interested in the exact location, would training for bounding boxes be irrelevant?</p>

<p>3) Finally, in architectures like YOLO, it seems that they predict the probability of each class on each grid (e.g. 7 x 7 for YOLO v1). What would be the purpose of bounding boxes in this architecture other than that it shows exactly where the object is? Obviously, the class has already been predicted so I'm guessing that it doesn't help classify better.</p>
"
2822,"<p>I'm studying <em>Artificial Intelligence. A Modern Approach</em>, Stuart Russell, Peter Norvig, specifically about search and planning arguments.</p>

<p>I don't understand the difference between the two terms. I was more confused when I saw that some search problems can be determined in planning way. My professor explained to me in a confusing way that the real difference on the search is that it uses an heuristic function, but my book says that planning use a heuristic too, for relaxing problem (in cap. 10.2.3).</p>

<p>I read <a href=""https://stackoverflow.com/questions/10282476/what-is-the-difference-between-search-and-planning"">this page</a> that says in a certain way what I'm saying.</p>

<p>Is planning and search the problem? If not, what are the differences and how are these problems related?</p>
"
2823,"<p>Let's propose, that I can define the state of a board in a board game, with 234 neurons. In theory, could I be able to train a neural network, with 468 inputs (two game boards), and 1 output, to tell me which board state is 'better'? The output should give me ~-1 if the second board is better than the first, ~0 if they are equal, and ~1 if the first board is better than the second.</p>

<p>If yes, what could be the number of ideal neurons on the hidden layers? What could be the ideal number of hidden layers?</p>
"
2824,"<p>Currently, we can build the <a href=""https://en.wikipedia.org/wiki/Artificial_intelligence"" rel=""nofollow noreferrer"">Artificial Intelligence (AI)</a> approaches that respectively explain their actions within the use of goal trees [1]. By moving up and down across the tree, it keeps tracking the last and next movements. Therefore, giving the ability to the machine for ""explain"" the actions.</p>

<p>Explainability regarding human levels, requires some cognitive effort, such as self-awareness, memory retrieval, a theory of mind and so on [2]. Humans are adept at selecting several causes from an infinite number of causes to be the explanation. However, this selection is influenced by certain cognitive biases. The idea of explanation selection is not new in <a href=""https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence"" rel=""nofollow noreferrer"">eXplainable Artificial Intelligence (XAI)</a> [3, 4]. But, as far as we are aware, there are currently no studies that look at the cognitive biases of humans as a way to select explanations from a set of causes.</p>

<p>Despite a clear definition and description of the <a href=""https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence"" rel=""nofollow noreferrer"">XAI</a> field, several questions remain present. The issues are summarized in just one sentence and listed as follows.</p>

<p>That said, our question is:</p>

<blockquote>
  <p>How can we create and build <a href=""https://en.wikipedia.org/wiki/Explainable_Artificial_Intelligence"" rel=""nofollow noreferrer"">XAI</a>?</p>
</blockquote>

<h1>References</h1>

<p>[1] Hadoux, Emmanuel, and Anthony Hunter. ""Strategic Sequences of Arguments for Persuasion Using Decision Trees."" AAAI. 2017.</p>

<p>[2] Miller, T., 2018. Explanation in artificial intelligence: Insights from the social sciences. Artificial Intelligence.</p>

<p>[3] Gunning, D., 2017. Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web.</p>

<p>[4] Samek, W., Wiegand, T. and Müller, K.R., 2017. Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. arXiv preprint arXiv:1708.08296.</p>
"
2825,"<p>I am a member of a robotics team that is measuring the amount of reflected IR light to determine the lightness/darkness of a given material. We eventually hope to be able to use this to follow a line using a pre-set algorithm, but the first step is determining whether the material is one of the binary options: light or dark.  </p>

<p>Given a large population of values between 0 and 1023, probably in two distinct groupings, how can I best go about classifying a given point as light or dark?</p>
"
2826,"<p><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noreferrer"">Pytorch's example for the REINFORCE algorithm</a> for reinforcement learning has the following code:</p>

<pre><code>import argparse
import gym
import numpy as np
from itertools import count

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical


parser = argparse.ArgumentParser(description='PyTorch REINFORCE example')
parser.add_argument('--gamma', type=float, default=0.99, metavar='G',
                    help='discount factor (default: 0.99)')
parser.add_argument('--seed', type=int, default=543, metavar='N',
                    help='random seed (default: 543)')
parser.add_argument('--render', action='store_true',
                    help='render the environment')
parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                    help='interval between training status logs (default: 10)')
args = parser.parse_args()


env = gym.make('CartPole-v0')
env.seed(args.seed)
torch.manual_seed(args.seed)


class Policy(nn.Module):
    def __init__(self):
        super(Policy, self).__init__()
        self.affine1 = nn.Linear(4, 128)
        self.affine2 = nn.Linear(128, 2)

        self.saved_log_probs = []
        self.rewards = []

    def forward(self, x):
        x = F.relu(self.affine1(x))
        action_scores = self.affine2(x)
        return F.softmax(action_scores, dim=1)


policy = Policy()
optimizer = optim.Adam(policy.parameters(), lr=1e-2)
eps = np.finfo(np.float32).eps.item()


def select_action(state):
    state = torch.from_numpy(state).float().unsqueeze(0)
    probs = policy(state)
    m = Categorical(probs)
    action = m.sample()
    policy.saved_log_probs.append(m.log_prob(action))
    return action.item()


def finish_episode():
    R = 0
    policy_loss = []
    rewards = []
    for r in policy.rewards[::-1]:
        R = r + args.gamma * R
        rewards.insert(0, R)
    rewards = torch.tensor(rewards)
    rewards = (rewards - rewards.mean()) / (rewards.std() + eps)
    for log_prob, reward in zip(policy.saved_log_probs, rewards):
        policy_loss.append(-log_prob * reward)
    optimizer.zero_grad()
    policy_loss = torch.cat(policy_loss).sum()
    policy_loss.backward()
    optimizer.step()
    del policy.rewards[:]
    del policy.saved_log_probs[:]


def main():
    running_reward = 10
    for i_episode in count(1):
        state = env.reset()
        for t in range(10000):  # Don't infinite loop while learning
            action = select_action(state)
            state, reward, done, _ = env.step(action)
            if args.render:
                env.render()
            policy.rewards.append(reward)
            if done:
                break

        running_reward = running_reward * 0.99 + t * 0.01
        finish_episode()
        if i_episode % args.log_interval == 0:
            print('Episode {}\tLast length: {:5d}\tAverage length: {:.2f}'.format(
                i_episode, t, running_reward))
        if running_reward &gt; env.spec.reward_threshold:
            print(""Solved! Running reward is now {} and ""
                  ""the last episode runs to {} time steps!"".format(running_reward, t))
            break


if __name__ == '__main__':
main()
</code></pre>

<p>I am interested in the function <code>finish_episode()</code>:</p>

<p>the line</p>

<pre><code> rewards = (rewards - rewards.mean()) / (rewards.std() + eps)
</code></pre>

<p>makes no sense to me.</p>

<p>I thought this might be <a href=""https://www.quora.com/Why-can-reinforcement-of-the-baseline-reduce-variance"" rel=""nofollow noreferrer"">baseline reduction</a>, but I can't see why divide by the standard deviation.</p>

<p>If it isn't baseline reduction, then why normalize the rewards, and where should the baseline reduction go?</p>

<p>Please explain that line</p>
"
2827,"<p>In <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noreferrer"">Open AI's actor-critic</a> and in <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"" rel=""nofollow noreferrer"">Open AI's REINFORCE</a>, the rewards are being normalized like so</p>

<pre><code>rewards = (rewards - rewards.mean()) / (rewards.std() + eps)
</code></pre>

<p><strong>ON EVERY EPISODE INDIVIDUALLY</strong></p>

<p>This is probably <a href=""https://www.quora.com/Why-can-reinforcement-of-the-baseline-reduce-variance"" rel=""nofollow noreferrer"">baseline reduction</a>, but <a href=""https://stats.stackexchange.com/questions/383143/pytorch-reinforce-code-question"">I'm not entirely sure why they divide by std()</a>. </p>

<p>Assuming this is baseline reduction, <strong>please explain why is this done PER EPISODE?</strong></p>

<p>What if one episode yields rewards in the (absolute, not normalized) range of <code>[0,1]</code>, and the next episode yields rewards in the range of <code>[100,200]</code>?</p>

<p>This method seems to ignore the absolute difference between the episodes' rewards</p>

<p>So again, the question:</p>

<ol>
<li><p>Why is the baseline reduction done PER EPISODE INDIVIDUALLY?</p></li>
<li><p>Why do they divide by std()?</p></li>
</ol>

<hr>

<p>Edit: NOT A DUPLICATE:
This question mainly asks about the individual calculation of reward in every episode.
I referenced the offered duplicate myself, as I am the author of it as well, and I found that the answers may be related. However, they are very much NOT the same quesion.</p>
"
2828,"<p>Following <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noreferrer"">Pytorch's actor critic</a>, I understand that the critic is a function mapping from the <strong>state</strong> space to the reward space, meaning, the critic approximates the <strong>state-value funcion</strong>.</p>

<p>However, according to <a href=""https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwia87rurYngAhUH36QKHd5QDCIQFjAAegQIChAC&amp;url=https%3A%2F%2Farxiv.org%2Fpdf%2F1512.07679&amp;usg=AOvVaw23yfpdaGmo9gs3_AIz8edQ"" rel=""nofollow noreferrer"">This paper</a> (you don't need to read it, just a glance at the nice picture at page 2 is enough), the critic is a function mapping from the <strong>action</strong> space to the reward, meaning it approximates the <strong>action value funcion</strong></p>

<p>I am confused.</p>

<p>When people say ""actor critic"" - what do they mean by ""critic""?</p>

<p>Is the term ""Critic"" ambiguous in RL?</p>
"
2829,"<p>If one examines SSD: Single Shot MultiBox Detector code from <a href=""https://github.com/weiliu89/caffe/tree/ssd"" rel=""nofollow noreferrer"">GitHub repository</a>, it can be seen that, for a testing phase (evaluating network on test data set), there is a parameter <code>test batch size</code>. It is not mentioned in the paper.</p>

<p>I am not familiar with using batches during  network evaluation. Can someone explain what is the reason behind using it and what are advantages and disadvantages?</p>
"
2830,"<p>I am in the process of implementing the DQN model from scratch in PyTorch with the target environment of Atari Pong. After a while of tweaking hyper-parameters, I cannot seem to get the model to achieve the performance that is reported in most publications (~ +21 reward; meaning that the agent wins almost every volley). </p>

<p>My most recent results are shown in the following figure. Note that the x axis is episodes (full games to 21), but the total training iterations is ~6.7 million.</p>

<p><a href=""https://i.stack.imgur.com/3TAEJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3TAEJ.png"" alt=""enter image description here""></a></p>

<p>The specifics of my setup are as follows:</p>

<h3>Model</h3>

<pre><code>class DQN(nn.Module):
    def __init__(self, in_channels, outputs):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(in_features=64*7*7 , out_features=512)
        self.fc2 = nn.Linear(in_features=512, out_features=outputs)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(-1, 64 * 7 * 7)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x    # return Q values of each action
</code></pre>

<h3>Hyperparameters</h3>

<ul>
<li>batch size: 32</li>
<li>replay memory size: 100000</li>
<li>initial epsilon: 1.0</li>
<li>epsilon anneals linearly to 0.02 over 100000 steps</li>
<li>random warmstart episodes: ~50000</li>
<li>update target model every: 1000 steps</li>
<li>optimizer = optim.RMSprop(policy_net.parameters(),  lr=0.0025, alpha=0.9, eps=1e-02, momentum=0.0)</li>
</ul>

<h3>Additional info</h3>

<ul>
<li>OpenAI gym Pong-v0 environment</li>
<li>Feeding model stacks of 4 last observed frames, scaled and cropped to 84x84 such that only the ""playing area"" is visible.</li>
<li>Treat losing a volley (end-of-life) as a terminal state in the replay buffer.</li>
<li>Using <a href=""https://pytorch.org/docs/stable/nn.html#smooth-l1-loss"" rel=""nofollow noreferrer"">smooth_l1_loss</a>, which acts as Huber loss</li>
<li>Clipping gradients between -1 and 1 before optimizing</li>
<li>I offset the beginning of each episode with 4-30 no-op steps as the papers suggest</li>
</ul>

<p>Has anyone had a similar experience of getting stuck around 6 - 9 average reward per episode like this? </p>

<p>Any suggestions for changes to hyperparameters or algorithmic nuances would be greatly appreciated!</p>
"
2831,"<p>Is this approach nonsense since reason produces algorithms as a method of solving problems. The algorithm is a sequence of events that perform a task till completion of the problem. Reason is intelligence and algorithms do not substitute reason or create it.</p>
"
2832,"<p>I am trying to make a personal ML project where my objective is using a photo from an invoice, for instance, a Walmart invoice, classify it as being a Walmart invoice and extract the total amount spent. I would then save this information in a relational database and infer some statistics about my spendings. The goal would be to classify invoices not only from Walmart but from the most frequent shops where I spend money and then extract the total amount spent. I already do this process manually, I insert my spendings in a relational database. I have a bunch of photos from different invoices that I have recorded over the past year for this purpose (training a model).</p>

<p>What algorithms would you guys recommend? From my point of view, I think that I need some natural language processing to extract the total amount spent and maybe a convolutional neural network to classify the invoice as being from a specific store?</p>

<p>Thanks!</p>
"
2833,"<p>I am using <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noreferrer"">Open AI's</a> code to do a RL task on an environment that I built myself.</p>

<p>I tried some network architectures, and they all converge, faster or slower on CartPole.</p>

<p>On my environment, the reward seems not to converge, and keeps flickering forever.</p>

<p>I suspect the neural network is too small, but I want to confirm my belief before going the route of researching the architecture.</p>

<p>How can I confirm that the architecture is the problem and not anything else in a neural network reinforcement learning task?</p>
"
2834,"<p>The paper states that <em>every potential function</em> won't alter the optimal policy. I lack of understanding why is that.</p>

<p>The definition:</p>

<p><span class=""math-container"">$$R' = R + F$$</span> with <span class=""math-container"">$$F = \gamma\Phi(s') - \Phi(s)$$</span></p>

<p>where <span class=""math-container"">$\gamma$</span> may be <span class=""math-container"">$0.9$</span></p>

<p>If I have the following setup:</p>

<ul>
<li>on the left is my <span class=""math-container"">$R$</span>.</li>
<li>on the right my potential function <span class=""math-container"">$\Phi(s)$</span></li>
<li>the top left is the start state, the top right is the goal state</li>
</ul>

<p><a href=""https://i.stack.imgur.com/WFhWc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WFhWc.png"" alt=""enter image description here""></a></p>

<p>The reward for the red route is: <span class=""math-container"">$(0 + 0.9 * 100 - 0) + (1 + 0.9 * 0 - 100) = -9$</span>.</p>

<p>And the reward for the blue route is: <span class=""math-container"">$(-1 + 0) + (1 + 0) = 0$</span>.</p>

<p>So, for me, it seems like the blue route is better than the optimal red route and thus the optimal policy changed. Do I have erroneous thoughts here?</p>
"
2835,"<p>I’m looking for advice regarding my ML project.</p>

<p>Using a special wristband, I am able to collect a bunch of physiological data from human subjects. I want to develop an application to recognize when these physiological signals change in a meaningful way and only then ask the user how he/she is feeling. This data will later be used for machine learning testing. The problem is, that I am struggling to find appropriate ways to classify current data input as meaningful and ask for information only when relevant user input is to be gathered, not more and not less.</p>

<p>For me, this seems to be a <a href=""https://en.wikipedia.org/wiki/Novelty_detection"" rel=""nofollow noreferrer"">novelty detection</a> problem, combined with a <a href=""https://en.wikipedia.org/wiki/Binary_classification"" rel=""nofollow noreferrer"">binary classification</a> problem. I have to recognize what values coming from the data stream are to be considered <em>normal</em>, and therefore not bother the user with unnecessary input requests. I would also use novelty detection to recognize the data coming out of the normal zone and ask the user about it. This new data is then not considered novelty anymore, and binary classification will tell if the user is to be asked about his emotions when getting the same data in the future.</p>

<p>So, these are my questions:<br>
-  What do you think about my reasoning of the problem? Do you have other perspectives on how to handle these problems? I have been told this could also be considered an <a href=""https://en.wikipedia.org/wiki/Anomaly_detection"" rel=""nofollow noreferrer"">anomaly detection</a> problem, for example.</p>

<p>-  What algorithms would you use to separate normal from more meaningful physiological data? <a href=""https://en.wikipedia.org/wiki/Support-vector_machine"" rel=""nofollow noreferrer"">Support Vector Machines</a> perhaps? Maybe some <a href=""https://en.wikipedia.org/wiki/Decision_theory"" rel=""nofollow noreferrer"">decision theory</a>?</p>

<p>-  Do you know any books or papers on similar matters? Even if I have found some after hours and hours of research, you may be able to point me to something different than those I have.</p>

<p>It is worth noting that data collection is supposed to be done when no other factors are messing with signal readings, such as sport.</p>

<p>Any help would be much appreciated.</p>

<p>Best regards,<br>
Augusto</p>
"
2836,"<p>I'm running Kera's <code>LSTM</code> (not <code>CuDNNLSTM</code>) but I notice my GPU is under load. I need recurrent dropout, so I can only stick with <code>LSTM</code>. Is the 'normal' <code>LSTM</code> assisted by GPU? If so, how are <code>LSTM</code> and <code>CuDNNLSTM</code> different? I presume <code>CuDNNLSTM</code> uses the CUDNN API (and <code>LSTM</code> doesn't?</p>

<p>Similarly, is the normal <code>LSTM</code> supposed to be faster on GPU or CPU?</p>
"
2837,"<p>Inputs:</p>

<ul>
<li>Time series of spectra representing human speech</li>
<li>Semantic network (as a directed graph) associations</li>
</ul>

<p>Outputs:</p>

<ul>
<li>Modified version of the semantic network input</li>
</ul>

<p>Edge types in the graph (semantic, not neural, network connections):</p>

<ul>
<li>Adjacency</li>
<li>Analogy of</li>
<li>Generalization</li>
<li>Example of</li>
<li>Composed of</li>
</ul>

<p>Ideally, each edge would have the following two numeric attributes:</p>

<ul>
<li>Median recency</li>
<li>Probability or strength</li>
</ul>

<p>Characteristics of the permissible network topology:</p>

<ul>
<li>Multiple edges types connecting the same two vertices</li>
<li>Cycles</li>
</ul>

<p>This goes beyond parsing to the modification of semantic models in ways that simulate listening to language.</p>
"
2838,"<p>In an MLP with ReLU activation functions after each hidden layer (except the final),</p>

<p>Let's say the final layer should output positive and negative values.</p>

<p>With ReLU intermediary activations, this is still possible because the final layer, despite taking in positive inputs only, can combine them to be negative.</p>

<p>However, would using leaky ReLU allow faster convergence? Because you can pass in negative values as input to the final layer instead of waiting till the final layer to make things negative</p>
"
2839,"<p>I'm confused about these two, Is there any difference between them?
and how can I learn more about them?</p>
"
2840,"<p><strong>This question is hardly broad. It specifically asks whether a very specific device is possible and finishes with a specific question of what the reward function might look like.  This site lacks questions of this nature, so its novelty should not be a quality upon which it is dismissed, especially on the fabricated basis of broadness.</strong></p>

<hr>

<p>Platonic solids are regular, convex, and equilateral polyhedrons with congruent faces and a congruent number of edges meeting at the vertices.</p>

<p>We can see them in toys; the six faces of the cube introduces most children to platonic solids as building blocks, without ever telling them that their blocks are cubes or platonic solids.</p>

<p>We can see them as plastic calendars; the dodecahedron has twelve faces, one for each month.</p>

<p>We don't see them in nuclear detonation designs, at least not most of us; the twenty faces of the icosahedron can correspond to the wave fronts of the twenty simultaneous detonations that must converge to a point to force the purified U, Pu, or H across the threshold of the density-temperature surface to self-sustain its reaction.</p>

<p>The tetrahedron and octahedron, although interesting, are not seen much, since they do not work as cleanly in CAD/CAM scenarios.</p>

<p><a href=""https://i.stack.imgur.com/qtvfa.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qtvfa.gif"" alt=""enter image description here""></a></p>

<p>Applying this geometric abstraction to robotics, one of the most nimble and efficient walkers and workers in 3-D space are spiders<sup>2</sup>, and the freedom of motion in their legs are not fully in the platonic paradigm, but could be as follows.</p>

<p><strong>Sun Loving Platonic Robots</strong></p>

<p>Imagine an octahedral robot with photovoltaic (PV) cells centered on each of its eight faces and a leg at each of its six vertices. The six legs have hip joints with robotic control over bend and rotation. (Bend is the angle between the direction the vertex points and the direction of the base of the leg.)</p>

<p>There is a knee part of the way down each leg, the angle of which can be controlled. At the end of each leg is a wheel with a break and rubber tire. There is no reason two power the wheel because the robot can run with the breaks on and then roll with them off and the wheels positioned in the direction of momentum. Alternatively, the entire robot can roll like a ball in a spherical wave fashion. Its platonic symmetry facilitates simulation of a ball that has a moving bump positioned to use gravitational force to produce the desired mobility accelerations.</p>

<p>The robot's only interest is maximally sustaining its electrical charge. All the systems and the AI that drives them are designed to learn one thing: Find light and stay in it to sustain charge. If it discharges to much too often, its Li battery will eventually not hold a charge, it becomes decrepit, and dies, until someone replaces its battery at which time it is reborn with all the knowledge of its previous life.</p>

<p>This is a really awesome robot in many ways.</p>

<ul>
<li>Mathematical symmetry</li>
<li>Walking, rolling, coasting, and climbing possibilities</li>
<li>Energy thrift</li>
<li>Potential extension of the design to produce utility</li>
<li>The PV power levels comprise eight-pixel omnidirectional vision</li>
<li>Simplicity of purpose: to maximally sustain its electrical charge</li>
<li>Potential for locking legs with others of its kind</li>
</ul>

<p>This last one allows the robots to discover that collaborative behavior can be used to stack and make larger machines to open doors and get outside when the sun is out. That's important because these really awesome robots likes the sun, for obvious reasons. In fact, increasing photovoltaic energy and the prospect of it is the only personal value of each robot and potentially the only social one.</p>

<p><a href=""https://i.stack.imgur.com/7Ds2E.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Ds2E.jpg"" alt=""enter image description here""></a></p>

<p>Possible extensions of the design to produce utility might include providing a power connection so that if the battery is charged and the risk of discharge is low, the robot can offload some of its energy to charge other devices. With a sufficient robot population, they can significantly feed a power grid.</p>

<p>This is also a great AI research platform. The symmetry reduces the cost of manufacture since many parts are in common between the eight faces and many others are in common between the six legs. The symmetry also reduces the complexity of control system hardware and software and therefore the AI system complexity.</p>

<p>The single mindedness, the self sufficiency, the potential for social collaboration and specialization, and other features lend themselves to a plethora of AI research objectives that have both commercial and scientific value.</p>

<p><strong>AI Design</strong></p>

<p>We have eight pixels that also provide power, so their light detection and their charge potential are coincident. We have six legs, each of which have four freedoms of motion.</p>

<ul>
<li>Hip bend</li>
<li>Hip rotation</li>
<li>Knee bend</li>
<li>Break pressure</li>
</ul>

<p>Radial and linear control is through sequences of cubic splines of the form</p>

<p><span class=""math-container"">$$ \lambda = \lambda_{\emptyset} + v_i t + a_i t^2 + b_i t^3 \\
   s_i \leq t_i &lt; u_i \; \text{,}$$</span></p>

<p>where <span class=""math-container"">$i$</span> is the spline index in the stream of splines and <span class=""math-container"">$s$</span> and <span class=""math-container"">$u$</span> are the limits of time for each. The coefficients must be calculated so that the concatenation of any two adjacent splines in the sequence produce both continuity and smoothness at the intersection.</p>

<p>Two hyper-parameters, maximum absolute velocity and maximum absolute acceleration, for each of the four freedoms of motion cannot be exceeded. The motor control system handles the execution of the sequences from a queue, so the AI merely has to work to keep the queue from running to empty.</p>

<p>In addition to the eight light intensity inputs measured at the photovoltaic cells, there are the battery condition indicators, which consists of voltage and inferred internal resistance. There is also indication of the time before the queue empties reported back from each motor control interface, along with a positioning fault indication for each.</p>

<p>This means there are 6 x 4 x 4 = 96 numerical outputs and 8 + 2 + 2 x 96 = 202 numerical inputs, but there is so much symmetry, there may be many valid ways to exploit symmetry as AlphaZero did with Go, such that learning and continuous control is resource thrifty.</p>

<blockquote>
  <p>What's the AI design that goes in between to produce the walking, energy acquisition planning, and coasting to conserve energy, and what would the reward function look like? <sup>1</sup></p>
</blockquote>

<hr>

<p><strong>Footnotes</strong></p>

<p>[1] If these questions were answered, future enhancements to the design may lead to collaborative behavior, climbing, and secondary goals like helping others of its kind to avoid discharge or providing extra energy to other systems or robot species.</p>

<p>[2] A picture of a web showing the nimble mid-air construction capabilities of spiders.</p>

<p><a href=""https://i.stack.imgur.com/YN8uk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YN8uk.png"" alt=""enter image description here""></a></p>
"
2841,"<p>In my input tensor, I would like to use both integer values as well as booleans. For example, if there is a spelling difference between 2 texts, I want to set the value to true, and otherwise false. In the same tensor, I would like to assign a value to, for example, the maximum number of consecutive messages, which will be an integer. </p>

<p>Am I allowed to use 0's and 1's for the booleans together with integers, or will it have any negative impact on the working of the network? The ANN wont see any difference between the binary and nonbinary values, but is it a problem?</p>
"
2842,"<p>Say I want to train a NN that generates outputs of some sort (say, even numbers).
Note that the network does not classify outputs, but, rather GENERATES the output.</p>

<p>I want let it run forward and generate some number, then either give it a positive reward of 1 for an even number, and a reward of -1 for an odd number, to make i output only even numbers over time.</p>

<p><strong>What would be an architecture for such a NN?</strong></p>

<p>I am getting caught in the part where here is actually no input, and I can't really start with a hidden layer, can I?</p>

<p>I am quite confused and would appreciate guidance</p>
"
2843,"<p>I am trying to implement NEAT algorithm in Python from scratch. However, I am stuck. When a new innovation number is created it has two nodes which represents the connection. Also this innovation number has a weight. </p>

<p>However, I know that innovation numbers are global variables, in other words when a innovation number is created, </p>

<pre><code>ex. Innovation ID:1 - Node:1 to Node:4 - weight: 0.5
</code></pre>

<p>it will have a ID which will be used by other connections to represent the connection between Node:1 to Node:4. </p>

<p>When this innovation is used by another neural network, will it also use the weight of the innovation 1, which is 0.5 in this example?</p>
"
2844,"<p>I'm interested in creating a convolutional neural network or LSTM to locate text in an image. I don't want to OCR the text yet, just find the text regions. Yes, I know Tesseract and other systems can do this, but I want to learn how it works by building my own. All of the tutorials and articles I've seen so far have the CNN output to a classification - ""image contains a cat"", ""image contains a dog"". Okay that's nice, but it doesn't say anything about where it was found. </p>

<p>Can anyone point me to some information that describes the output layer of a NN that can give location information? Like, x-y co-ordinates of text boxes?</p>
"
2845,"<p>i needed to make a system for recognizing people based on hundreds of text by finding similarities in their written text grammatically or similarities between words they choose for writing , i don't want it so accurate but wanted to know if it is possible ?</p>

<p>for example finding one person with two account or more on a forum or somthin in that case (texts already gathered)
 just wondering if it's possible and what field should i research in for.</p>

<p>thanks in advance </p>
"
2846,"<p>I have the following program for my neural network:  </p>

<pre><code>n_steps = 9
n_inputs = 36
n_neurons = 50
n_outputs = 1
n_layers = 2
learning_rate = 0.0001
batch_size =100
n_epochs = 1000#200 
train_set_size = 1000
test_set_size = 1000
tf.reset_default_graph()
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs],name=""input"")
y = tf.placeholder(tf.float32, [None, n_outputs],name=""output"")
layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons,activation=tf.nn.relu6, use_peepholes = True,name=""layer""+str(layer))
         for layer in range(n_layers)]    layers.append(tf.contrib.rnn.LSTMCell(num_units=n_neurons,activation=tf.nn.relu6, use_peepholes = True,name=""layer""+str(layer)))
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:,n_steps-1,:]
</code></pre>

<p>I want to know whether my network is fully connected or not?<br>
When I try to see the variables, I see:  </p>

<pre><code>multi_layer_cell.weights
</code></pre>

<p>The output is:  </p>

<pre><code>[&lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/kernel:0' shape=(86, 200) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/bias:0' shape=(200,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/w_f_diag:0' shape=(50,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/w_i_diag:0' shape=(50,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_0/layer0/w_o_diag:0' shape=(50,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/kernel:0' shape=(100, 200) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/bias:0' shape=(200,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/w_f_diag:0' shape=(50,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/w_i_diag:0' shape=(50,) dtype=float32_ref&gt;,
 &lt;tf.Variable 'rnn/multi_rnn_cell/cell_1/layer1/w_o_diag:0' shape=(50,) dtype=float32_ref&gt;]
</code></pre>

<p>I didn't understood whether each layer is getting the complete inputs or not.<br>
I want to know whether the following figure is correct for the above code:<br>
<a href=""https://i.stack.imgur.com/ub8Qb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ub8Qb.png"" alt=""enter image description here""></a> </p>

<p>If this is not then what is the figure for the network? Please let me know.</p>
"
2847,"<p>Is there a connection between the approximator network sizes in a RL task and the speed of convergence to an (near) optimal policy or value function?</p>

<p>When thinking about this, I came across the following thoughts:</p>

<ol>
<li><p>If the network would be too small, the problem won't get enough representation and would never be solved, and the network would converge to its final state quickly.</p></li>
<li><p>If the network would be infinitely big (assuming no vanishing gradients and the likes), the network would converge to some (desirable) over-fitting, and the network would converge to its final state very slowly, if at all.</p></li>
<li><p>This probably means there is some golden middle ground.</p></li>
</ol>

<p>Which leads me to the interesting question:</p>

<p><strong>4. Assuming training time is insignificant relative to running the environment (like in real life environments), then if a network of size M converges to an optimal policy in average after N episodes, would changing M make a predictable change on N?</strong></p>

<p>Is there any research, or known answer to this?</p>

<p>How to know that there is no more need to increase the network size?</p>

<p>How to know if the current network is too large?</p>

<p>Note: please regard question 4 as the main question here.</p>
"
2848,"<p>I have 2 tabular datasets, one is clean and one is drifted.
They are records of sensor measurements. I move the sensor around in the room and collected thousands of measurements.</p>

<p>I have a sensor that is supposed to track a signal from just one main source. But there are many sources that interfere with the main source in the dirty room.</p>

<p>In clean data, I have only one source when recording the measurements.</p>

<p>In dirty data, I have many interference sources when recording the measurements.</p>

<p>E.g. if the clean data has only 2 features, one of the row is 5,10. When it's affected by other sources, their value can be 7 and 8 or something like that. But it's not just a white noise that disappears later. It's a permanent drift that will never be gone unless I eliminate the interfering sources from the room.</p>

<p>That means if I measure the value it will always report 7 or 8 in the same dirty room every single time.</p>

<p>I want to separate out only the main source's measurement. So given 7 and 8, I want the output to be 5,10. But I don't have the input/output pair to train a machine learning model.
So this should be an unsupervised learning problem.</p>

<p>My idea is to train an unsupervised model to know what a <strong>clean</strong> data looks like. Then when given <strong>dirty</strong> data, it can convert the dirty data to a corresponding clean data like the example I gave.
So the model essentially learns to ignore all the other sources and report me the main source only. The number of sources is not known (maybe the clean area I thought was clean is actually containing other sources but I am fine if the model can convert the dirty to the <em>almost</em> clean data I provided)</p>

<p>Please give me a link or topics about this or explain your idea on how to make this unsupervised model.</p>

<p>PS. There are a lot more than 2 features for the sensor, that's why I think it's possible to know the clean data from the dirty one. And I also have time series data as I move the sensor around collecting measurements, let me know if you know how to make use of that to clean the data.</p>
"
2849,"<p>Can someone explain what is the process of learning? What does it mean to learn something?</p>
"
2850,"<p>I'm doing transfer learning using Inception on Tensorflow. The code that I used for training is <a href=""https://raw.githubusercontent.com/tensorflow/hub/master/examples/image_retraining/retrain.py"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/tensorflow/hub/master/examples/image_retraining/retrain.py</a> </p>

<p>If you take a look at the Argument Parser section at the bottom of the code, you will find these parameters :</p>

<ul>
<li>testing_percentage</li>
<li>validation_percentage</li>
<li>test_batch_size</li>
<li>validation_batch_size</li>
</ul>

<p>So far, I understand that testing and validation percentage is the amount of images that we want to train at 1 time. But I don't really understand the use of test batch size and validation batch size. What is the difference between percentage and batch size?</p>
"
2851,"<p>The address <a href=""https://discuss.openai.com/"" rel=""nofollow noreferrer"">https://discuss.openai.com/</a> at this moment does not work. To be more precise, ""discuss.openai.com"" does not resolve to a IP address, even using proxies.</p>

<p>Google Cached pages shows that at least until 2018-12-23 the was able to be accessed by google crawlers. </p>

<p>I tried to find some public official response to this, but I'm not able.</p>

<ul>
<li>My question is: <strong>what is the status of discuss.openai.com in 2019?</strong></li>
</ul>

<p>One extra important question is, if this is permanent, what will happen to all content from people who trusted OpenAI to host a online forum? (for example, at least exist a place to have access archived content?</p>

<hr>

<p>Note: Possible related question (but for another subdomain) is <a href=""https://ai.stackexchange.com/questions/4027/why-did-the-openais-gym-website-close"">Why did the openai&#39;s gym website close?</a></p>

<hr>

<p>Edit 1: thanks to @Neil Slater, I looked on my e-mail history, and found this. Not sure if is the full text of the thread of if is just this. But the text for who is using screen readers or automatic translation is:</p>

<blockquote>
  <p>Date: 2018-07-06</p>
  
  <p>Category: General Discussion</p>
  
  <p>Topic: Forum closing in a week</p>
  
  <p>Message: We will be closing these forums in ~1 week, as we no longer have time to maintain them. Please save any content that you wish to have access to.</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/SXIWI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SXIWI.png"" alt=""enter image description here""></a></p>
"
2852,"<p>Language is how we communicate to one other in the lab, at home, and in educational settings.</p>

<p>Word selection matters. Sometimes we don't select well. If we are writers, we may edit a word later or the editor may, but once picked and published, the something we have given a particular name may stick, and we can't change it easily. An example is the GAN (generative adversarial networks).</p>

<p>The generative artificial network ideas predated Ian J. Goodfellow et. al.'s 2014 paper introducing GANs. Some of those ideas are listed in the paper's bibliography. In the paper, they state,</p>

<blockquote>
  <p>Generative adversarial networks has been sometimes confused with the related concept of <em>adversarial examples</em>.</p>
</blockquote>

<p>Why, after admitting that it is a cause for confusion, did they pick the term <em>adversarial</em> for the second adjective in the name of their design?</p>

<p>They also write,</p>

<blockquote>
  <p>In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model ...</p>
</blockquote>

<p>What does it mean to be pitted against an adversary? The straightforward answer is that we place two entities <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span> such that the pursuit of the objectives of <span class=""math-container"">$A$</span> by <span class=""math-container"">$A$</span> hurts or destroys <span class=""math-container"">$B$</span> and vice versa. Even if not a game of annihilation or at least a zero sum game, there is clearly no win-win scenario, where <span class=""math-container"">$A$</span> and <span class=""math-container"">$B$</span> both get much of what they want.</p>

<p>Mutual benefit and the bargaining or negotiation process, in game theory, is called a Nash Equilibrium<sup>1</sup>. It has been refined<sup>2</sup> since Nash, but the basic idea of positive sum games as being the basis of bargaining in purchases and treaty negotiations remains a primary political and economic concept. When trade occurs or agreements are reached, such is done so that both parties benefit, even though there is a push-pull process in reaching the equilibrium.</p>

<p>In the GAN design, each network's objective is represented by its loss function, and nothing else. The networks cannot die, kill, dismember, hate, or even get angry at one another. There is no adrenaline or testosterone. They are not proud of themselves or their country. Isn't the term blatantly anthropomorphic?</p>

<p>The mathematics reveals, as so often it does, the truth. The paper further explains,</p>

<blockquote>
  <p>If <span class=""math-container"">$G$</span> and <span class=""math-container"">$D$</span> have enough capacity, and at each step of Algorithm 1, the discriminator is allowed to reach its optimum given <span class=""math-container"">$G$</span>, and p_g is updated so as to improve the criterion</p>
  
  <p><span class=""math-container"">$$ \mathbb{E}_{x ∼ p_{data}} \Big[ \log D_G^∗ (x) \Big] + \mathbb{E}_{x ∼ p_g} \Big[ \log (1 − D_G^∗ (x)) \Big] \; \text{,} $$</span></p>
  
  <p>then p_g converges to p_{data}.</p>
</blockquote>

<p>Note that, in this convergence, neither <span class=""math-container"">$G$</span> nor <span class=""math-container"">$D$</span> are hurt or destroyed as in Chess, where pieces are lost and concession is symbolized by tipping one's own King, representing that the King has fallen, meaning killed. Both networks are in Nash Equilibrium. Some might call the interaction between <span class=""math-container"">$G$</span> and <span class=""math-container"">$D$</span> negotiation and the result harmony.</p>

<p>In straight mathematical and unbiased terms, GANs are successful because of convergence collaboration. If each network concedes a portion of its loss optimality so that an equilibrium can be reached, then we have a Nash Equilibrium, which is not zero sum. It is win-win. If both converge fully, then the win-win is even stronger.</p>

<p>The collaboration mushrooms to a win-win-win-win scenario once the AI engineer and the project stakeholder are added in, since the consequence of the GAN equilibrium is the generated output the stakeholder wants and for which the AI engineer receives monetary and professional appreciation.</p>

<p>What causes these militant terms and negatively charged, anthropomorphic ways of looking at benign processes?</p>

<p>For the case of the 2014 paper, is the answer related to the fact that, since the cyber conflicts between Russia and Estonia, the term <em>adversarial networks</em> was common in papers arising from or vying for military funding?<sup>3</sup></p>

<p>Could that word association been subliminally introduced from Ian Goodfellow's subconscious? This is not to fault him or his colleagues but to get to the bottom of the conflict between the mathematics which proves non-adversarialism in a paper that names the algorithm arising from the math as adversarial.</p>

<p>In the case of Chess, the Huns attacked the Gupta Empire (now northern India) in the fifth century and Chaturanga, the forerunner of Chess, was invented in that same region in the sixth century. That would be immaterial if Chess wasn't the thing most associated with AI until GANs started generating images.</p>

<p>Stepping back from anthropomorphism and mathematics, from a sociology perspective, enemies are just regular people that resent others with greater abilities, social grace, food supply, and other goods. Some believe that, from God's perspective, enemies are people sent so we can learn how to love and show compassion unconditionally. If we apply this particular belief to populations, the purpose of adversarialism is solely and precisely to lay it down, along with its weapons.</p>

<p>Life for most on planet earth starts with some difficulty and increases gradually, with perhaps a decade or two of reprieve, but then increases more quickly until it ends. One interesting and multidimensional question for each individual is how we act and speak to one another under those universal conditions. And that intelligence is ancient and far from artificial. Which brings us back to the primary question.</p>

<blockquote>
  <p>Is AI research culture predisposed to adversarialism while even the mathematics is more friendly?</p>
</blockquote>

<p>The selection of words is how we communicate, and the choices we make perpetuate things that are good for AI and the world and things that may not be so good.</p>

<hr>

<p><strong>Footnotes</strong></p>

<p>[1] Even Nash's paper, <a href=""http://www.lsi.upc.edu/~ia/nash51.pdf"" rel=""nofollow noreferrer""><em>Non-cooperative Games</em></a>, Annals of Mathematics, 1951, describes an equilibrium with mutual advantage but uses the term non-cooperative in the paper title instead of <em>Game Advantage Equilibria</em>.</p>

<p>[2] K Binmore, A Rubinstein, and A Wolinsky, in their <a href=""http://www.academia.edu/download/3238070/200351014175547732.pdf"" rel=""nofollow noreferrer""><em>The Nash bargaining solution in economic modelling</em></a>, RAND Journal of Economics, 1986, ""The players are induced to reach an agreement by their impatience for the [mutually beneficial] outcomes.""</p>

<p>[3] With regard to the cyber conflicts between Russia and Estonia, <a href=""https://www.bbc.com/news/39655415"" rel=""nofollow noreferrer"">the BBC news stated</a>, ""But since [the 2007 cyber Estinian incident,] cyber warfare has been used all over the world, including in Russia's war with Georgia in 2008, and in Ukraine. ""Cyber has become a really serious tool in disrupting society for military purposes,"" says Tanel Sepp, ... a cyber security official at Estonia's Ministry of Defense.""</p>
"
2853,"<p>As an experiment, I want to teach an ANN to play the game of <a href=""https://en.wikipedia.org/wiki/Nim"" rel=""nofollow noreferrer"">Nim</a>.</p>

<blockquote>
  <p>The normal game is between two players and played with three heaps of any number of objects. The two players alternate taking any number of objects from any single one of the heaps. The goal is to be the last to take an object.</p>
</blockquote>

<p>The game is easily solvable and I already wrote a small bot that can play Nim perfectly to provide data sets for supervised learning.</p>

<p>Now I am struggling with the design question, how should I output the solution to a specific board state. The answer always consists of two components:</p>

<ul>
<li>How many stones to take (a more or less arbitrary integer value)</li>
<li>Which heap to take the stones from (the index of the heap)</li>
</ul>

<p>What are available design choices in this regard and is there a state-of-the-art design for this type of problem?</p>
"
2854,"<p>By reading the abstract of <a href=""https://people.orie.cornell.edu/davidr/or474/nn_sas.pdf"" rel=""nofollow noreferrer"">Neural Networks and Statistical Models</a> paper it would seem that ANNs are statistical models.</p>

<p>In contrast <a href=""https://towardsdatascience.com/no-machine-learning-is-not-just-glorified-statistics-26d3952234e3"" rel=""nofollow noreferrer"">Machine Learning is not just glorified Statistics</a>.</p>

<p>I am looking for a more concise/summarized answer with focus on ANNs.</p>
"
2855,"<p>I am doing neural machine translation task from language S to language T via interlingua L. So - there is the structure:</p>

<pre><code>S -&gt;
encoding of S (crisp) -&gt;
S-L encoder -&gt; S-L decoder -&gt;
encoding of L (non-crisp, coming from decoder) -&gt;
L -&gt;
encoding of L (crisp) -&gt;
L-T encoder -&gt; L-T decoder -&gt;
encoding of T (non-crisp, coming from decoder) -&gt;
T
</code></pre>

<p>All of this can be implemented in Pytorch more or less adapting the usual encoder-decoder NMT. So, the layer of interlingua L acts as a somehow symbolic/discrete layer inside the whole S-L-T neural network. My question is - how such system can be trained in end-to-end (S-T) manner? The gradient propagates from the T to the L and at the L one should do some kind of <strong>symbolic gradient</strong>? I. e. one should be able do compute the difference L1-L2?</p>

<p>I am somehow confused by such setting. My question is - is there similar networks which contain the symbolic representation as the intermediate layer and how one can train such system. I have heard about policy gradients but are they relevant to my setting?</p>

<p>Essentially - if I denote some neural network by symbols x(Wi)y, then the training of this network means, that I change Wi and x stays intact. I.e. the last member of backpropagation equation has the form d.../dw1. But if I combine (<strong>chain!</strong>) 2 neural networks x(Wi)y-y(Wj)z, then the the last backpropagation term for the y(Wj)z has the form (d.../dw1+d.../dy) and hence both the w1 and y should be changed/updated by the gradient descent too. So, doesn't some ambiguity arise here? Is such chaining of neural networks possible? Is is possible to train end-to-end chains of neural networks?</p>

<p>I am also thinking about use of evolutionary training.</p>
"
2856,"<p>I'm trying to train a neural network on evaluating chess positions if rather white (0.0) or black would win (1.0)</p>

<p>Currently the input consists of 4 bits per chess field (piece id 0 - 12, equals 64*4). Factors like castling are being ignored for now. Also, all training sets are random positions from popular games where it's white's turn and the desired output is the outcome of the game (0.0, 0.5, 1.0).</p>

<p>Are my input values the right choice?
How many hidden layers / neurons for each layer should be used and what's the best learning rate?
What type of NN's and which activation function would you recommend for this project?</p>
"
2857,"<p>I would like to use chatbots for a research project. The chatbots should trigger different types of emotions during a 10 minute period of chatting. I'm thinking about using Mitsuku, Zo or Cleverbot (I prefer using Facebook Messenger or Skype).</p>

<p>One option could be to let the participants do random smalltalk chatting. On the other hand, one could let the participants talk about specific topics where the bot is known to react differently (e.g. offensive, funny etc.)</p>

<p>Does somebody have a good idea how different emotions can be triggered using such a chatbot (or at least one specific emotion)?</p>

<p>Additionally, an interesting alternative are game-based chatbots (like Lifeline). Does somebody know a good chatbot, ideally for Facebook Messenger or Skype, which is game-based (e.g. a playable, branching story)? The interaction should be text-based (i.e. writing with the chatbot) and not only pressing buttons. I'm thankful also for recommendations of other chatbots which could be useful.</p>
"
2858,"<p>I am new in the field of genetic algorithms, and I want to learn to use them in practice.
<strong>How the genetic algorithm work and why it is applied ?</strong></p>
"
2859,"<p>From what I understand, Monte Carlo Tree Search Algorithm is a solution algorithm for model free reinforcement learning (RL).</p>

<p>Model free RL means agent doesnt know the transition and reward model. Thus for it to know which next state it will observe and next reward it will get is for the agent to actually perform an action.</p>

<p>my question is:
if that is the case, then how come the agent knows which state it will observe during the rollout, since rollout is just a simulation, and the agent never actually perform that action ? (it never really <strong>interact</strong> with the environment: e.g it never really move the piece in a Go game during rollout or look ahead, thus cannot observed anything). </p>

<p>It can only assume observing anything when not actually interacting with environment (during simulation) if it knows the transition model as I understand it. The same arguments goes for the rewards during rollout/ simulation.</p>

<p>in this case, doesnt rollout in Monte Carlo Tree Search algorithm suggests that the agent knows the transition model and reward model and thus a solution algorithm for model <strong>based</strong> reinforcement learning and not model <strong>free</strong> reinforcement learning ?</p>

<p>** it makes sense in Alphago, since the agent is trained to estimate what it would observed. but MCTS (without policy and value netwrok) method assumes that agent knows what it would observed even though no additional training is included.</p>
"
2860,"<p><strong><em>Status</em></strong>:</p>

<p>For a few weeks now, I have been working on a Double DQN agent for the <code>PongDeterministic-v4</code> environment, which you can find <a href=""https://github.com/hridayns/Reinforcement-Learning/tree/master/Atari"" rel=""nofollow noreferrer"">here</a>.</p>

<p>A single training run lasts for about 7-8 million timesteps (about 7000 episodes) and takes me about 2 days, on Google Collab (K80 Tesla GPU and 13 GB RAM). At first, I thought this was normal because I saw a lot of posts talking about how DQNs take a long time to train for Atari games.</p>

<p><strong><em>Revelation</em></strong>:</p>

<p>But then after cloning the OpenAI baselines <a href=""https://github.com/openai/baselines"" rel=""nofollow noreferrer"">repo</a>, I tried running <code>python -m baselines.run --alg=deepq --env=PongNoFrameskip-v4</code>  and this took about 500 episodes and an hour or 2 to converge to a nice score of +18, without breaking a sweat. Now I'm convinced that I'm doing something terribly wrong but I don't know what exactly.</p>

<p><strong><em>Investigation</em></strong>:</p>

<p>After going through the DQN baseline <a href=""https://github.com/openai/baselines/tree/master/baselines/deepq"" rel=""nofollow noreferrer"">code</a> by OpenAI, I was able to note a few differences:</p>

<ul>
<li>I use the <code>PongDeterministic-v4</code> environment but they use the <code>PongNoFrameskip-v4</code> environment </li>
<li>I thought a larger <strong>replay buffer size</strong> was important, so I struggled (with the memory optimization) to ensure it was set to <strong>70000</strong> but they set it to a mere <strong>10000</strong>, and still got amazing results.</li>
<li>I am using a normal <strong>Double DQN</strong>, but they seem to be using a <strong>Dueling Double DQN</strong>.</li>
</ul>

<p><strong><em>Results/Conclusion</em></strong></p>

<p>I have my doubts about such a huge increase in performance with just these few changes. So I know there is probably something wrong with my existing implementation. Can someone <strong>point me in the right direction</strong>?</p>

<p>Any sort of help will be appreciated. Thanks!</p>
"
2861,"<p>In LMS(least mean square) since, we use a quadratic error function, and quadratic functions are generally parabola in (some convex like shape). I wonder whether that is the reason why we use least square error metric? If that is not the case(its not ALWAYS convex or reason WHY we use LMS), what is the reason then? why this metric changes for deep learning/neural networks but works for regression problems?</p>

<p>[EDIT]: Will this always be a convex function or is there any possibility that it will not be convex?</p>
"
2862,"<p>Gradient training changes indiscriminately all the weights and nodes of the neural network. But one can imagine the situations when the training should be shaped, e.g.:</p>

<ul>
<li>One can put constraints on some of the weights. E.g. human brain contains regions whose inner connections are more dense that external connections with different regions. One can try to mimic this region-shaped structure in neural networks as well and hence one can require that inter-regional weights (in opposit to intra-regional weights) are close to zero (except, possibly, for some channels among regions);</li>
<li>One can put constraints on some of the weights in such manner that some layer of neurons have specific structure. E.g. consider the popular encoder-decoder architecture of neural machine translation e.g. <a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html</a> We can see that that the whole output of the encoder is expressed as a single layer of neurons which is forwarded to the input of the decoder. So - one can require that the set of all the possible outputs of the encoder (e.g. the possible values of this single layer of the neurons) forms some kind of structure, e.g. some grammar of some <em>interlingua</em>. This example is for illustration only, I have in mind more complex neural network which has one layer of neurons which indeed should output the encoded words of some interlingua. So, one is required to guid all the weights of the encoder in such manner that this single layer has only allowable values.</li>
</ul>

<p>So - my question is - <strong>are there methods that guide the gradient descent training with additional information about the weights or about the nodes</strong> (i.e. about the whole subsets of weights that have some impact on specific layer of nodes)? E.g. <strong>about methods that impress the region structure on the neural network or that constrains the values of some nodes to be in specific range only?</strong></p>

<p>Of course, it <strong>is quite easy to include such constraints in evolutionary neural networks</strong> - one can simply reject the neural networks with weights that violates the constraints. But <strong>is it possible to do this in gradient-like training?</strong></p>
"
2863,"<p>I am creating a VAE for time series data using CNNs. The data has 4800 timesteps and 4 features. It is standardized and normalized. The network I am using is implemented in Keras as follows. I have used a MSE reconstruction error:</p>



<pre><code># network parameters
(_, seq_len, feat_init) = X_train.shape
input_shape = (seq_len, feat_init)
intermediate_dim = 512
batch_size = 128
latent_dim = 10
epochs = 10
img_chns = 3
filters = 32
num_conv = (2, 2)
epsilon_std = 1

inputs = Input(shape=input_shape)
conv1 = Conv1D(16, 3, 2, padding='same', activation = 'relu', data_format = 'channels_last')(inputs)
conv2 = Conv1D(32, 2, 2, padding='same', activation = 'relu', data_format = 'channels_last')(conv1)
conv3 = Conv1D(64, 2, 2, padding='same', activation = 'relu', data_format = 'channels_last')(conv2)
flat = Flatten()(conv3)
hidden = Dense(intermediate_dim, activation='relu')(flat)
z_mean = Dense(latent_dim, name = 'z_mean')(hidden)
z_log_var = Dense(latent_dim, name = 'z_log_var')(hidden)

def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),
                              mean=0., stddev=epsilon_std)
    return z_mean + K.exp(z_log_var) * epsilon

z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])

decoder_hid = Dense(intermediate_dim, activation='relu')(z)
decoder_upsample = Dense(38400, activation='relu')(decoder_hid)
decoder_reshape = Reshape((600,64))(decoder_upsample)

deconv1 = Conv1D(filters=32, kernel_size=2, strides=1,
             activation=""relu"", padding='same', name='conv-decode1')(decoder_reshape)
upsample1 = UpSampling1D(size=2, name='upsampling1')(deconv1)
deconv2 = Conv1D(filters=16, kernel_size=2, strides=1,
             activation=""relu"", padding='same', name='conv-decode2')(upsample1)
upsample2 = UpSampling1D(size=2, name='upsampling2')(deconv2)
deconv3 = Conv1D(filters=8
                 , kernel_size=2, strides=1,
             activation=""relu"", padding='same', name='conv-decode3')(upsample2)
upsample3 = UpSampling1D(size=2, name='upsampling3')(deconv3)
x_decoded_mean_squash = Conv1D(filters=4
                 , kernel_size=4, strides=1,
             activation=""relu"", padding='same', name='conv-decode4')(upsample3)

class CustomVariationalLayer(Layer):
    def __init__(self, **kwargs):
        self.is_placeholder = True
        super(CustomVariationalLayer, self).__init__(**kwargs)

    def vae_loss(self, x, x_decoded_mean_squash):
        x = K.flatten(x)
        x_decoded_mean_squash = K.flatten(x_decoded_mean_squash)
        xent_loss = mse(x, x_decoded_mean_squash)
        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
        return K.mean(xent_loss + kl_loss)

    def call(self, inputs):
        x = inputs[0]
        x_decoded_mean_squash = inputs[1]
        loss = self.vae_loss(x, x_decoded_mean_squash)
        self.add_loss(loss, inputs=inputs)
        return x

outputs = CustomVariationalLayer()([inputs, x_decoded_mean_squash])

# entire model
vae = Model(inputs, outputs)
vae.compile(optimizer='adadelta', loss=None)
vae.summary()
</code></pre>

<p>I wanted to ask whether it is possible for the network to nearly perfectly reconstruct the test timeseries when passed through the entire VAE network, but still output junk when using a random Normal input. For further details, here is one of the inputs and outputs when passing a test signal through the network.</p>

<p><a href=""https://i.stack.imgur.com/MDd0C.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MDd0C.png"" alt=""inputs and outputs""></a></p>

<p>Here is a reconstruction generated purely from a random sample.</p>

<p><a href=""https://i.stack.imgur.com/RoPs8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RoPs8.png"" alt=""enter image description here""></a></p>

<p>How can this be? Even if there was a posterior collapse, the VAE should still be able to generate a good output sample with a random input. To further test this I decided to split the network into two parts (encoder and decoder), and then pass the test image through it. The encoder and decoder networks were made by simply splitting the trained VAE network as follows:</p>



<pre><code>idx = 9 
input_shape = vae.layers[idx].get_input_shape_at(0)

layer_input = Input(shape=(input_shape[1],)) 

x = layer_input
for layer in vae.layers[idx:-1]:
    x = layer(x)

decoder = Model(layer_input, x)
decoder.summary()
</code></pre>



<pre><code>idx = 0
input_shape = vae.layers[idx].get_input_shape_at(0)

layer_input = Input(shape=input_shape)

x = layer_input
for layer in vae.layers[idx + 1:7]:
     x = layer(x)

encoder = Model(layer_input, x)
encoder.summary()
</code></pre>

<p>Interestingly, I also got junk output here. I'm not sure how it is possible. If the model itself is getting a near perfect reconstruction, surely just passing an image through the encoder, extracting the latent mean, and then passing that latent mean through the decoder should also create a near perfect image?</p>

<p>Is there something I am missing here?</p>
"
2864,"<p>Can I treat a stochastic policy (over a finite action space of size <span class=""math-container"">$n$</span>) as a deterministic policy (in the set of probability distribution in <span class=""math-container"">$\mathbb{R}^n$</span>)? </p>

<p>It seems to me that nothing is broken by making this mental translation, except that the ""induced environment"" now has to take a stochastic action and spit out the next state, which is not hard using on the original environment. Is this legit? If yes, how does this ""deterministify then DDPG"" approach compare to, for example, A2C?</p>
"
2865,"<p>I have found multiple papers which use this dataset (because it's apparently public) to set benchmarks in forecasting but they don't mention where it can be found. I have done some searching on my own and haven't been able to find it. Any help would be appreciated.
Some of the papers are:</p>

<p><a href=""https://arxiv.org/pdf/1704.04110.pdf"" rel=""nofollow noreferrer"">Deep AR paper by Amazon</a></p>

<p><a href=""https://www.researchgate.net/publication/251527888_Forecasting_the_intermittent_demand_for_slow-moving_inventories_A_modelling_approach#pfb"" rel=""nofollow noreferrer"">Another one by Hyndman et. al.</a></p>

<p><a href=""https://pdfs.semanticscholar.org/5940/2496779a6d92afd70a551b64be4356e1a781.pdf"" rel=""nofollow noreferrer"">Another one by Amazon</a></p>
"
2866,"<p>I'm doing bachaleor thesis on traffic sign detection using single shot detector called YOLO. These single shot detectors can perform detection of objects in image and so they have specific way of training, ie. training on full images. Thats quite problem for me, because the biggest real dataset with full traffic sign images is <a href=""https://btsd.ethz.ch/shareddata/"" rel=""nofollow noreferrer"">Belgian one</a> with 9000 images in 210 classes, which is unfortunately not enough to train good detector.</p>

<p>To overcome this problem, I've created <a href=""https://github.com/kocica/DatasetGenerator"" rel=""nofollow noreferrer"">DatasetGenerator</a>, which does quite good job in generating synthetic datasets, you can see in the <a href=""https://github.com/kocica/DatasetGenerator/tree/master/results/v5"" rel=""nofollow noreferrer"">results directory</a>.</p>

<p>Recently I came across GAN's which can (besides others) generate or extend existing dataset and I would like to use these networks to compare with my dataset generator. I've tried <a href=""https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f"" rel=""nofollow noreferrer"">this introduction to GANs</a> succesfully.</p>

<p>The problem is it's unsupervised learning and so there are no annotations. It means it's able to extend my dataset of traffic signs, but the generated dataset won't be annotated at all, which is problem.</p>

<p>So my question is: Is there any way how to use GAN's to extend my dataset of full traffic sign images with annotations of traffic sign class and position? Actually the class is not important, because I can do it separately for each class, but what matters is the position of traffic sign in generated image.</p>
"
2867,"<p>Classical logic is an important part of university curriculum. It is about the distinction between zero and one and is useful to get a deeper understanding how a computer is working. If a student becomes familiar with boolean tables they can calculate for example that <em>1 <span class=""math-container"">$\land$</span> 0</em> is equal to <em>0</em>. In certain programming languages, the <code>byte</code> datatype consists of 1 byte which contains 8 bit and each of them can have the values 0 or 1. Such information can be explained very well to students. If someone hasn't understood the concept, they can ask and the teacher will explain it again.</p>

<p>The principle in higher education is, that assured knowledge of the world is taught. The students can profit from it, and are able to build systems in the real world. They can use their knowledge about 0 and 1 to write software. The underlying mathematics which is logic, boolean algebra and analysis helps the student to pass exams and be prepared for future challenges at work.</p>

<p>Fuzzy logic contradicts the principles of higher education. It is published in predatory journals which are not working with quality standards and have no peer-review. It is located outside the scientific community. Is it possible to integrate fuzzy knowledge into the normal curriculum? Will the students profit from uncertainty?</p>
"
2868,"<p>I recently came across a paper on <a href=""https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42945.pdf"" rel=""nofollow noreferrer"">Deep Ranking</a>. I was wondering whether this could be used to classify book covers as book titles. (For example, if I had a picture for the cover of the second HP book, the classifier would return Harry Potter and the Chamber of Secrets.)</p>

<p>For example, say I have a dataset of book covers along with the book titles in text. Could that data set be used for this deep ranking algorithm, or is there a much better way to approach my problem? I'm quite new to this whole thing, and this one of my first projects in this field.</p>

<p>What I'm trying to create is a mobile app where people can take a picture of a book cover, have an algorithm/neural net classify the title of the book, and then have some other algorithm connect that to the book's Goodreads page.</p>

<p>Thanks for the help!</p>
"
2869,"<p>I'm training an agent using RL and the SARSA function to update a Q function, but I'm confused how you handle the final state. In this case when the game ends and there is no S'.</p>

<p>For example, the agent performed an action based on the state S, and because of that the agent won or lost and there is no S' to transition to. So how do you update the Q function with the very last reward in that scenario because the state hasn't actually changed. In that case S' would equal S even though an action was performed and the agent received a reward (they ultimately won or lost, so quite important update to make!).</p>

<p>Do I add an extra inputs to the State agent_won and game_finished and that's the difference between S and S' for the final Q update?</p>

<p>EDIT: to make clear this is in reference to a multi-agent/player system.  So the final action the agent takes could have a cost/reward associated with it, but the subsequent actions other agents then take could further determine a greater gain or loss for this agent and whether it wins or loses. So the final state and chosen action, in effect, could generate different rewards without the agent taking further actions.</p>
"
2870,"<p><a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noreferrer"">Open Ai's (working) actor critic code</a> calculates the losses like so:</p>

<pre><code>actor_loss = -log_prob * discounted_reward
policy_loss = F.smooth_l1_loss(value, torch.tensor([discounted_reward]))
</code></pre>

<p>Both are different from <a href=""https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d"" rel=""nofollow noreferrer"">the regular formulas</a> which are:</p>

<p>Actor_Loss (parameterized by <span class=""math-container"">$\theta$</span>):</p>

<blockquote>
  <p><span class=""math-container"">$log[\pi_\theta(s_t,a_t)]Q_w(s_t,a_t)$</span></p>
</blockquote>

<p>Critic Loss (parameterized by <span class=""math-container"">$w$</span>):</p>

<blockquote>
  <p><span class=""math-container"">$r(s_t,a_t) + \gamma Q_w(s_{t+1},a_{t+1}) - Q_w(s_{t},a_{t})$</span></p>
</blockquote>

<p>Where <span class=""math-container"">$r(s_t,a_t)$</span> is the immediate reward following taking the action.</p>

<p><br></p>

<p>Meaning,</p>

<p>For the actor, <code>the immediate critic evaluation of the transaction</code> was replaced with <code>the discounted reward</code>, and</p>

<p>For the critic, the discounted evaluation of the value from the next state</p>

<blockquote>
  <p><span class=""math-container"">$r(s_t,a_t) + \gamma Q_w(s_{t+1},a_{t+1})$</span></p>
</blockquote>

<p>was replaced by</p>

<blockquote>
  <p><code>the discounted reward</code></p>
</blockquote>

<p>and</p>

<p>an l1 loss is then calculated, effectively discarding the sign of the (equation) loss.</p>

<p>Questions:</p>

<ol>
<li><p>Why are the changed equation elements?</p></li>
<li><p>Why is the sign discarded for the critic loss?</p></li>
</ol>
"
2871,"<p>After I read paper by Gatys, <a href=""https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf"" rel=""nofollow noreferrer"">Image Style Transfer Using Convolutional Neural Networks</a>, I notice there aren't any  explanations for the constant in Eq. (4):</p>

<p><span class=""math-container"">$$E_l = \frac{1}{4N_l^2M_l^2}\sum_{i,j}(G_{ij}^l-A_{ij}^l)^2$$</span></p>

<p>Some people said it's used for normalization, but why <span class=""math-container"">$4$</span>? I saw some implementations, for example in Udacity's course, they use different notation.</p>

<p>I hope somebody here can help, thank you!</p>
"
2872,"<p>I have a project to make a classification ( scoring ) system to measure driving behaviour aggressivity using data like velocity, acceleration, weather and traffic code for speed limits according to the specificity of roads, i'm an intermediate leveled in python, what is the best approach should i follow to get a better estimation of weights I'll be giving to those data to get my aggressivity factor? </p>
"
2873,"<p>I work on a project where the neural network must be quantized on 8 or 16 bits for an embedded platform, thus I lose some precision.</p>

<p>EDIT: since our platform does not have floating point arithmetic we need to quantize the weights. by quantizing i mean taking the max absolute value of the weights and divide it by the maximum signed number representable on 8 or 16 bits. this operation will give us a quantization factor (qf).
the final quantized weights will be integer(value * qf)</p>

<p>If my weights are very sparse and have a very bad distribution I lose more precision.</p>

<p>For example to the left here is the distribution of weights for one layer, and to the left is the distribution of weights after I added to the loss function the kurtosis and skew measures of the weights, and it improved a bit the shape of the distribution while keeping the same accuracy, even a bit higher.<a href=""https://i.stack.imgur.com/XPQOG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XPQOG.png"" alt=""enter image description here""></a></p>

<p>Does anybody have any other suggestions? Has anyone tackled this problem before?</p>

<p>Thank you</p>
"
2874,"<p>Is the role played by activation function significant only during the training of neural network or they play their role during testing (after training we supply data for prediction) the network.</p>

<p>I understand that a linear line cannot separate data scattered in complex manner but
Then why we don't used simple polynomials.</p>

<p>why specifically sigmoid, or tanh or ReLu what exactly they are doing ?</p>

<p>What Activation functions do when we are supplying data during training and </p>

<p>And when we supply test data once we have trained the network and we input test data for prediction?</p>
"
2875,"<p>I've watched this video of the recent contest of AlphaStar Vs Pro players of StarCraft2, and during the discussion David Silver of DeepMind said that they train AlphaStar on TPUs.</p>

<p>My question is, how is it possible to utilise a GPU or TPU for reinforcement learning when the agent would need to interact with an environment, in this case is the StarCraft game engine?</p>

<p>At the moment with my training of a RL agent I need to run it on my CPU, but obviously I'd love to utilise the GPU to speed it up. Does anyone know how they did it?</p>

<p>Here's the part where they talk about it, if anyone is interested:</p>

<p><a href=""https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;t=7030s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;t=7030s</a></p>
"
2876,"<p>Suppose we have a data set consists of columns</p>

<blockquote>
  <p>TransactionId, CardNo, TransactionDate</p>
</blockquote>

<p>then how can we calculate the customer purchase interval (means if customer A purchased on Jan 1st and after 10 days he again purchased, and then he again purchased after 15 days.) and how to predict the next visit of customer A by analyzing the purchasing intervals of customer A.</p>

<p>Any help will be appreciated.</p>
"
2877,"<p>I've watched this video of the recent contest of AlphaStar Vs Pro players of StarCraft2, and during the discussion David Silver of DeepMind explains the process of creating new agents (forked/branched) from their existing agents that employ reinforcement learning and MDP.</p>

<p>My question is, what method do they use to make new agents that differ from the 'parent' agent? You can obviously copy an agent exactly and it'll have the same characteristics, but obviously the idea is for a new agent to explore more of the feature space and employ new strategies. So the way I thought it could be done is copying an agent and adjust the <code>ϵ</code> value of that agent so that it has some increased probability of taking a random action.  Are there any other methods that might be employed?</p>

<p>Here's the part in the video where they discuss it:</p>

<p><a href=""https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;t=4654s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;t=4654s</a></p>
"
2878,"<p>I have a sensor that reads electromagnetic field strength from each position.</p>

<p>And the field is stable and unique for each position. So the reading is simply a function of the position like this: <code>reading = emf(x,y,z)</code></p>

<p>The reading consists of 3 numbers (not position).</p>

<p>I want to find the inverse function of <code>emf</code> function. Which means I want to find function <code>pos</code> that is defined like this: <code>x,y,z = pos(reading)</code></p>

<p>I don't have access to both <code>emf</code> and <code>pos</code> function. I think that I want to gradually estimate the <code>pos</code> function using a neural network.</p>

<p>So I have input <code>reading</code> and acceleration <code>ax,ay,az</code> of the sensor through space from an IMU. The acceleration is not so accurate.
I want to use these 2 inputs to help me figure out the position of the sensor over time. You can assume that the starting position is at 0,0,0 on the first reading.</p>

<p>In short, input is <code>reading</code> and <code>ax,ay,az</code> on each timestep, output will be adjustment on the weights of <code>pos</code> function or output will be position directly.</p>

<p>I've been reading about <strong>SLAM</strong> (simultaneous localization and mapping) algorithm and I think that it might help in my case because my problem is probabilistic. 
If I know accurately the acceleration, I would not need any probability, but the acceleration is not accurate.</p>

<p>So I want to know how do I model this problem in term of <strong>SLAM</strong>?
I don't have a camera to do vision-based SLAM though.</p>

<p>Why I think this is tractable? If the first reading is <code>1,1,1</code> and the position is at origin <code>0,0,0</code>, and I move the sensor, the position can drift because the sensor has never seen other reading before, but after I go back to origin, the reading will be <code>1,1,1</code> again so the sensor should report the origin <code>0,0,0</code> as output. During the movement of the sensor, the algorithm should filter the acceleration so that all the previous positions makes sense.</p>
"
2879,"<p>Recently I watched a video of ISrael's Iron Dome. Which is mainly created to protect citizens from missile attacks by enimies. Though, it sounds so wierd but, actually it's not controlled by anyone. It's totally automated. Whenever any missile tracked by their radars it automatically deploy a missile to action and so error free. I cannot understand how it works.
As same as others missiles or any special AI is used?</p>
"
2880,"<p>In <a href=""https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits"">experience replay</a>, the update rule follows the loss:</p>

<blockquote>
  <p><span class=""math-container"">$$
L_i(\theta_i) = \mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \sim U(D)} \left[ \left(r_t + \gamma \max_{a_{t+1}} Q(s_{t+1}, a_{t+1}; \theta_i^-) - Q(s_t, a_t; \theta_i)\right)^2 \right]
$$</span></p>
</blockquote>

<p>I can't get my head around the order of calculation of the terms in that equation:</p>

<p>An experience element is </p>

<blockquote>
  <p><span class=""math-container"">$(s_t, a_t, r_t, s_{t+1} )$</span></p>
</blockquote>

<p>where</p>

<blockquote>
  <p><span class=""math-container"">$s_t$</span> is the state at time <span class=""math-container"">$t$</span></p>
  
  <p><span class=""math-container"">$a_t$</span> is the action taken from <span class=""math-container"">$s_t$</span> at time <span class=""math-container"">$t$</span></p>
  
  <p><span class=""math-container"">$r_t$</span> is the reward received by taking that action from <span class=""math-container"">$s_t$</span> at time
  <span class=""math-container"">$t$</span></p>
  
  <p><span class=""math-container"">$s_{t+1}$</span> is the next state</p>
</blockquote>

<p>In <a href=""https://www.quora.com/Why-are-actor-critic-methods-in-RL-off-policy"" rel=""nofollow noreferrer"">the on policy case</a>, <strong>as i understand it</strong>, Q of the equation above is the same Q, which is the only approximator.</p>

<p>As I understand the algorithm, at time <span class=""math-container"">$t$</span> we save an experience </p>

<blockquote>
  <p><span class=""math-container"">$(s_t, a_t, r_t, s_{t+1} )$</span>.</p>
</blockquote>

<p>Then, later, at time <span class=""math-container"">$t+x$</span> we attempt to learn from that experience.</p>

<p>However, at the time of saving the experience, <span class=""math-container"">$Q(s_t,a_t)$</span> was something different than at the time of attempting to learn from that experience, because the parameters <span class=""math-container"">$\theta$</span> of <span class=""math-container"">$Q$</span> have since changed. This could actually be written as</p>

<blockquote>
  <p><span class=""math-container"">$Q_t(s_t, a_t) \neq Q_{t+x}(s_t,a_t)$</span></p>
</blockquote>

<p>Because the Q value is different, I don't see how the reward signal at time <span class=""math-container"">$t$</span> is of any relevance for <span class=""math-container"">$Q_{t+x}(s_t,a_t)$</span> at <span class=""math-container"">$t+x$</span>, the time of learning.</p>

<p>Also, it is likely that following a policy which is derived from <span class=""math-container"">$Q_t$</span> would lead to <span class=""math-container"">$a_{t}$</span>, whereas following a policy which is derived from <span class=""math-container"">$Q_{t+x}$</span> would not. </p>

<p>I don't see in the experience replay algorithm that the Q value <span class=""math-container"">$Q_t(s_t, a_t)$</span> is saved, so I must assume that is is not.</p>

<p><strong>Why does calculating the Q value again at a later time make sense FOR THE SAME SAVED REWARD AND ACTION?</strong></p>
"
2881,"<p>I am training a deep convolutional generative adversarial network (DC-GAN) (Original Paper: <a href=""https://arxiv.org/pdf/1511.06434.pdf"" rel=""nofollow noreferrer"">1</a>) using PyTorch on an image dataset for a research project. Despite which category of images I attempt to train on, the GAN seems to hit a point of instability between 1000-2000 training epochs, such as in the example shown below. Do you have any insight into what is causing this instability? Are there any best practices of tuning hyper-parameters for DC-GANs to avoid instability or is it mostly guess and test? </p>

<p>Thanks!</p>

<p><a href=""https://i.stack.imgur.com/GOUV9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GOUV9.png"" alt=""DC-GAN Training Instability""></a></p>
"
2882,"<p>I am reading <a href=""https://core.ac.uk/download/pdf/143454908.pdf"" rel=""nofollow noreferrer"">BayesChess: A computer chess program based
on Bayesian networks
</a> (Fernandez, Salmeron; 2008)</p>

<p>It is a chess playing engine using Bayesian networks. The following is mentioned about heuristic function in setion 3.</p>

<blockquote>
  <p>Here the heuristic is defined in terms of 838 parameters.</p>
  
  <p>There are 5 parameters indicating the value of each piece (pawn,
  queen, rook, knight, and bishop -the king is not evaluated, as it must
  always be on the board), 1 parameter for controlling whether the king
  is under check, 64 parameters for evaluating the location of each
  piece on each square on the board (i.e., a total of 786 parameters,
  corresponding to 64 squares × 6 pieces each colour × 2 colours) and
  finally 64 more parameters that are used to evaluate the position of
  the king on the board during the endgame.</p>
</blockquote>

<p>The above sentence contains about the parameters used by the heuristic function. But I didn't find the actual definition. What is the actual definition for the heuristic function?</p>
"
2883,"<p>Up to now, I have been using (my version of) <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noreferrer"">open AI's code</a>, with the suggested CartPole.</p>

<p>I have been using Monte Carlo methods, which, for cartpole, seemed to work fine.</p>

<p>Trying to move to temporal difference, Cartpole seems to fail to learn (with simple TD method) (or I stopped it too soon, but still unacceptable).</p>

<p>I assume that is the case because in Cartpole, for every timestamp, we get a reward of 1, which has very little immediate information about weather or not the action was good or not.</p>

<p><strong>Which gym environment is the simplest that would probably work with TD learning?</strong></p>

<p>by simplest I mean that there is no need for a large NN to solve it. No conv nets, no RNNS. just a few small layers of a fully connected NN, just like in cartpole. something I can train on my home cpu, just to see it starting to converge.</p>
"
2884,"<p><a href=""https://amreis.github.io/ml/reinf-learn/2017/11/02/reinforcement-learning-eligibility-traces.html"" rel=""nofollow noreferrer"">TD lambda</a> is a way to interpolate between TD(0) - bootstraping over a single step, and, TD(max), bootstraping over the entire episode length, or, Monte carlo.</p>

<p>Reading the link above, I see that an eligibility trace is kept for each state in order to calculate its ""contribution to the future"".</p>

<p>But, if we use an approximator, and not a table for state-values, then can we still use eligibility traces?</p>

<p>If so, how would the loss be calculated? (and thus the gradients)</p>

<p>Specifically, I would like to use actor critic (or advantage actor critic)</p>
"
2885,"<p>In my country Expert System class is mandatory class if you want to take AI specialization in most universities. In class I learned how to make a rule base system, forward chaining, backward chaining, <a href=""https://id.wikipedia.org/wiki/Prolog"" rel=""nofollow noreferrer"">Prolog</a>, etc. I enjoyed the class until I read a few comments in Stackoverflow that Expert System is no longer included in the AI field, some top university have stopped teaching that again.</p>

<p>I found the old screenshot one of the comments:
<a href=""https://i.stack.imgur.com/EdsVc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EdsVc.jpg"" alt=""enter image description here""></a></p>

<p>Is that true? why? And from industrial view, with the rise of machine learning, is the expert system still in use today?</p>
"
2886,"<p><a href=""https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits"">Experience replay</a> is buffer (or a ""memory"") of transactions <span class=""math-container"">$e_t = (s_t, a_t, r_t, s_{t+1})$</span>.</p>

<p><a href=""https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d"" rel=""nofollow noreferrer"">The equations for calculating the loss in actor critic</a> are an </p>

<p><em>actor</em> loss (parameterized by <span class=""math-container"">$\theta$</span>) <span class=""math-container"">$$log[\pi_\theta(s_t,a_t)]Q_w(s_t,a_t)$$</span> and a <em>critic</em> loss (parameterized by <span class=""math-container"">$w$</span>) <span class=""math-container"">$$r(s_t,a_t) + \gamma Q_w(s_{t+1},a_{t+1}) - Q_w(s_{t},a_{t}).$$</span></p>

<p>As I see it, there are two more elements that need to be saved for later use:</p>

<ol>
<li><p>The expected Q value at the time <span class=""math-container"">$t$</span>: <span class=""math-container"">$Q_w(s_{t},a_{t})$</span></p></li>
<li><p>The log probability for action <span class=""math-container"">$a_t$</span>: <span class=""math-container"">$log[\pi_\theta(s_t,a_t)]$</span></p></li>
</ol>

<p>If they are not saved, how will we be able to later calculate the loss for learning? I didn't see anywhere stating to save those, and I must be missing something.</p>

<p>Do these elements need to be saved or not?</p>
"
2887,"<p>I've come up with an idea on how we could use a combination of Deep Learning and body sensors to create a walking talking living humanoid. Here goes:</p>

<p>First, we will recruit 1 billion people and have them wear a special full face mask and suit. This suit will contain touch sensors along the skin, cameras, smell sensors, taste sensors on the mask, basically every data and information that a human receives will be collected electronically, whether it is what they see, what they smell what they feel and so on.</p>

<p>These suits will also have potentiometers and other sensors to measure the movement made by the person. Every hand movement, leg movement, muscle movement will be recorded and saved in a database as well.</p>

<p>After 50 years or so, all collected input and output data from every single person who participated in this experiment will be saved in a computer. We then create a neural network and then train it on the input and output data from the database.</p>

<p>Next, we create a robot that has motorized muscles and hand/leg joints that are the same specs as to our previous suits and also the touch, smell, sight and other sensors integrated inside of this robot.</p>

<p>Once everything is ready, we will load the trained neural network onto the robot and switch it on. During inference, the neural network will take data from sensors all over the robot's body and translate it into movement in legs, hands, and muscle.</p>

<ul>
<li>Could these techniques in conjunction with the data collection I describe, produce and AGI android?  </li>
</ul>

<p>Essentially, how feasible is current technology to produce a robot that will behave, speak, live, move like a normal human being?</p>
"
2888,"<p>As in sigmoid function when input x is very large or very small the curve is flat that means low gradient descent but when it is in between the slope is more so,
My question is how this thing helps us in neural network.</p>
"
2889,"<p>I wasn't sure how to title this question so pardon me please.</p>

<p>You may have seen at least one video of those ""INSANE A.I created simulation of {X} doing {Y &amp; Z} like the following ones:</p>

<p>A.I learns how to play Mario
A.I swaps faces of {insert celebrity} in this video after 16hrs.
etc...</p>

<p>I want to know what I have to learn to be able to create for example a program that takes xyz-K images of a person as training data and changes it with another person's face in a video.</p>

<p>Or create a program that on a basic level creates a simulation of 2 objects orbiting /attracting each other /colliding like this:
<img src=""https://i.imgur.com/X6TXNUa.gif"" alt=""enter image description here""></p>

<p>What field/topic is that?
 I suspect deep learning but I'm not sure. I'm currently learning machine learning with Python.</p>

<p>I'm struggling because linear regression &amp; finances /stock value prediction is really not interesting compared to teaching objects in games to do archive something or create a program that tries to read characters from images.</p>
"
2890,"<p>I am trying to implement Contractive auto-encoders in PyTorch but I don't know what I'm doing is right or not. The architecture of the auto-encoder is given below:</p>

<pre><code>class AE(nn.Module):
    def __init__(self):
        super(AE, self).__init__()
        self.encoder = nn.Sequential(nn.Linear(784, 256),nn.Linear(256, 128), nn.Linear(128, 64))
        self.decoder = nn.Sequential(nn.Linear(64, 128), nn.Linear(128, 256), nn.Linear(256, 784))
        self.sigmoid = nn.Sigmoid()

    def forward(self, input):
        h1 = self.encoder(input)
        h2 = self.decoder(h1)
        sigmoid = self.sigmoid(h2)
       return h1,sigmoid
</code></pre>

<p>I am trying to implement the contractive loss function the code for which is given below:</p>

<pre><code>mse_loss = nn.MSELoss()
lam= 1e-3

def contractive_loss(W, x, recons_x, h, lam):
    mse = mse_loss(recons_x, x)
    dh = h*(1-h)
    w_sum = torch.sum(W**2, dim=1)
    w_sum = w_sum.unsqueeze(1)
    contractive_loss_value = torch.sum(torch.mm(dh**2, w_sum),0)
    return mse + contractive_loss_value.mul(lam)
</code></pre>

<p>The training module is given below:</p>

<pre><code>def train(model=ae_model, epoch=0, train_loader= train_loader):
    model.train()
    train_loss= 0
    total= 0

    for i, (data, label) in enumerate(train_loader):
        data= data.to(device).view(-1, 28*28)
        label = label.to(device).view(-1, 28*28)
        optimizer.zero_grad()
        hidden_representation, recons_x = model(data)

        W = model.state_dict()['decoder.2.weight']
        loss = contractive_loss(W , label, recons_x, hidden_representation, lam)

        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        total += label.size(1)
</code></pre>

<p>Any help will be appreciated.</p>
"
2891,"<p>What is machine learning and deep learning? I hope there are similar questions out there. But I couldn't able visualize. Can someone explain me the difference between Machine learning and Deep learning? </p>
"
2892,"<p>I have a convolutional encoder (a CNN) consisting of DenseBlocks and a total of 50 layers (cf. FC-DenseNet103). The receptive field of the encoder (after last layer) is 660 according to Tensorflow function compute_receptive_field_from_graph_def(..)) whereas the input image is 64x64 pixels. Obviously the receptive field is way too big. </p>

<p>How can the receptive field be reduced to say 46 but the capacity of the encoder be more or less kept at the same level? By capacity I simply mean the number of parameters of the model. The capacity requirement is justified due to the complex dataset to be processed.</p>

<p>Using less layers or smaller kernels reduces the receptive field size but also the capacity. Should I then just increase the number of filters in the remaining layers in order to keep the capacity?</p>
"
2893,"<p>I have N property graphs and i must calculate similarity between these graphs using deep learning. My questions are: How can i represent these graphs for deep learning or something like feature extraction. And how can i calc similarity? Anyone has some idea?</p>
"
2894,"<p>I have got multi-class object detector. One model accuracy evaluation of detection consists of: <strong>mAP, FP, FN, TP</strong> for each class divided to two graphs and looks like this (I've used <a href=""https://github.com/rafaelpadilla/Object-Detection-Metrics"" rel=""nofollow noreferrer"">this repo</a> for evaluation).</p>

<p><a href=""https://i.stack.imgur.com/rBRlb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rBRlb.png"" alt=""enter image description here""></a></p>

<p>Now, I've got many of these evaluations (multiple times these two graphs for different models) and I would like to easily compare all these trained models (results) and put them to <strong>one</strong> graph.</p>

<p>I've searched through the whole Internet, but wasn't able to find suitable method of placing all the values to one graph. Also, the values of these three classes can be put together (eg. result mAP for this evaluation would be (75+ 68+ 66) / 3 = ~70%), so I would have just single value of each mAP, FN, FP, TP for one whole model evaluation.</p>

<p>What comes to my mind is the following graph (or maybe some kind of plot):</p>

<p><a href=""https://i.stack.imgur.com/mYQLs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mYQLs.png"" alt=""enter image description here""></a></p>

<p><strong>Note:</strong> It may not make sense to place mAP together with TP, etc. into one graph, but I would like to have all these values together to easily compare all the model evaluations. Also I am not really looking for a script, I can do the graph manually from values, but script would be more helpful. What really matters is, how to create meaningful graph with all the data :). If the post is more suitable for different kind of site, please, let me know.</p>
"
2895,"<p>Is it possible to sample from a distribution inside a neural network forward function? Assume that there is a NN and a sample is needed to be derived from it at every forward-pass to randomly set a layer-specific hyper-parameter.</p>

<p>Is this operation differentiable</p>
"
2896,"<p>Are there chatbots for Facebook Messenger or Skype available which are game-based, i.e. with which it is possible to play a short funny game? It should be possible to play the game for at least 10 minutes in sequence and it should be writing based and not based on clicking at boxes. That means the agent should be pretty clever, like Microsoft Zo bot, but instead of conducting random smalltalk a game should be played.</p>

<p>Second, are there bots for Facebook Messenger or Skype available which are nasty and unfriendly, i.e. which are offending?</p>

<p>Thank you a lot in advance. I'm thankful for any help.</p>
"
2897,"<p>I want to implement AI in client side. Any Javascript libraries that has been already used.</p>

<p>What are the libraries can be used and some samples?</p>
"
2898,"<p>The Softsign (a.k.a. ElliotSig) activation function is really simple: </p>

<pre><code>            x
f(x) = -----------
         1 + |x|
</code></pre>

<p>It is bounded [-1,1], has a first derivative, it is monotonic, and it is computationally extremely simple.</p>

<p>Why it is not widely used in neural networks? Is it because it is not infinitely derivable?</p>
"
2899,"<p>I want to customize the 'Pendulum-v0' environment such that the action (the torque) from previous time step as well as from the current timestep serve as the inputs in the Env.step() function.</p>

<p>My problem statement is that I want to generate torque from the controller which has a white Gaussian noise of magnitude 1 and then filter it with the torque generated in the previous timestep as follows:</p>

<p>tor_ = tor_c + a*WGN ;</p>

<p>tor(t) = lambda*tor_ + (1-lambda)*tor(t-1) ;</p>

<p><a href=""https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#L37"" rel=""nofollow noreferrer"">https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#L37</a></p>

<p>We can see that the 'u' is an array of some numbers as the input which is afterward clipped from -max_torque to max_torque and then only the first element is taken as the torque value to calculate the states and the reward function for the given time-step.</p>

<p>My question is what does the value of the other elements signify? Are they
torque values from the previous time steps or is it that the length of the 'u' array is just 1 and its value is restricted between -max_torque to max_torque?</p>

<p>In conclusion, I just wanna access the action (the torque value) from the previous time-step. Is it possible ? If yes, how?</p>
"
2900,"<p>In any neural network, each neuron in the network represents some part of non-linear feature of the input. Ex: Like in mnist data, Consider the stem of number 9 is cut into multiple pieces and different part is represented by different neurons in the first hidden layer(Just an example from 3B1B neural networks video). My questions are:</p>

<ol>
<li><p>What determines which neuron get to represent which part of the stem??</p></li>
<li><p>Is it possible that if we pass in the same input multiple times, each neuron can represent different part of the stem??
<em>OR</em> 
Is it that its all the magic of chain rule (i.e At the beginning, all neuron represent some trash feature and as updation of weights occur and then particular features have become synonymous to a particular neuron.) If so, How does this happen?
Thanks in advance</p></li>
</ol>
"
2901,"<p>Greetings and best wishes to AI community,</p>

<p>Given a worksheet with columns</p>

<ul>
<li>item name</li>
<li>item price</li>
<li>item supplier</li>
</ul>

<p>I'd like:</p>

<ul>
<li>to find same/similar items from different suppliers assuming they may be named differently by supplier</li>
</ul>

<p>What would you recommend to accomplish this task?</p>

<p>Thank you</p>
"
2902,"<p>I'm looking for some help... Someone can refer me or recommend recent studies that relate to the models at runtime with the AI, that would really help me a lot. I also appreciate your comments on the subject and any point of view would also be useful.</p>
"
2903,"<p>I am trying to build a film review classifier where I determine if a given review is positive or negative (w/ Python). I'm trying to avoid any other ML libraries so that I can better understand the processes. Here is my approach and the problems that I am facing:</p>

<ol>
<li>I mine thousands of film reviews as training sets and classify them as positive or negative. </li>
<li>I parse through my training set and for each class, I build an array of unique words. </li>
<li>For each document, I build a vector of TF-IDF values where the vector size is my number of unique words.</li>
<li>I use a Gaussian classifier to determine: <span class=""math-container"">$$P(C_i|w)=P(C_i)P(w|C)=P(C_i)*\dfrac{1}{\sqrt{2\pi}\sigma_i}e^{-(1/2)(w-\mu_i)^T\sigma_i^{-1}(w-\mu_i)}$$</span> where <span class=""math-container"">$w$</span> is the my document in a vector, <span class=""math-container"">$C_i$</span> is a particular class, <span class=""math-container"">$\mu_i$</span> is the mean vector and <span class=""math-container"">$\sigma_i$</span> is my covariance matrix. </li>
</ol>

<p>This approach <em>seems</em> to makes sense. My problem is that my algorithm is much too slow. As an example, I have sampled over 1,500 documents and I have determined over 40,000 unique words. This mean that each of my document vectors has 40,000 entries and if I were to build a covariance matrix, it would have dimensions 40,000 by 40,000. Even I were able to generate the entirety of <span class=""math-container"">$\sigma_i$</span>, but then I would have to compute the matrix product in the exponent, which will take an extraordinarily long time just to classify one document. </p>

<p>I have experimented with a multinomial approach, which is working well. I very curious on how to make this work more efficiently. I realise the matrix multiplication runtime can't be improved, and I was hoping for insight on how others are able to do this.</p>

<p>Some things I have tried:</p>

<ul>
<li>Filtered any stop words (but this still leaves me with tens of thousands of words)</li>
<li>Estimated <span class=""math-container"">$\sigma_i$</span> by summing over a couple of documents. </li>
</ul>
"
2904,"<p>Could advanced AI decide to kill all humans? </p>
"
2905,"<p>I just finished the three-part series of <a href=""https://www.coursera.org/learn/probabilistic-graphical-models-3-learning"" rel=""nofollow noreferrer"">Probabilistic Graphical Models</a> courses from Stanford over on Coursera. I got in to them because I realized there is a certain class of problem for which the standard supervised learning approaches don't apply, for which graph search algorithms don't work, problems that don't look like RL control problems, that don't even exactly look like the kind of clustering I came to call ""unsupervised learning"".</p>

<p>In my AI courses in the <a href=""https://www.gatech.edu/"" rel=""nofollow noreferrer"">Institute</a>, we talked briefly about Bayes Nets, but it was almost as if professors considered that preamble to hotter topics like Neural Nets. Meanwhile I heard about ""Expectation Maximization"" and ""Inference"" and ""Maximum Likelihood Estimation"" all the time, like I was supposed to know what they were talking about. It frustrated me not to be able to remember statistics well enough to <em>feel</em> these things, so I decided to fill the hole by delving deeper in to PGMs.</p>

<p>Throughout, Koller gives examples of how to apply PGMs to things like image segmentation and speech recognition, examples that seem completely dated now because we have CNNs and LSTMs, even <a href=""https://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/"" rel=""nofollow noreferrer"">deep nets that encode notions of uncertainty about their beliefs</a>.</p>

<p>I gather PGMs are good when:</p>

<ol>
<li>You know the structure of the problem and can encode domain knowledge that way.</li>
<li>You need a generative model.</li>
<li>You want to learn more than just one X -> Y mapping, when you instead need a more general-purpose model that can be queried from several sides to answer different kinds of questions.</li>
<li>You want to feed the model inputs that look more like probability distributions than like samples.</li>
</ol>

<p>What else are they good for? Have they not been outstripped by more advanced methods for lots of problems now? In which domains or for which specific kinds of problem are they still king? How are they complementary to modern advanced methods? Should I dedicate time to reading any of Koller &amp; Friedman's massive tome on this subject? How dated is this set of MOOCs?</p>
"
2906,"<p>I work with neural networks for real-time image processing on embedded softwares and I tested different architectures (Googlenet, Mobilenet, Resnet, custom networks...) and different hardware solutions (boards, processors, AI accelerators...). I noticed that the performance of the system, in terms of inference time, does not depend only on the processor but also on other factors. </p>

<p><em>For example</em>, I have two boards from different manifacturers, B1 (with a cheap processor) and B2 (with a better processor), and two neural networks, N1 (very light with regular convolutions and fully connected layers) and N2 (very large, with inception modules and many layers). The inference time for N1 is better on B1, while for N2 it is better on N2. Moreover, it happens that, as the software is executed, the inference time changes over time.</p>

<p>So my question is: <strong>in an embedded system, what are the aspects that impact on the inference time, and how?</strong> I am interested not only in the hardware features but also in the neural network architecture (convolutional filter size, types of layers and so on).</p>
"
2907,"<p>I trained some Gaussian process model with the Python library GPFlow on a dataset consisting of <span class=""math-container"">$(X, Y)$</span>, inputs and outputs, in a regression setting. This model gives me pretty good predictions in the sense that the relative error is small almost everywhere. I want to use the uncertainty as well, which is given in a GPFlow setting in the form of a standard deviation (STD) associated with every prediction. Here's my problem: I normalised both inputs and outputs before training (separately) using sklearn's StandardScaler (effectively making the data normally distributed with <span class=""math-container"">$0$</span> mean and unit STD). So the STD given by the model pertains to the scaled data. How do I ""rescale"" the uncertainty estimates of the GP to the actual data? Using the inverse_transform function of the output scaler makes little sense. This issue might be easier solvable if I scaled with a MinMaxScaler (squishing all data points into the unit interval) by dividing by the length of the range of the original output set (at least I think it works that way). But how about the case of the StandardScaler? Any insights will be appreciated!</p>
"
2908,"<p>How we should train a CNN model when training dataset contains only limited number of cases, and the trained model is supposed to predict class (label) for several other cases, which has not seen before? </p>

<p>Supposing there was hidden independent features describing the label repeated in previously seen cases of dataset.</p>

<p>For example let's consider we want to train a model to movement time series signals so it can predict some sort of activities (labels), and we have long record of movement signals (e.g. hours) for limited number of persons (e.g. 30) during various type of activities (e.g. 5), we may say these signals carry three type of hidden features:</p>

<ol>
<li><strong>Noise-features:</strong> Common features between every persons/activities </li>
<li><strong>Case-features:</strong> features mostly correlated with persons</li>
<li><strong>Class-features:</strong> features mostly correlated with activities</li>
</ol>

<p>We want to train the model such it learn mostly <code>Class-features</code> and eliminate 1st and 2nd types of features. </p>

<p>In conventional types of supervised-learning CNN learns all features how dataset represents them. In my test the model learned those 30 person activities very well, but on new persons it only predict randomly (i.e. 20% success). Over-fitted?</p>

<p>It seems there are three straight workaround to this:</p>

<ol>
<li>Extracting <code>class-features</code> and using a shallow classifier.</li>
<li>Increasing dataset wideness by recording signal on other persons: it can get so expensive or impossible in some situations. </li>
<li>Signal augmentation: by augmenting signals such it does not change <code>Class-features</code>, and making augmented <code>Case-features</code>. it seems to me harder than 1st workaround. </li>
</ol>

<p>Is there any other workaround on this type of problem?</p>

<p>For example specific type of training to use, to learn the model how different cases similarly follow <code>class-features</code> during class changes, eliminating <code>case-features</code> which varies case by case.</p>

<p>Sorry for very long question!</p>
"
2909,"<p>In actor critic, <a href=""https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d"" rel=""nofollow noreferrer"">The equations for calculating the loss in actor critic</a> are an </p>

<p><em>actor</em> loss (parameterized by <span class=""math-container"">$\theta$</span>) </p>

<p><span class=""math-container"">$$log[\pi_\theta(s_t,a_t)]Q_w(s_t,a_t)$$</span> </p>

<p>and a <em>critic</em> loss (parameterized by <span class=""math-container"">$w$</span>) </p>

<p><span class=""math-container"">$$r(s_t,a_t) + \gamma Q_w(s_{t+1}, a_{t+1}) - Q_w(s_{t}, a_t).$$</span></p>

<p>This is <a href=""https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits"">bootstrapping in experience replay</a>:</p>

<p><span class=""math-container"">$$
L_i(\theta_i) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \left[ \left(r + \gamma \max_{a'} Q(s', a'; \theta_i^-) - Q(s, a; \theta_i)\right)^2 \right]
$$</span></p>

<p>It is clear that bootstrapping is comparable to the critic loss, except that the <span class=""math-container"">$max$</span> operation is lacking from the critic.</p>

<p><strong>As i see it, (correct me if I'm wrong)</strong>:</p>

<p><span class=""math-container"">$Q(s_t,a_t) = V(s_{t+1}) + r_t$</span> where <span class=""math-container"">$a_t$</span> is the actual action that had been taken.</p>

<p>The critic, as I understand, estimates <span class=""math-container"">$V(s)$</span></p>

<p><strong>My question:</strong></p>

<p><strong>What exactly is the critic calculating?</strong></p>

<p><strong>What In actor critic outputs <span class=""math-container"">$Q(s_{t+1},a_{t+1})$</span>?</strong></p>

<p>It seems to me like the critic calculates the average next state <span class=""math-container"">$s_{t+1}$</span> value, over all possible actions, with their corresponding probabilities, yielding</p>

<p><span class=""math-container"">$Q(s_t, a_t) = r_t + \sum_{a_{t+1} \in A}P(a_{t+1}|s_t)V(s_{t+1})$</span></p>

<p>Which would mean that in order to get <span class=""math-container"">$Q(s_{t+1}, a_{t+1})$</span> for the above formula, I would need to calculate</p>

<p><span class=""math-container"">$Q(s_{t+1}, a_{t+1}) = r_{t+1} + \sum_{a_{t+2} \in A}P(a_{t+2}|s_{t+1})V(s_{t+2})$</span></p>

<p>Where <span class=""math-container"">$V(s_{t+2})$</span> is the critic output on <span class=""math-container"">$s_{t+2}$</span>, a state we get to by taking action <span class=""math-container"">$a_{t+1}$</span> from state <span class=""math-container"">$s_{t+1}$</span> but I am not sure that is indeed the meaning of the critic output and still it is unclear to me how to get <span class=""math-container"">$Q(s_{t+1}, a_{t+1})$</span> from actor critic.</p>

<p>If indeed that is what's being calculated, then why is it mathematically true that an improvement is being made? Or why does it make sense (even if not mathematically always true)?</p>

<hr>

<p>Practical use:</p>

<p>I want to use actor critic with experience replay in an environment with a large action space (could be continuous). Therefore, I cannot use the <span class=""math-container"">$max$</span> term. I need to understand the correct equation for the critic loss, and why it works.</p>
"
2910,"<p>Imagine that the agent receives a positive reward upon reaching a state . Once the state  has been reached the positive reward associated with it vanishes and appears somewhere else in the state space, say at state ′. The reward associated to ′ also vanishes when the agent visits that state once and re-appears at state . This goes periodically forever. Will discounted Q-learning converge to the optimal policy in this setup? Is yes, is there any proof out there, I couldn't find anything.</p>
"
2911,"<p>I am reviewing my Neural Network lectures and I have a doubt: My book's (Haykin) batch PTA describes a cost function which is defined over the set of the misclassified inputs. </p>

<p>I have always been taught to use MSE &lt; X as a stopping condition for the training process. Is the batch case different? Should I use as stopping condition size(misclassified) &lt; Y (and as a consequence when the weight change is very little)?</p>

<p>Moreover, the book uses the same symbol for both the training set and the misclassified input set. Does this mean that my training set changes each epoch? </p>

<p>Than you for any answer!</p>
"
2912,"<p>Most robotics challenges like Robocup soccer, micromouse and the Amazon picking challenge are oriented on fully autonomous systems. A motion controller gets started, the operator takes away his hands from the keyboard and then the system is solving the task by it's own. Such a technology might be state-of-the-art but my interests go more into the direction of low-tech robotics. That's a control paradigm which is working with a mouse and a keyboard no Artificial Intelligence is in the loop. This is called a teleoperated robot and the human takes his hands never away from the keyboard.</p>

<p>Realizing a plain teleoperation system is not that hard. Because the user input has to be transmitted only to the servo motor. To make things more complicated the system should provide annotations. That means, the human operator is doing a task and in the status bar a natural language description of the detected events is shown. For example “gripper pushes object”, “object in gripper”, “gripper releases object”.</p>

<p>The only problem is, that i don't how to annotate a teleoperation robot system. Is some kind of best practice method available for creating a semantic event recognition? Are neural networks are great choice?</p>
"
2913,"<p>I'm seeing a lot of examples involving games, or robot problems. What about how to make something else conform to this framework? How do you transform say a csv file of psychological data to determine the best life actions you can get from self report questionnaire?</p>
"
2914,"<p>A lot of research has been done to create the optimal (or ""smartest"") RL agent, using methods such as A2C. An agent can now beat humans at playing Go, Chess, Poker, Atari Games, DOTA, etc. But I think these kind of agents will never be a friend of humans, because humans won't play with a agent that always beats them. </p>

<p>How could we create an agent that doesn't outperform humans, but it has the human level skill, so that when it plays agains a human, the human is still motivated to beat it?</p>
"
2915,"<p>I have two Neural Network models (I use LSTM) that have different result on validation set (~100 samples data):</p>

<ul>
<li>Model A: Accuracy: ~91%, Loss: ~0.01</li>
<li>Model B: Accuracy: ~83%, Loss: ~0.003</li>
</ul>

<p>The size and the speed of both model is almost the same. So, which model should I choose?</p>
"
2916,"<p><strong>Note</strong>: I think the title is a bit too generic, so I'm open to suggestions on how to improve it.</p>

<p>I'm currently working with <a href=""https://arxiv.org/abs/1703.06870"" rel=""nofollow noreferrer"">Mask RCNN</a>, which does instance segmentation, but I believe the question is more related to the ""detection"" part of the network.</p>

<p>When detecting objects in an image, the network is trained on multiple scales, but I seem to remember there is a minimum scale of the objects to be detectable, depending (I think) on the anchors used when training and convolution prameters in the backbone of the network.</p>

<p>Is there a way to compute the minimum detectable scale? Or is it just related to the data used for training?</p>
"
2917,"<p>I have been researching about <strong>determining some key points on an image</strong>, in this case I'm gonna use <strong>cloth (top side of human body)</strong> pictures. I want to detect some corner points on those. </p>

<p>Example:</p>

<p><a href=""https://i.stack.imgur.com/bN9go.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bN9go.png"" alt=""Points on a t-shirt""></a></p>

<p>I have <strong>two</strong> solutions on my mind. <strong>One</strong> CNN with transpose layers resulting in <strong>heatmap</strong> where I can get points. <strong>The second</strong> is to get <strong>24 number</strong> as output from the model <strong>meaning 12(x,y) point</strong>. I don't know which one will be better. </p>

<p><strong>In face point detection, they use the second method. In human pose estimation, they use method one.</strong> So what do you suggest me to use? or do you have any new ideas? Thanks</p>
"
2918,"<p>I'm new to Reinforcement Learning and I'm having an issue trying to code a Q-Learning algorithm in Python. I don't understand why when I use this as my Q matrix update function (alp is my learning rate, rew is my reward(t+1)):</p>

<pre><code>Q[state, action] = Q[state, action] + alp*(rew)
</code></pre>

<p>I get a good result for my objective function but when I use the actual Q matrix update function (discount factor = 1):</p>

<pre><code>Q[state, action] = Q[state, action] + alp*(rew+Q[next_state, next_action]-Q[state, action])
</code></pre>

<p>I get terrible results! Any idea why? </p>

<p>This is my reward function, where I'm working with inventory values (Im = negative inventory, Ip = positive inventory):</p>

<pre><code>def reward(t):
    if Im[t] &gt; 0:
        return 0
    if Ip[t] == 0 and Im[t] == 0:
        return 100
    if Ip[t] &gt; 0 and Ip[t] &lt;= 5:
        return 25
    if Ip[t] &gt; 5 and Ip[t] &lt;= 10:
        return 10
    if Ip[t] &gt; 10:
        return 0
</code></pre>

<p>This is the part of my code for the Q-matrix Training: <a href=""https://pastebin.com/tE5aymdi"" rel=""nofollow noreferrer"">https://pastebin.com/tE5aymdi</a> Am I doing something wrong?</p>

<p>My objective function is a cost that I want to minimize <code>h*sumIp+b*sumIm</code>, h and b are constants and sumIp and sumIm are the <code>Im</code>/<code>Ip</code> for every day summed together, i.e., <code>h*sumIp</code> is my total cost of holding inventory and <code>b*sumIm</code> is my total cost of backorders.</p>

<p>My states are my available stock for each day, based on the average demand (dm). These are just the first 2 states, to give an example:</p>

<pre><code>def returnState(t):
    if Ip[t] &lt;= (dm*0.1):
        return 0
    elif Ip[t] &gt; (dm*0.1) and Ip[t] &lt;= (dm*0.2):
        return 1
</code></pre>

<p>I then choose which action should be chosen in that state by doing this (depending on if it will explore or exploit)</p>

<pre><code>rd = random.random()
if rd &lt; exploitation_p:
    action = np.where(Q[state,] == np.max(Q[state,]))[1]
    if np.size(action) &gt; 1:
        action = np.random.choice(action,1)
elif rd &gt;= exploitation_p:
    av_act = np.where(Q[state,] &lt; 999999)[1]
    action = np.random.choice(av_act,1)
action = int(action)
</code></pre>

<p>My actions are the quantity <code>Qt</code> to order so I then do <code>Qt[t] = returnAction(action)</code> where my returnAction function is the following (again just the first 2 as examples):</p>

<pre><code>def returnAction(action):
    Qbase = dm
    if action == 0:
        return Qbase
    if action == 1:
        return Qbase * 0.9
</code></pre>

<p>My <code>Ip</code> and <code>Im</code> depend on my ordered <code>Qt</code>, which is why I wanna optimize my ordered quantities. So then I can calculate my <code>Ip[t+1]</code> (as well as the reward(t+1)) and I repeat the same state and action selection processes to find out my <code>next_state</code> and <code>next_action</code>.</p>
"
2919,"<p>What are the practical and theoretical properties that commonly used loss functions have (in particular, in the context of neural networks)? </p>
"
2920,"<p>In the book ""Reinforcement Learning: An Introduction"", by Sutton and Barto, they provided the ""Q-learning prioritized sweeping"" algorithm, in which the model saves the next state and the immediate reward, for each state and action, that is, <span class=""math-container"">$Model(S_{t},A_{t}) \leftarrow S_{t+1}, R_{t+1}$</span>. </p>

<p>If we want to use ""SARSA prioritized sweeping"", should we save ""next state, immediate reward, and next action"", that is, <span class=""math-container"">$Model(S_{t},A_{t}) \leftarrow  S_{t+1}, R_{t+1}, A_{t+1}$</span>?</p>
"
2921,"<p>In this <a href=""https://www.youtube.com/watch?v=Jw3ZnWFjDfM"" rel=""nofollow noreferrer"">video</a>, the lecturer states that <span class=""math-container"">$R(s)$</span>, <span class=""math-container"">$R(s, a)$</span> and <span class=""math-container"">$R(s, a, s')$</span> are equivalent representations of the reward function. Intuitively, this is the case, according to the same lecturer, because <span class=""math-container"">$s$</span> can be made to represent the state and the action. Furthermore, apparently, the Markov decision process would change depending on whether we use one representation or the other.</p>

<p>I am looking for a formal proof that shows that these representations are equivalent. Moreover, how exactly would the Markov decision process change if we use one representation over the other? Finally, when should we use one representation over the other and why are there three representations? I suppose it is because one representation may be more convenient than another in certain cases: which cases? How do you decide which representation to use?</p>
"
2922,"<p>I'm building a customer assistant chatbot in Python, so a text classification task, and I have available more or less 7 hundred sentences of average length 15 words (unbalanced class).
What do you think, knowing that I have to do an oversampling, is this dataset large enough?</p>
"
2923,"<p>I have a dataset which I have loaded as a data frame in Python. It consists of 21392 rows (the data instances, each row is one sample) and 1972 columns (the features). The last column i.e. column 1972 has string type labels (14 different categories of target labels). I would like to use a CNN to classify the data in this case and predict the target labels using the available features. This is a somewhat unconventional approach though it seems possible. However, I am very confused on how the methodology should be as I could not find any sample code/ pseudo code guiding on using CNN for Classifying non-image data, either in Tensorflow or Keras. Any help in this regard will be highly appreciated. Cheers!</p>
"
2924,"<p>I've seen recently a video with Dr. Joanne Pransky on Youtube about a woman who is working as a robot psychiatrist. Her daily job is to speak with robots about their mental problems. That means, if a robot owner beliefs, that his robot needs advice from a doctor he can visit the psychiatrist and has to pay the bill after the visit. If i understood the woman in the video correct, she is speaking with robots about programming issues. Some of the newer models have emotion oriented subroutines, similar to the famous Sophia robot from Hanson robotics. But even a normal EV3 Mindstorms brick can have advanced problems. For example if it was programmed with behavior based algorithm which are forcing the robot to stay away from the wall. This can result into troubles with low level control loops.</p>

<p>At the first look the video was funny. Because what AI researchers have promised is the opposite. According to most researchers the social role of robots is that they are working as a doctor. And the researchers also have promised that robots will work in the factory for humans. What was presented in the video was the opposite. That means, robots have a demand for medical help and they have a demand for pizza which are produced in the factory by human workers. Is this the future? Are robots useless machines as default which are producing costs like a house dog?</p>
"
2925,"<p>In my understanding, Q-learning gives you a deterministic policy. However, can we use some technique to build a meaningful stochastic policy from the learned Q values? I think that simply using a softmax won't work. </p>
"
2926,"<p>I tried to do transfer learning using 2 methods :</p>

<ul>
<li>GoogleNet with Caffe framework training on Nvidia DIGITS</li>
<li>Inception V3 with Tensorflow framework (no DIGITS)</li>
</ul>

<p>My dataset is quite large with 1 million that separated to 28 classes.I trained both of them with similar parameters, but GoogleNet performs better at real Inference test.</p>

<p>Is there any case or chance that Googlenet actually performs better than V3? What is probably the cause of it?</p>
"
2927,"<p>Let assume that we have dataset of variables (random events), I apriori would like to set dependency conditions between some of them and perform structure learning to figure out the rest of the net. How it can be done practically (e.g. some libs like bnlearn etc.) or at least in theory?</p>

<p>I was trying to google it but haven't found anything related</p>
"
2928,"<p>I have chromosomes with floating point representation with values between <code>0</code> and <code>1</code>. For example-</p>

<p>Let 
<code>p1=[0.1,0.2,0.3</code> and <code>p2=[0.5,0.6,0.7]</code> be two parents. Both comply with the set of constraints. However, the children produced by 1 point crossover, we get <code>c1=[0.1, 0.6, 0.7]</code> and <code>c2=[0.5, 0.2, 0.3]</code> out of which 1 or both may not comply with the given constraints.</p>

<p>A similar scenario can also occur with small perturbation of values due to mutation strategy. Correct me if I am wrong in the belief that such kind of scenarios might arise irrespective of the strategy employed for crossover and mutation. What are the options to handle such kind of cases?</p>

<p>Edit-
In my case, the major constraint is- <span class=""math-container"">$$ p1[1]*p1[2] - k*p1[0] \geq 0 $$</span> for any chromosome <span class=""math-container"">$p1$</span>. For the example above we can take <span class=""math-container"">$k=0.3$</span> which renders <code>c2</code> infeasible. </p>
"
2929,"<p>An algorithmic Deep Q trader I have been working on has recently been modified to allow the passing in of multiple input states (Price, Volume, RSI) I am also looking to include further indicators later.</p>

<p>When working with multiple inputs with Keras, are there particular layers or structure I should be implementing within the model? Structure is currently like this (Where state size = 14 and action size = 3):</p>

<pre><code>    visible = Input(shape=(14,))
    visible2 = Input(shape=(14,))
    visible3 = Input(shape=(14,))

    input_layer = concatenate([visible, visible2, visible3])
    x = Embedding(input_dim=self.state_size, output_dim=self.action_size)(input_layer)
    a = Dense(64, activation=""relu"")(input_layer)
    b = Dense(32, activation=""relu"")(a)
    c = Dense(8, activation=""relu"")(b)
    output = Dense(self.action_size, activation=""linear"")(c)
    model = Model(inputs=[visible, visible2, visible3], outputs=output)
    model.compile(loss=""mse"", optimizer=Adam(lr=0.001))
    return model. 
</code></pre>

<p>I was considering including an additional dense layer after each visible layer before concatenation, however was unsure how this would affect the learning rate, I am also considering using dropout layers to help reduce the effects of overfitting on a smaller dataset.</p>

<p>Many thanks for any tips - Apologies I have only recently became introduced to this field.</p>
"
2930,"<p>I've used a table to represent the Q function, while an agent is being trained to catch the cheese without touching the walls.</p>

<p>The first and last row (and column) of the matrix are associated with the walls. I placed in last cell a cheese that agent must catch while being training.</p>

<p>So far, I've done it with dynamic states and, when necessary, I resized matrix with new states. I've used four actions (up, left, right and down). </p>

<p>I would like now to use an ANN to represent my Q function. How do I do that? What should be the input and output of such neural network?</p>
"
2931,"<p>LSTM is supposed to be the right tool to capture path-dependency in time-series data.</p>

<p>I decided to run a simple experiment (simulation) to assess the extent to which LSTM is better able to <em>understand</em> path-dependency.</p>

<p>The setting is very simple. I just simulate a bunch (N=100) paths coming from 4 different data generating processes. Two of these processes represent a <em>real</em> increase and a <em>real</em> decrease, while the other two <em>fake</em> trends that eventually revert to zero.</p>

<p>The following plot shows the simulated paths for each category:</p>

<p><a href=""https://i.stack.imgur.com/cqa2v.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cqa2v.png"" alt=""Simulated paths""></a></p>

<p>The candidate machine learning algorithm will be given the first 8 values of the path ( t in [1,8] ) and will be trained to predict the subsequent movement over the last 2 steps. </p>

<p>In other words:</p>

<ul>
<li><p>the feature vector is <code>X = (p1, p2, p3, p4, p5, p6, p7, p8)</code></p></li>
<li><p>the target is <code>y = p10 - p8</code></p></li>
</ul>

<p>I compared LSTM with a simple Random Forest model with 20 estimators. Here are the definitions and the training of the two models, using Keras and scikit-learn:</p>

<pre><code># LSTM
model = Sequential()
model.add(LSTM((1), batch_input_shape=(None, H, 1), return_sequences=True))
model.add(LSTM((1), return_sequences=False))
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
history = model.fit(train_X_LS, train_y_LS, epochs=100, validation_data=(vali_X_LS, vali_y_LS), verbose=0)
</code></pre>

<pre><code># Random Forest
RF = RandomForestRegressor(random_state=0, n_estimators=20)
RF.fit(train_X_RF, train_y_RF);
</code></pre>

<p>The results are the summarized by the following scatter plots:</p>

<p><a href=""https://i.stack.imgur.com/81fRi.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/81fRi.png"" alt=""enter image description here""></a></p>

<p>As you can see, the Random Forest model is clearly outperforming the LSTM. The latter seems to be not able to distinguish between the <em>real</em> and the <em>fake</em> trends.</p>

<h3>Do you have any idea to explain why this is happening?</h3>

<h3>How would you modify the LSTM model to make it better at this problem?</h3>

<p>Some remarks:</p>

<ul>
<li>The data points are divided by 100 to make sure gradients do not explode</li>
<li>I tried to increase the sample size, but I noticed no differences</li>
<li>I tried to increase the number of epochs over which the LSTM is trained, but I noticed no differences (the loss becomes stagnant after a bunch of epochs)</li>
<li>You can find the code I used to run the experiment <a href=""https://pastebin.com/QL2DuKDE"" rel=""nofollow noreferrer"">here</a> </li>
</ul>
"
2932,"<p>I have a data analysis problem that I can reduce to one similar to analyzing the trajectories in the images below. These images show the tracks of subatomic particles interacting in a bubble chamber. </p>

<p>It's pretty obvious that by eye, easily discernible patterns can be seen. I want very much to know more about how classification and segmentation can be done using neural networks for this type of image. </p>

<p>These images are binary. The trajectory is either at a point in the image or it isn't. As can be seen, trajectories cross over one another, Some data appears to be missing in otherwise smooth curves, at arbitrary points along those curves. (My data may be more sparse in this respect.)</p>

<p>A typical paper on bubble chamber analysis that I would find deals with the analysis of the physics after trajectories have been classified and segmented. </p>

<p>Can anyone identify some papers that address this or something similar in the context of neural networks? I am not able to find anything recent on automated methods at all, but my google fu may not be up to the challenge. (By the way, I am less interested in some of the parametric methods like Hough Transforms. I'd like to focus on the neural approach.)</p>

<p>(I posted <a href=""https://ai.stackexchange.com/questions/10036/pixel-level-detection-of-each-object-of-the-same-class-in-an-image"">this previous question</a> which wasn't quite as specific as this one. I hope there is some available research in this area related to physics that might give me some insights that are more directly related to my problem.)</p>

<p><a href=""https://i.stack.imgur.com/g936O.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g936O.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/uyuk6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uyuk6.jpg"" alt=""enter image description here""></a></p>
"
2933,"<p>I'm a beginner in machine learning and I was trying to make a test neural network for digits recognition from scratch using Numpy. I used MNIST dataset for training and testing. Input layer have 28*28 neurons which correspond to each pixel of image that must be recognized. Output layer have 10 neurons which correspond to each digit (0-9), and return values from 0 to 1 which mean chance that the corresponding digit is displayed on the image. Class Layer represents a separate layer and contains links to previous and next layer (if prevLayer is None, the current layer is input layer; if nextLayer is None, the current layer is output). The forward() method is responsible for passing data through neural network. The backprop() method is responsible for training of neural network via Backpropagation algorithm. A Layer object contains weights (W) between previous and current layer (input layer object doesn't contain weights). 'data_in' property contains a vector of calculated values before passing them into activation function. Property 'data' contains the values after activation function. But, unfortunately, it doesn't work: returned value of loss function doesn't decrease during training and neural network returns the same result during testing. I assume that bugs might be associated with backprop() and softmax_derivatime() methods. I tried in vain to find all bugs. Here's my code:</p>

<pre><code>import numpy as np

def ReLU(x):
    return np.maximum(0, x)

def ReLU_derivative(x):
    return np.greater(x, 0).astype(int)

def softmax(x):
    shift = x - np.max(x)
    return np.exp(shift) / np.sum(np.exp(shift))

def softmax_derivative(x):
    sm_array = softmax(x)
    J = np.zeros((x.size, x.size))
    for i in range(x.size):
        for j in range(x.size):
            delta = np.equal(i, j).astype(int)
            J[j, i] = sm_array[0][i] * (delta - sm_array[0][j])
    return J

class Layer:
    def __init__(self, size, prev_layer=None):
        self.size = size
        self.prevLayer = prev_layer
        self.nextLayer = None
        self.data = None
        self.data_in = None
        if prev_layer is not None:
            self.prevLayer.nextLayer = self
            self.W = np.random.random((self.prevLayer.size, size))
            self.W_bias = np.array([np.random.random(size)])
        else:
            self.W = None
            self.W_bias = None

    def forward(self):
        if self.prevLayer is not None:
            self.data_in = np.dot(self.prevLayer.data, self.W)
            self.data_in += np.dot([[1]], self.W_bias)
            if self.nextLayer is not None:
                self.data = ReLU(self.data_in)
                self.nextLayer.forward()
            else:
                self.data = softmax(self.data_in)
        else:
            self.nextLayer.forward()

    def backprop(self, expected_output=None, prev_delta=None):
        if prev_delta is None:
            #print(self.data_in)
            delta = np.dot(-(expected_output - self.data), softmax_derivative(self.data_in))
            delta_bias = delta
        else:
            delta = np.dot(prev_delta, self.nextLayer.W.T) * ReLU_derivative(self.data_in)
            delta_bias = np.dot(prev_delta, self.nextLayer.W_bias.T) * ReLU_derivative(self.data_in)
        training_velocity = 0.1
        W_dif = np.dot(self.prevLayer.data.T, delta) * training_velocity
        W_bias_dif = np.dot([[1]], delta_bias) * training_velocity
        if self.prevLayer.prevLayer is not None:
            self.prevLayer.backprop(prev_delta=delta)
        self.W -= W_dif
        self.W_bias -= W_bias_dif

f_images = open(""train-images.idx3-ubyte"", ""br"")
f_images.seek(4)
f_labels = open(""train-labels.idx1-ubyte"", ""br"")
f_labels.seek(8)
images_number = int.from_bytes(f_images.read(4), byteorder='big')
rows_number = int.from_bytes(f_images.read(4), byteorder='big')
cols_number = int.from_bytes(f_images.read(4), byteorder='big')

input_layer = Layer(rows_number*cols_number)
hidden_layer1 = Layer(rows_number*cols_number*7//10, input_layer)
hidden_layer2 = Layer(rows_number*cols_number*7//10, hidden_layer1)
output_layer = Layer(10, hidden_layer2)
digits = np.array([np.zeros(10)])

input_image = np.array([np.zeros(rows_number * cols_number)])
for k in range(images_number):
    for i in range(rows_number):
        for j in range(cols_number):
            input_image[0][i*cols_number+j] = int.from_bytes(f_images.read(1), byteorder='big') / 255.0 * 2 - 1
    input_layer.data = input_image
    input_layer.forward()
    current_digit = int.from_bytes(f_labels.read(1), byteorder='big')
    digits[0][current_digit] = 1
    output_layer.backprop(expected_output=digits)
    print(np.sum((digits - output_layer.data)**2)/2)
    digits[0][current_digit] = 0
    if((k+1) % 1000 == 0):
        print(str(k+1) + "" / "" + str(images_number))
f_images.close()
f_labels.close()

f_images = open(""t10k-images.idx3-ubyte"", ""br"")
f_images.seek(4)
f_labels = open(""t10k-labels.idx1-ubyte"", ""br"")
f_labels.seek(8)
images_number = int.from_bytes(f_images.read(4), byteorder='big')
rows_number = int.from_bytes(f_images.read(4), byteorder='big')
cols_number = int.from_bytes(f_images.read(4), byteorder='big')

for k in range(images_number):
    for i in range(rows_number):
        for j in range(cols_number):
            input_image[0][i*cols_number+j] = int.from_bytes(f_images.read(1), byteorder='big')
    input_layer.data = input_image
    input_layer.forward()
    current_digit = int.from_bytes(f_labels.read(1), byteorder='big')
    print(output_layer.data)

f_images.close()
f_labels.close()
</code></pre>

<p>I would appreciate for any help. Thanks in advance!</p>
"
2934,"<p>Suppose that we are doing machine translation. We have a conditional language model  with attention where we are are trying to predict a sequence <span class=""math-container"">$y_1, y_2, \dots, y_J$</span> from <span class=""math-container"">$x_1, x_2, \dots x_I$</span>: <span class=""math-container"">$$P(y_1, y_2, \dots, y_{J}|x_1, x_2, \dots x_I) = \prod_{j=1}^{J} p(y_j|v_j, y_1, \dots, y_{j-1})$$</span> where <span class=""math-container"">$v_j$</span> is a context vector that is different for each <span class=""math-container"">$y_j$</span>. Using an RNN with a encoder-decoder structure, each element <span class=""math-container"">$x_i$</span> of the input sequence and <span class=""math-container"">$y_j$</span> of the output sequence is converted into an embedding <span class=""math-container"">$h_i$</span> and <span class=""math-container"">$s_j$</span> respectively: <span class=""math-container"">$$h_i = f(h_{i-1}, x_i) \\ s_j = g(s_{j-1},[y_{j-1}, v_j])$$</span> where <span class=""math-container"">$f$</span> is some function of the previous input state <span class=""math-container"">$h_{i-1}$</span> and the current input word <span class=""math-container"">$x_i$</span> and <span class=""math-container"">$g$</span> is some function of the previous output state <span class=""math-container"">$s_{j-1}$</span>, the previous output word <span class=""math-container"">$y_{j-1}$</span> and the context vector <span class=""math-container"">$v_j$</span>.</p>

<p>Now, we want the process of predicting <span class=""math-container"">$s_j$</span> to ""pay attention"" to the correct parts of the encoder states (context vector <span class=""math-container"">$v_j$</span>). So: <span class=""math-container"">$$v_j = \sum_{i=1}^{I} \alpha_{ij} h_i$$</span> where <span class=""math-container"">$\alpha_{ij}$</span> tells us how much weight to put on the <span class=""math-container"">$i^{th}$</span> state of the source vector when predicting the <span class=""math-container"">$j^{th}$</span> word of the output vector. Since we want the <span class=""math-container"">$\alpha_{ij}$</span>s to be probabilities, we use a softmax function on the similarities between the encoder and decoder states: <span class=""math-container"">$$\alpha_{ij} = \frac{\exp(\text{sim}(h_i, s_{j-1}))}{\sum_{i'=1}^{I} \exp(\text{sim}(h_i, s_{j-1}))}$$</span></p>

<p>Now, in additive attention, the similarities of the encoder and decoder states are computed as: <span class=""math-container"">$$\text{sim}(h_i, s_{j}) = \textbf{w}^{T} \text{tanh}(\textbf{W}_{h}h_{i} +\textbf{W}_{s}s_{j})$$</span></p>

<p>where <span class=""math-container"">$\textbf{w}$</span>, <span class=""math-container"">$\textbf{W}_{h}$</span> and <span class=""math-container"">$\textbf{W}_{s}$</span> are learned attention parameters using a  one-hidden layer feed-forward network. </p>

<p>What is the intuition behind this definition? Why use the <span class=""math-container"">$\text{tanh}$</span> function? I know that the idea is to use one layer of a neural network to predict the similarities. </p>

<p><strong>Added.</strong> This description of machine translation/attention is based on the Coursera course <a href=""https://www.coursera.org/learn/language-processing/lecture/1nQaG/attention-mechanism"" rel=""nofollow noreferrer"">Natural Language Processing</a>.</p>
"
2935,"<p>In terms of sample complexity, is it just as easy to learn with observation space A as observation A with 10 zero's appended?</p>

<p>For example: OpenAI Gym's fetch robotics environment has a state space observation space.</p>

<p>If there is a block, the block's state is appended.
If there is not a block, 0's are appended instead of the same size as the block's state.</p>

<p>However, I noticed when training that pick and place works with 1 block using HER, but when appending another block of 0s it does not work. </p>
"
2936,"<p>I am studying a knowledge base (KB) from the book ""Artificial Intelligence: A Modern Approach"" (by Stuart Russell and Peter Norvig) and from <a href=""https://www.cs.princeton.edu/courses/archive/fall16/cos402/lectures/402-lec12.pdf"" rel=""nofollow noreferrer"">this series of slides</a>.</p>

<p>A formula is satisfiable if there is some assignment to the variables that makes the formula evaluate to true. For example, if we have the boolean formula <span class=""math-container"">$A \land B$</span>, then the assignments <span class=""math-container"">$A=true$</span> and <span class=""math-container"">$B= true$</span> make it satisfiable. Right?</p>

<p>But what does it mean for a KB to be consistent? The definition (given at slide 14 of <a href=""https://www.cs.princeton.edu/courses/archive/fall16/cos402/lectures/402-lec12.pdf"" rel=""nofollow noreferrer"">this series of slides</a>) is: </p>

<blockquote>
  <p>a KB is consistent with formula <span class=""math-container"">$f$</span> if <span class=""math-container"">$M(KB \cup \{ f \})$</span> is non-empty (there is a world in which KB is true and <span class=""math-container"">$f$</span> is also true).</p>
</blockquote>

<p>Can anyone explain this part to me with an example?</p>
"
2937,"<p>I've recently started reading a book about deep learning. The book is titled ""Grokking Deep Learning"" (by Andrew W Trask). In chapter 3 (pages 44 and 45), it talks about multiplying vectors using dot product and element-wise multiplication. For instance, taking 3 scalar inputs (vector) and 3 vector weights (matrix) and multiplying. </p>

<p>From my understanding, when multiplying vectors the size needs to be identical. The concept I have a hard time understanding is multiplying vectors by a matrix. The book gives an example of an 1x4 vector being multiplied by 4x3 matrix. The output is an 1x3 vector. I'm am confused because I assumed multiplying vector by matrix needs the same number of columns, but I have read that the matrices need rows equal to the vectors columns. </p>

<p>If I do not have an equal number of columns, how does my deep learning algorithm multiply each input in my vector by a corresponding weight?</p>
"
2938,"<p>In the context of RL, there is the notion of on-policy and off-policy algorithms. I roughly understand the difference between on-policy and off-policy algorithms. Moreover, in RL, there's also the notion of online and offline learning. </p>

<p>What is the relation (including the differences) between online learning and on-policy algorithms? Similarly, what is the relation between offline learning and off-policy algorithms? </p>

<p>Finally, is there any relation between online (or offline) learning and off-policy (or on-policy) algorithms? For example, can an on-policy algorithm perform ""offline"" learning? If yes, can you explain why? </p>
"
2939,"<p>I am trying to model a simple Neural Net to classify data amongst 14 classes. The data is quite high dimensional, with 21392 rows and 1970 columns, with the last column being the labels (which have encoded into integral values for classification purposes). I am referring to the proposed architecture in <a href=""https://www.kaggle.com/azzion/iris-data-set-classification-using-neural-network"" rel=""nofollow noreferrer"">https://www.kaggle.com/azzion/iris-data-set-classification-using-neural-network</a>, where it is used on Iris data to get a conventional learning curve. But, my learning curve is coming to be something unusual, which I haven't seen before.</p>

<p><a href=""https://i.stack.imgur.com/KUaT6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KUaT6.png"" alt=""enter image description here""></a>
Does my learning curve graph signify that the model performs poorly (as it doesn't go down like a conventional one using the Gradient Descent Optimizer), or is it just some point I am missing? Any comments in this regard would be appreciated. Thanks!</p>

<pre><code>import pandas as pd
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize

df = pd.read_csv(""final_merged_data.csv"", parse_dates = ['StartTime'],index_col = 'StartTime')

# print first 5 rows
df.head()
# list unique values of classes in FunctionalGroup
df.FunctionalGroup.unique()
df.dropna()
df[""Class""] = df[""Class""].map({
    ""A"": 0,
    ""B"": 1,
    ""C"": 2,
    ""D"": 3,
    ""E"": 4,
    ""F"": 5,
    ""G"": 6,
    ""H"":7,
    ""I"": 8,
    ""J"" : 9,
    ""K"":10,
    ""L"" : 11,
    ""M"" : 12,
    ""N"" : 13
}).astype(int)

x_train = df.iloc[:,0:-1]
y_train = df.iloc[:,-1]

new_y = []
for i in y_train:
    a = [0, 0, 0, 0, 0, 0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0]
    a[i] = 1
    new_y.append(a)

columns = list(x_train)
X = pd.DataFrame.as_matrix(x_train, columns=columns)
Y = np.array(new_y)

#flatten the features for feeding into network base layer

X_train_flatten = X.reshape(X.shape[0],-1).T
Y_train_flatten = Y.reshape(Y.shape[0],-1).T
print(""No of training (X):""+str(X_train_flatten.shape))
print(""No of training (X):""+str(Y_train_flatten.shape))

#Normalize
XX_train_flatten = normalize(X_train_flatten)
YY_train_flatten = normalize(Y_train_flatten)


# creating the placeholders for X &amp; Y
def create_placeholders(n_x, n_y):
    X = tf.placeholder(shape=[n_x, None], dtype=tf.float32)
    Y = tf.placeholder(shape=[n_y, None], dtype=tf.float32)

    return X, Y


# initialize parameter
def initialize_parameters():
    W1 = tf.get_variable(""W1"", [14, 1970], initializer=tf.zeros_initializer())  # tf.contrib.layers.xavier_initializer())
    b1 = tf.get_variable(""b1"", [14, 1], initializer=tf.zeros_initializer())

    parameters = {""W1"": W1,
                  ""b1"": b1}

    return parameters


# forward propogation
def forward_propagation(X, parameters):
    W1 = parameters['W1']
    b1 = parameters['b1']

    Z1 = tf.add(tf.matmul(W1, X), b1)

    return Z1


# compute function
def compute_cost(Z1, Y):
    logits = tf.transpose(Z1)
    labels = tf.transpose(Y)

    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))

    return cost

# Running the NN !!
tf.reset_default_graph()
(n_x, m) = X_train_flatten.shape       # shape of X
n_y = Y_train_flatten.shape[0]         # shape of Y
X, Y = create_placeholders(n_x,n_y)    # creating placeholder
tf.set_random_seed(42)
p = initialize_parameters()            # initialize parameter
Z6 = forward_propagation(X,p)          # forward prop
y_softmax = tf.nn.softmax(Z6)          # softmax function
cost = compute_cost(Z6,Y)              # cost function
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.01).minimize(cost)  # gradient descent, backprop, update,optimiz
sess = tf.Session()
sess.run(tf.global_variables_initializer())    #initializae
par = sess.run(p)
Y_pred = sess.run(Z6,feed_dict={X:X_train_flatten})    #forward prop test
cost_value = sess.run(cost,feed_dict={Z6:Y_pred,Y:Y_train_flatten})  #cost function test - First cost function
costs =[]
for i in range(0,2000):                # 2000 epoch !!
    _,new_cost_value = sess.run([optimizer, cost], feed_dict={X: X_train_flatten, Y: Y_train_flatten})
    costs.append(new_cost_value)

p = sess.run(p)                        # parameter saving
y_softmax = sess.run(y_softmax,feed_dict={X: X_train_flatten, Y: Y_train_flatten})    # running softmax

plt.plot(np.squeeze(costs))            # plot
plt.ylabel('cost')
plt.xlabel('iterations (per tens)')
plt.title(""Learning rate ="" + str(.01))
plt.show()
plt.savefig('LearningRate_SimpleNN.png')

#testing prediction !!
correct_prediction = tf.equal(tf.argmax(y_softmax), tf.argmax(Y_train_flatten))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

print(""the Accuracy is :""+str(sess.run(accuracy, feed_dict={X: X_train_flatten, Y: Y_train_flatten})))
</code></pre>
"
2940,"<p>The update rules for Q-learning and SARSA each are as follows:</p>

<p>Q Learning:</p>

<p><span class=""math-container"">$$Q(s_t,a_t)←Q(s_t,a_t)+α[r_{t+1}+γ\max_{a'}Q(s_{t+1},a')−Q(s_t,a_t)]$$</span></p>

<p>SARSA:</p>

<p><span class=""math-container"">$$Q(s_t,a_t)←Q(s_t,a_t)+α[r_{t+1}+γQ(s_{t+1},a_{t+1})−Q(s_t,a_t)]$$</span></p>

<p>I understand the theory that SARSA performs 'on-policy' updates, and Q-learning performs 'off-policy' updates.</p>

<p>At the moment I perform Q-learning by calculating the target thusly: </p>

<pre><code>target = reward + self.y * np.max(self.action_model.predict(state_prime))
</code></pre>

<p>Here you can see I pick the maximum for the Q-function for state prime (i.e. greedy selection as defined by maxQ in the update rule). If I were to do a SARSA update and use the same on-policy as used when selecting an action, e.g. ϵ-greedy, would I basically change to this:</p>

<pre><code>if np.random.random() &lt; self.eps:
    target = reward + self.y * self.action_model.predict(state_prime)[random.randint(0,9)]
else:
    target = reward + self.y * np.max(self.action_model.predict(state_prime))
</code></pre>

<p>So sometimes it will pick a random future reward based on my epsilon greedy policy?</p>
"
2941,"<p>I'm using Q-learning to train an agent to play a board game (e.g. chess, draughts or go). </p>

<p>The agent takes an action while in state <span class=""math-container"">$S$</span>, but then what is the next state (that is,  <span class=""math-container"">$S'$</span>)? Is <span class=""math-container"">$S'$</span> now the board with the piece moved as a result of taking the action, or is <span class=""math-container"">$S'$</span> the state the agent encounters after the other player has performed his action (i.e. it's this agent's turn again)?</p>
"
2942,"<p>I am having trouble understanding inference graphs. In the diagram below I understand the graph on the left (forward graph) where the arrows describe the direction that data flows when training for example.</p>

<p>How should I understand the arrows on the inference graph to the right? Are they just the direction in which back-propagation is occurring?</p>

<p><a href=""https://i.stack.imgur.com/lxIRr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lxIRr.png"" alt=""enter image description here""></a></p>

<p><em><a href=""https://arxiv.org/pdf/1605.06432.pdf"" rel=""nofollow noreferrer"">Image reference</a></em></p>
"
2943,"<p>I am playing with a deep Q-learning algorithm in my own environment. The network can perform well as long as there is only one enemy. My agent can perform the following actions:</p>

<ol>
<li><code>do_nothing</code></li>
<li><code>prepare_for(e)</code></li>
<li><code>attack(e)</code> </li>
</ol>

<p>where <code>e</code> is some enemy. </p>

<p>In the case of two enemies, the action vector has 5 elements:</p>

<pre><code>|   0       |      1          |      2      |        3         |     4      |
-----------------------------------------------------------------------------
|do_nothing | prepare_for(e1) |  attack(e1) |  prepare_for(e2) | attack(e2) |
-----------------------------------------------------------------------------
</code></pre>

<p>After a couple of episodes, the agent always starts picking the first <code>do_nothing</code> action, which is not desired. Changing reward for <code>do_nothing</code> action is not helping, even using significantly higher negative reward, than for other actions. </p>

<p>There is no problem with the environment with only one enemy. (Only using columns 0, 1, 2). I feel like my action encoding can be the issue, but I can't figure it out, how to fix it. Any suggestions?</p>
"
2944,"<p>I've read some surveys about cognitive architectures from the past. SOAR, ACT-R, Clarion and ICARUS are prominent examples for realizing a symbolic system which can reasoning about different domains. For a better understanding, I've created a prototype for a new cognitive architecture from scratch. What the name of the software will be is unclear, but a first screenshot is available.</p>

<p>The system contains of a GUI in which knowledge containers are visible. Each box is a tkinter widget in which normal text can be entered. The content entered into the boxes is oriented to what my understanding of a cognitive architecture is. According to the literature such a system contains of goals, evens, tasks, rules and a linked list which stores procedures.</p>

<p>The problem is, that the prototype isn't working. That means, it is only a GUI without any functionality. The reason is, that my understanding of cognitive architectures is too low and something apart has to be implemented. My question is: what is the next step? What should i program to realize a working cognitive architecture?</p>

<p><a href=""https://i.stack.imgur.com/wvjQ4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wvjQ4.png"" alt=""screenshot""></a></p>
"
2945,"<p>Till where I could learn, reinforcement learning enables to optimize actions to get more reward. There are a good variety of RL algorithms: model-based and model-free; on-policy and off-policy; continuous and discrete action and state spaces, and so on.</p>

<p>The problem I am facing is like this one:</p>

<ul>
<li>Partially known discrete state space</li>
<li>Partially known action space</li>
<li>Unknown continuous rewards</li>
<li>Unknown transition probabilities</li>
</ul>

<p>Is there any reinforcement learning that meet these requirements? In that case, which ones and <strong>how</strong> the action and state spaces get scaled?</p>
"
2946,"<p>In the context of reinforcement learning, a policy, <span class=""math-container"">$\pi$</span>, is often defined as a function from the space of states, <span class=""math-container"">$\mathcal{S}$</span>, to the space of actions, <span class=""math-container"">$\mathcal{A}$</span>, that is, <span class=""math-container"">$\pi : \mathcal{S} \rightarrow \mathcal{A}$</span>. This function is the ""solution"" to a problem, which is represented as a Markov decision process (MDP), so we often say that <span class=""math-container"">$\pi$</span> is a solution to the MDP. In general, we want to find the optimal policy <span class=""math-container"">$\pi^*$</span> for each MDP <span class=""math-container"">$\mathcal{M}$</span>, that is, for each MDP <span class=""math-container"">$\mathcal{M}$</span>, we want to find the policy which would make the agent behave optimality (that is, obtain the highest ""cumulative future discounted reward"", or, in short, the highest ""return"").</p>

<p>It is often the case that, in RL algorithms, e.g. Q-learning, people often mention ""policies"" like <span class=""math-container"">$\epsilon$</span>-greedy, greedy, soft-max, etc., without ever mentioning that these policies are or not solutions to some MDP. It seems to me that these are two different types of policies: for example, the ""greedy policy"" always chooses the action with the highest expected return, no matter which state we are in; similarly, for the ""<span class=""math-container"">$\epsilon$</span>-greedy policy""; on the other hand, a policy which is a solution to a MDP is a map between states and actions. </p>

<p>What is then the relation between a policy which is the solution to a MDP and a policy like <span class=""math-container"">$\epsilon$</span>-greedy? Is a policy like <span class=""math-container"">$\epsilon$</span>-greedy a solution to any MDP? How can we formalise a policy like <span class=""math-container"">$\epsilon$</span>-greedy in a similar way that I formalised a policy which is the solution to a MDP?</p>

<p>I understand that ""<span class=""math-container"">$\epsilon$</span>-greedy"" can be called a policy, because, in fact, in algorithms like Q-learning, they are used to select actions (i.e. they allow the agent to behave), and this is the fundamental definition of a policy.</p>
"
2947,"<p>Can traditional neural networks be combined with spiking neural networks? And can there be training algorithms for such hybrid network? Does such hybrid network model biological brains? </p>

<p>As I understand, brains contain only spiking networks and traditional networks are more or less crude approximation of them. But we can imagine that evolutionary computing can surpass the biological evolution and so the new structure can be created that are better than mind. And that is why the question about such tradition-spiking hybrid neural networks should be interesting.</p>
"
2948,"<p>I am studying csp problems in artificial intelligence.I have so much efforts on written in N-Queens with csp.
I confused with the output of my AC-3 algorithm with MAC-3 algorithm.
Please check me with my coding.
You can download my code <a href=""https://github.com/ThuriyaThwin/ThesisCode/archive/master.zip"" rel=""nofollow noreferrer"">here.</a>The instructions to run are include in README.md. Thanks.</p>
"
2949,"<p>I have continual simulated data of million sentences of two simulated persons talking to each other in a room and I want to model one of the persons speech. Now, during this period things in the room can change. Let's say, one of them says ""Where is the book?"" The other one responds ""I placed the book on the bookshelf"". Now during time, the position of the book changes, so the question Where is the book? does not have stationary answer i.e the answer changes during time. However, in general the answer has to be ""The book is at some_location"" and not something else. Also, the mentioning that the book is placed on the bookshelf can be sometimes 10, 100 or 1000 sentences before the question ""Where is the book?""</p>

<p>How do you approach this kind of problem? Since the window can be too large I can not split data into training samples of 10, 100 or 1000 sentences. My guess is that I should use BPTT + LSTM and train in one shot without shuffling the data. I am not sure this is feasible, so I will greatly appreciate your help! I have also my doubts what if ""Where is the book?"" appears 20 sentences after (instead of 10,100 and 1000) in the test set (which is not same as the training set)? Also, should I use Reinforcement Learning (since I can generate the data) or Supervised learning?</p>

<p>Thanks a lot!</p>
"
2950,"<p>In the diagram below, there are three variables: <code>X3</code> is a function of (depends on) <code>X1</code> and <code>X2</code>, <code>X2</code> also depends on <code>X1</code>. More specifically, <code>X3 = f(X1, x2)</code> and <code>X2 = g(X1)</code>. Therefore, <code>X3 = f(X1, g(X1))</code>. </p>

<p><a href=""https://i.stack.imgur.com/Av2hx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Av2hx.png"" alt=""enter image description here""></a></p>

<p>If the probabilistic distribution of <code>X1</code> is known, is it possible to derive the the probabilistic distribution of <code>X3</code>?</p>
"
2951,"<p>I'm trying to define the q-learning multiple agent reinforcement learning models but I'm not sure if it is correct or not.
N the number of agents and <span class=""math-container"">$i \in N$</span> and the k is the time episode.</p>

<p><span class=""math-container"">$Q^{i}_{k+1}(s_{k}, a^{i}_{k}) =(1-\alpha_k)Q^{i}_k(s_{k}, a^{i}_{k})+ \alpha_k[{r^i}_k(s_{k},\\ a^{1}_{k},...,a^{N}_{k}) + \beta\max_{b^i}Q^{i}_k(s_{k+1},b^i)]$</span></p>

<p>Any help will be appreciated.</p>
"
2952,"<p>Robotic control problems are said to require continuous action spaces. But is that really necessary for granular control with real world robotics? At the end of the day, even human movement is restricted on some molecular level that does not correspond to truly 'continuous' / infinitely granular actions.</p>
"
2953,"<p>Is there any technical subtle difference in these two ? Or both are same ?</p>

<p>Is my following interpretation correct:</p>

<p>Automatic Transcription: Converts the speech to text by looking the whole spoken input </p>

<p>ASR: Converts the speech to text by looking into word by word choices</p>
"
2954,"<p>I want to study artificial intelligence but I don't know how to begin. I want to know study path for artificial intelligence.</p>
"
2955,"<p>Suppose <span class=""math-container"">$G_t$</span>, the discounted return at time <span class=""math-container"">$t$</span> is defined as: <span class=""math-container"">$$ G_t \triangleq R_t+\gamma R_{t+1}+\gamma^{2}R_{t+2} + \cdots = \sum_{j=1}^{\infty} \gamma^{k}R_{t+k}$$</span></p>

<p>where <span class=""math-container"">$R_t$</span> is the reward at time <span class=""math-container"">$t$</span> and <span class=""math-container"">$0 &lt; \gamma &lt; 1$</span> is a discount factor. Let the state-value function <span class=""math-container"">$v(s)$</span> be defined as: <span class=""math-container"">$$v_{\pi}(s) \triangleq \mathbb{E}[G_t|S_{t}=s]$$</span></p>

<p>In other words, it is the expected discounted return given that we start in state <span class=""math-container"">$s$</span> with some policy <span class=""math-container"">$\pi$</span>. Then  <span class=""math-container"">$$v_{\pi}(s) = \mathbb{E}_{\pi}[R_t+\gamma G_{t+1}|S_{t}=s]$$</span></p>

<p><span class=""math-container"">$$ = \sum_{a} \pi(a|s) \sum_{s',r} p(r,s'|s,a)[r+\ \gamma v_{\pi}(s')]$$</span></p>

<blockquote>
  <p><strong>Question 1</strong>. Are the states <span class=""math-container"">$s'$</span> drawn from a from a joint probability distribution <span class=""math-container"">$P_{sa}$</span>? In other words, if you are in an
  initial state <span class=""math-container"">$s$</span>, take an action <span class=""math-container"">$\pi(s)$</span>, then <span class=""math-container"">$s'$</span> is the random
  state you would end up in according to the probability distribution
  <span class=""math-container"">$P_{sa}$</span>?</p>
</blockquote>

<p>Also let <span class=""math-container"">$q_{\pi}(s,a)$</span>, the action-value function be defined as: <span class=""math-container"">$$q_{\pi}(s,a) \triangleq \mathbb{E}_{\pi}[G_t|S_t = s, A_t = a]$$</span> </p>

<p><span class=""math-container"">$$=\sum_{s',r} p(r,s'|s,a)[r+\ \gamma v_{\pi}(s')]$$</span></p>

<blockquote>
  <p><strong>Question 2.</strong> What are the advantages of looking at <span class=""math-container"">$q_{\pi}(s,a)$</span> versus <span class=""math-container"">$v_{\pi}(s)$</span>?</p>
</blockquote>
"
2956,"<p>Before a robot can act with meaning, some planning is needed. The idea is, that the decision making process is independent from action. The task of figuring out what the best decision is, was introduced under the term Cybernetics and is focused on intelligence in animals and artificial systems. In a robot, the act of thinking is realized with computer programs.</p>

<p>If we are researching how exactly a computer program determines what the next action for a robot is, the term reinforcement learning is often referenced. The idea is, to store all situations in a q-table and if the robot is in a certain situation he can look into the q-matrix and sees in the row what action is the right one. This is called a policy because it answers the question what action is needed in a certain state.</p>

<p>Some researchers have recognized that Q-learning is not the best way for dealing with complex problems. A policy which is similar to a lookup table will provide only simple formed answers to difficult challenges. The more elaborated form of storing knowledge is called an indirect policy. The idea is here not to store state-action values in a table but use a prediction model. The first thing what the robot software is answering are potential actions and for each action an outcome is calculated. The predicted result of the actions is stored in graph and the best node is identified with a solver.</p>

<p>In a concrete situation a direct policy knows what the correct action is, but the indirect policy don't. The indirect policy has to figure out first what the prediction of future states will be. My question is: is an indirect policy which is using a prediction model superior to a direct policy known as a q-table?</p>
"
2957,"<p>I am studying csp problems in artificial intelligence.I find difficulties in studying with AC-3 algorithm.I already understand that AC-3 algorithm maintains a queue of all arcs and do revise.But,I become confuse when seeing AC3+BT (backtracking) and AC-3 +Maintaining arc consistency algorithm.I want to understand the main difference between them. </p>
"
2958,"<p>well I am trying to create a neural network with the purpose to map <a href=""https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_orb/py_orb.html"" rel=""nofollow noreferrer"">orb features</a> (well they could be SURF or SIFT also but I would prefer SURF for dimensionality reasons) from 2 different modes. </p>

<p>The images are supposed to represent the same scene so the features in either place should match the corresponding features from the other mode extracted from the same point. </p>

<p>Anyway the main task is to get the a set of orb features (set 1) from their corresponding second set of orb features (set 2). Besides the obvious question if an innner mapping like this exists (and supposing it does exist) </p>

<p>So far I have tried a NN with Dense layers only. Nothing fancy just some sequential dense layers stacked together. Anyway, I am not sure it's the best 1st choice to work with. I have some thoughts about using RNN (with LSTM or GRU) as the problem seems seq2seq. Also, 1D-CNN might worth a try (considering nearby values should have some covariance -at least some of them-). Is there some hint I could use to help me narrow down the problem.</p>

<p>Also, I haven't applied any normalization to my data. Would this make a difference in performance for example? Or I did not consider <code>Huber loss</code> instead (as it seems more relevant). Anyway I know that those are (some) of the hyper-parameters I need to tune but I would like to hear if any of you have any general idea how to proceed from here.</p>

<pre><code>model = Sequential()
model.add(Dense(256, input_shape=input_shape, activation=""relu""))
model.add(Dense(128, input_shape=input_shape, activation=""relu""))
model.add(Dropout(0.25))
model.add(Dense(64, activation=""relu""))
model.add(Dense(32, activation=""relu""))
model.add(Dropout(0.25))
model.add(Dense(16, activation=""relu""))
model.add(Dense(32, activation=""relu""))
model.add(Dropout(0.25))
model.add(Dense(64, activation=""relu""))
model.add(Dense(32, activation=""relu""))

RMSprop = keras.optimizers.RMSprop(lr=0.0005, rho=0.9, epsilon=None, decay=0.0)
model.compile(optimizer=RMSprop, loss='mse', metrics=['mae'])
</code></pre>

<p>My current model is this one (in Keras). </p>
"
2959,"<p>Suppose we have a deterministic environment where knowing <span class=""math-container"">$s,a$</span> determines <span class=""math-container"">$s'$</span>. Is it possible to get two different rewards <span class=""math-container"">$r\neq r'$</span> in some state <span class=""math-container"">$s_{\text{fixed}}$</span>? Assume that <span class=""math-container"">$s_{\text{fixed}}$</span> is a fixed state I get to after taking the action <span class=""math-container"">$a$</span>. Note that we can have situations where in multiple iterations we have: <span class=""math-container"">$$(s,a) \to (s_1, r_1) \\ (s,a) \to (s_{\text{fixed}}, r_1) \\ (s,a) \to (s_{\text{fixed}}, r_2) \\ (s,a) \to (s_3, r_3) \\ \vdots$$</span></p>

<p>My question is, would <span class=""math-container"">$r_1 =r_2$</span>?</p>
"
2960,"<p>From <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">the reinforcement learning book section 13.3</a>:</p>

<p><a href=""https://i.stack.imgur.com/Ex4a5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ex4a5.png"" alt=""enter image description here""></a></p>

<p>Using pytorch, I need to calculate a loss, and then the gradient is calculated internally.</p>

<p>How to obtain the loss from equations which are stated in the form of an iterative update with respect to the gradient?</p>

<p>In this case:</p>

<p><span class=""math-container"">$\theta \leftarrow \theta + \alpha\gamma^tG\nabla_{\theta}ln\pi(A_t|S_t,\theta)$</span></p>

<p>What would be the loss?</p>

<p>And in general, what would be the loss if the update rule were</p>

<p><span class=""math-container"">$\theta \leftarrow \theta + \alpha C\nabla_{\theta}g(x|\theta)$</span></p>

<p>for some general (derivable) function <span class=""math-container"">$g$</span> parameterized by theta?</p>
"
2961,"<p><a href=""http://The%20Reinforcement%20Learning%20Book%20by%20Richard%20Sutton%20et%20al"" rel=""nofollow noreferrer"">The Reinforcement Learning Book by Richard Sutton et al, section 13.5</a> shows an online actor critic algorithm.</p>

<p><a href=""https://i.stack.imgur.com/sOtRd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sOtRd.png"" alt=""""></a></p>

<p>Why do the weights updates depend on the discount factor via <span class=""math-container"">$I$</span>?</p>

<p>It seems that the more we get closer to the end of the episode, the less we value our newest experience <span class=""math-container"">$\delta$</span>.</p>

<p>This seems odd to me. I thought discounting in the  recursive formula of <span class=""math-container"">$\delta$</span> itself is enough.</p>

<p><strong>Why does the weights update become less significant as the episode progresses?</strong>
Note this is not eligibility traces, as those are discussed separately, later in the same episode.</p>
"
2962,"<p><a href=""https://ai.stackexchange.com/questions/10531/in-online-one-step-actor-critic-why-does-the-weights-update-become-less-signifi"">In my related question</a>, I asked about the one step actor critic from <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">The Reinforcement Learning Book by Richard Sutton et al, section 13.5</a>:</p>

<p><a href=""https://i.stack.imgur.com/64azc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/64azc.png"" alt=""""></a></p>

<p>The learning is becoming less significant as the episode progresses, by means of making <span class=""math-container"">$I$</span> smaller by the discounting factor, as the episode progresses.</p>

<p>How would this discounting generalize to experience replay?</p>

<p>Meaning, if we want to update <span class=""math-container"">$\theta$</span> and <span class=""math-container"">$w$</span> by some experience e=<span class=""math-container"">$(s, a, r, s')$</span>, for which we don't know by how much to discount, how would we accomplish a correct update?</p>

<p>Should we remember the discount amount <span class=""math-container"">$I$</span> in the experience?</p>

<hr>

<p>Please note the critic here is different from the critic <a href=""https://ai.stackexchange.com/questions/10372/what-information-should-be-cached-in-experience-replay-for-actor-critic"">here</a>, because it estimates the state-value function <span class=""math-container"">$V(s)$</span>, rather than the action-state-value function <span class=""math-container"">$Q(s,a)$</span></p>
"
2963,"<p>We recently founded a company in the area of <strong>additive manufacturing</strong>. Our development focuses on making the process easier and liberate time to the user, which I personally believe in as the ultimate promise of any computerised process, including AI, for the public. </p>

<p>We have gone through the usual channels including regular job offers and personal contacts, visiting universities and meetups. However there appears to be a lack of participation of <em>real</em> specialists in most of those gatherings as the ones interested in learning about AI are doing just that, and don't show up somewhere where we can find them, although we believe to have a very interesting task at hand.</p>

<p>It appears to be remarkably hard to find specialists in the field, as AI specialists are either employed by huge multinational companies (instead of startups), have no interest in additive manufacturing (because it implies something else than only code) and the challenge of machines (making machines) or just do not pop up anywhere.</p>

<p><strong>How would a specialist of AI look for someone else in the specific field of additive manufacturing/motion planning/creative strategy and its employment?</strong> <em>(Or course in the case they do not already know someone.)</em></p>
"
2964,"<p>Is there any research in this area?</p>
"
2965,"<p>I am a beginner, just started studying around NLP, specifically various language models. So far, my understanding is that -  the goal is to understand/produce natural language.</p>

<p>So far the methods I have studied speak about correlation of words, using correct combination to make a meaningful sentence. I also have the sense that the language modeling does not really care about the punctuation marks (or did I miss it?)</p>

<p>Thus I am curious is there a way they can classify sentence types such as Declarative, Imperative, Interrogative or Exclamatory?</p>
"
2966,"<p>The prosecution or defense could just input all the documentary evidence and Project Debater would generate coherent arguments supporting their side. As a result, fewer lawyers would be required to do the work.</p>
"
2967,"<p>For example, you train on dataset 1 with an adaptive optimizer like Adam. Should you reload the learning schedule etc from the end of training on dataset1 when attempting transfer to dataset2? Why or why not?</p>
"
2968,"<p>I am purchasing Titan RTX GPU. Everything seems fine with that except float32 &amp; float64 performance which seems lower vis-a-vis some of its counter parts. I wanted to understand if single precision and double precision performance of GPU affect deep learning training or efficiency ? We work mostly with images, however not limited to that.</p>
"
2969,"<p>In the paper ""Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"", they define the loss function for the policy network as </p>

<p><span class=""math-container"">$$
J_\pi(\phi)=\mathbb E_{s_t\sim \mathcal D}\left[D_{KL}\left(\pi_\phi(\cdot|s_t)\Big\Vert {\exp(Q_\theta(s_t,\cdot)\over Z_\theta(s_t)}\right)\right]
$$</span></p>

<p>Applying the reparameterization trick, let <span class=""math-container"">$a_t=f_\phi(\epsilon_t;s_t)$</span>, then the objective could be rewritten as</p>

<p><span class=""math-container"">$$
J_\pi(\phi)=\mathbb E_{s_t\sim \mathcal D, \epsilon \sim\mathcal N}[\log \pi_\phi(f_\phi(\epsilon_;s_t)|s_t)-Q_\theta(s_t,f_\phi(\epsilon_t;s_t))]
$$</span></p>

<p>They compute the gradient of the above objective as follows</p>

<p><span class=""math-container"">$$
\nabla_\phi J_\pi(\phi)=\nabla_\phi\log\pi_\phi(a_t|s_t)+(\nabla_{a_t}\log\pi_\phi(a_t|s_t)-\nabla_{a_t}Q(s_t,a_t))\nabla_\phi f_\phi(\epsilon_t;s_t)
$$</span></p>

<p>The thing confuses me is the first term in the gradient, where does it come from? To my best knowledge, the second large term is already the gradient we need, why do they add the first term?</p>
"
2970,"<p>An activation function is a function from <span class=""math-container"">$R \rightarrow R$</span>. It takes as input the inner products of weights and activations in the previous layer. It outputs the activation. </p>

<p>A softmax however, is a function that takes input from <span class=""math-container"">$R^p$</span>, where <span class=""math-container"">$p$</span> is the number of possible outcomes that need to be classified. Therefore, strictly speaking, it cannot be an activation function.</p>

<p>Yet everywhere on the net it says the softmax is an activation function. Am I wrong or are they?</p>
"
2971,"<p>I've noticed that when modelling a continuous action space, the default thing to do is to estimate a mean and a variance where each is parameterized by a neural network or some other model. </p>

<p>I also often see that it is one network <span class=""math-container"">$\theta$</span> models both. The REINFORCE objective can be written as </p>

<p><span class=""math-container"">$$\nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi} [\nabla_\theta \log \pi(a_t|s_t) * R_t] $$</span></p>

<p>For discrete action space this makes sense since the output of the network is determined by a softmax. However, if we explicitly model the output of the network as a Gaussian, then the gradient of the log likelihood is of a different form,</p>

<p><span class=""math-container"">$$\pi_\theta(a_t|s_t) = Normal(\mu_\theta(s_t), \Sigma_\theta(s_t))$$</span></p>

<p>and the log is:</p>

<p><span class=""math-container"">$$\log \pi_\theta(a_t | s_t) =  -\frac{1}{2} (a_t-\mu_\theta)^\top \Sigma^{-1}_\theta(a_t-\mu_\theta) + \log 2 \pi \det({\Sigma_\theta})$$</span></p>

<p>In the slides provided here (slide 18):
<a href=""http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf"" rel=""nofollow noreferrer"">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf</a></p>

<p>IF the variance is held constant, then we can solve this analytically:</p>

<p><span class=""math-container"">$$\nabla_\theta \log \pi_\theta(a_t|s_t) = (a_t - \mu_\theta) \Sigma^{-1} \phi(s)$$</span></p>

<p>But, are things always modelled assuming a constant variance? If it's not constant then we have to account for the inverse of the covariance matrix 
as well as the determinant? </p>

<p>I've taken a look at code online and from what I've seen, most of them assume the variance is constant. </p>

<hr>

<p>@NielSlater</p>

<p>Using the reparameterization trick we would use a normal distribution with fixed parameters 0 and 1. 
<span class=""math-container"">$$ a_t \sim \mu_\theta(s_t) + \Sigma_\theta(s_t) * Normal(0, 1) $$</span> </p>

<p>Which is the same as if we <em>had</em> actually sampled directly from a distribution, <span class=""math-container"">$ \pi_\theta(a_t | s_t) = Normal(\mu_\theta(s_t), \Sigma_\theta(s_t))$</span> and let's us calculate the corresponding <span class=""math-container"">$\log \pi_\theta(a_t|s_t)$</span> and <span class=""math-container"">$\nabla_\theta \log \pi_\theta(a_t | s_t)$</span> without having to differentiate through the actual density.</p>
"
2972,"<p>I wonder how self-driving cars determine the path to follow. Yes, there's GPS, but GPS can have hiccups and a precision larger than expected. Suppose the car is supposed to turn right at an intersection on the inner lane, how is the exact path determined? How does it determine the trajectory of the inner lane?</p>
"
2973,"<blockquote>
  <p>Show which literals can be inferred from the following knowledge bases, using both reasoning patterns and truth tables. Show all steps in your reasoning and explain your answers.</p>
  
  <p>1) P &amp; Q<br>
  2) Q →R v S<br>
  3) P → ~R </p>
</blockquote>

<p>This is from my reasoning pattern tutorial, my text book shows similar question except that there is a single literal with workings, so I can somehow read through but I'm not familiar with some terms. I don't understand how I can infer all literals with the above information. I also don't fully understand what is and-elimination, modus ponens and unit resolution. </p>

<p>Is there anyone who is kind enough to use the above question as an example so that I can have a clearer picture?</p>
"
2974,"<p>I'm new to the whole world of AI and neural networks, and I'm looking to get started with training a CNN on a cloud service like AWS or GCP but I'm totally overwhelmed by the choices for VM instances. In GCP, there are options that seem to have the same amount of memory but are classifed as standard, highmem, or highcpu -- what's the difference? As well what would be some reasons to opt for having a GPU as well? </p>
"
2975,"<p>I am trying to write a genetic algorithm that generates 100 items, assigning random weights and utilities to them. And then try to pick items how out these 100 items while maximising the utility and not picking items over 500ks. The program should return an array of boolean values, where true represents items to be picked and false represents item not to be picked. </p>

<p>Can someone help with this or point me to a link of something that has been written like this before?</p>
"
2976,"<p>My understanding of the main idea behind A2C / A3C is that we run small segments of an episode to estimate the return using a trainable value function to compensate for the unseen final steps of the episode.</p>

<p>While I can see how this could work in continuing tasks with relatively dense rewards, where you can still get some useful immediate rewards from a small experience segment, does this approach work for episodic tasks where the reward is only delivered at the end? For example, in a game where you only know if you win or lose at the end of the game, does it still make sense to use the A2C / A3C approach?</p>

<p>It's not clear to me how the algorithm could get any useful signal to learn anything if almost every experience segment has zero reward, except for the last one. This would not be a problem in a pure MC approach for example, except for the fact that we might need a lot of samples. However, it's not clear to me that arbitrarily truncating episode segments like in A2C / A3C is a good idea in this case.</p>
"
2977,"<p>One of the most common misconceptions about reinforcement learning (RL) applications is that, once you deploy them, they continue to learn. And, usually, I'm left having to explain this. As part of my explanations, I like to show where it is being used and where not. </p>

<p>I've done a little bit of research on the topic, but the descriptions seem fairly academic, and I'm left with the opinion that reinforcement learning is not really suitable for financial services in regulated markets. </p>

<p>Am I wrong? If so, I would like to know where RL is being used? Also, if it is, how are the RL algorithms governed?</p>
"
2978,"<p>Which representation is most biologically plausible for actor nodes? For example, actions represented across several output nodes which may be either</p>

<ol>
<li><p>mutually exclusive with each other (e.g., go north, go south, etc),
achieved by winner-takes-all. </p></li>
<li><p>NOT mutually exclusive with each other (e.g. left leg forward, right leg forward); these actions may occur concurrently. To go north, the correct combination of nodes must be active.</p></li>
</ol>

<p>Similarly which representation is most plausible for critic output nodes?</p>

<ol start=""3"">
<li><p>A single output node that outputs a real number representing the
reward. </p></li>
<li><p>A set of output nodes each representing a separate value, achieved by winner-takes-all.</p></li>
</ol>

<p>Or do other representations better align with real brains ?</p>
"
2979,"<p><a href=""https://distill.pub/2017/feature-visualization/"" rel=""nofollow noreferrer"">Feature visualization</a> allows to better understand neural networks by generating images that maximize the activation of a specific neuron, and therefore understand what are the abstract features that produce a high activation.</p>

<p>The examples that I saw so far are related to classification tasks. So my question is: can these concepts be applied to other convolutional neural network tasks, like semantic segmentation or image embedding (triplet loss)? What can I expect if I apply visualization algorithms to these networks?</p>
"
2980,"<p>I have a few robots that are roaming a space (e.g. a room). I would like to cluster, using a clustering algorithm, these robots based on features, such as the distance between them, their battery life, etc. How would I cluster these robots? Which algorithm would you suggest? Furthermore, which features should I take into account to cluster such robots?</p>
"
2981,"<p>I want to generate a VoicePrint file (trained model) based on the audio recording files that I am going to feed to either <a href=""https://azure.microsoft.com/en-in/services/cognitive-services/speaker-recognition/"" rel=""nofollow noreferrer"">Azure Speaker Verification</a> (Cognitive services) OR <a href=""https://voiceit.io/apidemo"" rel=""nofollow noreferrer"">Voice.it API</a></p>

<ol>
<li>Will I get anything in return if I pass the recording file in via the
API? Does either platform gives me a VoicePrint file (or trained model) that I can save in my database? </li>
<li>If yes, then in which format I will get the data? Is there any
API available for that?</li>
</ol>

<p>P.S: I went through the API documentation but can't find anything.  </p>
"
2982,"<p>I trained a DCGAN on MNIST and CelebA dataset with 28x28 image size. Both the models were able to train successfully. I used many tips from <a href=""https://github.com/soumith/ganhacks"" rel=""nofollow noreferrer"">https://github.com/soumith/ganhacks</a> to make both the G and D's losses converge.
Respective scripts are <a href=""https://github.com/piyush-kgp/GAN-Lab/blob/master/dcgan/dcgan.py"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/piyush-kgp/GAN-Lab/blob/master/dcgan/celeb_dcgan.py"" rel=""nofollow noreferrer"">here</a></p>

<p>Now I want to train my model on CelebA with image size of 224x224 so that I can generate higher resolution fake celebrity images. For this I added 2 more <code>Conv2DTranspose + BatchNorm + LeakyRelu</code> stacks to my generator and 1  <code>Conv2DTranspose + BatchNorm + LeakyRelu + Dropout</code> stack to my discriminator. I did not alter anything else. On training, the discriminator is quickly reaching near 100% accuracy and the generator's accuracy saturates near 0. As expected from these values, the generated images are just noise. Code:</p>

<p><a href=""https://github.com/piyush-kgp/GAN-Lab/blob/master/dcgan/celeb_dcgan_hd.py"" rel=""nofollow noreferrer"">https://github.com/piyush-kgp/GAN-Lab/blob/master/dcgan/celeb_dcgan_hd.py</a></p>

<p>Am I missing something obvious. How do I stabilize the network.</p>
"
2983,"<p>I have to calculate the affluence in localities of Metro city. To calculate affluence, I am considering a parameter per capita income. </p>

<p>Where I can get a dataset of it? What are other parameters I should consider for the problem? </p>

<p>Any guidance will be fruitful for me.</p>
"
2984,"<p>At present I use decaying ϵ-greedy to pick what action to take, but I've been looking into ways of better exploring the state-action space.  I've read up on hoeffding's inequality / UCB, and probability matching, but these methods seem to involve keeping a count of the number of times an action was performed for a given state. But what if the state-space is massive and keeping a record of states and counts isn't feasible?</p>

<p>If ϵ has decayed to near zero already then it will begin to behave greedily and the actions further down the tree which are hit on very rarely could then be 'locked in' without any future exploration. But if I set the ϵ-decay very small, then I'll continue to explore state/actions higher up the tree that the agent is already confident about, which is wasteful as instead it would have been better to proceed to exploring the rest of the tree that it's unsure about.</p>

<p>I seem to have somewhat of a paradox of wanting to use UCB or probability matching to ensure my agent better explores because I have a large state-action space, but I'm unable to use it precisely <em>because</em> I have a large state-action space and recording/looking up counts isn't really feasible. D'oh!</p>
"
2985,"<p>In reinforcement learning, we often define two functions, the ""state-value function""</p>

<p><span class=""math-container"">$$V^\pi(s) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} 
\gamma^{k}R_{t+k+1}|S_t=s]$$</span></p>

<p>and the ""state-action value function"":</p>

<p><span class=""math-container"">$$Q^\pi(s,a) = \mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}|S_t=s, A_t=a]$$</span></p>

<p>where <span class=""math-container"">$\mathbb{E}_{\pi}$</span> means that these functions are defined as the ""<a href=""https://en.wikipedia.org/wiki/Expected_value"" rel=""nofollow noreferrer"">expectation</a>"" with respect to a fixed policy <span class=""math-container"">$\pi$</span> of what is often called the ""return"", <span class=""math-container"">$\sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}$</span>, where <span class=""math-container"">$\gamma$</span> is a ""discount factor"" and <span class=""math-container"">$R_{t+k+1}$</span> is the reward received from the environment (while the agent interacts with it) from time <span class=""math-container"">$t$</span> onwards. </p>

<p>So, both the <span class=""math-container"">$V$</span> and <span class=""math-container"">$Q$</span> functions are defined as expectations of the return (or the ""cumulative future discounted reward""), but these expectations have different ""conditions"" (or are conditioned on different variables). The <span class=""math-container"">$V$</span> function is the expectation (with respect to a fixed policy <span class=""math-container"">$\pi$</span>) of the return given that the current state (the state at time <span class=""math-container"">$t$</span>) is <span class=""math-container"">$s$</span>. The <span class=""math-container"">$Q$</span> function is the expectation (with respect to a fixed policy <span class=""math-container"">$\pi$</span>) of the return conditioned on the fact that the current state the agent is in is <span class=""math-container"">$s$</span> and the action the agent takes at <span class=""math-container"">$s$</span> is <span class=""math-container"">$a$</span>.</p>

<p>I know that the <em>Bellman optimality equation</em> for <span class=""math-container"">$V^*$</span> (the optimal value function) can be expressed as the Bellman optimality equation for <span class=""math-container"">$Q^{\pi^*}$</span> (the optimal state-action value function associated with the optimal policy <span class=""math-container"">$\pi^*$</span>) as follows</p>

<p><span class=""math-container"">$$
V^*(s) = \max_{a \in \mathcal{A}(s)} Q^{\pi^*}(s, a)
$$</span></p>

<p>This is actually shown (or proved) at page 76 of the book ""Reinforcement Learning: An Introduction"" (1st edition) by Andrew Barto and Richard S. Sutton.</p>

<p>Are there any other functions, apart from the <span class=""math-container"">$V$</span> and <span class=""math-container"">$Q$</span> functions defined above, in the RL context? If so, how are they related? </p>

<p>For example, I've heard of the ""advantage"" or ""continuation"" functions. How are these functions related to the <span class=""math-container"">$V$</span> and <span class=""math-container"">$Q$</span> functions? When should one be used as opposed to the other? </p>

<p>Note that I'm <em>not just</em> asking about the ""advantage"" or ""continuation"" functions, but, if possible, any existing function that is used in RL that is similar (in purpose) to these mentioned functions, and how they are related to each other. </p>
"
2986,"<p>Disclaimer : This is my first question on this site so I apologize if it doesn't fit the general policy. </p>

<p>Years ago I had the idea of introducing the Lex function (standing for law in Latin or a shortcut for ""Logical EXistence"" as it is basically the only law existing in math) such that given a proposition P in a theory T, Lex(P/T)=1 if and only if the existence of P in T entails no contradiction (and equals 0 otherwise). My goal back then was to define a ""coherence calculus"" that would act as a certificate of coherence of a given non necessarily mathematical statement.</p>

<p>Tonight I heard about Winograd sentences like ""the trophy can't fit in the brown suitcase as it is too big"" which are grammatically but not semantically ambiguous to a human who gets the meaning of such a sentence through the context. In that case we know that it is the trophy that is too big cause were it the suitcase, the latter could contain the trophy, hence turning the sentence into something illogical.</p>

<p>We can do this because we know that the volume, as an instance of a mathematical measure, of the contained object has to be smaller than the container as it occupies a subset thereof, which is the very meaning of ""(fit) in"". But an AI doesn't have such a knowledge. I was thus wondering if it could anyhow acquire it through browsing potentially the whole internet via what I'd call ""quantum leaps"" between not directly related pieces of information, the probability of such a leap being an increasing function of the ""semantic proximity"" of those pieces of information. I don't yet have a satisfying definition of this semantic proximity but maybe we can get one as follows :</p>

<p>Let G(K) be the semantic graph of K as a graph (V,E) the vertices being concepts defining a path if the two concepts are semantically related. For example from ""rain"" we go first to ""water drop"" then to ""spherical shape"" or to ""bad weather"" and then to ""grey skies"" and further to ""feeling moody"". I guess I'm reinventing the wheel but as I work outside AI I am quite ignorant of the state of the art and related terminology.</p>

<p>So getting back to quantum leaps they could be leaps from one semantic graph to another sharing similar nodes or paths, which requires mapping the set of all semantic graphs containing a given word and establishing a metric on this set. Defining the proximity as the reciprocal of the distance, we could get a rough notion of a quantum leap between any two elements of this set of graphs.</p>

<p>Once a quantum leap has occurred, we adjoint step by step the new graphs to the already existing ones and evaluate the coherence of the result by generating propositions whose terms are seen as nodes of such graphs and computing the value of the Lex function for each such proposition. </p>

<p>Finally we keep propositions whose Lex value is 1 and eliminate the ones whose Lex value is 0 to get a ""maximal semantically cohesive graph"" that should help the AI endowed with such a process to decide the meaning of Winograd sentences.</p>

<p>My question is : is the implementation of the above realistic or totally out of reach of current (as of February 14th 2019) tech?</p>
"
2987,"<p>I want to train a convolutional neural network (CNN) in PyTorch to predict frequency spectrum data related to an input image. Rather than assigning one label to each image (Dog, Cat, Car, Airplane, etc.), I would like to assign a matrix of labels (one label per frequency) to each image. In PyTorch, how do I assign a matrix of data as a label to each input image in my dataset? I have been trying to do this using ImageFolder. Thanks!</p>
"
2988,"<p><strong>TL;DR: read the bold. The rest are details</strong></p>

<p><strong>I am trying to implement</strong> <a href=""http://incompleteideas.net/book/bookdraft2017nov5.pdf"" rel=""nofollow noreferrer"">Reinforcement Learning:An Introduction, section 13.5</a> <strong>myself</strong>:</p>

<p><a href=""https://i.stack.imgur.com/JDK5k.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JDK5k.png"" alt=""enter image description here""></a></p>

<p>on <a href=""https://gym.openai.com/envs/CartPole-v0/"" rel=""nofollow noreferrer"">OpenAi's cartpole</a></p>

<p><strong>The algorithm seems to be learning something useful</strong> (and not random), as shown in these graphs (different zoom on the same run):</p>

<p><a href=""https://i.stack.imgur.com/wkyXW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wkyXW.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ZWAxU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZWAxU.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/Sxkmf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sxkmf.png"" alt=""enter image description here""></a></p>

<p>Which show the reward per episode (y axis is the ""time alive"", x axis is episode number).</p>

<p><strong>However</strong>, as can be seen, </p>

<ol>
<li><p>The learning does not seem to stabilize.</p></li>
<li><p>It looks like every time the reward maxes out (200), it immediately drops.</p></li>
</ol>

<hr>

<p><strong>My relevant code for reference</strong> (inspired by <a href=""https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py"" rel=""nofollow noreferrer"">pytorch's actor critic</a>)</p>

<p>note: <em>in this question, <code>xp_batch</code> is ONLY THE VERY LAST (s, a, r, s'), meaning experience replay is not in use in this code</em>!</p>

<p>The actor and critic are both distinct neural networks which </p>

<pre><code>def learn(self, xp_batch):#in this question, xp_batch is ONLY THE VERY LAST (s, a, r, s')
    for s_t, a_t, r_t, s_t1 in xp_batch:
        expected_reward_from_t = self.critic_nn(s_t)
        probs_t = self.actor_nn(s_t)
        expected_reward_from_t1 = torch.tensor([[0]], dtype=torch.float)
        if s_t1 is not None:  # s_t is not a terminal state, s_t1 exists.
            expected_reward_from_t1 = self.critic_nn(s_t1)

        m = Categorical(probs_t)
        log_prob_t = m.log_prob(a_t)

        delta = r_t + self.args.gamma * expected_reward_from_t1 - expected_reward_from_t

        loss_critic = delta * expected_reward_from_t
        self.critic_optimizer.zero_grad()
        loss_critic.backward(retain_graph=True)
        self.critic_optimizer.step()

        delta.detach()
        loss_actor = delta * log_prob_t
        self.actor_optimizer.zero_grad()
        loss_actor.backward()
        self.actor_optimizer.step()

def select_action(self, state):
    probs = self.actor_nn(state)
    m = Categorical(probs)
    action  = m.sample()
    return action
</code></pre>

<hr>

<p><strong>My questions are:</strong></p>

<ol>
<li><p><strong>Am I doing something wrong, or is this to be expected</strong>?</p></li>
<li><p>I know this can be improved with eligibility traces/experience replay+off policy learning. <strong>Before making those upgrades, I want to make sure the current results make sense.</strong></p></li>
</ol>
"
2989,"<p>Intuitively, how do temporal-difference and Monte Carlo methods work in reinforcement learning? How can they be used to solve the reinforcement learning problem?</p>
"
2990,"<p>What is the common representation used for the state in articulated robot environments? My first guess is that it's a set of the angles of every joint. Is that correct? My question is motivated by the fact that one common trick that helps training neural nets in general is to normalize the inputs, like setting mean = 0 and std dev = 1, or scaling all the input values to <span class=""math-container"">$[0, 1]$</span>, which could be easily done in this case too if all the inputs are angles in <span class=""math-container"">$[0, 2 \pi]$</span>. But, what about distances? Is it common to, for example, use as input some distance of the agent to the ground, or a distance to some target position? In that case, the scale of the distances can be arbitrary and vary a lot. What are some common ways to deal with that?</p>
"
2991,"<p>In section ""<em>5.2 Monte Carlo Estimation of Action Values</em>"" of the second edition of the reinforcement learning book by Sutton and Barto, this is stated:</p>

<blockquote>
  <p>If a model is not available, then it is particularly useful to estimate action values (the values of state– action pairs) rather than state values. With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP.</p>
</blockquote>

<p>However, I don't see how this is true in practice. I can see how it'd work trivially for discrete state and action spaces with deterministic environment dynamics, because we could compute <span class=""math-container"">$\pi(s) = \underset{a}{\text{argmax}}\ V(\text{step}(s, a))$</span> by just looking at all possible actions and choosing the best one. As soon as I think about continuous state and action spaces with stochastic environment dynamics, computing the <span class=""math-container"">$\text{argmax}$</span> seems to be become very complicated and impractical. For the particular case of continuous states and discrete actions, I think estimating an action value might be more practical to do even if a forward model of the environment dynamics is available, because the <span class=""math-container"">$\text{argmax}$</span> becomes easier (I'm especially thinking of the approach taken in deep Q learning). </p>

<p>Am I correct in thinking this way or is it true that if a model is available it's not useful to estimate action values if state values are already available?</p>
"
2992,"<p>I often see the terms episode, trajectory and rollout to refer to basically the same thing, a list of (state, action, rewards). Are there any concrete differences between the terms or can they be used interchangeably?</p>

<p>In the following paragraphs, I'll summarize my current slightly vague understanding of the terms. Please point any inaccuracy or missing details in my definitions.</p>

<p>I think <em>episode</em> has a more specific definition in that it begins with an initial state and finishes with a terminal state, where the definition of whether or not a state is initial or terminal is given by the definition of the MDP. Also, I understand an episode as a sequence of <span class=""math-container"">$(s, a, r)$</span> sampled by interacting with the environment following a particular policy, so it should have a non-zero probability of occurring in the exact same order.</p>

<p>With <em>trajectory</em>, the meaning is not as clear to me, but I believe a trajectory could represent only part of an episode and maybe the tuples could also be in an arbitrary order; even if getting such sequence by interacting with the environment has zero probability, it'd be ok, because we could say that such trajectory has zero probability of occurring.</p>

<p>I think <em>rollout</em> is somewhere in between, since I commonly see it used to refer to a sampled sequence of <span class=""math-container"">$(s, a, r) $</span>from interacting with the environment under a given policy, but it might be only a segment of the episode, or even a segment of a continuing task, where it doesn't even make sense to talk about episodes.</p>
"
2993,"<p>The Turing-machine is well known in the literature. On the turing-machine it is possible to calculate the busy beaver problem. A more recent approach to implement a computer is a Random boolean network. This looks similar to a neural network but contains of logic gates. The advantage over propositional logic is that a feedback loop is in the network, which makes the system turing-powerful. Writing the sourcecode for realizing a RBN is not that hard.</p>

<p>The problem is similar to neural networks. After executing the demo program, the cpu runs at 100% but the weights in the network can't be found. In the sourcecode, the boolean network should determine the weights for a calculator. Some example input/output patterns are given. The idea is, that the 4 first neurons are the input and the 4  last neurons are the output. On a standard PC the network wasn't able to find the correct pattern. That means, it's possible to build with “and, or, not, xor” operators a full-adder, but the boolean network doesn't find so by it's own.</p>

<p>Question: Are random boolean networks useless, because there is no algorithm available for determine the parameters?</p>

<pre><code>#!/usr/bin/env Python
#-*- coding: utf-8 -*-
import random

class Network:
  def __init__(self,maxnode):
    self.maxnode=maxnode
    self.op=(""and"",""or"",""xor"",""not"")
    self.node=[] # (input1, input2, operator, boolearnvalue)
  def show(self):
    for i in range(len(self.node)):
      print i, self.node[i]
  def randomnetwork(self):
    self.node=[] 
    for i in range(self.maxnode):
      nodea=random.randint(0,self.maxnode-1)
      nodeb=random.randint(0,self.maxnode-1)
      op=random.choice(self.op)
      self.node.append((nodea,nodeb,op,0))
  def setinput(self,listinput):
    for i in range(len(self.node)):
      temp=self.node[i]
      self.node[i]=(temp[0],temp[1],temp[2],listinput[i])
  def getoutput(self):
    result=[]
    for i in range(len(self.node)):
      result.append(self.node[i][3])
    return result
  def update(self):
    for i in range(len(self.node)):
      self.updatenode(i)
  def updatenode(self,nodeid):
    # get input
    pos=self.node[nodeid][0]
    input1= self.node[pos][3] 
    pos=self.node[nodeid][1]
    input2= self.node[pos][3] 
    # operator
    operator=self.node[nodeid][2]
    if operator==self.op[0]: result=min(input1,input2)
    if operator==self.op[1]: result=max(input1,input2)
    if operator==self.op[2]: result=input1 ^ input2 # xor
    if operator==self.op[3]: result=1-input1
    # stpre result
    temp=self.node[nodeid]
    self.node[nodeid]=(temp[0],temp[1],temp[2],result)

class Solver:
  def __init__(self):
    self.input=[]
    self.output=[]
    # first 4 nodes are input, last 4 nodes are output
    self.input.append((0,1,0,1,0,0,0,0)) 
    self.output.append((0,0,0,0,0,0,1,0)) # 1+1=2
    self.input.append((0,1,1,0,0,0,0,0)) 
    self.output.append((0,0,0,0,0,0,1,1)) # 1+2=3
    self.input.append((0,0,0,1,0,0,0,0)) 
    self.output.append((0,0,0,0,0,0,0,1)) # 0+1=1
    self.input.append((1,1,0,1,0,0,0,0)) 
    self.output.append((0,0,0,0,0,1,0,0)) # 3+1=4

    self.mynet=Network(8)
    self.run()

  def run(self):
    # self.mynet.show()
    for trial in range(1000000):
      self.mynet.randomnetwork()
      score=self.getscore(False)
      if score&gt;=16:
        print trial, score
        self.getscore(True)
      if trial%100000==0: print trial
  def getscore(self,verbose):
    result=0
    for i in range(len(self.input)): # over all examples
      self.mynet.setinput(self.input[i])
      for iupdate in range(10): # iteration 
        self.mynet.update()
      outputnet=self.mynet.getoutput()
      similarity=self.similarity(self.output[i],outputnet)
      result += similarity
      if verbose==True:
        print i,self.input[i]
        print i,outputnet
    return result
  def similarity(self,lista,listb):
    count=0
    for i in range(4,len(lista)): # only check second half
      if lista[i]==listb[i]:
        count +=1
    return count

mysolver = Solver()
</code></pre>
"
2994,"<p>Is the optimal policy always stochastic (that is, a map from states to a probability distribution over actions) if the environment is also stochastic? </p>

<p>Intuitively, if the environment is <em>deterministic</em> (that is, if the agent is in a state <span class=""math-container"">$s$</span> and takes action <span class=""math-container"">$a$</span>, then the next state <span class=""math-container"">$s'$</span> is always the same, no matter which time step), then the optimal policy should also be deterministic (that is, it should be a map from states to actions, and not to a probability distribution over actions).</p>
"
2995,"<p>From example 3.15 of Richard Sutton's book, there's a grid (right) that shows the value function for each state. </p>

<p><a href=""https://i.stack.imgur.com/6HPwT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6HPwT.png"" alt=""enter image description here""></a></p>

<p>He mentions this equation being used, along with a uniform random policy.</p>

<p><a href=""https://i.stack.imgur.com/GhXjR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GhXjR.png"" alt=""enter image description here""></a></p>

<p>However, I'm not sure how to actually obtain the values in the grid for each state? If anyone could translate the math to a real example (preferably in code) it would help immensely! </p>

<p>Thank you! :)</p>
"
2996,"<p>so I'm trying to make a program where I can predict what cars from an auction are going to be liked by a specific client based on the previous cars he has bought. I am coding this program in python. 
The information I have is the year, make, model and damage of the car of which I web scrape from auction websites. I also have the same information for the purchase history of the client which I will use to train the neural network to predict if a new car from an auction is going to be liked by the client. My question is: what could be a good way to approach the coding of a Neural Network to have these inputs?</p>
"
2997,"<p>This is not meant to be negative or a joke but rather looking for a productive solution on AI development, engineering and its impact on human life:</p>

<p>Lately with my Google searches, the AI model keeps auto filling the ending of my searches with:</p>

<p>“...in Vietnamese”</p>

<p>And </p>

<p>“...in a Vietnamese home”</p>

<p>The issue is I have never searched for that but because of my last name the model is creating this context. </p>

<p>The other issue is that I’m a halfy and my dad is actually third generation, I grew up mainstream American and don’t even speak Vietnamese. I’m not even sure what a Vietnamese home means. </p>

<p>My buddy in a similar situation of South Asian and noticed the same exact thing more so with YouTube recommended videos. </p>

<p>We already have enough issues in the US with racism, projections of who others expect us to be based on any number of things, stereotyping and putting people in boxes to limit them - I truly believe AI is adding to the problem, not helping. </p>

<p>How can we fix this. Moreover, how can we use AI to bring out peoples true self, talents and empower and free them them to create their life how they like ?</p>
"
2998,"<p>I'd like to use machine learning to guess a mathematical pattern: the input are certain polynomials in four variables <span class=""math-container"">$q_1,q_2,q_3,q_4$</span>, the output can be zero or one.</p>

<p>Allowed polynomials are such that (i) all their non-zero coefficients are equal to one, (ii) they do not contain monomials of the form <span class=""math-container"">$q_1^j$</span> for <span class=""math-container"">$j \geq 0$</span>, and (iii) if an allowed polynomial contains a monomial <span class=""math-container"">$m=q_1^a q_2^b q_3^c q_4^d$</span> for some non-negative integers <span class=""math-container"">$a,b,c,d$</span>, then it also contains <span class=""math-container"">$m'=q_1^{a-1} q_2^b q_3^c q_4^d$</span>, provided this does not violate (ii) and <span class=""math-container"">$a \geq 1$</span>; similarly for <span class=""math-container"">$b \to b-1$</span>, <span class=""math-container"">$c \to c-1$</span>, and <span class=""math-container"">$d \to d-1$</span>.</p>

<p>Here's an example batch, given by pairs {input, output}: <span class=""math-container"">$\{q_2,1\},\{q_3,1\},\{q_4,1\}$</span></p>

<p>Here's a second batch: <span class=""math-container"">$\{q_2+q_1 q_2,0\},\{q_2+q_2^2,0\},\{q_2+q_3,1\},\{q_3+q_1 q_3,0\},\{q_3+q_3^2,0\},\{q_2+q_4,1\},\{q_3+q_4,1\},\{q_4+q_1 q_4,0\},\{q_4+q_4^2,1\}$</span></p>

<p>I can construct larger and larger batches using Mathematica, and I'd like to know how to practically go from here, to instructing an AI to guess a simple function of <span class=""math-container"">$q$</span>'s that reproduces the behavior, namely that can guess the correct output for previously unknown admissible polynomials.</p>

<p>What are the typical batch size and computational power required for such a program to succeed?</p>

<p>My idea is to use a function <span class=""math-container"">$\phi$</span> from the space of allowed polynomials <span class=""math-container"">$\mathcal P$</span> to the set <span class=""math-container"">$\mathbb Z_2=\{0,1\}$</span>, of the form <span class=""math-container"">$\phi:\mathcal P \to \mathbb Z_2$</span>, <span class=""math-container"">$p=\sum_{i \in I} m_i \mapsto \phi(p):= \sum_{i \in I' \subset I} m_i|_1 \mod 2$</span>, where <span class=""math-container"">$m_i|_1$</span> means the <span class=""math-container"">$i$</span>-th monomial inside <span class=""math-container"">$p$</span> evaluated at <span class=""math-container"">$q_1=q_2=q_3=q_4=1$</span>, and come up with the form of <span class=""math-container"">$I'$</span> as function of <span class=""math-container"">$I$</span>.</p>

<p>Notice there's no linear structure on <span class=""math-container"">$\mathcal P$</span>.</p>

<p>Remark: of course instead of polynomials one could use punctured solid partitions.</p>
"
2999,"<p>I have been trying to solve a sequential decision-making problem that involves taking ""sub-actions"". I would like to share my experience and really appreciate any suggestions/insights.</p>

<p>Suppose we model it as a Traveling salesman problem (TSP) that has 5 nodes (however, we don't need to go back to the starting node in the end), and at each node, we also have to decide what to do over 3 possible actions. </p>

<p>For example, one possible path is: </p>

<p>(Node 2, Action 0) <span class=""math-container"">$\rightarrow$</span> (Node 4, Action 2) <span class=""math-container"">$\rightarrow$</span> (Node 3, Action 1) <span class=""math-container"">$\rightarrow$</span> (Node 1, Action 0) <span class=""math-container"">$\rightarrow$</span> (Node 0, Action 2)</p>

<p>One challenging part is that we cannot get the intermediate reward and only the final evaluation score is provided. Therefore formulating this problem with TD algorithms sounds not that intuitive.</p>

<p>Methods I have tried includes:</p>

<ol>
<li><p>Genetic algorithm (GA): this method works well overall, but not that ""attractive"" as a piece of research work</p></li>
<li><p>Monte Carlo Tree Search (MCTS): I implemented the vanilla version of MCTS and set the random rollout to 10k, it can surpass the GA's performance, but with a tradeoff of 10X computation time</p></li>
<li><p>I went on to evaluate the cross-entropy method (CEM) by modeling the sequence generation as a Markov chain and sub-action as an independent Bernoulli distribution, but the results are rather poor</p></li>
</ol>

<p>Several future work directions I can think of now:</p>

<ul>
<li><p>Speed up the MCTS, perhaps by pruning or parallel processing?</p></li>
<li><p>Methods like SeqGAN, but was wondering how to deal with the sub-actions, or, how to generate tuples</p></li>
<li><p>Try TD approach by assigning 0-reward for the intermediate states and only evaluate the final state?</p></li>
</ul>

<p>Would really love to hear your opinions and advice! Cheers!</p>
"
3000,"<p>I am in the process of writing my own basic machine learning library in Python as an exercise to gain a good conceptual understanding. I have successfully implemented backpropagation for activation functions such as <span class=""math-container"">$\tanh$</span> and the sigmoid function. However, these are normalised in their outputs. A function like ReLU is unbounded so its outputs can blow up really fast. In my understanding, a classification layer, usually using the SoftMax function, is added at the end to squash the outputs between 0 and 1. </p>

<p>How does backpropagation work with this? Do I just treat the SoftMax function as another activation function and compute its gradient? If so, what is that gradient and how would I implement it? If not, how does the training process work? If possible, a pseudocode answer is preferred.</p>
"
3001,"<p>This is an excerpt taken from <a href=""https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"" rel=""nofollow noreferrer"">Sutton and Barto</a> (pg. 3):</p>

<blockquote>
  <p>Another key feature of reinforcement learning is that it explicitly considers the whole
  problem of a goal-directed agent interacting with an uncertain environment. This is in 
  contrast with many approaches that address subproblems without addressing how they 
  might fit into a larger picture. For example, we have mentioned that much of machine 
  learning research is concerned with supervised learning without explicitly specifying how 
  such an ability would finally be useful. Other researchers have developed theories of 
  planning with general goals, but without considering planning's role in real-time decision-
  making, or the question of where the predictive models necessary for planning would 
  come from. Although these approaches have yielded many useful results, their focus on 
  isolated subproblems is a significant limitation.</p>
</blockquote>

<p>I have an idea of Supervised Learning, but what exactly does the author mean by Planning? And how is the RL approach different from planning and Supervised Learning? </p>

<p>(Illustration with an example would be nice).</p>
"
3002,"<p>I am trying to understand how RNNs are used for sequence modelling.</p>

<p>On a <a href=""https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"" rel=""nofollow noreferrer"">tutorial here</a>, it mentions that if you want to translate say a sentence from English to French you can use an encoder-decoder set-up as they described.</p>

<p>However what if you want to do a sequence to sequence modelling where your inputs and outputs are of the same domain but you just want to predict the next output of a sequence.</p>

<p>For example if I want to use sequence modelling to learn the sine function. So say I have 20 y-coordinates from <span class=""math-container"">$y = sin(x)$</span> from 20 evenly spaced out x-coordinates and I want to predict the next 10 or so y-coordinates. Would I use an encoder-decoder setup here? </p>
"
3003,"<p>In reinforcement learning (RL), there are model-based and model-free algorithms. In short, model-based algorithms use a transition model (e.g. a probability distribution) and the reward function, even though they do not necessarily compute (or estimate) them. On the other hand, model-free algorithms do not use such transition model or reward function, but they directly estimate e.g. the state or state-action value functions by interacting with the environment, which allows the agent to infer the dynamics of the environment.</p>

<p>Given that model-based RL algorithms do not necessarily estimate or compute the transition model or reward function, in the case these are unknown, how can them be computed or estimated (so that they can be used by the model-based algorithms)? In general, what are examples of algorithms that can be used to estimate the transition model and reward function of the environment (represented as either an MDP, POMDP, etc.)?</p>
"
3004,"<p><strong>Introduction:</strong></p>

<p>The notion that various social complex systems (e.g. society, family, business company, state, etc) could be regarded as ones exhibiting consistent traits of behaviour of their own - suggesting that they are entities unto themselves, some sort of organisms on their own or even intelligent entities on their own - is not new. I have personally stumbled upon papers of scholars who straightforwardly speak of such systems as if they were already proven to be singular entities.</p>

<p>That kind of assumption has entered the vernacular, as well, long time ago - e.g. ""the state wants to..."" , ""society responds to conflict by..."", ""the family dynamics seeks balance through..."", etc. </p>

<p>Therefore, we could even assume at one point, that such social systems are not only organisms of their own, but even some sort of <strong>artificial intelligence entities</strong> (as long as we could see them as an artificial product of human activity).</p>

<p>We are generally used to seeing ourselves as conscious entities and we are also good at exploring entities of less complexity than ourselves. But when it comes to entities which consists of us as mere components, we are not ready to mentally process that idea - it sounds as either too abstract or too sci-fi (think of Stanisław Lem's work).</p>

<p><strong>Question:</strong></p>

<ul>
<li><p>While the average Joe could easily say <em>""The state wants to..."" or ""Society responds to..."", etc</em>, how exactly do we prove (or at least gather some sort of supporting evidence) that a complex social system really exhibits a behaviour of its own? </p></li>
<li><p>Under what conditions could we regard it as some sort of spontaneously born artificial intelligence? </p></li>
<li><p>If that were true, how could we predict if that AI would procreate and bring about other social system forms which are also entities unto themselves? How could we possibly become aware if that has already happened?</p></li>
</ul>
"
3005,"<p>I am reading about the actor-critic architecture. I am confused about how the actor determines the action using the value (or future reward) from the critic network.</p>

<p>Below you have the most popular picture of actor-critic network. </p>

<p><a href=""https://i.stack.imgur.com/pKKmS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pKKmS.png"" alt=""Actor Critic Network""></a></p>

<p>It looks like the input of the actor network is only the ""state"" variable (<span class=""math-container"">$s_t$</span>), it has nothing to do with the critic network.</p>

<p>However, from the equation below</p>

<p><a href=""https://i.stack.imgur.com/OKy4l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OKy4l.png"" alt=""Policy Gradient with Critic Network""></a></p>

<p>the actor seems to be related to critic network. </p>

<p>I have a few questions</p>

<ol>
<li><p>Does actor network has two inputs, state variable and future reward (output from critic network), or only the state variable?</p></li>
<li><p>If the actor network does take future reward as input, how does it use it? Only during the training stage, or in the action making stage?</p></li>
<li><p>Is there a ""policy iteration"" procedure happens during decision making stage, i.e. for every state <span class=""math-container"">$s_t$</span>, policy network will make several attempts with critic network, and output the best policy? </p></li>
</ol>
"
3006,"<p>What is self-supervision in machine learning? Is it related to supervised learning? How is it different from supervised learning?</p>
"
3007,"<p>In this tutorial from Jeremy Howard: <a href=""https://pytorch.org/tutorials/beginner/nn_tutorial.html"" rel=""nofollow noreferrer"">What is torch.nn really?</a> he has an example towards the end where he creates a CNN for mnist. In <code>nn.Conv2d</code> he makes the inchannels and outchannels: <code>(1,16), (16,16), (16,10)</code>. I get that the last one has to be 10 because there are 10 classes and we want 'probabilities' of each class. But why go up to 16 first? How do you choose this value? And why not just go from 1 to 10, 10 to 10, and 10 to 10? Does this have to do with the kernel_size and stride?</p>

<p>All of the images are <code>28x28</code> so I can't see any correlation between these values and 16 either.</p>

<pre><code>class Mnist_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)

    def forward(self, xb):
        xb = xb.view(-1, 1, 28, 28)
        xb = F.relu(self.conv1(xb))
        xb = F.relu(self.conv2(xb))
        xb = F.relu(self.conv3(xb))
        xb = F.avg_pool2d(xb, 4)
        return xb.view(-1, xb.size(1))
<span class=""math-container"">```</span>
</code></pre>
"
3008,"<p>I have the following problem: </p>

<p>Consider the problem of placing 2 knights and 2 queens on a 4 x 4 chessboard, each piece (knight or queen) per row (denoted by R1, R2, R3, and R4) as shown in the figure, such that no two pieces are attacking each other. One knight must be placed in row R1, the other in R2, one queen in R3, and the other queen in R4.</p>

<p>Formulate this problem as a Constraint Satisfaction problem. Describe the set of variables, and their domain. List the set of constraints. Draw the constraint graph.</p>
"
3009,"<p>I have a system (like a bank) that people (customers) are entered into the systems by a Poisson process, so the time between the arrival of people (two consecutive customers) will be a random variable. The state of the problem is related to just the system (bank), and the action, made inside the system, can be e.g. offering the customer promotion or not (just based on the state of the system, not the status of customers). </p>

<p>To model the problem through RL, 1) it is possible to discretize time horizon into very short time interval (for example 5 minutes as a stage) such that in each time interval, just a single customer enter to our system. On the other hand, 2) it is possible that stages are defined as the time when a customer enters our system. </p>

<p>My questions are:</p>

<ol>
<li><p>Is the second approach an semi-MDP (SMDP)? If I want to solve it with RL, should I use <em>hierarchical RL</em>?</p></li>
<li><p>In the first approach, if a customer enters in a time interval, it is easy to update the Q values. However, what should we do, if we are in state <span class=""math-container"">$S$</span> and take action <span class=""math-container"">$A$</span>, but no customer enters our system, so we do not receive any reward for the pair of <span class=""math-container"">$(S, A)$</span>? There would be no difference if we would take action <span class=""math-container"">$A_{1}$</span>, <span class=""math-container"">$A_{2}$</span>, and so on. This can happen for several consecutive time intervals. I think it is more challenging when we consider eligibility traces.</p></li>
</ol>
"
3010,"<p>I have the aim to build a model to predict global horizontal irradiance (ghi) using satellite images and other features namely the day of the year and time of the day.</p>

<p>For extracting the satellite images features, I aim at using one of the available network in keras (e.g. Xception). Then I can concatenate the features output from the CNN to the other features (I already made the modifications to keras to handle those requirements.</p>

<p>Now, essentially, ghi is also a time-series problem so I assumed using time-series techniques could help. But I don't know where to look for those architectures (CNN + Time series + other features) so :</p>

<ul>
<li><p>given my problem and desires, do you have ressources to help me tackle this issue?</p></li>
<li><p>one thing that is not yet clear to me is if I have don't freeze Xception internal layers, are the fiters parameters automatically learned by the model? I read here and there that filters are optimized also but I never found in keras a clear statement about that.</p></li>
</ul>
"
3011,"<p>First of all, there is a lot of misunderstanding about the Graph search and Tree search. The difference between these two is not about Graph and Tree. They have two different algorithms. You can find the difference in the first answer in this link: <a href=""https://stackoverflow.com/questions/10680180/what-is-the-difference-between-graph-search-and-tree-search"">What is the difference between graph-search and tree search?</a>.</p>

<p>So now I want to be sure that I have understood these two searches well. We have a graph search and a tree search for each searching algorithm. For example BFS graph search and BFS tree search. Or DFS graph search and DFS Tree search (DFS tree search is not complete for example). Iterative deepening graph search and Iterative deepening tree search. </p>

<p>Please answer if you are well informed in this subject because as you see in the above link  many users have answered the question wrong (the difference between graph search and Tree search)</p>
"
3012,"<p>I'm working on my own implementation of NEAT algorithm based on the original 2002 paper called ""<a href=""http://nn.cs.utexas.edu/downloads/papers/stanley.gecco02_1.pdf"" rel=""nofollow noreferrer"">Efficient Reinforcement Learning through Evolving Neural Network Topologies</a>"" (by Kenneth O. Stanley and Risto Miikkulainen). The way the algorithm is designed it may generate loops in connection of hidden layer. Which obviously will cause difficulties in calculating the output.</p>

<p>I have searched and came across two types of approaches. One set like this <a href=""https://ai.stackexchange.com/q/6231/2444"">example</a> claim that the value should be calculated like a time series usually seen in RNNs and the circular nodes should use ""old"" values as their ""current"" output. But, this seems wrong since the training data is not always ordered and the previous value has nothing to do with current one.</p>

<p>A second group like this <a href=""https://ai.stackexchange.com/q/6329/2444"">example</a> claim that the structure should be pruned with some method to avoid loops and cycles. This approach apart from being really expensive to do, is also against the core idea of the algorithm. Deleting connections like this may cause later structural changes.</p>

<p>I my self have so far tried setting the unknown forward values as 0 and this hides the connection (as whatever weight it has will have no effect on the result) but have failed also for two reasons. One is my networks get big quickly destroying the ""smallest network required"" idea and also not good results.</p>

<p>What is the correct approach? </p>
"
3013,"<p>Lets say we have a oracle <span class=""math-container"">$S$</span> that, given any function <span class=""math-container"">$F$</span> and desired output <span class=""math-container"">$y$</span>,  can find an input to <span class=""math-container"">$x$</span> that causes <span class=""math-container"">$F$</span> to output <span class=""math-container"">$y$</span> if it exists, or otherwise returns nil. I.e.:</p>

<p><span class=""math-container"">$$S(F, y) = x \implies F(x) = y$$</span>
<span class=""math-container"">$$S(F, y) = nil \implies !\exists  x \hspace{10px}s.t.\hspace{10px} F(x) = y$$</span></p>

<p>And <span class=""math-container"">$S$</span> takes <span class=""math-container"">$1$</span> millisecond to run (plus the amount of time it takes to read the input and write the output), regardless of <span class=""math-container"">$F$</span> or <span class=""math-container"">$y$</span>. <span class=""math-container"">$F$</span> is allowed to include calls to <span class=""math-container"">$S$</span> in itself.</p>

<p>Clearly with this we can solve any NP-Complete problem in constant time (plus the amount of time it takes to read the input and write the output), and in fact we can go further and efficiently solve any optimization problem:</p>

<pre><code>def IsMin(Cost, MeetsConstraints, x):
  def HasSmaller(y):
    return MeetsConstraints(x) and Cost(y) &lt; Cost(x) and y != x
  return MeetsConstraints(x) and S(HasSmaller, True) == nil

def FindMin(Cost, MeetsConstraints):
  def Helper(x):
    return IsMin(Cost, MeetsConstraints, x)
  return S(Helper, True)
</code></pre>

<p>Which means we can do something like:</p>

<pre><code>def FindSmallestRecurrentNeuralNetworkThatPerfectlyFitsData(Data):
  def MeetsConstraints(x):
    return IsRecurrentNeuralNetwork(x) and Error(x, Data) == 0
  return FindMin(NumParamaters, MeetsConstraints)
</code></pre>

<p>And something similar for any other kind of model (random forest, random ensemble of functions, etc.). We can even solve the halting problem with this, which probably means that there is some proof similar to the halting problem proof that shows such an oracle could not exist. Lets assume this exists anyway, as a thought experiment. </p>

<p>But I'm not sure how to take it from here to something that achieves endless self improvement. What exactly the ""singularity"" even means I suppose is tricky to define formally, but I'm interested in any simple definitions, even if they don't quite capture it.</p>

<p>A sidenote, here is one more function we can do:</p>

<pre><code>IsEquivalent(G, H):
    def Helper(x):
      return G(x) != H(x)
    return P(Helper, True) == nil
</code></pre>
"
3014,"<p>The ""AI Singularity"" or ""<a href=""https://en.wikipedia.org/wiki/Technological_singularity"" rel=""nofollow noreferrer"">Technological Singularity</a>"" is a vague term that roughly seems to refer to the idea of:</p>

<ol>
<li><p>Humans can design algorithms</p></li>
<li><p>Humans can improve algorithms</p></li>
<li><p>Eventually algorithms we design might end up being as good as humans at designing and improving algorithms</p></li>
<li><p>This might lead to these algorithms designing better versions of themselves, eventually becoming far more intelligent than humans. This improvement would continue to grow at an increasing rate until we reach a ""singularity"" where an AI is capable of making technological progress at a rate far faster than we could ever imagine</p></li>
</ol>

<p>Also known as an <a href=""https://wiki.lesswrong.com/wiki/Intelligence_explosion"" rel=""nofollow noreferrer"">Intelligence Explosion</a>. This rough idea has been heavily debated as to its feasibility, how fast it'll take (if it does happen), etc.</p>

<p>However I'm not aware of any formal definitions of the concept of ""singularity"". Are there any? If not, do we have close approximations?</p>

<p>I have seen <a href=""https://en.wikipedia.org/wiki/AIXI"" rel=""nofollow noreferrer"">AIXI</a> and the <a href=""https://en.wikipedia.org/wiki/G%C3%B6del_machine"" rel=""nofollow noreferrer"">G&ouml;del machine</a>, but these both require some ""reward signal"" &mdash; it is unclear to me what reward signal one should choose to bring about a singularity, or really how those models are even relevant here. Because even if we had an oracle that can solve any formal problem given to it, it's unclear to me how we could use that to cause a singularity to happen <a href=""https://ai.stackexchange.com/questions/10643/is-a-very-powerful-oracle-sufficient-to-trigger-the-ai-singularity?noredirect=1&amp;lq=1"">(see this question for more discussion on that note)</a>.</p>
"
3015,"<p>I am generating images that consist of points where the object's location is where the most overlap of points occurs. <a href=""https://i.stack.imgur.com/7FyHl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7FyHl.png"" alt=""Generated image example""></a>
In this example, the object location is (25, 51). I am trying to train a model to just finds the location, I don't care about classification. Additionally, the shape of the overlapping points where the object is located never changes and will always be that shape.</p>

<p><strong>What is a good model for this objective?</strong> 
Many of the potential models I've been looking at (CNN, YOLO, and RCNN) are more concerned with classification than location. Should I search the image for the overlapping dots, create a bound box around them, then retrieve the boxes coordinates? </p>

<p>Thanks :)</p>
"
3016,"<p>I have some trouble understanding the benefits of Bayesian networks.</p>

<p>Am I correct that the key benefit of the network is that one does not need to use 
chain rule of probability in order to calculate joint distributions?</p>

<p>So, using the chain rule:</p>

<p><span class=""math-container"">$$
P(A_1, \dots, A_n) = \prod_{i=1}^n (A_i \mid \cap_{j=1}^{i-1} A_j)
$$</span></p>

<p>leads to the same result as the following (assuming the nodes are structured by an Bayesian network)?</p>

<p><span class=""math-container"">$$
P(A_1, \dots, A_n) = \prod_{i=1}^n P(A_i \mid \text{parents}(A_i))
$$</span></p>
"
3017,"<p>I would like to implement a variant of policy iteration that can choose one or more actions in each state. An example would be to heal and move in the game of Doom.</p>

<p>Parameterizing the power set of all single actions would be one idea, but I was wondering if somebody achieved good results on a similar problem, perhaps by simply defining some lower bound on the output layer and taking all actions with values larger than that bound (i.e. with actions and activation values {shoot=0.2, heal=0.51, move=0.6, jump=0.4} I would choose heal and move if the bound was 0.5)</p>

<p>Another idea was to collect these actions iteratively, i.e. choosing an action from a softmax output based on the state <span class=""math-container"">$s$</span> (taking action ""healing"") and then constructing and using some temporary state <span class=""math-container"">$s_t$</span> to evaluate that state to find another action (e.g. ""moving""). This would require some dummy action that is just used to signal the end of that iteration procedure (i.e. choosing action <span class=""math-container"">$n+1$</span> will not add any other action to the set <span class=""math-container"">$\{ \text{healing}, \text{moving} \}$</span>, but it will lead to the execution of those two actions and transition to the next state <span class=""math-container"">$s'$</span>.</p>
"
3018,"<p>I asked a question a while ago here and since then I've been solving the issues within my code but I have just one question... This is the formula for updating the Q-Matrix in Q-Learning:</p>

<p><span class=""math-container"">$$Q(s_t, a_t) = Q(s_t, a_t) + \alpha \times (R+Q(s_{t+1}, max_a)-Q(s_t, a_t))$$</span></p>

<p>However, I saw a Q-Learning example that uses a different formula, which I'm applying to my own problem and I'm getting good results:</p>

<p><span class=""math-container"">$$Q(s_t, a_t) = R(s_t,a_t) + \alpha \times Q(s_{t+1}, max_a)$$</span></p>

<p>Is this valid?</p>
"
3019,"<p>In a neural network, the number of neurons in the hidden layer corresponds to the complexity of the model generated to map the inputs to output(s).  More neurons creates a more complex function (and thus the ability to model more nuanced decision barriers) than a hidden layer with less nodes.</p>

<p>But what of the hidden layers?  What do more hidden layers correspond to in terms of the model generated?</p>
"
3020,"<p>Hello I am new to reinforcement learning and robotics. So far I have an understanding of the concept on 2D world. You can make agent move one step in one direction. However, how do you define movement action of a robot arm? I am a bit lost over here. Any useful links or keywords would be very appreciated! :)</p>
"
3021,"<p>I have read a lot on Actor Critic and I'm not convinced that there is a qualitative difference doing direct gradient updates on the network and slightly adjusting a soft-max output in the direction of the advantage function and doing gradient descent on the error. </p>

<p>Can anyone explain why updating the gradient directly is necessary? </p>
"
3022,"<p>I am toying around with creating a probability of win calculator for proposals that we do. the information on each proposal is housed in our corporate SharePoint (which I am the admin) </p>

<p>Is there a way to pull directly from SharePoint as the data source rather than have to export to xls then upload each time the data updates?</p>
"
3023,"<p>In some newer robotics literature, the term system identification is used in a certain meaning. The idea is not to use a fixed model, but to create the model on the fly. So it is equal to a model-free system identification. Perhaps a short remark for all, who doesn't know what the idea is. System identification means, to create a prediction model, better known as a forward numerical simulation. The model takes the input and calculates the outcome. It's not exactly the same like a physics engine, but both are operating with a model in the loop which is generating the output in realtime.</p>

<p>But what is policy learning? Somewhere, I've read that policy learning is equal to online system identification. Is that correct? And if yes, then it doesn't make much sense, because reinforcement learning has the goal to learn a policy. A policy is something which controls the robot. But if the aim is to do system identification, than the policy is equal to the prediction model. Perhaps somebody can lower the confusion about the different terms ...</p>

<p><strong>Example</strong> Q-learning is a good example for reinforcement learning. The idea is to construct a q-table and this table controls the robot movements. But, if online-system-identification is equal to policy learning and this is equal to q-learning, then the q-table doesn't contains the servo signals for the robot, but it provides only the prediction of the system. That means, the q-table is equal to a box2d physics engine which can say, what x/y coordinates the robot will have. This kind of interpretation doesn't make much sense. Or does it make sense and the definition of a policy is quite different?</p>
"
3024,"<p>Fuzzy logic is typically used in control theory and engineering applications, but is it connected fundamentally to classification systems?</p>

<p>Once I have a trained neural network (multiple inputs, one output), I have a nonlinear function that will turn a set of inputs  into a number that will estimate how close my set of given inputs are to the trained set. </p>

<p>Since my output number characterizes ""closeness"" to the training set as a continuous number, isn't this kind of inherently some sort of fuzzy classifier? </p>

<p>Is there a deep connection here in the logic, or am I missing something? </p>
"
3025,"<p>I have a neural network that is already trained to predict two continuous outputs from a set of 7 continuous features. </p>

<p>Is there any way to apply the network to predict one of the input features, given other 6 features and the two outputs?</p>
"
3026,"<p>I am learning Deep RL following this tutorial: <a href=""https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8"" rel=""nofollow noreferrer"">https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8</a></p>

<p>I understand everything but one detail:</p>

<p>This image shows the difference between a classic Q learning table and a DNN.
It states that a Q table needs a state-action pair as input and outputs the corresponding Q value whereas a Deep Q network needs the state as feature input and outputs the Q value for each action that can be made in that state.</p>

<p>But shouldn't the state AND the action together be the input to the network and the network just outputs a single Q value?</p>

<p><a href=""https://i.stack.imgur.com/A9ulk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A9ulk.png"" alt=""enter image description here""></a></p>
"
3027,"<p>I have two word embeddings w1 and w2 with dimension 100 as input into my convolutional neural network. It should learn the similarity between these two words. </p>

<p>I am now concerned with the applied convolution operation. What is a reasonable filter size, in which way should the convolution operation perform on the two word embeddings?</p>

<p>Thanks for your advice based on literature/source.</p>
"
3028,"<p>Some (stock market) traders have the ability to produce a high percentage of winning trades (80%+, positive return) over years. I had the chance to look into real money trades of two such traders and I also got trading instructions from them for research. </p>

<p>Now the interesting part is that if you strictly follow their rules then you usually end up with more losers than winners on the long run. But after a while you get some kind of subconscious ""feeling"" for winners which also shows in the results. I assume that this ""feeling"" is a hidden function which can be modeled.</p>

<p>My question is: Is there work about how to model such ""gut feeling"" and subconscious knowledge by means of machine learning (especially with little training data sets)? Is there relevant literature about this topic?</p>

<p>Regards,</p>
"
3029,"<p>I'm looking into using PPO implementations like OpenAi's <a href=""https://spinningup.openai.com/en/latest/user/installation.html"" rel=""nofollow noreferrer"">SpinningUp</a> and <a href=""https://github.com/openai/baselines"" rel=""nofollow noreferrer"">Baselines</a>. However, I fear that these implementations require packages which are not available for Windows. So I'm wondering if in general Linux should be used for working with DRL algorithms. Also, I'm not planing on using OpenAI's gym environments. Environments, created with the ml-agents toolkit from Unity and its gym wrapper are going to be used.</p>

<p>What is your opinion on this? Should I rather work on Linux or Windows? And what do you think about alternatives such as: VM on Windows with Ubuntu or Subsystem on Windows with Ubuntu (WSL)?</p>
"
3030,"<p>I just got interested in machine learning. For days, I searched many sites about the prerequisites and skills (including mathematical skills) needed to learn machine learning. I want to take my learning to an extremely professional level. So please help me clarify my questions and give me some tips about it.</p>
"
3031,"<p>I am presently using a LSTM model to classify high dimensional tabular data which is not text/images (dimensions 21392x1970). I also tried XGBoost (Gradient boosting) in Python separately for the same classification task (classify into one of 14 categories of different categorical values). I have come across the provision of using feature_selection_ method in XGBoost, which can provide me the F1 scores of the most relevant features in prediction. </p>

<p>I would like to create a hybrid model which combines the LSTM with the XGBoost but am confused as to how I can do something such as using the most important features for classification (probably getting these through XGBoost) and then feeding to an LSTM ?)by a combined approach. Any ideas, suggestions and comments are appreciated!</p>
"
3032,"<p>I am working through the famous RL textbook by Sutton &amp; Barto. Currently, I am on the <a href=""http://www.incompleteideas.net/book/ebook/node44.html"" rel=""nofollow noreferrer"">value iteration chapter</a>. To gain better understanding, I coded up a small example, inspired by <a href=""http://www.incompleteideas.net/book/ebook/node41.html"" rel=""nofollow noreferrer"">this article</a>.</p>

<p>The problem is the following</p>

<blockquote>
  <p>There is a rat (R) in a grid. Every square is accessible. The goal is to find the cheese (C). However, there is also a trap (T). The game is over whenever the rat either find the cheese or is trapped (these are my terminal states). </p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/8CqfK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8CqfK.png"" alt=""""></a></p>

<p>The rat can move up, down, left, and right (always by one square). </p>

<p>I modeled the reward as follows:</p>

<pre><code>-1 for every step
5 for finding the cheese
-5 for getting trapped
</code></pre>

<p>I used value iteration for this and it worked out quite nice.</p>

<p>However, now I would like to add another cheese to the equation. In order to win the game, the rat has to collect <em>both</em> cheese pieces.</p>

<p><a href=""https://i.stack.imgur.com/nAlf9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nAlf9.png"" alt=""""></a></p>

<p>I am unsure how to model this scenario. I don't think it will work when I use both cheese squares and the trap square as terminal states, with rewards for both cheese squares.</p>

<p>How can I model this scenario? Should I somehow combine the two cheese states into one?</p>
"
3033,"<p>Does anyone work out ways of relating trained networks by symbolic logic?</p>

<p>For example:
If I train a network on pictures of dogs, and I train a network on pictures of shirts. You could imagine that the simplest way of (without going through the process from scratch), identifying ""dog AND shirt"" would be to perform an AND operation on the last output of the individual cat &amp; dog neural nets.</p>

<p>So ""dog AND shirt"" would amount to AND'ing the output of two nets (Which I believe is described <a href=""https://ai.stackexchange.com/questions/9701/combining-different-trained-neural-networks"">here</a>). </p>

<p>But this operation AND could be replaced with a more complicated operation. And in principle I could ""train"" a network to act as one of these operations. </p>

<p>For instance maybe I could figure out the net that describes some changeable output ""X"" being ""on the shirt."" This would be sort of like a ""functional"" in mathematics (in which we are operating are considering the behavior of a network whos input could be any network).   </p>

<p>If I can figure out this ""functional"" then I would be able to use is symbolically and determine queries like ""dog on the shirt""? - ""cat on the shirt""?</p>

<p>It seems like to me there's a lot of sense to turn specific neural networks into more ""abstract"" objects - and that there would be a lot of power in doing so. </p>
"
3034,"<p>We are given a game between exactly two players, with end result either one of them wins or it is a draw.</p>

<p>I have these things already done:</p>

<ul>
<li>A <code>Player(List weights)</code> can be created given a set of weights. These weights determine how good a player will play (using them internally in an evaluation function with alpha beta search).</li>
<li>A function <span class=""math-container"">$p: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{N} \times \mathbb{N}$</span>, say <code>playMatch(List weights1, List weights2) -&gt; (int, int)</code> which creates two players and lets player A and B play two games against each other and returns the points of A and B (say 2 for a win, 1 for a draw, 0 for a lost game).</li>
<li>An algorithm (I have a genetic algorithm for now) which can optimise a function <span class=""math-container"">$f: \mathbb{R}^m \to \mathbb{R}$</span>, let us call this the fitness function.</li>
</ul>

<p>So obviously there is a mismatch of the signatures of the function I have and the function I can optimise.</p>

<p><strong>Goal</strong> I want to let players with different weights play against each other to improve the best set of weights.</p>

<p><strong>Question</strong> Now what is usually done to optimise a player by letting it play against different versions of itself?</p>

<p><strong>Ideas</strong>
My idea was to save an initial version of the (weights of the) player in a global variable, and then implement a fitness function which plays against the saved player, and when it wins then it replaces the saved player.</p>

<p>However, in this way I am making a function which is a moving target: every time the saved player is updated, actually the complete fitness function changes, so I am not sure whether this makes sense as a proper fitness function at all (will a genetic algorithm handle this?). </p>

<p>Also I am not convinced this will converge at all, because there is an element of chance which player will win because of random move ordering in alpha beta. So I am confused how other people do this? Did I get the setup wrong?</p>
"
3035,"<p>The NHS, Centre for Data Ethics, and Nuffield Council have put together a code of conduct for AI use in health care.</p>

<p>My question is whether item 7 as follows is possible and in what ways might it be:</p>

<p>7.1 Demonstrate the learning methodology of the algorithm being built.</p>

<p>7.2 Aim to show in a clear and transparent way how outcomes are validated.</p>
"
3036,"<p>I am training a video prediction model.</p>

<p>According to the loss plots, the model convergences very fast while the final loss is not small enough and the generation is not good.</p>

<p>Actually, I have test the <code>lr=1e-04</code>and <code>lr=1e-05</code>, the loss plots drop down a little more slowly, but it's still not ideal. But I think <code>lr=1e-05</code>should be small enough, isn't it?</p>

<p>How should I fix my model or the hyper parameters?</p>
"
3037,"<p>I am trying to understand if there is any difference in the the interpretation of accuracy and loss on synthetic data vs real data.</p>
"
3038,"<p><a href=""https://i.stack.imgur.com/yoGv8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yoGv8.png"" alt=""enter image description here""></a></p>

<p>For the UNSW-NB15 dataset i receive spikes in the loss function during training. 
The algorithms see part of this UNSW dataset a single time. 
Loss function is plotted after every batch. 
For other datasets I don't experience this problem. 
I've tried different optimizers and loss functions but this problem remains with this dataset.</p>

<p>I'm using the .fit_generator() function from keras. Is there anyone experience this problem using keras with this function ? </p>

<p>thanks in advance.</p>
"
3039,"<p>State of the art technology in the industry is equal to automation. Lots of machines are used which have increased the productivity. For example a crane, a truck, electric light, a food packaging machine, refrigerators, oven and container ships. No factory owner and no worker is motivated to renounce of these technology, because it helps them to increase the profit, to reduce the costs and to make life easier. The shared similarity of automation with high productivity is the absence of software. The cited machines are working all-mechanical. That means, they didn't have an onboard microcontroller, they didn't have neural networks or a VXworks realtime operating system. Instead the technology in automation is based on classical engineering, it's a combination of electric powersource, motors and a mechanic to execute a job. This allows the crane to lift up a load, and the oven to bake bread.</p>

<p>In contrast, the robotics revolution and especially Artificial Intelligence is about technology which goes beyond classical automation. Via definition, a robot is a software-defined machine which contains of an operating system, a middleware and lots of AI related software packages for vision, motion planning, symbolic planning, and reinforcement learning. The contrast between a robot like the EV3 mindstorms brick and automation technology used in the industry is huge. The main contrast is the software: the EV3 brick has to be programmed, while the forklift not.</p>

<p>What robotics engineers are trying to do is to extend classical automation with robots. The idea is to bring biped walking robots, grasping robots, autonomous cars and humanoid robots into the real life. That means into the factory and into the home. Is it possible to shape this transition soft? Soft means, that the world is not surprised by it but can adapt slowly. Or let me ask the question from a different perspective: From the failed robotics projects in the past, e.g. Halle 54 at VW, or robots at home it is known that introducing robotics into the reality is harder than expected. Until now, not a single household robot is produced for a mass market. Is it possible that a soft transition isn't possible?</p>
"
3040,"<p>This question is related to <a href=""https://ai.stackexchange.com/q/7640/2444"">What does &quot;stationary&quot; mean in the context of reinforcement learning?</a>, but I have a more specific question to clarify the difference between a non-stationary policy and a state that includes time.</p>

<p>My understanding is that, in general, a non-stationary policy is a policy that doesn't change. My first (probably incorrect) interpretation of that was that it meant that the state shouldn't contain time. For example, in the case of game, we could encode time as the current turn, which increases every time the agent takes an action. However, I think even if we include the turn in the state, the policy is still non-stationary so long as sending the same state (including turn) to the policy produces the same action (in case of a deterministic policy) or the same probability distribution (stochastic policy).</p>

<p>I believe the notion of stationarity assumes an additional implicit background state that counts the number of times we have evaluated the policy, so a more precise way to think about a policy (I'll use a deterministic policy for simplicity) would be:</p>

<p><span class=""math-container"">$$ \pi : \mathbb{N} \times S \rightarrow \mathbb{N} \times A $$</span>
<span class=""math-container"">$$ \pi : (i, s_t) \rightarrow (i + 1, s_{t+1}) $$</span></p>

<p>instead of <span class=""math-container"">$\pi : S \rightarrow A$</span>.</p>

<p>So, here is the question: <strong>Is it true that a stationary policy must satisfy this condition?</strong></p>

<p><span class=""math-container"">$$ \forall i, j \in \mathbb{N}, s \in S, \pi (i, s) = \pi(j, s) $$</span></p>

<p>In other words, the policy must output the same result no matter when we evaluate it (either the ith or jth time). Even if the state <span class=""math-container"">$S$</span> contains a counter of the turn, the policy would still be non-stationary because for the same state (including turn), no matter how many times you evaluate it, it will return the same thing. Correct?</p>

<p>As a final note, I want to contrast the difference between a state that includes time, with the background state I called <span class=""math-container"">$i$</span> in my definition of <span class=""math-container"">$\pi$</span>. For example, when we run an episode of 3 steps, the state <span class=""math-container"">$S$</span> will contain 0, 1, 2, and the background counter of number of the policy <span class=""math-container"">$i$</span> will also be set to 2. Once we reset the environment to evaluate the policy again, the turn, which we store in the state, will go back to 0, but the background number of evaluations won't reset and it will be 3. My understanding is that in this reset is when we could see the non-stationarity of the policy in action. If we get a different result here it's a non-stationary policy, and if we get the same result it's a stationary policy, and such property is independent of whether or not we include the turn in the state. Correct?</p>
"
3041,"<p>I am confused as to how neural networks consider the different features by that have access to at the input layer.</p>

<p>Consider this example: I have three features: an image, a dollar amount, and a rating.  However, since the one feature is an image, I need to represent it with very high dimensionality, for example with 128x128=16,384 pixel values.  (I am just using 'image' as an example, my question holds for any feature that needs high dimensional representation: word counts, one-hot encodings, etc.)</p>

<p>Will the 16,384 'features' representing the image completely overwhelm the other 2 features that are the dollar amount and rating?  Ideally, I would think the network would consider each of the three true features relatively equally.  Would this issue naturally resolve itself in the training  process?  Would training become much more difficult of a task?</p>
"
3042,"<p>The definition machine learning is as follows:</p>

<blockquote>
  <p>A computer program is said to learn from experience E with respect to some  task T and performance measure P, if its performance at task T, as measured by P, improves with experience E.</p>
</blockquote>

<p>Here, we talk about what it means for a program to learn rather than a machine, and a program and a machine aren't equivalent, so this is a definition about ""program learning"" rather than ""machine learning"". How is this consistent?</p>
"
3043,"<p>I have a dataset with hundreds of thousands of training examples. There are 27 input variables and one output variable which is always a 0 or a 1, based on whether an event happened or not.</p>

<p>My network therefore has 27 inputs and 1 output. I want the network's output to be a confidence guess of how likely the event is to happen, for example if the output is 0.23 then that represents that the network thinks the event has a 23% chance of happening.</p>

<p>I am using back propagation to train the neural network. It does appear to work well and the network outputs a higher number when the event is more likely and a lower number when the event is less likely.</p>

<p>Would it be a valid concern that my training data only has 0 or 1 values as outputs, when this is not truly what I want the network to output?</p>

<p>My concern comes from the fact that back propagation attempts to reduce the square of the error between the network's output, and the value of the output in the training data, which is always a 0 or a 1. Because it is the <strong>square</strong> of the error it is trying to reduce, I'm concerned that it's probability output may not be a linear mapping to the true probability of the event happening based on the 27 inputs it is seeing.</p>

<p>Is this a valid concern? And are there any techniques I can use to get a neural network to output a linear confidence guess between 0 and 1 when my test data only has outputs of 0 or 1?</p>

<p>I am using the sigmoid activation function for all of my neurons, would there be a better choice of activation function for this problem?</p>

<p>Edit: Thanks to Xpector's answer, I now understand that not all back propagation aims to reduce the square of the error, it depends on the loss function used. I am including a part of the back propagation code I have used here which calculates the error:</p>

<pre><code>var neuronOutput = layerOutputs[i];
var error = (neuronOutput - desiredOutput[i]);
errors[i] = error * Maths.SigmoidDerivative(neuronOutput);
</code></pre>

<p>This is from an open source RProp implementation. I am not sure what loss function is being used here.</p>
"
3044,"<p>I wanna solve a <strong>problem of regression</strong> to predict a factor. I decide to go with <strong>Deep Neural Networks</strong> as solution for my problem.</p>

<p>The features in this problem represent loop characteristic such us loop nest level, loop sizes. The loops hold also instruction (operations) that in itself represent many characteristics like number of variables, loads, stores, etc.</p>

<p>Those instructions maybe positions in the innermost loop or in the middle or under the outermost loop. </p>

<p>We extract here characteristics of Computations in Tiramisu language.</p>

<p>For example, if we have two iterator variables:</p>

<pre><code>var i(""i"", 0, 20), j(""j"", 0, 30);
</code></pre>

<p>and we have the following computation declaration:</p>

<pre><code>computation S(""S"", {i,j}, 4);
</code></pre>

<p>This is equivalent to writing the following <code>C</code> code:</p>

<pre><code> for (i=0; i&lt;20; i++)
      for (j=0; j&lt;30; j++)
         S(i,j) = 4;
</code></pre>

<p>The aspect of receptivity here we can have something like this: </p>

<pre><code> computation S(""S"", {i,j}, 4+M); 
</code></pre>

<p>where ""M"" is computation also.</p>

<p>We considered those features to represent Computations in Tiramisu language.</p>

<pre><code>/** Computations=loops **/
   ""nest_level"" : 3,   // Number of nest levels
   ""perfect_nested"" : 1 ,  // 1 if the loop is perfectly nested , 0 instead
   ""loops_sizes"" : [200,100,300] // Sizes of for loops 
   ""lower_bound"" : [5,0,0], // Bounds of the iterator (in this e.g [2, 510])
   ""upper_bound"" : [205,100,300], 
   ""nb_intern_if"" : 1000, //number of if statements in the nest
   ""nb_exec_if"" : 300, // Estimation of number if 
   ""prec_if"" : 1,  // 1=true if the nest is preceded by if statement  
   ""nb_dependencies_intern"" : 5, // number of dependencies between loops levels in the nest 
   // ""dependencies_extern"" : , // number of extern nest dependencies  
    ""nb_computations"" : 3,  // number of operations (computations) in the nest 
    //std::map&lt;std::string, computation_features *&gt; computations_features; // list of operations Features in the nest
</code></pre>

<p>And this to represent operations: </p>

<pre><code>/** Instructions **/
""n"" : 1, &lt;-- Number of computations
    ""compt_array"" : [
      {
              // Should we add to which level should belong the instructions ?

              ""comp_id"" : 1,  // Unique id for the instructions
              ""nb_var"" : 5,   // Number of the variables in the instructions
              ""nb_const"" : 2, // Number of constantes in the instructions
              ""nb_operands"" : 3, // Number of operands of the operatiion ( including direct values)
              ""histograme_loads"" :  [2,1,5,8], // number of load ops. i.e. acces to inputs per type
              ""histograme_stores"" :  [2,1,5,8], // number of load ops. i.e. acces to inputs per type
              ""nb_library_call"" : 5;  // number of the computation library_calls 
              ""wait_library_argument"" : 2, // number of ar 
              ""operations_histogram"" : [ // number of arithmetic operations per type
                    [0, 2, 0, 0],  // p_int32
                    [0, 0, 0, 0],  // float, for example
                    [0, 0, 0, 0],  // ...
                    [0, 0, 0, 0],
                    [0, 0, 0, 0],
                    [0, 0, 0, 0], // ...
                    [0, 0, 0, 0]  // boolean    
              ]              
      }
  ]
</code></pre>

<p>We may also represent iterator as a characteristic of computation.</p>

<p>The problem in those features we have: </p>

<ol>
<li><p>Loops (Computations) can hold many operations ==> the size of operation vector is variable.</p></li>
<li><p>Instructions (Operations) can be in the level 2, 3 under the innermost 
I mean we can have this situation: </p></li>
</ol>

<blockquote>
<pre><code>for (i=0; i &lt; 20; i++)
    S(i, j) = 4;
for (j=0; j &lt; 30; j++)
    ...
</code></pre>
</blockquote>

<p>or this one:</p>

<blockquote>
<pre><code>for (i=0; i&lt;20; i++)
      for (j=0; j&lt;30; j++)
         S(i,j) = 4;
</code></pre>
</blockquote>

<p>Or many other situations with many instructions ==> their is dependencies between the position of the instruction and the level (iterator) in which it is, in the other way operation hold the id of the iterator :§.</p>

<ol start=""3"">
<li>The operation on itself can be composed with another Computation(Loop nest) which on itself hold instruction and so forth ==> Resistivity.</li>
</ol>

<p>After some research i have found that that DNN has fixed input size. RNN, recursive NN can handle with varying length of inputs. But what about others </p>

<p>how should I present that as input?</p>
"
3045,"<p>How to detect liveness of face using face landmark points? 
I am getting face landmarks from android camera frames. And I want to detect liveness using these landmark points. 
How to tell if a human is making a specific movement that can be useful for liveness detection? </p>
"
3046,"<p>I have created LSTM model using the following Tensorflow code:  </p>

<pre><code>def load_data_old(df_, seq_len):
    data_raw = df_.values 
    data = []
    for index in range(len(data_raw) - seq_len): 
        data.append(data_raw[index: index + seq_len])

    data = np.array(data);
    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));  
    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));
    train_set_size = data.shape[0] - (valid_set_size + test_set_size);

    x_train = data[:train_set_size,:-1,:-1]
    y_train = data[:train_set_size,-1,-1:]

    x_valid = data[train_set_size:train_set_size+valid_set_size,:-1,:-1]
    y_valid = data[train_set_size:train_set_size+valid_set_size,-1,-1:]

    x_test = data[train_set_size+valid_set_size:,:-1,:-1]
    y_test = data[train_set_size+valid_set_size:,-1,-1:]
    return[x_train, y_train, x_valid, y_valid, x_test, y_test]

x_train, y_train, x_valid, y_valid, x_test, y_test = load_data(df,seq_len)
X = tf.placeholder(tf.float32, [None, n_steps, n_inputs],name=""input"")
y = tf.placeholder(tf.float32, [None, n_outputs],name=""output"")
n_inputs = x_train.shape[2]
n_outputs = y_train.shape[1]
layers = [tf.contrib.rnn.LSTMCell(num_units=n_neurons,activation=tf.nn.leaky_relu, use_peepholes = True,name=""layer""+str(layer))
             for layer in range(n_layers)]
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons]) 
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = tf.identity(outputs[:,n_steps-1,:], name=""prediction"")
loss = tf.reduce_mean(tf.square(outputs - y)) # loss function = mean squared error 
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) 
training_op = optimizer.minimize(loss)
</code></pre>

<p>I have given the input data as:  </p>

<pre><code>input1,input2,input3,input4,input5,input6,input7,input8,output
1,2,3,4,5,6,7,8,1
2,3,4,5,6,7,8,9,0
3,4,5,6,7,8,9,10,-1
4,5,6,7,8,9,10,11,-1
5,6,7,8,9,10,11,12,1
6,7,8,9,10,11,12,13,0
7,8,9,10,11,12,13,14,1
</code></pre>

<p>When I am creating data sequences, I am getting:  </p>

<pre><code>[array([[ 1,  2,  3,  4,  5,  6,  7,  8,  1],
       [ 2,  3,  4,  5,  6,  7,  8,  9,  0],
       [ 3,  4,  5,  6,  7,  8,  9, 10, -1]], dtype=int64), array([[ 2,  3,  4,  5,  6,  7,  8,  9,  0],
       [ 3,  4,  5,  6,  7,  8,  9, 10, -1],
       [ 4,  5,  6,  7,  8,  9, 10, 11, -1]], dtype=int64), array([[ 3,  4,  5,  6,  7,  8,  9, 10, -1],
       [ 4,  5,  6,  7,  8,  9, 10, 11, -1],
       [ 5,  6,  7,  8,  9, 10, 11, 12,  1]], dtype=int64), array([[ 4,  5,  6,  7,  8,  9, 10, 11, -1],
       [ 5,  6,  7,  8,  9, 10, 11, 12,  1],
       [ 6,  7,  8,  9, 10, 11, 12, 13,  0]], dtype=int64)]
</code></pre>

<p>I want to know how the LSTM is making association between the data and how data is read-- whether from right to left or vise versa? Meaning that whether my first column values goes first or the last column values followed by others?   </p>
"
3047,"<p>In the paper ""<a href=""https://arxiv.org/pdf/1612.00563.pdf"" rel=""nofollow noreferrer"">Self-critical sequence training for image captioning</a>"", on page 3, they define the loss function (of the parameters <span class=""math-container"">$\theta$</span>) of an image captioning system as the negative expected reward of a generated sequence of words (Equation (3)): </p>

<p><span class=""math-container"">$$L(\theta) = - \mathbb{E}_{w^s \sim p_{\theta}}[r(w^s)],$$</span></p>

<p>where <span class=""math-container"">$w^s = (w_1^s,..., w_T^s)$</span> and <span class=""math-container"">$w^s_t$</span> is the word sampled from the model at time step <span class=""math-container"">$t$</span>.</p>

<p>The derivation of the gradient of <span class=""math-container"">$L(\theta)$</span> concludes with Equation (7), where the gradient of <span class=""math-container"">$L(\theta)$</span> is approximated with a single sample <span class=""math-container"">$w^s \sim p_\theta$</span>: </p>

<p><span class=""math-container"">$$\nabla_{\theta}L(\theta) \approx -(r(w^s) - b) \ \triangledown_{\theta}  log \ p_{\theta}(w^s),$$</span></p>

<p>where <span class=""math-container"">$b$</span> is a reward baseline and <span class=""math-container"">$p_\theta(w^s)$</span> is the probability that sequence <span class=""math-container"">$w^s$</span> is sampled from the model. Up until here I understand what's going on. However, then they proceed with defining the partial derivative of <span class=""math-container"">$L(\theta)$</span> w.r.t. the input of the softmax function <span class=""math-container"">$s_t$</span> (final layer):</p>

<p><span class=""math-container"">$$\nabla_{\theta}L(\theta) = \sum^T_{t=1} \frac{\partial L(\theta)}{\partial s_t} \frac{\partial s_t}{\partial \theta}$$</span> </p>

<p>I still understand the equation above.</p>

<p>And Equation (8):</p>

<p><span class=""math-container"">$$\frac{\partial L(\theta)}{\partial s_t} \approx (r(w^s) - b) (p_\theta(w_t| h_t) - 1_{w^s_t}),$$</span></p>

<p>where <span class=""math-container"">$1_{w^s_t}$</span> is <span class=""math-container"">$0$</span> everywhere, but <span class=""math-container"">$1$</span> at the <span class=""math-container"">$w^s_t$</span>'th entry. How do you arrive at Equation (8)? </p>

<p>I'm happy to provide more information if necessary. In the paper ""<a href=""https://arxiv.org/pdf/1511.06732.pdf"" rel=""nofollow noreferrer"">Sequence level training with recurrent neural networks</a>"", on page 7, they derive a similar result.</p>
"
3048,"<p>I'm creating a schedule for a summer camp. Because of the high risk of rain, the higher priority activities need to be attempted first, so there is more time for later attempts if need be (temporarily ignoring the schedule in that situation). </p>

<p>Camp takes place over four days. My current idea is to map the days to a set of numbers (4, 3, 2, 1), and get a correlation between these numbers and the priorities of activities. But I'm not certain this is the best way to do it, nor what the best way of correlating the two are. I'm also not sure how I would factor this correlation in with the fitness function, along with the priorities themselves.</p>

<p>How should I proceed?</p>
"
3049,"<p>In an unknown environment, how do I avoid an agent to tend to terminate its trajectory in a negative state when time needs to be taken into account?</p>

<p>Suppose the following example to make my question clear:</p>

<ul>
<li>A mouse (M) starts in the bottom left of its world</li>
<li>Its goal is to reach the cheese (C) in the top right (+5 reward) while also avoiding the trap (T) (-5 reward)</li>
<li>It should do this as quickly as possible, so for every timestep it also receives a penalty (-1 reward)</li>
</ul>

<p>If the grid world is sufficiently large, it may actually take the mouse many actions to reach the cheese. </p>

<p>Is there a scenario where the mouse may choose to prefer the trap (-1*small + -5 cumulative reward) versus the cheese (-1*large + 5 cumulative reward)?  Is this avoidable?  How does this translate to an unknown environment where the number of time steps required to reach the positive terminal state is unknown?</p>
"
3050,"<p>Sutton and Barto state in the 2018-version of ""Reinforcement Learning: An Introduction"" in the context of Expected SARSA (p. 133) the following sentences:</p>

<blockquote>
  <p>Expected SARSA is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of <span class=""math-container"">$A_{t+1}$</span>. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does.</p>
</blockquote>

<p>I have three questions concerning this statement:</p>

<ol>
<li>Why is the action selection random with Sarsa? Isn't it on-policy and therefore <span class=""math-container"">$\epsilon$</span>-greedy?</li>
<li>Because Expected-Sarsa is off-policy the experience it learns from can be from any policy that at least explores everything in the limit e.g. random action-selection with equal probabilities for every action. How can Exected-Sarsa learning from such policy be <em>generally</em> better than normal Sarsa learning from an <span class=""math-container"">$\epsilon$</span>-greedy policy, especially with the same amount of experience?</li>
<li>Probably more general: How can on-policy and off-policy algorithms be compared in such way (e.g. through variance) even though their concepts and assumptions are so different?</li>
</ol>
"
3051,"<p>I've been looking at various bounding box algorithms, like the three versions of RCNN, SSD and YOLO, and I have noticed that not even the original papers include pseudocode for their algorithms. I have built a CNN classifier and I am attempting to incorporate bounding box regression, though I am having difficulties in implementation. I was wondering if anyone can whip up some pseudocode for any bounding box classifier or a link to one (unsuccessful in my search) to aid my endeavor.</p>

<p>Note: I do know that there are many pre-built and pre-trained versions of these object classifiers that I can download from various sources, I am interested in building it myself.</p>
"
3052,"<p>I am using rasa nlu for training an NLU system to detect intents and slots. Now, some languages have word endings with their nouns (like Finnish, e.g. ""in Berlin"" -> ""Berliinissä""). I have tried to annotate the characters in the training data as entities, but then I run the model, it doesn't detect the characters inside the word. When those characters are a separate word, only then they're detected. I am unable to think of an implementation to effectively detect named entities within a word. Suggestions needed.</p>
"
3053,"<p>Is there any guidance available for training on very noisy data, when Bayes error rate (lowest possible error rate for any classifier) is high? For example, I wonder if deliberately (not due to memory or numerical stability limitations) lowering the batch size or learning rate could produce a better classifier.</p>

<p>I found so far some general recommendations, not specific for noisy data: <a href=""https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network"">Tradeoff batch size vs. number of iterations to train a neural network</a></p>
"
3054,"<p>I came across this 2 algorithms but I cannot understand the difference between these  2 both in terms of implementation as well as intuitionally.
So what difference does the second point in both the slides referring to?</p>

<p><a href=""https://i.stack.imgur.com/kHuKM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kHuKM.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/dOhO5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dOhO5.png"" alt=""enter image description here""></a></p>
"
3055,"<p>I'm going through the paper <a href=""https://arxiv.org/pdf/1505.05424.pdf"" rel=""nofollow noreferrer""><em>Weight Uncertainty in Neural Networks</em></a> by Google Deepmind. In the final line of the proof of proposition 1, the integral and the derivative are swapped. Then the derivative is taken. But this somehow yields 2 derivatives of <span class=""math-container"">$f$</span> with respect to <span class=""math-container"">$\theta$</span>. I thought that this was the result of a product rule applied to <span class=""math-container"">$q(\epsilon)$</span> and <span class=""math-container"">$f(w,\theta)$</span> and then the chain rule. But that does not yield the same outcome as <span class=""math-container"">$\frac{\partial q(\epsilon)}{\partial \theta} = 0 $</span>. My question is: does anyone understand how the equation in the last line comes about?</p>

<p><a href=""https://i.stack.imgur.com/U7BeO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U7BeO.png"" alt=""Proposition 1""></a></p>
"
3056,"<p>Is it possible to build and AI application which would go to all the services/applications in a large enterprise and gather information about them?</p>
"
3057,"<p>Using standard tree search and experimenting with a few different heuristics, and noticing a lot of decrease key operations (almost 5-6%).  The data structure that I am using doesn't support decrease key, so I end up doing linear time iteration to remove, then add again with the reduced key.  Fringe is about 20K nodes, so linear time is not trivial.</p>

<p>Is there a standard fib heap data structure either in Java that others have tried and liked?  There is one in jgrapht: <a href=""https://jgrapht.org/javadoc/org/jgrapht/util/FibonacciHeap.html"" rel=""nofollow noreferrer"">https://jgrapht.org/javadoc/org/jgrapht/util/FibonacciHeap.html</a></p>

<p>Have others tried with this?</p>

<p>I could also switch to Python since I have the same code running in Python, if this was supported more easily in some standard Python lib.</p>
"
3058,"<p>I have a reinforcement learning agent with both a positive and a negative terminal state.  After each episode during training, I am recording whether a success or failure occurred, and then I can compute a running ratio of success to failure.</p>

<p>I am seeing a phenomenon where, at some point in time, my agent achieves a reasonably high success rate (~80%) for a 100-episode running average.  However, with further training, it seems to 'train itself out' of this behavior and ends the training sequence with a very low success rate (~10-20%).</p>

<p>I am using an epsilon-greedy strategy whereby epsilon decays linearly from 1.0 to 0.1 for the first 10% of episodes and then remains at 0.1 for the remaining 90%.  As such, the 'training out' appears to occur some time where exploration only occurs with 10% probability.</p>

<p>What could be causing this undesirable behavior? How can I combat it?</p>
"
3059,"<p>I have a questionnaire consisting with over 10 questions. The questionnaire is being answered by a lot of people - which I have manually graded. Each question can give the user up to 10 points depending on how they have answered. Let's say that my dataset is big enough, how would I go about using a Neural Network to automatically grade these questions for me?</p>

<p>I have used Neural Networks (CNN) before in relation to image classifying. But when dealing with text classifying, where should I start? Is there some sort of tutorial out there that covers this with a similar example?</p>

<p>Thanks in advance.</p>
"
3060,"<p>Say I have a set of data generated by someone. It could be either bytes from a photo, or readings from bio-sensors, and I have a huge amount of said information, from many people or subjects. Which AI algorithms could be used to learn that a set of data belongs to a subject. I would have the information map that a huge set of data belongs to  Bob, and another belongs to Alice to train the system.</p>
"
3061,"<p>In PPO, every epoch runs many gradient descent iterations to update the policy parameters and the value function. It's not clear to me why we can update both the policy and value at the same time.</p>

<p>Let me elaborate why I think it could be a problem.</p>

<p>The gradient estimator in PPO is:</p>

<p><span class=""math-container"">$$ \hat{g} = \hat{\mathbb{E}}_t \left[ \nabla_{\theta} \log_{\pi_{\theta}} (a_t | s_t) \hat{A}_t \right] $$</span></p>

<p>where the advantage could be approximated with bootstrapping using a learned value function:</p>

<p><span class=""math-container"">$$ \hat{A}_t = r_t + \hat{V}(s_{t + 1}) - \hat{V}(s_t) $$</span></p>

<p>Implicit in these definitions for <span class=""math-container"">$\hat{A}$</span> and <span class=""math-container"">$\hat{V}$</span> is that they are with respect to a policy with certain parameters <span class=""math-container"">$\hat{A}^{\pi_{\theta}}$</span>, <span class=""math-container"">$\hat{V}^{\pi_{\theta}}$</span>. I can see that if these functions had a good estimate of the advantage and value for that policy <span class=""math-container"">$\pi_\theta$</span>, the gradient estimator would have a good baseline. However, in the implementations I've seen, the policy parameters and the parameters of the learned value function are updated at the same time. This means that the baseline used to update the policy parameters are baselines corresponding to an old policy and not the current one. A more intuitive thing to do for me would be to first run a few iterations to update the value function parameters, and then run a few iterations to update the policy, since now we'd have a better value function estimator of the policy we want to improve. Does it make sense to do it the way I described (first update the value estimator and then the policy) or does updating both at the same time work just fine in practice?</p>
"
3062,"<p>I have a set of images. These images are not standard ones but very specific. Some of them have defects and some others don't. I'm new to deep learning software and I would like to know what's the best software to give a set of images for training the soft (images with and without defects) and classify them.</p>

<p>I've been told that Cognex vidi suite is appropiate but it's not free. Are there any free alternatives? Thanks</p>

<p>EDIT: I've just found out that what I need is a software that can do outlier image detection.</p>
"
3063,"<p>I'm trying to replicate the DeepMind paper results, so I implemented my own DQN. I left it training for more than 4 million frames (more than 2000 episodes) on SpaceInvaders-v4 (OpenAI-Gym) and it couldn't finish a full episode. I tried two different learning rates (0.0001 and 0.00125) and seems to work better with 0.0001, but the median score never raises above 200.
I'm using a double DQN.
Here is my code and some photos of the graphs I'm getting each session. 
Between sessions I'm saving the network weights; I'm updating the target network every 1000 steps.
I can't see if I'm doing something wrong, so any help would be appreciated. I'm using the same CNN construction as the DQN paper.</p>

<p>Here's the <strong>action selection</strong> function; it uses a batch  of 4  80x80 processed experiences in grayscale to select the action (s_batch means for state batch):</p>

<pre><code>    def action_selection(self, s_batch):
        action_values = self.parallel_model.predict(s_batch)
        best_action = np.argmax(action_values)
        best_action_value = action_values[0, best_action]
        random_value = np.random.random()

        if random_value &lt; AI.epsilon:
            best_action = np.random.randint(0, AI.action_size)
        return best_action, best_action_value
</code></pre>

<p>Here is my <strong>training function</strong>. It uses the past experiences as training; I tried to implement that if it lose any life, it wouldn't get any extra rewards, so in theory, the agent would try to not die:</p>

<pre><code>    def training(self, replay_s_batch, replay_ns_batch):
        Q_values = []
        batch_size = len(AI.replay_s_batch)
        Q_values = np.zeros((batch_size, AI.action_size))

        for m in range(batch_size):

            Q_values[m] = self.parallel_model.predict(AI.replay_s_batch[m].reshape(AI.batch_shape))
            new_Q = self.parallel_target_model.predict(AI.replay_ns_batch[m].reshape(AI.batch_shape))
            Q_values[m, [item[0] for item in AI.replay_a_batch][m]] = AI.replay_r_batch[m]

            if np.all(AI.replay_d_batch[m] == True):
                Q_values[m, [item[0] for item in AI.replay_a_batch][m]] = AI.gamma * np.max(new_Q)    

        if lives == 0:
            loss = self.parallel_model.fit(np.asarray(AI.replay_s_batch).reshape(batch_size,80,80,4), Q_values, batch_size=batch_size, verbose=0)

        if AI.epsilon &gt; AI.final_epsilon:
            AI.epsilon -= (AI.initial_epsilon-AI.final_epsilon)/AI.epsilon_decay
</code></pre>

<p>replay_s_batch it's a batch of (batch_size) experience replay states (packs of 4 experiences), and replay_ns_batch it's full of 4 next states. The batch size is 32.</p>

<p>And here are some results, after training:
<a href=""https://i.stack.imgur.com/VqRoj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VqRoj.png"" alt=""""></a><br>
In blue, the loss (I think it's correct; it's near-zero). Red dots are the different match scores (as you can see, it does sometimes really good matches). In green, the median (near 190 in this training, with learning rate = 0.0001)
<a href=""https://i.stack.imgur.com/Mf72r.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mf72r.png"" alt=""enter image description here""></a> 
Here is the last training, with lr = 0.00125; the results are worse (it's median it's about 160). Anyway the line it's almost straight, I don't see any variation in any case.
So anyone can point me to the right direction? I tried a similar approach with pendulum and it worked properly. I know that with Atari games it takes more time but a week or so I think it's enough, and it seems to be stuck.
In case someone need to see another part of my code just tell me.</p>

<p><strong>Edit:</strong> With the suggestions provided, I modified the action_selection function. Here it is:</p>

<pre><code>def action_selection(self, s_batch):
    if np.random.rand() &lt; AI.epsilon:
        best_action = env.action_space.sample()
    else:
        action_values = self.parallel_model.predict(s_batch)
        best_action = np.argmax(action_values[0])
    return best_action
</code></pre>

<p>To clarify my last edit: with action_values you get the q values; with best_action you get the action which corresponds to the max q value. Should I return that or just the max q value?</p>
"
3064,"<p>Here is what I understand (what I think I understand).</p>

<p>We first train out model on our images using transfer learning.</p>

<p>So now we have a pre-trained model.</p>

<p>For each image in out dataset, we compute <strong>selective search</strong> on it, which makes 2000 region proposals.
    These 2000 region proposals are feed through our pre-trained NN ,</p>

<p>However we only collect the output (feature maps) from the last convolution layer. These outputs are saved to a hard-disk.</p>

<p>These feature maps are fed into a SVM for another round of training, were another label, ""no object"" is added.</p>

<p>We also have regression model that trains based on the window coordinates that we also annotated.</p>

<p>So we have SVN and a regression model (two models) that we train.</p>

<p>1)Is the above correct?</p>

<p>2) Are each of these 2000 region proposals hand-labeled (correct label (cat, dog etc) or no-object) before feeding it into the SVM?</p>

<p>3) Is the regression model tied into the SVM model? Basically out loss is a combination of both regression coords and SVM classification?</p>
"
3065,"<p>In reinforcement learning, in general, successive states (actions and rewards) are highly correlated. An ""<a href=""http://www.incompleteideas.net/lin-92.pdf"" rel=""nofollow noreferrer"">experience replay</a>"" buffer was used, in the <a href=""https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf"" rel=""nofollow noreferrer"">DQN architecture</a>, to avoid training the neural network (NN), which represents the <span class=""math-container"">$Q$</span> function, with correlated (or non-independent) data. In statistics, the i.i.d. (""independently and identically distributed"") assumption is often made. See e.g. <a href=""https://stats.stackexchange.com/q/213464/82135"">this question</a>. <a href=""https://qr.ae/TUfiQ2"" rel=""nofollow noreferrer"">This</a> is another related question. Intuitively, if consecutive data points are often correlated, then we, as humans, might learn slowly (because the differences between the consecutive data points are not sufficient to infer more about the associated distribution).</p>

<p>Mathematically, why exactly do (feed-forward) neural networks (or multi-layer perceptrons) require i.i.d. data (when being trained)? Is this only because <a href=""https://www.ijcai.org/Proceedings/07/Papers/121.pdf"" rel=""nofollow noreferrer"">we use back-propagation to train NNs</a>? If yes, why would back-propagation require i.i.d. data? Or is actually the optimisation algorithm (like gradient-descent) which requires i.i.d. data? Back-propagation is just the algorithm used to compute the gradients (which is e.g. used by GD to update the weights), so I think that back-propagation isn't really the problem.</p>

<p>When using recurrent neural networks (RNNs), we apparently do not make this assumption, given that we expect consecutive data points to be highly correlated. So, why do feed-forward NNs required the i.i.d. assumption but not RNNs? </p>

<p>I'm looking for a rigorous answer (ideally, a proof) and not just the intuition behind it. If there is a paper which answers this question, you can simply link us to it.</p>
"
3066,"<p>I am currently trying to understand the method of generating anchor boxes for object detection. I am looking at a code where the author has done this task in a very flexible way. But I am having problems understanding some part of it.</p>

<p>As every part of code is highly dependent each other, misunderstanding a small part leads to confusion and then I have to start from beginning. Please help...</p>

<p>This is the function used by the author in his code:</p>

<pre><code>    def generate_anchor_boxes_for_layer(self,
                                        feature_map_size,
                                        aspect_ratios,
                                        this_scale,
                                        next_scale,
                                        this_steps=None,
                                        this_offsets=None,
                                        diagnostics=False):
        '''
        Computes an array of the spatial positions and sizes of the anchor boxes for one predictor layer
        of size `feature_map_size == [feature_map_height, feature_map_width]`.

        Arguments:
            feature_map_size (tuple): A list or tuple `[feature_map_height, feature_map_width]` with the spatial
                dimensions of the feature map for which to generate the anchor boxes.
            aspect_ratios (list): A list of floats, the aspect ratios for which anchor boxes are to be generated.
                All list elements must be unique.
            this_scale (float): A float in [0, 1], the scaling factor for the size of the generate anchor boxes
                as a fraction of the shorter side of the input image.
            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if
                `self.two_boxes_for_ar1 == True`.
            diagnostics (bool, optional): If true, the following additional outputs will be returned:
                1) A list of the center point `x` and `y` coordinates for each spatial location.
                2) A list containing `(width, height)` for each box aspect ratio.
                3) A tuple containing `(step_height, step_width)`
                4) A tuple containing `(offset_height, offset_width)`
                This information can be useful to understand in just a few numbers what the generated grid of
                anchor boxes actually looks like, i.e. how large the different boxes are and how dense
                their spatial distribution is, in order to determine whether the box grid covers the input images
                appropriately and whether the box sizes are appropriate to fit the sizes of the objects
                to be detected.

        Returns:
            A 4D Numpy tensor of shape `(feature_map_height, feature_map_width, n_boxes_per_cell, 4)` where the
            last dimension contains `(xmin, xmax, ymin, ymax)` for each anchor box in each cell of the feature map.
        '''
        # Compute box width and height for each aspect ratio.

        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.
        size = min(self.img_height, self.img_width)
        # Compute the box widths and and heights for all aspect ratios
        wh_list = []
        for ar in aspect_ratios:
            if (ar == 1):
                # Compute the regular anchor box for aspect ratio 1.
                box_height = box_width = this_scale * size
                wh_list.append((box_width, box_height))
                if self.two_boxes_for_ar1:
                    # Compute one slightly larger version using the geometric mean of this scale value and the next.
                    box_height = box_width = np.sqrt(this_scale * next_scale) * size
                    wh_list.append((box_width, box_height))
            else:
                box_width = this_scale * size * np.sqrt(ar)
                box_height = this_scale * size / np.sqrt(ar)
                wh_list.append((box_width, box_height))
        wh_list = np.array(wh_list)
        n_boxes = len(wh_list)

        # Compute the grid of box center points. They are identical for all aspect ratios.

        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.
        if (this_steps is None):
            step_height = self.img_height / feature_map_size[0]
            step_width = self.img_width / feature_map_size[1]
        else:
            if isinstance(this_steps, (list, tuple)) and (len(this_steps) == 2):
                step_height = this_steps[0]
                step_width = this_steps[1]
            elif isinstance(this_steps, (int, float)):
                step_height = this_steps
                step_width = this_steps
        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.
        if (this_offsets is None):
            offset_height = 0.5
            offset_width = 0.5
        else:
            if isinstance(this_offsets, (list, tuple)) and (len(this_offsets) == 2):
                offset_height = this_offsets[0]
                offset_width = this_offsets[1]
            elif isinstance(this_offsets, (int, float)):
                offset_height = this_offsets
                offset_width = this_offsets
        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.
        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_size[0] - 1) * step_height, feature_map_size[0])
        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_size[1] - 1) * step_width, feature_map_size[1])
        cx_grid, cy_grid = np.meshgrid(cx, cy)
        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down
        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down

        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`
        # where the last dimension will contain `(cx, cy, w, h)`
        boxes_tensor = np.zeros((feature_map_size[0], feature_map_size[1], n_boxes, 4))

        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, n_boxes)) # Set cx
        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, n_boxes)) # Set cy
        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w
        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h

        # Convert `(cx, cy, w, h)` to `(xmin, ymin, xmax, ymax)`
        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')

        # If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries
        if self.clip_boxes:
            x_coords = boxes_tensor[:,:,:,[0, 2]]
            x_coords[x_coords &gt;= self.img_width] = self.img_width - 1
            x_coords[x_coords &lt; 0] = 0
            boxes_tensor[:,:,:,[0, 2]] = x_coords
            y_coords = boxes_tensor[:,:,:,[1, 3]]
            y_coords[y_coords &gt;= self.img_height] = self.img_height - 1
            y_coords[y_coords &lt; 0] = 0
            boxes_tensor[:,:,:,[1, 3]] = y_coords

        # `normalize_coords` is enabled, normalize the coordinates to be within [0,1]
        if self.normalize_coords:
            boxes_tensor[:, :, :, [0, 2]] /= self.img_width
            boxes_tensor[:, :, :, [1, 3]] /= self.img_height

        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.
        if self.coords == 'centroids':
            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.
            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')
        elif self.coords == 'minmax':
            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).
            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')

        if diagnostics:
            return boxes_tensor, (cy, cx), wh_list, (step_height, step_width), (offset_height, offset_width)
        else:
            return boxes_tensor

</code></pre>

<p>Here, I can't understand the terms this_steps and this_offsets. I tried understanding them in the way it was described in comments but then I could not be able to understand the rest of the code from how the center x and y are calculated and how the boxes are generated from it.</p>

<p>Please help.. Thank you..</p>

<p><strong>Edit</strong>: I also didn't get why the author normalized the anchor box size using image width and image height. I mean we perform normalization in such cases in order to make our anchor boxes independent of different size of images. Then, why did he use image width and height to normalize the anchor box size?</p>
"
3067,"<p>I was reading the paper by Kalchbrenner et al. titled <a href=""https://arxiv.org/pdf/1404.2188.pdf"" rel=""nofollow noreferrer"">A Convolutional Neural Network for Modelling Sentences</a> and I am struggling to understand their definition of convolutional layer.</p>

<p>First, let's take a step back and describe what I'd expect the 1D convolution to look like, just as defined in <a href=""https://arxiv.org/pdf/1408.5882.pdf"" rel=""nofollow noreferrer"">Yoon Kim (2014)</a>. </p>

<blockquote>
  <p>sentence. A sentence of length n (padded where necessary) is represented as</p>
  
  <p><span class=""math-container"">$x_{1:n} = x_1 \oplus  x_2 \oplus \dots ⊕ x_n,$</span> (1)</p>
  
  <p>where <span class=""math-container"">$\oplus$</span> is the concatenation operator. In general, let <span class=""math-container"">$x_{i:i+j}$</span> refer to the concatenation of words <span class=""math-container"">$x_i, x_{i+1}, \dots, x_{i+j}$</span>. A convolution operation involves a filter <span class=""math-container"">$w \in \mathbb{R}^{hk}$</span>, which is applied to a window of h words to produce a new feature. For example, a feature ci is generated from a window of words <span class=""math-container"">$x_{i:i+h−1}$</span> by</p>
  
  <p><span class=""math-container"">$c_i = f(w \cdot x_{i:i+h−1} + b)$</span> (2).</p>
  
  <p>Here <span class=""math-container"">$b \in \mathbb{R}$</span> is a bias term and <span class=""math-container"">$f$</span> is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sentence <span class=""math-container"">$\{x_{1:h}, x_{2:h+1}, \dots, x_{n−h+1:n}\}$</span> to produce a feature map</p>
  
  <p><span class=""math-container"">$c = [c_1, c_2, \dots, c_{n−h+1}]$</span>, (3)</p>
  
  <p>with <span class=""math-container"">$c \in \mathbb{R}^{n−h+1}$</span>.</p>
</blockquote>

<p>Meaning a single feature detector transforms every window from the input sequence to a single number, resulting in <span class=""math-container"">$n-h+1$</span> activations.</p>

<p>Whereas in Kalchbrenner's paper, the convolution is described as follows:</p>

<blockquote>
  <p>If we temporarily ignore the pooling layer, we may state how one computes each d-dimensional column a in the matrix a resulting after the convolutional and non-linear layers. Define <span class=""math-container"">$M$</span> to be the matrix of diagonals:</p>
  
  <p><span class=""math-container"">$M = [diag(m:,1), \dots, diag(m:,m)]$</span> (5)</p>
  
  <p>where <span class=""math-container"">$m$</span> are the weights of the d filters of the wide convolution. Then after the first pair of a convolutional and a non-linear layer, each column <span class=""math-container"">$a$</span> in the matrix <strong>a</strong> is obtained as follows, for some index <span class=""math-container"">$j$</span>:</p>
  
  <p><a href=""https://i.stack.imgur.com/7EwxO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7EwxO.png"" alt=""enter image description here""></a></p>
  
  <p>Here <span class=""math-container"">$a$</span> is a column of first order features. Second order features are similarly obtained by applying Eq. 6 to a sequence of first order features <span class=""math-container"">$a_j, \dots, a_{j+m'−1}$</span> with another weight matrix <span class=""math-container"">$M'$</span>. Barring pooling, Eq. 6 represents a core aspect of the feature extraction function and has a rather general form that we return to below. Together with pooling, the feature function induces position invariance and makes the range of higher-order features variable.</p>
</blockquote>

<p>As described in <a href=""https://stats.stackexchange.com/questions/345977/what-does-the-matrix-m-diagm-1-ldots-diagm-m-look-like"">this question</a>, the matrix <span class=""math-container"">$M$</span> has dimensionalty of <span class=""math-container"">$d$</span> by <span class=""math-container"">$d * m$</span> and the vector of concatenated <span class=""math-container"">$w$</span>'s has dimensionality <span class=""math-container"">$d * m$</span>. Thus the multiplication produces a vector of dimensionality d (for a single convolution of a single window!).</p>

<p>Architecture visualization from the paper seems to confirm this understanding:</p>

<p><a href=""https://i.stack.imgur.com/3iHN7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3iHN7.png"" alt=""enter image description here""></a></p>

<p>The two matrices in the second layer represent two feature maps. Each feature map has dimensionality <span class=""math-container"">$(s + m - 1) \times d$</span>, and not <span class=""math-container"">$(s + m - 1)$</span> as I would expect.</p>

<p>Authors refer to a ""conventional"" model where feature maps have only one dimension as Max-TDNN and differentiate it from their own.</p>

<p>As the authors point out, feature detectors in different rows are fully independent from each other until the top layer. Thus they introduce the Folding layer, which merges each pair of rows in the penultimate layer (by summation), reducing their number in half (from <span class=""math-container"">$d$</span> to <span class=""math-container"">$d/2$</span>).</p>

<p><a href=""https://i.stack.imgur.com/e1Dyn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/e1Dyn.png"" alt=""architecture""></a></p>

<p>Sorry for the prolonged introduction, here are my two main questions:</p>

<ol>
<li><p>What is the possible motivation for this definition of convolution (as opposed to Max-TDNN or e.g. Yoon Kim's model)</p></li>
<li><p>In the Folding layer, why is it satisfying to only have dependence between pairs of corresponding rows? I don't understand the gain over no dependence at all.</p></li>
</ol>
"
3068,"<p>Is it possible to create a voice-recognition virtual assistant using local storage?</p>
"
3069,"<h3>Abstract</h3>

<p>I wish to design a neural network that will categorize messages based on criteria I have predefined. It should feature the ability to be proactively trained as it continues its lifecycle. This means a human can intervene in its categorization attempts and determine whether or not it was correct and have it adjust its weights accordingly (without having to retrain all over again).</p>

<h3>Inputs</h3>

<p>It is know that ALL input will follow these rules:</p>

<ol>
<li>Always of <span class=""math-container"">$N$</span> length</li>
<li>All messages are transformed to eliminate unnecessary complexity</li>
</ol>

<p>Here's a brief overview of how an example message might be processed.</p>

<p>Starting with a message <span class=""math-container"">$M$</span>:</p>

<blockquote>
  <p>That's an interesting perspective. I think that you should consider adding more details to your point about the cat being too silly.</p>
</blockquote>

<p>The text is then transformed so that extra details are removed:</p>

<blockquote>
  <p>thats an interesting perspective i think that you should consider adding more details to your point about the cat being too silly</p>
</blockquote>

<p>Then it's converted into a vector (appending <span class=""math-container"">$0$</span> to reach length <span class=""math-container"">$N$</span>) ready to be processed by the neural network:</p>

<pre>[116, 104, 97, 116, 115, 32, 97, . . . , 0, 0, 0, 0]</pre>

<h3>Ouputs</h3>

<p>In my network, I wish all the outputs to be weighted on how well they fit in each category. I <strong>need</strong> multiple outputs. I'm not really focusing on one particular category per-say, but how well the message fits in all of them. </p>

<p>Following the input <span class=""math-container"">$M$</span> I used as an example, I'd expect the outputs to look <em>something</em> like this after my vector has fed-forward:</p>

<pre>Suggestive:  0.89042<br>Opinionated: 0.68703</pre>

<p>The weight values for each output indicate the strength of the category in the overall message.</p>

<p>From message <span class=""math-container"">$M$</span>:</p>

<blockquote>
  <p>That's an interesting perspective.</p>
</blockquote>

<p>Would weigh the opinionated category as <span class=""math-container"">$0.68703$</span>.</p>

<p>And:</p>

<blockquote>
  <p>I think that you should consider adding more details to your point about the cat being too silly.</p>
</blockquote>

<p>Would weigh the suggestive category as <span class=""math-container"">$0.89042$</span>.</p>

<h3>Summary and Questions</h3>

<p>I'm interested in the architectural design choices of a network that would support my feature set. The main goal is to be able to train my network to categorize messages based on pre-trained (and live-trained) data. I'd like to know things like:</p>

<ol>
<li>What type of Neural Network I should use for this purpose? I've researched <a href=""https://en.wikipedia.org/wiki/Long_short-term_memory"" rel=""nofollow noreferrer"">LSTM</a> &amp; <a href=""https://en.wikipedia.org/wiki/Recurrent_neural_network"" rel=""nofollow noreferrer"">Recurrent</a> networks; which have been mentioned to be good at processing sequences (ie. messages).</li>
<li>What considerations should I account for when creating this network?.</li>
<li>How can the overall network support live-training so I can tell my network when its wrong and have it 'correct' itself without having to retrain completely? </li>
</ol>
"
3070,"<p>I understand what an admissible heuristic is, I just don't know how to tell if one heuristic is admissible or not. So, in this case, I'd like to know why Nilsson heuristic isn't admissible.</p>
"
3071,"<p>I have a time series of human pose data which are recorded from real humans.
I want to train the model with unsupervised learning on the training data.
Let's call this the ""real"" training data.
The fake data is generated from moving/rotating joints of the pose.</p>

<p>After the model is trained on the real data, I would want to feed the model ""fake"" or ""real"" data and let it tells me how likely is the data to be real.
E.g. if the data looks very real, the model tells me a probability value close to 1. If it's fake, return something close to 0.</p>

<p>I want to do this so that I can iteratively adjust the input data such that it maximizes this probability value. The application is to have a fake data, adjust it enough times until it looks real.</p>

<p>I know about GAN but I don't know how to apply it to tabular data or data that are time series (not images)</p>

<p>Please suggest me what kind of model or problem definition that suits my needs. Also any topic that I need to learn about.</p>

<p>I'm modeling this in keras. Current approach is to randomly change the input until the model classifies the data as real.</p>
"
3072,"<p>I have a complex wargame already developed in a aging Objective-C and I would like to improve the AI</p>

<p>I have built the logic for self-play, fitness evaluation and evolution
The hard-time is the ability to run a lot of experiences of self-play with limited ressources (single Mac). Time is a factor but also memory. I am facing some random crashed after a given number of games</p>

<p><strong>I was wondering</strong> 
- if people have faced the same issues with a large number of runs with objective-C 
- if other people have tried to use Genetic Programming or Reinforcement Learning with Objective-C or C#</p>
"
3073,"<p>I have these Monte Carlo Tree Search, and I need to expand it, but I don't understand the step 1 and 2, why it it goes to the first Node and then make a new Node? instead of go to the depthest left Leaf?</p>

<p>I thought it need to go to the Most Probability Leaf.</p>

<p>MonteCarlo Tree Search:
<a href=""https://i.stack.imgur.com/y4qe8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/y4qe8.jpg"" alt=""enter image description here""></a></p>

<p>Monte Carlo Tree Step 1, it added a New Node, now are 9 Cases:
<a href=""https://i.stack.imgur.com/qsO5s.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qsO5s.jpg"" alt=""enter image description here""></a>
Monte Carlo Tree Step 2, it added a New Node, now are 2 Cases under the First Node:<a href=""https://i.stack.imgur.com/9D1b6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9D1b6.jpg"" alt=""enter image description here""></a></p>
"
3074,"<p>I am developing an AI tool for <strong>anomaly detection in a distributed system</strong>.  The system supports an interface that combines several individual logs into a single log file generating approx. 7000 entries/min. The logs entries are partially system generated (d-Bus, IPC, ….)  and human written statements (Status not received, initialized successfully, ….). The developers use the generated log for debugging. The entries have been configured to have a similar format depending on the generated system (timestamp, ids, component, context, verbosity level, description, ….). </p>

<p><em>Background:</em><br>
1. The history of the identified anomalies is minimal and not archived.<br>
2. Not many similar event templates in log files.<br>
3. Software execution rules are not clearly documented.<br>
4. The log events are co-related.  </p>

<p>What are the recommended algorithms (Statistical, NLP, ML, Neural networks) that can be used to efficiently perform pattern extraction on the entries and identify existing and new anomalous behavior?</p>
"
3075,"<p>A bit of clarification on pytorch's <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss"" rel=""nofollow noreferrer""><code>BCEWithLogitsLoss</code></a>: I am using : <code>pos_weights = torch.tensor([len_n/(len_n + len_y), len_y/(len_n + len_y)])</code> to initialize the loss, with <code>[1.0, 0.0]</code> being the negative class and <code>[0.0, 1.0]</code> being the positive class, and <code>len_n, len_y</code> being respectively the length of negative and positive samples. </p>

<p>The reason to use <code>BCEWithLogitsLoss</code> in the first place is precisely because I assume that it is compensating an imbalance between the quantity of positive and negative samples by avoiding the network from simply ""defaulting"" to the most abundant class type in the training set. I want to control the priorization of the loss on detecting the less abundant class correctly. In my case, negative train samples exceed positive samples by a factor of 25 to 1, so it is very important that the network predicts a high fraction of positive samples correctly, rather than having a high overall prediction rate (even by defaulting always to negative, that would lead to 96% prediction if I only cared about that).</p>

<p><strong>Question</strong> Is it correct my assumption about <code>BCEWithLogitsLoss</code> using the <code>pos_weights</code> parameter to control training class imbalances? Any insight into how the imbalance is being addressed in the loss evaluation?</p>
"
3076,"<p>The blocked N-queens is a variant of the N-queens problem. In the blocked N-queens problem, we also have a NxN chess board and N queens. Each square can hold at most one queen. Some squares on the board are blocked and can not hold any queens. Conditionality is that queens do not dare to attack each other. At the entrance to this problem are the queues and blocked areas. </p>

<p>How do I model the blocked N queens problem as a search problem, so that I can apply search algorithms like BFS?</p>
"
3077,"<p>I'd like to ask you how to use data from range &lt;-1,1> in neural networks to have the best results? I came up with three scenarios:</p>

<ol>
<li>Use it as it is and check if activation functions work with &lt;-1,1> data.</li>
<li>Move it to range &lt;0,1> with 0.5 as middle value (taking a role of 0)</li>
<li>Split it into two inputs - one is positive, one negative, and when one keeps the module of previous value second is equal to 0.</li>
</ol>

<p>What is your experience?</p>
"
3078,"<p>I want to build a DNN model that I will later integrate into a C++ program.</p>

<p>I heard that PyTorch model is hard to load it on C++ and the integration requires extra code, and it's complicated.</p>

<p>I have searched a bit on the internet and I have found this post: <a href=""https://pytorch.org/tutorials/advanced/cpp_export.html"" rel=""nofollow noreferrer"">Loading a PyTorch Model in C++</a>. I am still unsure whether or not I will have any problems in practice if I opt for PyTorch.</p>
"
3079,"<p>What would be one good daily life example of SAT problem? I've thought about this one:
The problem of placing a bunch of different kinds of glasses in a shared cabinet in such a way that some constraints would be satisfied, such as putting the longer ones in the back of the shorter ones so it will be easier to take them when we need. 
Is it a good one, or can you think of any other better one?</p>
"
3080,"<p>Is it a common practice to do 3-dimensional imagary data acquisition and implementing the dataset to certain high resolution motor driven humanoid candidates to mimic the human micro expressions or walk signatures ? If so, what would be the exact law studies that distinguishes the original dataset that determines if a person has committed a certain crime or not rather than the AI itself ?</p>
"
3081,"<p>We wanna build a DNN model to predict unrolling factor though our features represent variable length of inputs. Knowing that we have to give our features at once <strong>""0 padding""</strong> look like the only solution that may solve our problem.</p>

<p>Many people suggest to use a <strong>masking layer</strong> so that padded values are ignored.</p>

<p>I have find some posts talking about using 0 padding in RNN, though I do not know if it's applied same way in case of MLPs.</p>

<p>How does it work masking layer with 0 padding in case of MLPs? f What are advantages of disadvantages of this solution ? In which cases this solution is useful and when it is not ? </p>
"
3082,"<p>You may have heard of GPT2, a new language model. It has recently attracted attention from the general public as the foundation that published the paper, <a href=""https://blog.openai.com/better-language-models/"" rel=""nofollow noreferrer"">OpenAI</a>, ironically refused to share the whole model fearing dangerous implications. Along the paper, they also published a manifesto to justify their choice: ""Better Language Models and Their Implications"". And soon a lot of media were publishing articles discussing the choice and its effectiveness to actually prevent bad implications. I am not here to discuss the ethical components of this choice but the actual performance of the model.</p>

<p>The model got my attention too and I downloaded the small model to play with. To be honest I am far from impressed by the results. Some times the first paragraph of the produced text appears to make sense, but nine times out of ten it is giberish by the first or the second sentence. Exemples given in the paper seems to be ""Lucky"" outputs, cherry picked by human hands. Overall, the paper may suffer from a very strong publication bias.</p>

<p>However most article we can read on the internet seems to take its powerfullness for granted. <a href=""https://www.technologyreview.com/s/612975/ai-natural-language-processing-explained/"" rel=""nofollow noreferrer"">The MIT technology review</a> wrote : ""The language model can write like a human [...]"", <a href=""https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction"" rel=""nofollow noreferrer"">The Guardian</a> wrote ""When used to simply generate new text, GPT2 is capable of writing plausible passages that match what it is given in both style and subject. It rarely shows any of the quirks that mark out previous AI systems, such as forgetting what it is writing about midway through a paragraph, or mangling the syntax of long sentences."". The model appears generally qualified as a ""breaktrough"". These writings do not match my personnal experimentation as produced texts are rarely consistent / syntaxically correct. </p>

<p>My question is : whitout the release of the whole model for ethical reasons, how do we know if the model is really that powerfull ?</p>
"
3083,"<p>Why do we use the seed function in the 'Pendulum-v0' environment?</p>

<p><a href=""https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#L25"" rel=""nofollow noreferrer"">https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py#L25</a></p>
"
3084,"<p>While going through the basics of machine learning, I came across these three topics that I am having trouble understanding.</p>

<p>Among the three, learning by analogy seems pretty intuitive to me looking at the terms itself. The other two are quite confusing. </p>

<p>I have tried searching on the internet but that haven't helped me anyway. I however have read the contents in these two sites : <a href=""http://users.cs.cf.ac.uk/Dave.Marshall/AI2/node144.html#SECTION000161000000000000000"" rel=""nofollow noreferrer"">Inductive Learning</a> and <a href=""http://users.cs.cf.ac.uk/Dave.Marshall/AI2/node148.html"" rel=""nofollow noreferrer"">Explanation based Learning</a>. The terms there are too difficult to understand for me.</p>

<p>Would you please help me understand the concept of these approaches and the difference between them?</p>

<p>A link to some explanatory article/notes/blog post are appreciated too. Thank you for your valuable help.</p>
"
3085,"<p>I order to build better artifical agents (AA) we need the right tasks and data to train on. The task I have in mind is the well-know game ""I spy with my little eye"", where agent A has to guess the thing agent B is seeing by asking questions about it's nature.</p>

<p>I have a mobile game in mind which would likely be popular enough to provide large amounts of data. This data could be  used to improve some aspects of artifical agents: (i) concise object representations consisting of characteristics humans deem relevant (ii) immitate human reasoning process based on the questions being asked to drill down to the object in mind. (iii) could be used for other things like chatbots, Q&amp;A, IR and to create better embeddings. And since humans and AA's alike could play it against each other, some interesting scenarios are possible. </p>

<p>Let it sink in for a second. What do you think about the usefullness of such labeled data and task?</p>
"
3086,"<p>How to detect presence of object (highly occluded) in a scene?</p>

<p>There are specific features (small patterns, etc), which allow to say that object is present, but it is not enough for detection for YOLO or RPCNN.</p>

<p>How to detect small specific pattern in a whole image efficiently?</p>
"
3087,"<p>I want to apply the concept that exists in the <a href=""https://dialogflow.com/"" rel=""nofollow noreferrer"">Dialogflow</a> API in my e-commerce website.</p>

<p>I get some references in this regard :</p>

<ol>
<li>Tokenization</li>
<li>Part Of Speech</li>
<li>Named Entity Recognition</li>
<li>Rule based </li>
</ol>

<p>I just saw that I just didn't understand how to implement it on the website. 
so I still don't know how the truth is.</p>

<p>please give me a method or explanation that can help me create a chatbot for ecommerce that can give action when a user asks for a product and wants to place an order or something else.</p>

<p>Please give me some explanation or method or references :(</p>
"
3088,"<p>I am trying to solve the spiral exercise in TensorFlow but I can not add features like in this answer <a href=""https://ai.stackexchange.com/a/10000/22691"">https://ai.stackexchange.com/a/10000/22691</a>. How can I do that?</p>
"
3089,"<p>I have just found the paper and documentation about GAN 2.0, the new face creator from Nvidia.</p>

<p>On the website <a href=""https://thispersondoesnotexist.com/"" rel=""nofollow noreferrer"">https://thispersondoesnotexist.com/</a> they have used this approach to create realistic faces. Unfortunately, the website does not exist anymore.</p>

<p>Is there another webpage demonstrating the new face creator from Nvidia?</p>
"
3090,"<p>Algorithm consists of three steps:
goal test, generate, and ordering function.
In that order. It seems a shame to generate a node that is in fact a solution, but to fail to recognize it because the ordering function fails to place it first. </p>
"
3091,"<p>Hi all I am using python 2.7 with Theano 
I have an input images and kernels </p>

<p>I want to do this thing </p>

<p>W1*X1+W2*X2+.......b=1
OR 
W1*X1+W2*X2+.......b=-1</p>

<p>For all the positions i,j 
I have implemented my code </p>

<pre><code>import theano.tensor as T

 def compu(W, filter_shape, input , input Shape) :
wReshaped = W.dimshuffle(0,4,1,2,3)
wReshapedShape = (filter_shape[0], filter_shape[4], filter_shape[1], 
filter_shape[2], filter_shape[3])

inputReshaped = input.dimshuffle(0, 4, 1, 2, 3)
inputSampleReshapedShape = (inputShape[0],
                            inputShape[4],
                            inputShape[1],
                            inputShape[2],
                            inputShape[3]) 

    output =W1*X1+W2*X2+......+b

    return (output, outputShape)
</code></pre>

<p>How can I do this part </p>

<p>output =W1*X1+W2*X2+......+b</p>

<p>what is the function in Theano to do that </p>

<p>Please any help </p>

<p>the output must either 1 or -1 from all the locations i,j </p>
"
3092,"<p>Why is the representation of hidden layers made in terms of circles?</p>

<p>What is the difference between a circle and a box in diagrams of neural networks?</p>
"
3093,"<p>I have already implemented a relatively simple DQN on Pacman.</p>

<p>Now I would like to clearly understand the difference between a DQN and the techniques used by AlphaGo zero/AlphaZero and I couldn't find a place where the features of both approaches are compared. </p>

<p>Also sometimes, when reading through blogs, I believe different terms might in fact be the same mathematical tool which adds to the difficulty of clearly understanding the differences. For example, variations of DQN e.g. Double DQN also uses two networks like alpha zero.</p>

<p>Has someone a good reference regarding this question ? Be it a book or an online ressource.</p>
"
3094,"<p>In the context of Deep Q Network, a target network is usually utilized. The target network is a slow changing network with a changing rate as its hyperparameter. This includes both replacement update every <span class=""math-container"">$N$</span> iterations and slowly update every iteration.</p>

<p>Since the rate is hard to fine tune manually, is there an alternative technique that can eliminate the use of target network or at least makes it less susceptible to the changing rate?</p>
"
3095,"<p>A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot ceiling. He would like to get the bananas. The room contains two stackable, movable, climbable 3-foot-high crates.</p>

<p>Give the initial state, goal test, successor function, and cost function for each of the following. Choose a formulation that is precise enough to be implemented.</p>
"
3096,"<p>Suppose I want to build a neural network regression model that takes one input and return one output.</p>

<p>Here's the training data:</p>

<pre><code>0.1 =&gt; 0.1
0.2 =&gt; 0.2
0.1 =&gt; -0.1
</code></pre>

<p>You will see that there are 2 inputs <code>0.1</code> that matches to different output values <code>0.1</code> and <code>-0.1</code>. So what will happen with most machine learning models is that they will predict the average when <code>0.1</code> is fed to the model. E.g. the output of <code>0.1</code> will be <code>(0.1 + (-0.1))/2 = 0</code>.</p>

<p>But this <code>0</code> as an average answer is an incorrect answer. I want the model to be telling me that the input is ambiguous/insufficient to infer the output. Ideally, the model would report it as a form of confidence.</p>

<p><strong>How do I report predictability confidence from the input?</strong></p>

<p>The application that I find very useful in many areas is that I could then later ask the model to show me inputs that are easy to predict and inputs that are ambiguous. This would make me able to collect the data that are making sense.</p>

<p>One way I know is to train the model then check the error on each training data, if it's high then it probably means that the input is ambiguous. But if you know any other papers or better techniques, I would be appreciated to know that!</p>
"
3097,"<p>I am reading the paper ""<a href=""https://i.stack.imgur.com/qZGnM.png"" rel=""nofollow noreferrer"">Transformation Invariance in Pattern Recognition – Tangent Distance and Tangent Propagation</a>"", where the tangent vector is calculated for the given curve <span class=""math-container"">$s(P,\alpha)$</span> at <span class=""math-container"">$\alpha=0$</span> by differentiating with respect to <span class=""math-container"">$\alpha$</span>, that is, <span class=""math-container"">$\frac{\partial s(P,\alpha)}{\partial\alpha}$</span>. For the curve, I have taken one <span class=""math-container"">$2D$</span> image and I am rotating it with matrix <span class=""math-container"">$R=\left[\matrix{cos(\alpha)\space -sin(\alpha)\\sin(\alpha) \space\space\space\space\space cos(\alpha)} \right]$</span>. </p>

<p>As my image is fixed, the curve is just a function of <span class=""math-container"">$\alpha$</span>. Therefore, to find the tangent vector, what I am doing is as follows:</p>

<ol>
<li><p>I am rotating the image by the matrix <span class=""math-container"">$R^{'}$</span> which is <span class=""math-container"">$R^{'}=\left[\matrix{-sin(\alpha)\space -cos(\alpha)\\cos(\alpha) \space\space\space\space\space -sin(\alpha)} \right]$</span> </p></li>
<li><p>This rotates the image by <span class=""math-container"">$90$</span> degree, which is not the expected result.</p></li>
</ol>

<p>I have done the same exercise by differentiating numerically and I am getting the expected answer which is as follows:<br><a href=""https://i.stack.imgur.com/oqYUc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqYUc.png"" alt=""Picture <span class=""math-container"">$\alpha$</span>=0""></a> 
<br><a href=""https://i.stack.imgur.com/qZGnM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qZGnM.png"" alt=""Tangent vector plot at <span class=""math-container"">$\alpha$</span>=0""></a></p>

<p>Please, help me to understand my mistake in taking derivative of the matrix and multiplying it with image.</p>
"
3098,"<p>How do you efficiently choose the hyper-parameters of a neural network (e.g. the learning rate, number of layer, weights, etc.)?</p>
"
3099,"<p>I hope this question is not too broad or general. I have a very large set of images all of which contain text (some have more, some less). All of them have been tagged as containing, say, English text or Korean. I wonder if convolutional neural networks would be a good approach to classify these images as containing English vs. Korean. Or is there any existing literature/method that does this already. Crucially though, I am not interested in ""understanding"" the text, so this is not an NLP task but, I suppose, a task of classifying orthographies in the images. </p>
"
3100,"<p>What is the fundamental difference between NN for classifying data and generating data?</p>

<p>Most examples show how neural networks can be used to classify data. Like is it an image of a dog or a cat. However, there are applications where NN are used to create images and even write short stories. </p>
"
3101,"<p>It seems to me that current work in semantics of natural language processing is based on Tarski's book such as ""Logic, Semantics, Metamathematics; Papers From 1923 To 1938"", which is far from satisfactory according to Michael Jordan (a leading expert in AI). Any thought on this?</p>
"
3102,"<p>How would a quantum computer potentially facilitate artificial consciousness, assuming it is possible? </p>
"
3103,"<p>The use of target network is to reduce the chance of value divergence which could happen with off-policy samples trained with <em>semi-gradient</em> objectives. In Deep Q network, semi-gradient TD is used and with experience replay the training could diverge. </p>

<p>Target network is a slow changing network designed to slowly track the main value network. In <a href=""https://arxiv.org/abs/1312.5602"" rel=""nofollow noreferrer"">Mnih 2013</a>, it was designed to match the main network every <span class=""math-container"">$N$</span> steps. There is another way which slowly updates the weight in the direction to match the main network every step. To someone, the latter is called <em>Polyak updates</em>. </p>

<p>I have done some very limited experiments and seen that with the same update rate, e.g. <span class=""math-container"">$N=10$</span>, Polyak update would update with the rate of 0.1, I usually see Polyak updates to give <em>smoother</em> progress and converge faster. My experiments are by no means conclusive. </p>

<p>I would thence ask if it is known which one to perform better, converge faster or has smoother progress, in a wider range of tasks and settings?</p>
"
3104,"<p>I stumbled around <code>Web Data StackExchange</code> site proposal on <code>Area51 SE</code> and was writing a suggestion of a question just with the words on title. I sort of migrated it here, as the question was so philosofical, that it would not be suitable to that site.</p>

<p>So, would General AI, a AI that thinks more like human, compared to the specific narrow AIs we have nowadays, get ease to formulate its thinking, if all or most of data in Internet would be having more and more semantic content?</p>

<p>Semantic data can contain meta data, ids, links to other data and typing in machine understandable format. I don't fully know how diverse the metadata can be and my question is would the needed <code>Intelligence</code> come through enough context and semantics?</p>
"
3105,"<p>I have a bayesian network, which has the following data: </p>

<p><span class=""math-container"">$P(S) = 0.07$</span></p>

<p><span class=""math-container"">$P(A) = 0.01$</span></p>

<p><span class=""math-container"">$P(F \mid S,A) = 1.0$</span></p>

<p><span class=""math-container"">$P(F \mid S, \lnot A) = 0.7$</span></p>

<p><span class=""math-container"">$P(F \mid \lnot S, A) = 0.9$</span></p>

<p><span class=""math-container"">$P(F \mid \lnot S, \lnot A) = 0.1$</span></p>

<p>And I'm asked to get <span class=""math-container"">$P(F \mid S)$</span>. Is it possible? How can I deduce it? </p>
"
3106,"<p>i am new to machine learning. i'm trying to identify driving pattern through accelerometer and gyroscope sensor. i have been collecting the data of both the sensors and have been storing them in .csv extension. i am not able to identify a pattern in the datasets since it has a lot of datas. i have three independent variables of accelerometer and with that i need to identify the sudden acceleration and sudden breaking and i have three independent variables of gyroscope and i need to identify aggressive turns. can you suggest as to how i need to analyse the pattern and find a algorithm which suits my requirement. this is how the dataset is</p>

<p><a href=""https://i.stack.imgur.com/L380x.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/L380x.png"" alt=""enter image description here""></a></p>
"
3107,"<p>I was wondering whether there is a method (not a table of recommendations) that could tell me what activation function to choose if the outputs of the neural network have some interpretation . For example these can be means of some normal distribution, probabilities in multinomial distribution, parameters of exponential distribution and so on. </p>
"
3108,"<p>I'm currently doing a research project related to Distributed Tracing. My research has led me to a point where I think ML might be suited for our problem.</p>

<p>I'm looking for papers that are similar to this (even if they have other applications):</p>

<p>I want to match packets exiting a black-box system (outputs) to packets that enter a black box (inputs). I can do that easily in a non concurrent setting which should help  me grow a training set (maybe for supervised learning), but I need an algorithm that, in a concurrent setting, can separate the different request ""flows"" if you will.</p>

<p>I hope this makes sense.</p>

<p>The closest thing to what I'm looking for is ""Aguilera, Marcos K., et al. ""Performance debugging for distributed systems of black boxes."" ACM SIGOPS Operating Systems Review. Vol. 37. No. 5. ACM, 2003."" but it's mostly suited for finding the dependency graph of the system, which I already know.</p>

<p>Thank you</p>
"
3109,"<p>A generative adversarial network (GAN) takes a vector of numbers as input and generates an image, based on the input. Each element of the vector causes some feature of the image to change, but the mapping between input and output is not clear, as often happens in deep learning. What is the best way to study the correlation between the vector elements and the output image features? The first approach that comes to mind is to manually change every element and check the result, however I am not sure that this is the best solution.</p>
"
3110,"<p>I've pieced together this A3C w/ PPO Gym Pendulum example, but I'm finding after a while, when attempting to get the action from the model, I get a NaN return:</p>

<pre><code>a = self.sess.run(self.sample_op, {self.tfs: s})[0]
</code></pre>

<p>It runs okay for a while, but then errors. To me that implies perhaps a invalid update happens at some point that corrupts the model. But I've put debugging output in the code and everything appears to be fine - it's not submitting any NaNs or outliers to the model as far as I can see.</p>

<p>After a bit of playing around I find that if I comment out the code to update the actor model then the code executes fine. Obviously it doesn't learn much, but it appears it's <em>something</em> to do with this update:</p>

<pre><code>[self.sess.run(self.atrain_op, {self.tfs: s, self.tfa: a, self.tfadv: adv}) for _ in range(UPDATE_STEP)]
</code></pre>

<p>Can anyone see what the problem might be?  I've been playing around with this for hours and it's got me stumped.</p>

<p>In code example below I set <code>N_WORKERS = 1</code> so the debug is easier to read, but if you want to hit the error faster then increase that figure to the number of CPU cores you have.</p>

<pre><code>""""""
Dependencies:
tensorflow 1.8.0
gym 0.9.2
""""""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import gym, threading, queue, math

EP_MAX = 600
EP_LEN = 200
N_WORKER = 1                # parallel workers
GAMMA = 0.9                 # reward discount factor
A_LR = 0.0001               # learning rate for actor
C_LR = 0.0001               # learning rate for critic
MIN_BATCH_SIZE = 64         # minimum batch size for updating PPO
UPDATE_STEP = 15            # loop update operation n-steps
EPSILON = 0.2               # for clipping surrogate objective
GAME = 'Pendulum-v0'

env = gym.make(GAME)
S_DIM = env.observation_space.shape[0]
A_DIM = 1 # not available in pendulum env.action_space.n


class PPONet(object):
    def __init__(self):
        self.sess = tf.Session()
        self.tfs = tf.placeholder(tf.float32, [None, S_DIM], 'state')
        self.tfa = tf.placeholder(tf.float32, [None, ], 'action')
        self.tfadv = tf.placeholder(tf.float32, [None, 1], 'advantage')        

        # critic        
        with tf.variable_scope('critic'):
            w_init = tf.random_normal_initializer(0., .1)
            lc = tf.layers.dense(self.tfs, 200, tf.nn.relu, kernel_initializer=w_init, name='layer1-critic')
            self.v = tf.layers.dense(lc, 1)

        with tf.variable_scope('ctrain'):
            self.tfdc_r = tf.placeholder(tf.float32, [None, 1], 'discounted_r')
            self.advantage = self.tfdc_r - self.v
            self.closs = tf.reduce_mean(tf.square(self.advantage))
            self.ctrain_op = tf.train.AdamOptimizer(C_LR).minimize(self.closs)

        # actor
        pi, pi_params = self._build_anet('pi', trainable=True)
        oldpi, oldpi_params = self._build_anet('oldpi', trainable=False)

        with tf.variable_scope('sample_action'):
            self.sample_op = tf.squeeze(pi.sample(1), axis=0)       # choosing action

        with tf.variable_scope('update_oldpi'):
            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]

        with tf.variable_scope('loss'):
            with tf.variable_scope('surrogate_pp'):
                ratio = pi.prob(self.tfa) / oldpi.prob(self.tfa)
                surr = ratio * self.tfadv

            self.aloss = -tf.reduce_mean(tf.minimum(        # clipped surrogate objective
                surr,
                tf.clip_by_value(ratio, 1. - EPSILON, 1. + EPSILON) * self.tfadv))

        with tf.variable_scope('atrain'):
            self.atrain_op = tf.train.AdamOptimizer(A_LR).minimize(self.aloss)

        self.sess.run(tf.global_variables_initializer())

    def update(self):
        global GLOBAL_UPDATE_COUNTER
        while not COORD.should_stop():
            if GLOBAL_EP &lt; EP_MAX:
                UPDATE_EVENT.wait()                     # wait until get batch of data
                self.sess.run(self.update_oldpi_op)     # copy pi to old pi
                data = [QUEUE.get() for _ in range(QUEUE.qsize())]      # collect data from all workers
                data = np.vstack(data)
                s, a, r = data[:, :S_DIM], data[:, S_DIM: S_DIM + 1].ravel(), data[:, -1:]
                adv = self.sess.run(self.advantage, {self.tfs: s, self.tfdc_r: r})

                print(""Updating: s: {}, a: {}, r: {}, adv: {}"".format(s, a, r, adv))

                # update actor and critic in a update loop
                [self.sess.run(self.atrain_op, {self.tfs: s, self.tfa: a, self.tfadv: adv}) for _ in range(UPDATE_STEP)] #ERR 
                [self.sess.run(self.ctrain_op, {self.tfs: s, self.tfdc_r: r}) for _ in range(UPDATE_STEP)]
                UPDATE_EVENT.clear()        # updating finished
                GLOBAL_UPDATE_COUNTER = 0   # reset counter
                ROLLING_EVENT.set()         # set roll-out available

    def _build_anet(self, name, trainable): # Build the current &amp; hold structure for the policies 
        with tf.variable_scope(name):
            l1 = tf.layers.dense(self.tfs, 200, tf.nn.relu, trainable=trainable)
            mu = 2 * tf.layers.dense(l1, A_DIM, tf.nn.tanh, trainable=trainable, name = 'mu_'+name)
            sigma = tf.layers.dense(l1, A_DIM, tf.nn.softplus, trainable=trainable,name ='sigma_'+name )
            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma) # Loc is the mean
        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name) # Collects the weights of the layers l1, mu / 2, sigma
        return norm_dist, params    

    def choose_action(self, s):
        s = s[np.newaxis, :]
        print(""Action s: {}"".format(s))
        a = self.sess.run(self.sample_op, {self.tfs: s})[0]
        if math.isnan(a):
            print(""Action is NaN - stopping"")
            exit()
        return np.clip(a, -2, 2)

    def get_v(self, s):
        if s.ndim &lt; 2: s = s[np.newaxis, :]
        return self.sess.run(self.v, {self.tfs: s})[0, 0]


class Worker(object):
    def __init__(self, wid):
        self.wid = wid
        self.env = gym.make(GAME).unwrapped
        self.ppo = GLOBAL_PPO

    def work(self):
        global GLOBAL_EP, GLOBAL_RUNNING_R, GLOBAL_UPDATE_COUNTER
        while not COORD.should_stop():
            s = self.env.reset()            
            ep_r = 0
            buffer_s, buffer_a, buffer_r = [], [], []
            for t in range(EP_LEN):
                if not ROLLING_EVENT.is_set():                  # while global PPO is updating
                    ROLLING_EVENT.wait()                        # wait until PPO is updated
                    buffer_s, buffer_a, buffer_r = [], [], []   # clear history buffer, use new policy to collect data

                a = self.ppo.choose_action(s)
                s_, r, done, _ = self.env.step(a)
                print(""Step returns: s_: {}, r: {}, done: {}"".format(s_, r, done))
                #self.env.render()
                buffer_s.append(s)
                buffer_a.append(a)
                buffer_r.append((r+8)/8)    # normalize reward, find to be useful
                s = s_
                ep_r += r

                GLOBAL_UPDATE_COUNTER += 1                      # count to minimum batch size, no need to wait other workers
                if t == EP_LEN - 1 or GLOBAL_UPDATE_COUNTER &gt;= MIN_BATCH_SIZE or done:
                    if done:
                        v_s_ = 0                                # end of episode
                    else:
                        v_s_ = self.ppo.get_v(s_)

                    discounted_r = []                           # compute discounted reward
                    for r in buffer_r[::-1]:
                        v_s_ = r + GAMMA * v_s_
                        discounted_r.append(v_s_)
                    discounted_r.reverse()

                    bs, ba, br = np.vstack(buffer_s), np.vstack(buffer_a), np.array(discounted_r)[:, None]
                    buffer_s, buffer_a, buffer_r = [], [], []
                    QUEUE.put(np.hstack((bs, ba, br)))          # put data in the queue
                    if GLOBAL_UPDATE_COUNTER &gt;= MIN_BATCH_SIZE:
                        ROLLING_EVENT.clear()       # stop collecting data
                        UPDATE_EVENT.set()          # globalPPO update

                    if GLOBAL_EP &gt;= EP_MAX:         # stop training
                        COORD.request_stop()
                        break

                    if done: break

            # record reward changes, plot later
            if len(GLOBAL_RUNNING_R) == 0: GLOBAL_RUNNING_R.append(ep_r)
            else: GLOBAL_RUNNING_R.append(GLOBAL_RUNNING_R[-1]*0.9+ep_r*0.1)
            GLOBAL_EP += 1
            print('{0:.1f}%'.format(GLOBAL_EP/EP_MAX*100), '|W%i' % self.wid,  '|Ep_r: %.2f' % ep_r,)


if __name__ == '__main__':
    GLOBAL_PPO = PPONet()
    UPDATE_EVENT, ROLLING_EVENT = threading.Event(), threading.Event()
    UPDATE_EVENT.clear()            # not update now
    ROLLING_EVENT.set()             # start to roll out
    workers = [Worker(wid=i) for i in range(N_WORKER)]

    GLOBAL_UPDATE_COUNTER, GLOBAL_EP = 0, 0
    GLOBAL_RUNNING_R = []
    COORD = tf.train.Coordinator()
    QUEUE = queue.Queue()           # workers putting data in this queue
    threads = []
    for worker in workers:          # worker threads
        t = threading.Thread(target=worker.work, args=())
        t.start()                   # training
        threads.append(t)
    # add a PPO updating thread
    threads.append(threading.Thread(target=GLOBAL_PPO.update,))
    threads[-1].start()
    COORD.join(threads)

    # plot reward change and test
    plt.plot(np.arange(len(GLOBAL_RUNNING_R)), GLOBAL_RUNNING_R)
    plt.xlabel('Episode'); plt.ylabel('Moving reward'); plt.ion(); plt.show()
    env = gym.make('CartPole-v0')
    while True:
        s = env.reset()
        for t in range(1000):
            env.render()
            s, r, done, info = env.step(GLOBAL_PPO.choose_action(s))
            if done:
                break
</code></pre>
"
3111,"<p>Usually, when I evaluate() a model, I would get a single loss that is already averaged over all samples. How do I get the loss per each sample and return all of them?</p>

<p>E.g. if my dataset has 100 samples, I would get 100 losses, for each of the sample.</p>
"
3112,"<p>I'm interested in knowing whether there exist any neural network, that solves (with  >=80% accuracy) any nontrivial problem, that uses very few nodes (where 20 nodes is not a hard limit). I want to develop an intuition on sizes of neural networks.</p>
"
3113,"<p>I was reading an article on <a href=""https://chatbotsjournal.com/why-i-should-should-not-invest-in-chatbot-df952cbabf10"" rel=""nofollow noreferrer"">Medium</a> and wanted to make it clear whether a bot created on IBM Watson is an intelligent one or unintelligent.</p>

<blockquote>
  <p>Simply put, there are 2 types of chatbots — unintelligent ones that act using predefined conversation flows (algorithms) written by developers building them and intelligent ones that use machine learning to interact with users.</p>
</blockquote>
"
3114,"<p>The reinforcement learning paradigm has the aim to determine the optimal actions for a robot. A typical example is a maze finding robot, but reinforcement learning can also be used for training a robot to play the pong game. The principle is based on a reward function. If the robot is able to solve a problem, he gets a score from the underlying game engine. The score can be positive, if the robot reaches the end of a maze, or it can be negative, if he is colliding with an obstacle. The principle itself is working quite well, that means for simpler applications it is possible to train a robot to play a game with reinforcement learning.</p>

<p>Chatbots are a different category of artificial intelligence. They are working not with actions but with natural language. Person #1 is opening a dialogue with “Hi, I'm Alice”, while person #2 is responding with “Nice to meet you”. What is missing here is an underlying game which is played. There is no reward available for printing out a certain sentence. In some literature the problem of language grounding was discussed seriously, but with an unclear result. It seems, that a classical game for example pong, and a chatbot conversation doesn't have much in common.</p>

<p>Is it possible to combine Reinforcement Learning with chatbot design? The problem is, that a speech-act should be connected to a reward. That means, a well formulated sentence gets +10 points but a weak sentence gets -10 points. How can this be evaluated?</p>
"
3115,"<p>The following plot shows error function output based on system weights.
Two equal local minima are shown in green pointers. Note that the red dots are not related to the question.</p>

<p>Does the right one generalize better compared to the left one? </p>

<p>My assumption is that for the right minimum if the weights change, the overall error increases less compared to the left minimum point. Would this somehow mean the system does better generalization if the right one is chosen as the optimum minimum?</p>

<p><a href=""https://i.stack.imgur.com/pDEFF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pDEFF.png"" alt=""enter image description here""></a></p>
"
3116,"<p>I find myself wondering if there exists a data structure with the following properties:</p>

<ul>
<li>Stores information</li>
<li>Conforms to the <a href=""https://en.wikipedia.org/wiki/Encoding_specificity_principle"" rel=""nofollow noreferrer"">encoding specificity principle</a></li>
</ul>

<p>Additionally to the existence of such a data structure I'm specifically wondering if neural networks have been used to model this biological principle at all and if it's been done in an efficient manner. </p>

<p>More broadly, this relates to an interest in neural networks as any type of storage mechanism such as the <a href=""https://en.wikipedia.org/wiki/Hopfield_network"" rel=""nofollow noreferrer"">Hopfield Network</a> (although it's not clear to me wether an HN models this principle, if someone can shed light on this I would appreciate it).</p>
"
3117,"<p>I want to try and compare different optimization methods in some datasets. I know that in scikit-learn there are some corresponding functions for grid and random search optimizations. However, I also need a package (or multiple ones) for different recent Bayesian optimization methods. Are there any good and stable ones to use? Which packages do you recommend? (If any recent for grid/random search, it is also okay.)</p>

<p>Thanks in advance.</p>
"
3118,"<p>The following plot shows error function output based on system weights.
Two equal local minima are shown in green pointers. Note that the red dots are not related to the question.</p>

<p>Considering the amount of convex in the local minima, is there any way to opt between these two local minima?</p>

<p><a href=""https://i.stack.imgur.com/7rMRC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7rMRC.png"" alt=""enter image description here""></a></p>
"
3119,"<p>I just read about <a href=""https://en.wikipedia.org/wiki/Parse_tree"" rel=""nofollow noreferrer"">Parse Tree</a> for parsing a sentence as an Input for NLP Task.</p>

<p>In my understanding, a valid Parse Tree of a sentence should have be validated by linguistic expert. So, I concluded, a sentence only has one Parse Tree structure.</p>

<p>But, is that correct? is it possible a sentence has more than one valid structures of parse tree with the same type (e.g. Constituency-based)?</p>
"
3120,"<p>I want to install some libraries such as xgboost, lightgbm, and catboost without connecting to the Internet in Anaconda because of some security issues. Can anybody help me how to do this? Moreover, is there any pre-requisite to install before installation of these libraries like visual studio, etc?</p>
"
3121,"<p>Im trying to learn a bit more about why someone would use a neuroevolution algorithm over a traditional machine learning algorithm.</p>

<p>i.e what situation would only apply to an algorithm such as NEAT, but no other machine learning algorithm.</p>
"
3122,"<p>There are various heuristic search algorithms, like hill climbing, greedy search, A* algorithm, but when it is best preferred to use hill climbing?</p>
"
3123,"<p>I installed the <code>speech_recognition</code> Python package. I am getting an error. I tried to solve it, but I am not able. The error is</p>

<pre><code>/opt/goeasy/arduniap$ python3.6 stt.py
ALSA lib pcm.c:2495:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear
ALSA lib pcm.c:2495:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe
ALSA lib pcm.c:2495:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side
ALSA lib pcm_route.c:867:(find_matching_chmap) Found no matching channel map
ALSA lib pcm_route.c:867:(find_matching_chmap) Found no matching channel map
ALSA lib pcm_route.c:867:(find_matching_chmap) Found no matching channel map
ALSA lib pcm_route.c:867:(find_matching_chmap) Found no matching channel map
</code></pre>

<p>Here's my code</p>

<pre><code>import speech_recognition as sr
import pyaudio

# Record Audio
r = sr.Recognizer()
with sr.Microphone() as source:
   print(""Say something!"")
   audio = r.listen(source)

try:`
   print(""You said: "" + r.recognize_google(audio))
except sr.UnknownValueError:
     print(""Google Speech Recognition could not understand audio"")
except sr.RequestError as e:
     print(""Could not request results from Google Speech Recognition service; {0}"".format(e))
</code></pre>

<p>I am using in Ubuntu with Python 3.6.</p>
"
3124,"<p>I have coded a very basic LSTM with forget gates (no libraries used). I'm trying to predict 0.5*sin(t + N) given 0.5*sin(t) as an exercise.</p>

<p>I have tweaked the model, changing the output layer activation function, weight initialization, number of memory blocks/cells, etc. However, I still couldn't manage to correct the output.</p>

<p>The problem is that the output range is much smaller than desired, [-0.2, 0.2] instead of [-0.5, 0.5]. The output also is slightly delayed, meaning it is predicting sin(t + N - 1) for example.</p>

<p>Is there something that I'm missing?</p>

<p>As an example, for output layer activation function as a centered logistic from (-1, 1), the validation output looks like</p>

<p><a href=""https://i.stack.imgur.com/Ay7By.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ay7By.png"" alt=""Validation output""></a></p>

<p>Training output looks like</p>

<p><a href=""https://i.stack.imgur.com/ds6oX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ds6oX.png"" alt=""Training output""></a></p>

<p>Additional information:</p>

<p>Topology: 1 input layer, 1 hidden layer each with 5 memory blocks each with 1 cell, 1 output layer each with 1 regular neuron.</p>

<p>Alpha: 1</p>

<p>Weights: generated with normal distribution, from [-1, 1]</p>

<p>Output layer activation function used: logistic [0, 1], centered logistic, tanh, ReLU, leaky ReLU, f(x) = x (identity)</p>
"
3125,"<p>To make A2C into A3C you make it asynchronous. From what I understand the 'correct' way to do that is to thread off workers with a copy of the policy and critic, and then return the state/action/reward tuples to the main thread, which then performs the gradients updates on the main policy and critic, and then repeat the process.</p>

<p>I understand why the copying would be necessary in a distributed environment, but if I were to always run it locally then could I just perform the updates on a global variable of the policy and critic, i.e. avoid the need for copying? Provided the concurrency of the updates was handled correctly, would that be fine?</p>
"
