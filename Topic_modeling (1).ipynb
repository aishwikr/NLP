{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko1IwygRZF6u",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llEFt_afNTz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "d4c0519f-80b5-453c-eaa3-3a5782b30003"
      },
      "source": [
        "!pip install -U gensim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "import sys\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from gensim.models import ldamodel\n",
        "import gensim.corpora\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (1.14.63)\n",
            "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.63 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.63)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
            "Installing collected packages: gensim\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-3.8.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5tNMxkyZTKK",
        "colab_type": "text"
      },
      "source": [
        "#Data Loading and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXp4Dbn8OMEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('abcnews-date-text.csv', \n",
        "error_bad_lines=False)\n",
        "# We only need the Headlines text column from the data\n",
        "data_text = data[['headline_text']]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVpSpdZWRFsa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96d87ba6-e6f7-4f6f-defa-52eac10d48af"
      },
      "source": [
        "data_text = data_text.astype('str')\n",
        "for idx in range(len(data_text)):\n",
        "    \n",
        "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
        "    data_text.iloc[idx]['headline_text'] = [word for word in data_text.iloc[idx]['headline_text'].split(' ') if word not in stopwords.words()]\n",
        "    \n",
        "    #print logs to monitor output\n",
        "    if idx % 1000 == 0:\n",
        "        sys.stdout.write('\\rc = ' + str(idx) + ' / ' + str(len(data_text)))\n",
        "#save data because it takes very long to remove stop words\n",
        "pickle.dump(data_text, open('data_text.dat', 'wb'))\n",
        "#get the words as an array for lda input\n",
        "train_headlines = [value[0] for value in data_text.iloc[0:].values]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "c = 1186000 / 1186018"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxFjZ_fZacu6",
        "colab_type": "text"
      },
      "source": [
        "#Implementing LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXj9ObdrRUk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_topics = 10  #initializing the no. of topics we need to cluster"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oclkB1or8IRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2word = gensim.corpora.Dictionary(train_headlines)\n",
        "corpus = [id2word.doc2bow(text) for text in train_headlines]\n",
        "lda = ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O2C0x_48V8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lda_topics(model, num_topics):\n",
        "    word_dict = {}\n",
        "    for i in range(num_topics):\n",
        "        words = model.show_topic(i, topn = 20)\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n",
        "    return pd.DataFrame(word_dict)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCQbtrFU9dlj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "c26f7fe8-8895-490f-e17f-98b7edc37340"
      },
      "source": [
        "get_lda_topics(lda, num_topics)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic # 01</th>\n",
              "      <th>Topic # 02</th>\n",
              "      <th>Topic # 03</th>\n",
              "      <th>Topic # 04</th>\n",
              "      <th>Topic # 05</th>\n",
              "      <th>Topic # 06</th>\n",
              "      <th>Topic # 07</th>\n",
              "      <th>Topic # 08</th>\n",
              "      <th>Topic # 09</th>\n",
              "      <th>Topic # 10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>news</td>\n",
              "      <td>years</td>\n",
              "      <td>election</td>\n",
              "      <td>police</td>\n",
              "      <td>health</td>\n",
              "      <td>trump</td>\n",
              "      <td>year</td>\n",
              "      <td>government</td>\n",
              "      <td>australia</td>\n",
              "      <td>fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bushfire</td>\n",
              "      <td>woman</td>\n",
              "      <td>donald</td>\n",
              "      <td>crash</td>\n",
              "      <td>says</td>\n",
              "      <td>sydney</td>\n",
              "      <td>first</td>\n",
              "      <td>day</td>\n",
              "      <td>wa</td>\n",
              "      <td>abc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>us</td>\n",
              "      <td>people</td>\n",
              "      <td>victoria</td>\n",
              "      <td>new</td>\n",
              "      <td>guilty</td>\n",
              "      <td>perth</td>\n",
              "      <td>tasmania</td>\n",
              "      <td>australian</td>\n",
              "      <td>world</td>\n",
              "      <td>queensland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>victorian</td>\n",
              "      <td>found</td>\n",
              "      <td>stories</td>\n",
              "      <td>car</td>\n",
              "      <td>life</td>\n",
              "      <td>change</td>\n",
              "      <td>top</td>\n",
              "      <td>adelaide</td>\n",
              "      <td>2019</td>\n",
              "      <td>two</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>federal</td>\n",
              "      <td>family</td>\n",
              "      <td>charged</td>\n",
              "      <td>accused</td>\n",
              "      <td>morrison</td>\n",
              "      <td>climate</td>\n",
              "      <td>women</td>\n",
              "      <td>drum</td>\n",
              "      <td>china</td>\n",
              "      <td>back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>emergency</td>\n",
              "      <td>canberra</td>\n",
              "      <td>royal</td>\n",
              "      <td>darwin</td>\n",
              "      <td>tasmanian</td>\n",
              "      <td>drought</td>\n",
              "      <td>hit</td>\n",
              "      <td>open</td>\n",
              "      <td>melbourne</td>\n",
              "      <td>attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>live</td>\n",
              "      <td>beach</td>\n",
              "      <td>shooting</td>\n",
              "      <td>dead</td>\n",
              "      <td>mental</td>\n",
              "      <td>chinese</td>\n",
              "      <td>test</td>\n",
              "      <td>ban</td>\n",
              "      <td>cup</td>\n",
              "      <td>school</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>nt</td>\n",
              "      <td>final</td>\n",
              "      <td>labor</td>\n",
              "      <td>injured</td>\n",
              "      <td>former</td>\n",
              "      <td>case</td>\n",
              "      <td>win</td>\n",
              "      <td>state</td>\n",
              "      <td>north</td>\n",
              "      <td>residents</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>death</td>\n",
              "      <td>hong</td>\n",
              "      <td>scott</td>\n",
              "      <td>help</td>\n",
              "      <td>minister</td>\n",
              "      <td>farmers</td>\n",
              "      <td>record</td>\n",
              "      <td>hobart</td>\n",
              "      <td>south</td>\n",
              "      <td>sex</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>markets</td>\n",
              "      <td>eve</td>\n",
              "      <td>commission</td>\n",
              "      <td>michael</td>\n",
              "      <td>speaks</td>\n",
              "      <td>australias</td>\n",
              "      <td>island</td>\n",
              "      <td>market</td>\n",
              "      <td>afl</td>\n",
              "      <td>brisbane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>report</td>\n",
              "      <td>kong</td>\n",
              "      <td>says</td>\n",
              "      <td>video</td>\n",
              "      <td>power</td>\n",
              "      <td>wins</td>\n",
              "      <td>coast</td>\n",
              "      <td>us</td>\n",
              "      <td>australian</td>\n",
              "      <td>trial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>missing</td>\n",
              "      <td>abuse</td>\n",
              "      <td>east</td>\n",
              "      <td>young</td>\n",
              "      <td>road</td>\n",
              "      <td>shows</td>\n",
              "      <td>gold</td>\n",
              "      <td>friday</td>\n",
              "      <td>us</td>\n",
              "      <td>street</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>gippsland</td>\n",
              "      <td>bill</td>\n",
              "      <td>say</td>\n",
              "      <td>killed</td>\n",
              "      <td>new</td>\n",
              "      <td>trade</td>\n",
              "      <td>tax</td>\n",
              "      <td>tuesday</td>\n",
              "      <td>thousands</td>\n",
              "      <td>child</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>three</td>\n",
              "      <td>nrl</td>\n",
              "      <td>children</td>\n",
              "      <td>murder</td>\n",
              "      <td>bush</td>\n",
              "      <td>review</td>\n",
              "      <td>finance</td>\n",
              "      <td>second</td>\n",
              "      <td>west</td>\n",
              "      <td>house</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>deal</td>\n",
              "      <td>country</td>\n",
              "      <td>president</td>\n",
              "      <td>driver</td>\n",
              "      <td>budget</td>\n",
              "      <td>regional</td>\n",
              "      <td>jailed</td>\n",
              "      <td>pay</td>\n",
              "      <td>john</td>\n",
              "      <td>national</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>fears</td>\n",
              "      <td>sentenced</td>\n",
              "      <td>brexit</td>\n",
              "      <td>near</td>\n",
              "      <td>party</td>\n",
              "      <td>community</td>\n",
              "      <td>bay</td>\n",
              "      <td>vision</td>\n",
              "      <td>qld</td>\n",
              "      <td>indigenous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>blog</td>\n",
              "      <td>media</td>\n",
              "      <td>briefing</td>\n",
              "      <td>assault</td>\n",
              "      <td>sexual</td>\n",
              "      <td>australian</td>\n",
              "      <td>old</td>\n",
              "      <td>cut</td>\n",
              "      <td>cricket</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>premier</td>\n",
              "      <td>violence</td>\n",
              "      <td>australians</td>\n",
              "      <td>start</td>\n",
              "      <td>weather</td>\n",
              "      <td>aged</td>\n",
              "      <td>charges</td>\n",
              "      <td>port</td>\n",
              "      <td>arrested</td>\n",
              "      <td>wall</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>parliament</td>\n",
              "      <td>jail</td>\n",
              "      <td>go</td>\n",
              "      <td>death</td>\n",
              "      <td>thursday</td>\n",
              "      <td>northern</td>\n",
              "      <td>alan</td>\n",
              "      <td>inquest</td>\n",
              "      <td>four</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>laws</td>\n",
              "      <td>million</td>\n",
              "      <td>turnbull</td>\n",
              "      <td>takes</td>\n",
              "      <td>warning</td>\n",
              "      <td>program</td>\n",
              "      <td>students</td>\n",
              "      <td>energy</td>\n",
              "      <td>home</td>\n",
              "      <td>alleged</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Topic # 01 Topic # 02   Topic # 03  ...  Topic # 08  Topic # 09  Topic # 10\n",
              "0         news      years     election  ...  government   australia        fire\n",
              "1     bushfire      woman       donald  ...         day          wa         abc\n",
              "2           us     people     victoria  ...  australian       world  queensland\n",
              "3    victorian      found      stories  ...    adelaide        2019         two\n",
              "4      federal     family      charged  ...        drum       china        back\n",
              "5    emergency   canberra        royal  ...        open   melbourne      attack\n",
              "6         live      beach     shooting  ...         ban         cup      school\n",
              "7           nt      final        labor  ...       state       north   residents\n",
              "8        death       hong        scott  ...      hobart       south         sex\n",
              "9      markets        eve   commission  ...      market         afl    brisbane\n",
              "10      report       kong         says  ...          us  australian       trial\n",
              "11     missing      abuse         east  ...      friday          us      street\n",
              "12   gippsland       bill          say  ...     tuesday   thousands       child\n",
              "13       three        nrl     children  ...      second        west       house\n",
              "14        deal    country    president  ...         pay        john    national\n",
              "15       fears  sentenced       brexit  ...      vision         qld  indigenous\n",
              "16        blog      media     briefing  ...         cut     cricket    business\n",
              "17     premier   violence  australians  ...        port    arrested        wall\n",
              "18  parliament       jail           go  ...     inquest        four        high\n",
              "19        laws    million     turnbull  ...      energy        home     alleged\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kW50Z8ta8Sq",
        "colab_type": "text"
      },
      "source": [
        "# Implementing NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Tk6Kjal9gdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the count vectorizer module needs string inputs, not array, so they are joined with a space.\n",
        "train_headlines_sentences = [' '.join(text) for text in train_headlines]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUew0InX9kF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='word', max_features=5000)\n",
        "x_counts = vectorizer.fit_transform(train_headlines_sentences)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgXzfHXh9nqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "x_tfidf = transformer.fit_transform(x_counts)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxKH4h-49qoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)  #normalizing the TfIdf values to unit length for each row"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raxYabrW9tjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8b824658-5425-4a24-fe80-b5a7ffc40d19"
      },
      "source": [
        "#obtain a NMF model.\n",
        "model = NMF(n_components=num_topics, init='nndsvd')\n",
        "#fit the model\n",
        "model.fit(xtfidf_norm)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0, max_iter=200,\n",
              "    n_components=10, random_state=None, shuffle=False, solver='cd', tol=0.0001,\n",
              "    verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW8aqvCvbpyT",
        "colab_type": "text"
      },
      "source": [
        "# Generating NMF topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yil4lwi9wkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nmf_topics(model, n_top_words):\n",
        "    \n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    feat_names = vectorizer.get_feature_names()\n",
        "    \n",
        "    word_dict = {}\n",
        "    for i in range(num_topics):\n",
        "        \n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
        "        words = [feat_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words\n",
        "    \n",
        "    return pd.DataFrame(word_dict)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NXKwv-491E-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "outputId": "354e0483-3f6b-4475-8209-12ad16690563"
      },
      "source": [
        "get_nmf_topics(model, 20)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic # 01</th>\n",
              "      <th>Topic # 02</th>\n",
              "      <th>Topic # 03</th>\n",
              "      <th>Topic # 04</th>\n",
              "      <th>Topic # 05</th>\n",
              "      <th>Topic # 06</th>\n",
              "      <th>Topic # 07</th>\n",
              "      <th>Topic # 08</th>\n",
              "      <th>Topic # 09</th>\n",
              "      <th>Topic # 10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>interview</td>\n",
              "      <td>police</td>\n",
              "      <td>new</td>\n",
              "      <td>abc</td>\n",
              "      <td>charged</td>\n",
              "      <td>rural</td>\n",
              "      <td>fire</td>\n",
              "      <td>says</td>\n",
              "      <td>court</td>\n",
              "      <td>crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>extended</td>\n",
              "      <td>missing</td>\n",
              "      <td>zealand</td>\n",
              "      <td>weather</td>\n",
              "      <td>murder</td>\n",
              "      <td>news</td>\n",
              "      <td>house</td>\n",
              "      <td>council</td>\n",
              "      <td>accused</td>\n",
              "      <td>car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>michael</td>\n",
              "      <td>search</td>\n",
              "      <td>laws</td>\n",
              "      <td>sport</td>\n",
              "      <td>death</td>\n",
              "      <td>national</td>\n",
              "      <td>crews</td>\n",
              "      <td>australia</td>\n",
              "      <td>faces</td>\n",
              "      <td>killed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>david</td>\n",
              "      <td>probe</td>\n",
              "      <td>year</td>\n",
              "      <td>entertainment</td>\n",
              "      <td>woman</td>\n",
              "      <td>nsw</td>\n",
              "      <td>destroys</td>\n",
              "      <td>water</td>\n",
              "      <td>murder</td>\n",
              "      <td>fatal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>james</td>\n",
              "      <td>investigate</td>\n",
              "      <td>hospital</td>\n",
              "      <td>business</td>\n",
              "      <td>stabbing</td>\n",
              "      <td>qld</td>\n",
              "      <td>threat</td>\n",
              "      <td>us</td>\n",
              "      <td>charges</td>\n",
              "      <td>woman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>john</td>\n",
              "      <td>death</td>\n",
              "      <td>york</td>\n",
              "      <td>news</td>\n",
              "      <td>assault</td>\n",
              "      <td>reporter</td>\n",
              "      <td>home</td>\n",
              "      <td>govt</td>\n",
              "      <td>front</td>\n",
              "      <td>road</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nrl</td>\n",
              "      <td>hunt</td>\n",
              "      <td>home</td>\n",
              "      <td>market</td>\n",
              "      <td>trial</td>\n",
              "      <td>nrn</td>\n",
              "      <td>school</td>\n",
              "      <td>plan</td>\n",
              "      <td>told</td>\n",
              "      <td>two</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>matt</td>\n",
              "      <td>shooting</td>\n",
              "      <td>deal</td>\n",
              "      <td>analysis</td>\n",
              "      <td>sydney</td>\n",
              "      <td>closer</td>\n",
              "      <td>suspicious</td>\n",
              "      <td>report</td>\n",
              "      <td>case</td>\n",
              "      <td>dead</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ivan</td>\n",
              "      <td>officer</td>\n",
              "      <td>centre</td>\n",
              "      <td>speaks</td>\n",
              "      <td>attack</td>\n",
              "      <td>health</td>\n",
              "      <td>factory</td>\n",
              "      <td>back</td>\n",
              "      <td>high</td>\n",
              "      <td>driver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>andrew</td>\n",
              "      <td>arrest</td>\n",
              "      <td>president</td>\n",
              "      <td>talks</td>\n",
              "      <td>two</td>\n",
              "      <td>drought</td>\n",
              "      <td>blaze</td>\n",
              "      <td>australian</td>\n",
              "      <td>hears</td>\n",
              "      <td>hospital</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>nathan</td>\n",
              "      <td>seek</td>\n",
              "      <td>life</td>\n",
              "      <td>top</td>\n",
              "      <td>shooting</td>\n",
              "      <td>tasmania</td>\n",
              "      <td>warning</td>\n",
              "      <td>health</td>\n",
              "      <td>sex</td>\n",
              "      <td>plane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>chris</td>\n",
              "      <td>assault</td>\n",
              "      <td>gets</td>\n",
              "      <td>stories</td>\n",
              "      <td>guilty</td>\n",
              "      <td>exchange</td>\n",
              "      <td>sydney</td>\n",
              "      <td>day</td>\n",
              "      <td>appeal</td>\n",
              "      <td>injured</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>luke</td>\n",
              "      <td>find</td>\n",
              "      <td>opens</td>\n",
              "      <td>quiz</td>\n",
              "      <td>teen</td>\n",
              "      <td>country</td>\n",
              "      <td>residents</td>\n",
              "      <td>call</td>\n",
              "      <td>alleged</td>\n",
              "      <td>found</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>scott</td>\n",
              "      <td>say</td>\n",
              "      <td>chief</td>\n",
              "      <td>wild</td>\n",
              "      <td>jailed</td>\n",
              "      <td>doctors</td>\n",
              "      <td>govt</td>\n",
              "      <td>wa</td>\n",
              "      <td>assault</td>\n",
              "      <td>highway</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>tim</td>\n",
              "      <td>station</td>\n",
              "      <td>years</td>\n",
              "      <td>friday</td>\n",
              "      <td>child</td>\n",
              "      <td>friday</td>\n",
              "      <td>destroyed</td>\n",
              "      <td>wins</td>\n",
              "      <td>fronts</td>\n",
              "      <td>truck</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>mark</td>\n",
              "      <td>charge</td>\n",
              "      <td>ceo</td>\n",
              "      <td>breakfast</td>\n",
              "      <td>found</td>\n",
              "      <td>sach</td>\n",
              "      <td>season</td>\n",
              "      <td>world</td>\n",
              "      <td>child</td>\n",
              "      <td>accident</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>matthew</td>\n",
              "      <td>car</td>\n",
              "      <td>named</td>\n",
              "      <td>learning</td>\n",
              "      <td>sex</td>\n",
              "      <td>tas</td>\n",
              "      <td>control</td>\n",
              "      <td>election</td>\n",
              "      <td>trial</td>\n",
              "      <td>hit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>smith</td>\n",
              "      <td>body</td>\n",
              "      <td>get</td>\n",
              "      <td>report</td>\n",
              "      <td>attempted</td>\n",
              "      <td>park</td>\n",
              "      <td>damages</td>\n",
              "      <td>calls</td>\n",
              "      <td>stabbing</td>\n",
              "      <td>bus</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>shane</td>\n",
              "      <td>attack</td>\n",
              "      <td>hope</td>\n",
              "      <td>radio</td>\n",
              "      <td>alleged</td>\n",
              "      <td>tuesday</td>\n",
              "      <td>ban</td>\n",
              "      <td>china</td>\n",
              "      <td>appears</td>\n",
              "      <td>jailed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>steve</td>\n",
              "      <td>nsw</td>\n",
              "      <td>rules</td>\n",
              "      <td>darwin</td>\n",
              "      <td>arrested</td>\n",
              "      <td>thursday</td>\n",
              "      <td>melbourne</td>\n",
              "      <td>urged</td>\n",
              "      <td>jails</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Topic # 01   Topic # 02 Topic # 03  ...  Topic # 08 Topic # 09 Topic # 10\n",
              "0   interview       police        new  ...        says      court      crash\n",
              "1    extended      missing    zealand  ...     council    accused        car\n",
              "2     michael       search       laws  ...   australia      faces     killed\n",
              "3       david        probe       year  ...       water     murder      fatal\n",
              "4       james  investigate   hospital  ...          us    charges      woman\n",
              "5        john        death       york  ...        govt      front       road\n",
              "6         nrl         hunt       home  ...        plan       told        two\n",
              "7        matt     shooting       deal  ...      report       case       dead\n",
              "8        ivan      officer     centre  ...        back       high     driver\n",
              "9      andrew       arrest  president  ...  australian      hears   hospital\n",
              "10     nathan         seek       life  ...      health        sex      plane\n",
              "11      chris      assault       gets  ...         day     appeal    injured\n",
              "12       luke         find      opens  ...        call    alleged      found\n",
              "13      scott          say      chief  ...          wa    assault    highway\n",
              "14        tim      station      years  ...        wins     fronts      truck\n",
              "15       mark       charge        ceo  ...       world      child   accident\n",
              "16    matthew          car      named  ...    election      trial        hit\n",
              "17      smith         body        get  ...       calls   stabbing        bus\n",
              "18      shane       attack       hope  ...       china    appears     jailed\n",
              "19      steve          nsw      rules  ...       urged      jails      train\n",
              "\n",
              "[20 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5PT31or95Y6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}